I0123 17:35:49.055722 140210684239872 inference_utils.py:69] Parsing gin configuration.
I0123 17:35:49.055824 140210684239872 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 17:35:49.056030 140210684239872 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 17:35:49.056064 140210684239872 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 17:35:49.056092 140210684239872 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 17:35:49.056118 140210684239872 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 17:35:49.056143 140210684239872 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 17:35:49.056169 140210684239872 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 17:35:49.056196 140210684239872 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 17:35:49.056221 140210684239872 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 17:35:49.056246 140210684239872 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 17:35:49.056272 140210684239872 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 17:35:49.056320 140210684239872 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 17:35:49.056458 140210684239872 resource_reader.py:55] Path not found: base_htrans.gin
I0123 17:35:49.056666 140210684239872 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 17:35:49.056771 140210684239872 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 17:35:49.063049 140210684239872 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 17:35:49.063175 140210684239872 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 17:35:49.063491 140210684239872 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 17:35:49.063596 140210684239872 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 17:35:49.063870 140210684239872 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 17:35:49.063970 140210684239872 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 17:35:49.064369 140210684239872 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 17:35:49.064469 140210684239872 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 17:35:49.068129 140210684239872 training_loop.py:334] ==== Training loop: initializing model ====
I0123 17:35:49.178504 140210684239872 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 17:35:49.179259 140210684239872 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 17:35:49.186098 140210684239872 training_loop.py:335] Process 0 of 1
I0123 17:35:49.186165 140210684239872 training_loop.py:336] Local device count = 1
I0123 17:35:49.186212 140210684239872 training_loop.py:337] Number of replicas = 1
I0123 17:35:49.186246 140210684239872 training_loop.py:339] Using random number seed 42
I0123 17:35:49.665446 140210684239872 training_loop.py:359] Initializing the model.
I0123 17:35:50.022518 140210684239872 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.022781 140210684239872 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 17:35:50.022895 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.022979 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023059 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023145 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023220 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023291 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023361 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023431 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023502 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023571 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023640 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023710 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:35:50.023749 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.023795 140210684239872 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:35:50.023909 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.023948 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.023980 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.025993 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.031300 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.042115 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.042420 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.046806 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.057396 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.057453 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.057492 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.057525 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.057589 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.058778 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.058857 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.059569 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.062031 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.067805 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.069528 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.069608 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.069651 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.069715 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.069844 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.070174 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.070224 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.072128 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.072226 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.075135 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.075219 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.075721 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.086175 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.094989 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.095090 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.095387 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.095468 140210684239872 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:35:50.095579 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.095618 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.095651 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.097491 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.099987 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.105596 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.105880 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.108530 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.112360 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.112416 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.112453 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.112484 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.112547 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.113110 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.113189 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.113548 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.114319 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.116825 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.117445 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.117521 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.117556 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.117616 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.117748 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.118077 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.118122 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.120064 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.120161 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.122678 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.122760 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.123198 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.125515 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.127423 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.127519 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.127818 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.127898 140210684239872 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:35:50.128009 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.128048 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.128080 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.130015 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.132384 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.138383 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.138638 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.141303 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.145154 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.145210 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.145247 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.145279 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.145341 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.146495 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.146639 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.147061 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.147880 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.150462 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.151139 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.151218 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.151253 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.151314 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.151449 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.151794 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.151839 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.153803 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.153901 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.156459 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.156544 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.157038 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.159355 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.161302 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.161400 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.161708 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.161792 140210684239872 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:35:50.161903 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.161942 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.161974 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.163910 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.166350 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.172059 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.172320 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.174988 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.178828 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.178887 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.178924 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.178955 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.179017 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.179580 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.179656 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.180021 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.180795 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.183372 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.184003 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.184083 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.184120 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.184180 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.184312 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.184826 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.184869 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.186922 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.187017 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.189592 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.189683 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.190156 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.192444 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.194379 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.194473 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.194765 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.194844 140210684239872 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:35:50.194952 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.194991 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.195022 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.196924 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.199371 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.205077 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.205339 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.208086 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.211861 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.211916 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.211952 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.211983 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.212045 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.212608 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.212683 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.213046 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.213824 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.216726 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.217355 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.217435 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.217471 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.217533 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.217671 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.218001 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.218045 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.219972 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.220064 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.222667 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.222747 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.223178 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.225471 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.227471 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.227567 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.227866 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.227947 140210684239872 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:35:50.228059 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.228099 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.228131 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.229987 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.232399 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.238073 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.238337 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.241064 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.244817 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.244871 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.244907 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.244939 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.245000 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.245600 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.245687 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.246051 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.246836 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.249335 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.249973 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.250051 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.250087 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.250148 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.250275 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.250601 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.250644 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.252545 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.252636 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.255227 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.255305 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.255741 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.258077 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.260011 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.260105 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.260402 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.260483 140210684239872 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:35:50.260595 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.260634 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.260666 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.262533 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.265026 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.270717 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.270980 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.273666 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.277460 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.277514 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.277551 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.277582 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.277648 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.278222 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.278298 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.278662 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.279433 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.281953 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.282580 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.282657 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.282692 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.282751 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.282877 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.283203 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.283246 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.285213 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.285306 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.287901 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.287980 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.288408 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.291057 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.292984 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.293085 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.293379 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.293459 140210684239872 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:35:50.293571 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.293611 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.293648 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.430447 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.433470 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.439345 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.439641 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.442350 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.446267 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.446326 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.446369 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.446403 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.446469 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.447074 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.447151 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.447518 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.448309 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.450945 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.451589 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.451672 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.451709 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.451771 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.451901 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.452240 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.452284 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.454228 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.454322 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.456941 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.457024 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.457468 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.459826 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.461767 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.461874 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.462172 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.462256 140210684239872 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:35:50.462370 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.462411 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.462443 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.464397 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.466818 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.472505 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.472774 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.475510 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.479312 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.479372 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.479410 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.479441 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.479505 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.480090 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.480169 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.480531 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.481313 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.483942 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.484568 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.484645 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.484682 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.484742 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.484871 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.485191 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.485235 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.487211 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.487324 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.489911 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.489995 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.490472 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.492783 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.494792 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.494889 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.495186 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.495278 140210684239872 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:35:50.495392 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.495432 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.495465 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.497330 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.499806 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.505494 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.505763 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.508824 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.512595 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.512650 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.512687 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.512718 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.512782 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.513380 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.513456 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.513842 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.514622 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.517152 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.517782 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.517861 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.517896 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.517957 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.518087 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.518412 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.518455 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.520385 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.520478 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.523080 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.523160 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.523597 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.525962 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.527891 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.527990 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.528285 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.528374 140210684239872 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:35:50.528489 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.528529 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.528561 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.530426 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.532904 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.538600 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.538872 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.541535 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.545341 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.545395 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.545431 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.545463 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.545524 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.546096 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.546177 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.546558 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.547344 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.549853 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.550499 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.550579 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.550616 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.550682 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.550817 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.551154 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.551198 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.553169 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.553262 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.556085 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.556165 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.556610 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.558968 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.560889 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.560984 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.561278 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.561358 140210684239872 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:35:50.561479 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.561519 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.561552 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.563480 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.565912 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.571562 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.571828 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.574492 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:35:50.578286 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.578343 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.578381 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.578414 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.578477 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.579038 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.579117 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.579475 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.580252 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.582768 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.583757 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.583842 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.583878 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.583941 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.584074 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.584398 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.584441 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.586378 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.586471 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.588996 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.589079 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.589571 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.591825 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.593761 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.593858 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.594179 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.594465 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.594535 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.594603 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.594660 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.594716 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.594771 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.594826 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.594879 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.594931 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.594984 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.595035 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.595088 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:35:50.595126 140210684239872 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:35:50.598683 140210684239872 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:35:50.646704 140210684239872 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.646791 140210684239872 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:35:50.646846 140210684239872 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:35:50.646951 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.646991 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.647022 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.647086 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.649533 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.655116 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.655378 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.658037 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:50.674728 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.674786 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.674823 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.674856 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.674920 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.676057 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.676135 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.676851 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.678882 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.683681 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.684994 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.685081 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.685118 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.685179 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.685311 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.685421 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.685461 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.687393 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.687488 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.689947 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.690027 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.690142 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.692370 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.694325 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.694422 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.694714 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.694796 140210684239872 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:35:50.694907 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.694948 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.694980 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.695046 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.697310 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.702831 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.703090 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.705826 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:50.718989 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.719044 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.719080 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.719112 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.719174 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.719731 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.719808 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.720169 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.720858 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.723372 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.723986 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.724065 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.724106 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.724166 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.724296 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.724406 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.724445 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.726395 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.726490 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.728905 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.728984 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.729094 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.731321 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.733266 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.733362 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.733658 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.733740 140210684239872 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:35:50.733852 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.733892 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.733924 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.733989 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.736268 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.741796 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.742054 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.744748 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:50.757484 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.757540 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.757576 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.757608 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.757698 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.758263 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.758339 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.758690 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.759385 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.761889 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.762575 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.762652 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.762687 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.762752 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.762882 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.762993 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.763031 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.765046 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.765140 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.767611 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.767691 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.767800 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.770051 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.771996 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.772093 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.772384 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.772466 140210684239872 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:35:50.772575 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.772615 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.772646 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.772710 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.774970 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.780437 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.780696 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.783397 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:50.796192 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.796247 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.796284 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.796315 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.796377 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.796929 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.797010 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.797366 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.798064 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.800575 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.801194 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.801272 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.801308 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.801368 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.801506 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.801618 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.801664 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.803609 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.803703 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.810612 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.810731 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.810857 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.813311 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.815235 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.815332 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.815633 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.815721 140210684239872 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:35:50.815834 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.815876 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.815908 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.815975 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.818644 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.824199 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.824472 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.827177 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:50.840137 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.840195 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.840233 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.840265 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.840332 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.840915 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.840992 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.841361 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.842068 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.844708 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.845343 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.845421 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.845457 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.845521 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.845665 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.845778 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.845817 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.847718 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.847814 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.850270 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.850350 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.850460 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.852770 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.854660 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.854755 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.855041 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.855122 140210684239872 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:35:50.855233 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.855273 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.855304 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.855368 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.857650 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.863158 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.863555 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.866291 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:50.879091 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.879147 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.879183 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.879216 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.879279 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.879856 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.879932 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.880292 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.880985 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.883521 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.884141 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.884218 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.884253 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.884312 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.884441 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.884556 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.884595 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.886577 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.886672 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.889097 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.889176 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.889286 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.891527 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.893399 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.893495 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.893794 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.893877 140210684239872 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:35:50.893989 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.894028 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.894059 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.894123 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.896375 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.901933 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.902193 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.904817 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:50.917528 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.917584 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.917619 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.917658 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.917722 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.918282 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.918358 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.918717 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.919409 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.921932 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.922926 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.923005 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.923040 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.923101 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.923230 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.923339 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.923383 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.925284 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.925380 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.927788 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.927868 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.927976 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.930189 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.932138 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.932234 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.932523 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.932604 140210684239872 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:35:50.932714 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.932754 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.932785 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.932848 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.935115 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.940614 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.940882 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.943607 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:50.956467 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.956523 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.956559 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.956591 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.956653 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.957256 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.957332 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.957700 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.958395 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.960902 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.961528 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.961604 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:50.961646 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:50.961709 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.961841 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:50.961956 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:50.962001 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.963887 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.963982 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.966463 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.966543 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:50.966653 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:50.968886 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:50.970780 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.970877 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:50.971165 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.971248 140210684239872 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:35:50.971360 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:50.971400 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:50.971431 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:50.971495 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.973746 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:50.979295 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.979555 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:50.982210 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:50.994948 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:50.995004 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:50.995042 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:50.995074 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.995136 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.995693 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.995771 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.996140 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.996841 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:50.999382 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.000061 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.000141 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.000176 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.000239 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.000370 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.000479 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.000518 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.002431 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.002525 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.004955 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.005034 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.005141 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.007367 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.009328 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.009423 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.009723 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.009807 140210684239872 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:35:51.009917 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.009957 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.009988 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.010054 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.012308 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.017803 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.018064 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.020758 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.033761 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.033822 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.033859 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.033891 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.033954 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.034564 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.034641 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.035000 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.035690 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.038194 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.038815 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.038892 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.038927 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.038986 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.039116 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.039225 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.039263 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.041165 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.041264 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.043738 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.043818 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.043925 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.046148 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.047995 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.048090 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.048382 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.048463 140210684239872 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:35:51.048574 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.048613 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.048645 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.048710 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.050966 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.056492 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.056751 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.059514 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.072236 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.072292 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.072328 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.072358 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.072421 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.072973 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.073048 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.073416 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.074116 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.076606 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.077266 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.077343 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.077378 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.077440 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.077570 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.077687 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.077727 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.079647 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.079747 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.082216 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.082296 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.082406 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.084628 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.086601 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.086698 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.086986 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.087067 140210684239872 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:35:51.087177 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.087217 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.087249 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.087312 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.089565 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.095051 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.095313 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.098039 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.110662 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.110718 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.110754 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.110786 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.110847 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.111397 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.111473 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.111832 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.112579 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.115096 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.115710 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.115787 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.115821 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.115880 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.116005 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.116112 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.116155 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.118042 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.118135 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.120566 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.120645 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.120754 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.123032 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.124904 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.125000 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.125287 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.125376 140210684239872 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:35:51.128262 140210684239872 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:35:51.183775 140210684239872 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.183861 140210684239872 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:35:51.183915 140210684239872 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:35:51.184018 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.184056 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.184088 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.184152 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.186857 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.192263 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.192524 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.195142 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.207505 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.207562 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.207598 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.207630 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.207692 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.208249 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.208325 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.208681 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.209356 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.211850 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.212458 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.212535 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.212570 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.212629 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.212755 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.212870 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.212910 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.214758 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.214851 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.217220 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.217298 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.217406 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.219636 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.221470 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.221565 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.221856 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.221938 140210684239872 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:35:51.222046 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.222085 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.222117 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.222180 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.224403 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.229734 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.229992 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.232667 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.244893 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.244949 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.244984 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.245016 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.245078 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.245626 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.245710 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.246065 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.246736 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.249225 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.249838 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.249917 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.249953 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.250014 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.250141 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.250250 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.250296 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.252124 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.252218 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.254607 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.254688 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.254799 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.257034 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.258877 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.258975 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.259264 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.259345 140210684239872 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:35:51.259454 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.259494 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.259526 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.259589 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.261821 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.267179 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.267436 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.270097 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.282393 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.282449 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.282486 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.282518 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.282581 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.283135 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.283211 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.283569 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.284246 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.286768 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.287380 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.287458 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.287494 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.287554 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.287682 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.287791 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.287829 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.289672 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.289771 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.292154 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.292233 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.292343 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.295043 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.296903 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.297000 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.297288 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.297369 140210684239872 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:35:51.297477 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.297517 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.297549 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.297613 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.299851 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.305206 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.305466 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.308154 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.320541 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.320599 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.320638 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.320681 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.320745 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.321295 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.321370 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.321734 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.322417 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.324958 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.325573 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.325654 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.325691 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.325750 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.325876 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.325985 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.326027 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.327915 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.328008 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.330410 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.330487 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.330595 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.332887 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.334758 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.334854 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.335140 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.335219 140210684239872 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:35:51.335327 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.335365 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.335395 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.335456 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.337682 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.343064 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.343319 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.345992 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.358421 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.358474 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.358510 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.358540 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.358602 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.359158 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.359233 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.359587 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.360276 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.362832 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.363451 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.363526 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.363560 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.363617 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.363743 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.363849 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.363886 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.365765 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.365862 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.368257 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.368335 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.368442 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.370724 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.372563 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.372657 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.372944 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.373024 140210684239872 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:35:51.373132 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.373170 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.373200 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.373263 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.375492 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.380884 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.381138 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.383833 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.396355 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.396408 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.396443 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.396473 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.396535 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.397082 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.397157 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.397520 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.398213 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.400797 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.401407 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.401482 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.401516 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.401573 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.401702 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.401809 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.401846 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.403720 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.403817 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.406224 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.406302 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.406410 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.409090 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.410962 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.411057 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.411345 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.411423 140210684239872 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:35:51.411530 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.411567 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.411598 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.411660 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.413900 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.419338 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.419594 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.422302 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.434905 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.434960 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.434994 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.435025 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.435087 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.435647 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.435723 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.436081 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.436772 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.439337 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.439959 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.440035 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.440069 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.440129 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.440259 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.440369 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.440406 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.442305 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.442396 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.444784 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.444861 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.444969 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.447247 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.449108 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.449201 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.449485 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.449563 140210684239872 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:35:51.449676 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.449714 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.449745 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.449807 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.452067 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.457553 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.457818 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.460504 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.473115 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.473168 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.473203 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.473233 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.473297 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.473860 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.473935 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.474292 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.474976 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.477515 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.478139 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.478215 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.478248 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.478306 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.478430 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.478536 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.478573 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.480446 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.480538 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.482938 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.483020 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.483130 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.485415 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.487268 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.487362 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.487645 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.487725 140210684239872 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:35:51.487833 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.487870 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.487900 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.487962 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.490178 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.495580 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.495835 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.498499 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.511141 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.511195 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.511229 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.511260 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.511323 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.511883 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.511958 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.512319 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.513007 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.515574 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.516192 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.516268 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.516302 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.516359 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.516483 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.516590 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.516626 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.518510 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.518602 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.520990 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.521072 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.521180 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.523846 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.525727 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.525822 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.526108 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.526189 140210684239872 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:35:51.526298 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.526336 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.526367 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.526430 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.528662 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.534109 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.534365 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.537042 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.549573 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.549627 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.549668 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.549699 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.549761 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.550322 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.550400 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.550760 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.551450 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.554006 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.554620 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.554695 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.554729 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.554786 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.554917 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.555024 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.555061 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.557430 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.557523 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.559911 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.559988 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.560106 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.562343 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.564175 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.564268 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.564554 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.564633 140210684239872 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:35:51.564739 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.564777 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.564807 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.564868 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.567094 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.572478 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.572733 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.575450 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.587878 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.587935 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.587970 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.588001 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.588068 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.588634 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.588709 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.589069 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.589759 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.592325 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.592948 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.593024 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.593059 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.593116 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.593242 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.593350 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.593388 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.595268 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.595360 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.597726 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.597804 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.597909 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.600183 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.602023 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.602116 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.602400 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.602479 140210684239872 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:35:51.602586 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:35:51.602623 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:35:51.602653 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:35:51.602713 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.604928 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:35:51.610351 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.610606 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:35:51.613294 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:35:51.625818 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:35:51.625873 140210684239872 attention.py:418] Single window, no scan.
I0123 17:35:51.625908 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:35:51.625939 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.626000 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.626559 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.626633 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.626991 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.627679 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.630228 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.630845 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.630922 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:35:51.630955 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:35:51.631013 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.631138 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:35:51.631244 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:35:51.631282 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.633145 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.633234 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.635624 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.635701 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:35:51.635812 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:35:51.638619 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:35:51.640480 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.640712 140210684239872 nn_components.py:261] mlp: residual
I0123 17:35:51.641002 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:51.641087 140210684239872 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:35:51.643907 140210684239872 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:35:56.062838 140210684239872 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 17:35:56.634847 140210684239872 training_loop.py:409] No working directory specified.
I0123 17:35:56.634976 140210684239872 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 17:35:56.635757 140210684239872 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 17:35:59.947021 140210684239872 training_loop.py:447] Only restoring trainable parameters.
I0123 17:35:59.947661 140210684239872 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 17:35:59.947745 140210684239872 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.947795 140210684239872 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.947840 140210684239872 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.947882 140210684239872 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.947922 140210684239872 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.947961 140210684239872 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948000 140210684239872 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948037 140210684239872 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.948073 140210684239872 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.948110 140210684239872 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948148 140210684239872 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.948185 140210684239872 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.948222 140210684239872 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.948259 140210684239872 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948295 140210684239872 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.948333 140210684239872 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948370 140210684239872 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948407 140210684239872 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.948444 140210684239872 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.948494 140210684239872 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948534 140210684239872 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.948573 140210684239872 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.948609 140210684239872 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.948645 140210684239872 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948681 140210684239872 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.948718 140210684239872 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948753 140210684239872 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948790 140210684239872 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.948825 140210684239872 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.948860 140210684239872 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.948896 140210684239872 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.948930 140210684239872 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.948965 140210684239872 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.949001 140210684239872 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949035 140210684239872 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.949070 140210684239872 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949105 140210684239872 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949141 140210684239872 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.949176 140210684239872 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.949211 140210684239872 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949247 140210684239872 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.949283 140210684239872 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.949318 140210684239872 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.949352 140210684239872 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949387 140210684239872 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.949428 140210684239872 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949465 140210684239872 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949501 140210684239872 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.949537 140210684239872 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.949572 140210684239872 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949607 140210684239872 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.949652 140210684239872 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.949692 140210684239872 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.949733 140210684239872 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949770 140210684239872 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.949806 140210684239872 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949841 140210684239872 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949876 140210684239872 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.949911 140210684239872 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.949946 140210684239872 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.949981 140210684239872 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.950016 140210684239872 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.950052 140210684239872 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.950087 140210684239872 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950122 140210684239872 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.950157 140210684239872 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950192 140210684239872 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950227 140210684239872 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.950263 140210684239872 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.950299 140210684239872 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950335 140210684239872 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.950373 140210684239872 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.950416 140210684239872 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.950453 140210684239872 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950489 140210684239872 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.950525 140210684239872 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950560 140210684239872 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950595 140210684239872 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.950630 140210684239872 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.950665 140210684239872 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950700 140210684239872 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.950736 140210684239872 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.950770 140210684239872 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.950806 140210684239872 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950841 140210684239872 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.950876 140210684239872 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950911 140210684239872 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.950946 140210684239872 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.950981 140210684239872 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.951015 140210684239872 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951050 140210684239872 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.951085 140210684239872 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.951120 140210684239872 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.951155 140210684239872 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951189 140210684239872 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.951225 140210684239872 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951261 140210684239872 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951297 140210684239872 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.951333 140210684239872 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.951373 140210684239872 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951410 140210684239872 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.951446 140210684239872 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.951481 140210684239872 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.951515 140210684239872 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951551 140210684239872 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.951587 140210684239872 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951623 140210684239872 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951658 140210684239872 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.951694 140210684239872 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.951730 140210684239872 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951765 140210684239872 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.951800 140210684239872 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:35:59.951835 140210684239872 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:35:59.951870 140210684239872 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951906 140210684239872 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.951942 140210684239872 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.951977 140210684239872 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.952013 140210684239872 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:35:59.952049 140210684239872 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:35:59.952085 140210684239872 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:35:59.952121 140210684239872 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:35:59.952150 140210684239872 training_loop.py:725] Total parameters: 152072288
I0123 17:35:59.952377 140210684239872 training_loop.py:739] Total state size: 0
I0123 17:35:59.976454 140210684239872 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 17:35:59.976697 140210684239872 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 17:35:59.977095 140210684239872 training_loop.py:652] Compiling mode beam_search with jit.
I0123 17:35:59.977430 140210684239872 training_loop.py:89] registering functions: dict_keys([])
I0123 17:35:59.994050 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b ? coll m o n
I0123 17:36:04.498471 140210684239872 ddar.py:60] Depth 1/1000 time = 4.4764111042022705
I0123 17:36:15.528173 140210684239872 ddar.py:60] Depth 2/1000 time = 11.029510736465454
I0123 17:36:31.619644 140210684239872 ddar.py:60] Depth 3/1000 time = 16.091217756271362
I0123 17:36:50.795311 140210684239872 ddar.py:60] Depth 4/1000 time = 19.175267457962036
I0123 17:37:10.059557 140210684239872 ddar.py:60] Depth 5/1000 time = 19.26335644721985
I0123 17:37:31.146979 140210684239872 ddar.py:60] Depth 6/1000 time = 20.770330905914307
I0123 17:37:53.157366 140210684239872 ddar.py:60] Depth 7/1000 time = 21.747473001480103
I0123 17:37:53.157711 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:37:53.157815 140210684239872 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 17:37:53.157851 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00
I0123 17:37:53.157882 140210684239872 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00
I0123 17:37:53.307466 140210684239872 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.307666 140210684239872 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 17:37:53.307767 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.307845 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.307917 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.307987 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.308057 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.308126 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.308196 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.308264 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.308331 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.308399 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.308466 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.308532 140210684239872 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:53.308575 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.308621 140210684239872 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:37:53.308734 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.308773 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.308803 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.310780 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.313315 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.319030 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.319303 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.322011 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.325992 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.326049 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.326086 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.326119 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.326181 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.326831 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.326906 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.327271 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.328053 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.330698 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.331335 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.331411 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.331445 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.331506 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.331631 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.331957 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.332000 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.333921 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.334013 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.336470 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.336548 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.336969 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.339427 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.341376 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.341469 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.341765 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.341845 140210684239872 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:37:53.341952 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.341989 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.342019 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.343786 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.346106 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.351718 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.351970 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.354537 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.358172 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.358226 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.358261 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.358291 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.358353 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.358954 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.359028 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.359381 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.360135 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.362604 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.363218 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.363293 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.363328 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.363386 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.363511 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.363830 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.363871 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.365852 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.365944 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.368391 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.368469 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.368896 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.371170 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.373096 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.373188 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.373475 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.373553 140210684239872 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:37:53.373671 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.373711 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.373741 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.375597 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.377923 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.383517 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.383767 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.386376 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.390038 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.390091 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.390125 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.390154 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.390214 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.390775 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.390850 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.391207 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.391977 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.394460 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.395480 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.395558 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.395593 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.395651 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.395778 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.396095 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.396137 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.398054 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.398149 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.400613 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.400690 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.401112 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.403443 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.405366 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.405458 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.405751 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.405831 140210684239872 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:37:53.405938 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.405976 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.406006 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.407771 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.410109 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.415788 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.416043 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.418594 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.422262 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.422315 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.422349 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.422379 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.422441 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.423045 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.423119 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.423471 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.424229 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.426733 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.427343 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.427422 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.427455 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.427513 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.427640 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.427953 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.427994 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.429999 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.430091 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.432532 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.432612 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.433036 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.435292 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.437206 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.437299 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.437587 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.437673 140210684239872 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:37:53.437781 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.437819 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.437849 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.439686 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.441997 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.447552 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.447813 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.450439 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.454074 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.454127 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.454162 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.454192 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.454253 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.454806 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.454880 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.455230 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.455978 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.458430 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.459095 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.459171 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.459208 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.459265 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.459392 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.459707 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.459748 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.461646 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.461738 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.464182 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.464258 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.464679 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.467007 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.468909 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.469001 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.469290 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.469369 140210684239872 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:37:53.469478 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.469516 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.469546 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.471323 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.473633 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.479233 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.479483 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.482044 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.485685 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.485740 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.485774 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.485805 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.485916 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.486469 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.486543 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.486897 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.487662 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.490124 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.490740 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.490815 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.490850 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.490908 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.491062 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.491379 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.491421 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.493367 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.493460 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.495913 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.495990 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.496414 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.498663 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.500565 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.500657 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.500944 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.501023 140210684239872 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:37:53.501129 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.501167 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.501197 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.503355 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.505675 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.511185 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.511438 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.514044 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.517660 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.517713 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.517748 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.517778 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.517840 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.518392 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.518467 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.518821 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.519575 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.522022 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.522690 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.522767 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.522801 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.522858 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.522985 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.523298 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.523339 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.525220 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.525309 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.527737 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.527815 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.528233 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.530534 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.532433 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.532526 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.532812 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.532891 140210684239872 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:37:53.532999 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.533038 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.533069 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.534828 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.537120 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.542740 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.542990 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.545519 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.549101 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.549159 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.549194 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.549224 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.549332 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.549889 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.549963 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.550314 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.551066 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.553488 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.554097 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.554172 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.554205 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.554261 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.554384 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.554695 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.554736 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.556673 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.556762 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.559193 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.559269 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.559688 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.561913 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.563786 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.563878 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.564163 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.564242 140210684239872 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:37:53.564348 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.564385 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.564415 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.566251 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.568533 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.574070 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.574321 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.576905 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.580475 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.580528 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.580568 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.580599 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.580660 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.581214 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.581288 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.581647 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.582404 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.584821 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.585436 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.585511 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.585545 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.585601 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.585736 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.586101 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.586143 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.588037 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.588127 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.590564 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.590642 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.591059 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.593293 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.595252 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.595344 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.595633 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.595712 140210684239872 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:37:53.595819 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.595857 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.595888 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.597649 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.599935 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.605538 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.605794 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.608320 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.611916 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.611969 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.612003 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.612041 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.612105 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.613022 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.613098 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.613457 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.614235 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.616707 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.617322 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.617398 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.617432 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.617489 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.617614 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.617935 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.617978 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.619883 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.619973 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.622468 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.622545 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.622961 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.625195 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.627095 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.627189 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.627480 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.627559 140210684239872 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:37:53.627666 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.627704 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.627735 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.629506 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.631923 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.637490 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.637749 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.640305 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.643902 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.643955 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.643990 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.644019 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.644134 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.644693 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.644766 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.645118 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.645878 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.648335 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.648947 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.649024 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.649057 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.649115 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.649241 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.649554 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.649594 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.651488 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.651580 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.654078 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.654155 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.654580 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.656830 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.658737 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.658830 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.659120 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.659199 140210684239872 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:37:53.659305 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.659342 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.659373 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.661139 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.663524 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.669010 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.669262 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.671822 140210684239872 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:53.675422 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.675476 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.675510 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.675541 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.675651 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.676206 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.676278 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.676629 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.677376 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.679812 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.680423 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.680499 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.680532 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.680590 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.680715 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.681029 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.681070 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.682976 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.683067 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.685567 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.685649 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.686069 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.688296 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.690197 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.690289 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.690579 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.690825 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.690891 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.690947 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691001 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691052 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691103 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691154 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691205 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691255 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691305 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691356 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691406 140210684239872 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:53.691442 140210684239872 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:37:53.694365 140210684239872 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:53.738729 140210684239872 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.738813 140210684239872 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:37:53.738865 140210684239872 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:37:53.738966 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.739004 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.739035 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.739095 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.741433 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.746784 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.747040 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.749602 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:53.762433 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.762488 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.762523 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.762553 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.762614 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.763180 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.763254 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.763626 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.764327 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.766909 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.767525 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.767603 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.767638 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.767697 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.767824 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.767931 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.767968 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.769816 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.769908 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.772294 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.772372 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.772479 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.774704 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.776543 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.776642 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.776933 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.777012 140210684239872 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:37:53.777121 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.777159 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.777189 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.777251 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.779463 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.784798 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.785054 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.787716 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:53.800312 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.800365 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.800400 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.800430 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.800492 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.801046 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.801120 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.801474 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.802214 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.804661 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.805276 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.805351 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.805385 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.805443 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.805570 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.805683 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.805721 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.807560 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.807650 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.810028 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.810106 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.810212 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.812425 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.814254 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.814352 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.814642 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.814721 140210684239872 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:37:53.814829 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.814867 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.814896 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.814957 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.817155 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.822520 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.822775 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.825403 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:53.837582 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.837636 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.837678 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.837708 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.837769 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.838321 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.838395 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.838749 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.839468 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.841898 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.842510 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.842586 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.842619 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.842676 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.842801 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.842908 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.842945 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.844772 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.844863 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.847257 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.847336 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.847442 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.849664 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.851497 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.851589 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.851886 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.851965 140210684239872 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:37:53.852074 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.852112 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.852142 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.852204 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.854438 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.859869 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.860137 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.862849 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:53.875252 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.875306 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.875339 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.875370 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.875431 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.875982 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.876057 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.876415 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.877143 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.879601 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.880207 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.880282 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.880316 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.880373 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.880500 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.880606 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.880644 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.882503 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.882594 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.884993 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.885070 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.885181 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.887516 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.889347 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.889439 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.889735 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.889823 140210684239872 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:37:53.889932 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.889971 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.890000 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.890064 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.892336 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.897670 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.897923 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.900550 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:53.913206 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.913259 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.913293 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.913324 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.913385 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.913944 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.914018 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.914371 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.915096 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.917567 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.918186 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.918263 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.918297 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.918354 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.918480 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.918586 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.918623 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.920447 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.920538 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.922934 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.923012 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.923119 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.925335 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.927188 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.927281 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.927571 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.927657 140210684239872 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:37:53.927767 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.927805 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.927835 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.927896 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.930124 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.935454 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.935708 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.938343 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:53.950581 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.950635 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.950670 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.950700 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.950762 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.951306 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.951380 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.951732 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.952458 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.954908 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.955518 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.955592 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.955626 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.955682 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.955807 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.955915 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.955952 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.957792 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.957883 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.960263 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.960339 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.960445 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:53.962674 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.964512 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.964604 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.964891 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.964969 140210684239872 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:37:53.965080 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:53.965119 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:53.965149 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:53.965211 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.967431 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:53.972807 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.973062 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:53.975694 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:53.987875 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:53.987928 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:53.987963 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:53.987994 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.988056 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.988605 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.988679 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.989033 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.989770 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.992250 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.992862 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.992937 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:53.992971 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:53.993028 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.993155 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:53.993263 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:53.993299 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:53.995148 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.995239 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:53.997607 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:53.997692 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:53.997800 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.000023 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.001847 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.001943 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.002232 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.002311 140210684239872 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:37:54.002424 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.002461 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.002491 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.002552 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.004794 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.010146 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.010403 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.013047 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.025666 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.025721 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.025756 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.025787 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.025849 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.026402 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.026476 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.026830 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.027564 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.030007 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.030616 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.030692 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.030726 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.030783 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.030909 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.031015 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.031052 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.032886 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.032976 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.035382 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.035458 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.035563 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.037797 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.039620 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.039713 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.040000 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.040078 140210684239872 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:37:54.040186 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.040229 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.040261 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.040324 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.042544 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.047899 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.048152 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.050783 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.063102 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.063157 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.063191 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.063221 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.063283 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.063834 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.063908 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.064262 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.064938 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.067464 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.068072 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.068147 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.068181 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.068238 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.068363 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.068469 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.068506 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.070355 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.070447 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.072824 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.072901 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.073006 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.075249 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.077075 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.077168 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.077457 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.077535 140210684239872 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:37:54.077647 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.077687 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.077724 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.077788 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.080005 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.085307 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.085561 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.088216 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.100460 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.100513 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.100548 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.100577 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.100638 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.101186 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.101261 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.101614 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.102303 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.104811 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.105423 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.105499 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.105532 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.105589 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.105748 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.105857 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.105894 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.107731 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.107823 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.110209 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.110287 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.110393 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.112609 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.114450 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.114543 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.114830 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.114909 140210684239872 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:37:54.115016 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.115054 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.115083 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.115151 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.117374 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.122735 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.122990 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.125628 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.138280 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.138334 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.138369 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.138399 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.138460 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.139004 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.139078 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.139434 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.140109 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.142634 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.143248 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.143324 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.143357 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.143415 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.143541 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.143647 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.143684 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.145529 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.145619 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.148019 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.148096 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.148202 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.150455 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.152312 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.152405 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.152698 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.152777 140210684239872 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:37:54.152885 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.152923 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.152953 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.153021 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.155264 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.160651 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.160910 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.163572 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.175873 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.175928 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.175963 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.175993 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.176055 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.176609 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.176685 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.177045 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.177732 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.180281 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.180892 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.180968 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.181002 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.181059 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.181185 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.181293 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.181331 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.183180 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.183272 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.185658 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.185737 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.185844 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.188082 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.189923 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.190016 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.190307 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.190392 140210684239872 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:37:54.193233 140210684239872 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:54.242333 140210684239872 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.242418 140210684239872 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:37:54.242477 140210684239872 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:37:54.242583 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.242620 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.242650 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.242712 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.244974 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.250374 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.250633 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.253205 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.265474 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.265530 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.265564 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.265596 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.265665 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.266214 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.266288 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.266637 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.267307 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.269757 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.270362 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.270436 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.270470 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.270527 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.270650 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.270756 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.270792 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.272679 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.272770 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.275155 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.275231 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.275338 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.277503 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.279340 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.279432 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.279718 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.279797 140210684239872 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:37:54.279908 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.279946 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.279975 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.280037 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.282276 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.287718 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.287975 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.290536 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.302822 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.302876 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.302910 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.302940 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.303001 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.303547 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.303621 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.303977 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.304646 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.307091 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.307703 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.307780 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.307814 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.307872 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.307996 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.308102 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.308140 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.310488 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.310581 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.312948 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.313025 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.313131 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.315290 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.317094 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.317186 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.317471 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.317549 140210684239872 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:37:54.317661 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.317705 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.317736 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.317798 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.319989 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.325357 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.325610 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.328153 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.340275 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.340329 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.340363 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.340394 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.340455 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.340999 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.341074 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.341424 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.342099 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.344513 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.345113 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.345188 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.345222 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.345280 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.345403 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.345508 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.345546 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.347453 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.347543 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.349903 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.349979 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.350085 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.352228 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.354053 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.354145 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.354430 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.354508 140210684239872 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:37:54.354614 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.354658 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.354689 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.354753 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.356943 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.362383 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.362638 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.365216 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.377373 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.377428 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.377463 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.377492 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.377553 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.378107 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.378182 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.378535 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.379207 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.381631 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.382242 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.382317 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.382350 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.382407 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.382531 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.382637 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.382675 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.384570 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.384661 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.387045 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.387121 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.387227 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.389379 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.391218 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.391310 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.391599 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.391677 140210684239872 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:37:54.391783 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.391821 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.391857 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.391922 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.394149 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.399544 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.399794 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.402354 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.414547 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.414600 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.414633 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.414664 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.414724 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.415270 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.415343 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.415695 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.416364 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.418797 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.419403 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.419476 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.419509 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.419565 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.419686 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.419791 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.419830 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.422163 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.422255 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.424621 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.424697 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.424803 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.426992 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.428829 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.428922 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.429212 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.429290 140210684239872 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:37:54.429397 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.429436 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.429466 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.429533 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.431761 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.437204 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.437465 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.440045 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.452272 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.452325 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.452360 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.452389 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.452450 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.452997 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.453071 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.453426 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.454103 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.456547 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.457150 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.457224 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.457257 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.457315 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.457438 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.457543 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.457580 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.459448 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.459539 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.461899 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.461977 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.462084 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.464230 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.466050 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.466144 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.466433 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.466512 140210684239872 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:37:54.466618 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.466656 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.466686 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.466749 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.468952 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.474331 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.474583 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.477111 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.489271 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.489325 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.489359 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.489388 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.489449 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.490004 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.490078 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.490432 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.491094 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.493507 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.494115 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.494191 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.494224 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.494281 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.494405 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.494510 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.494546 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.496423 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.496513 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.498884 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.498961 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.499068 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.501221 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.503035 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.503127 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.503412 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.503490 140210684239872 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:37:54.503597 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.503634 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.503664 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.503725 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.505911 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.511251 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.511507 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.514046 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.526210 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.526263 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.526298 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.526329 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.526389 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.526935 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.527008 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.527360 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.528021 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.530448 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.531050 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.531124 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.531157 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.531214 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.531337 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.531442 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.531479 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.533771 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.533862 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.536221 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.536296 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.536403 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.538567 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.540376 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.540467 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.540757 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.540835 140210684239872 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:37:54.540940 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.540978 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.541008 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.541068 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.543287 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.548824 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.549078 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.551692 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.563888 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.563942 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.563977 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.564007 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.564069 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.564620 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.564693 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.565049 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.565733 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.568172 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.568778 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.568850 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.568883 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.568940 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.569062 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.569167 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.569202 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.571103 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.571193 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.573579 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.573661 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.573770 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.575937 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.577774 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.577867 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.578161 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.578238 140210684239872 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:37:54.578344 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.578382 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.578413 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.578475 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.580691 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.586092 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.586347 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.588904 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.601227 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.601282 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.601316 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.601347 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.601406 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.601963 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.602037 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.602399 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.603066 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.605527 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.606136 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.606211 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.606244 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.606302 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.606429 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.606536 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.606573 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.608485 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.608574 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.611027 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.611106 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.611216 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.613401 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.615300 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.615395 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.615695 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.615773 140210684239872 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:37:54.615880 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.615916 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.615946 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.616008 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.618219 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.623611 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.623869 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.626448 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.638788 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.638842 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.638876 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.638906 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.638964 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.639507 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.639579 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.639933 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.640608 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.643059 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.643662 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.643736 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.643768 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.643825 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.643947 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.644052 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.644089 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.646406 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.646497 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.648870 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.648945 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.649051 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.651224 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.653029 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.653120 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.653410 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.653487 140210684239872 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:37:54.653595 140210684239872 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:54.653633 140210684239872 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:54.653671 140210684239872 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:54.653734 140210684239872 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.655957 140210684239872 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:54.661389 140210684239872 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.661656 140210684239872 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:54.664231 140210684239872 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:54.676609 140210684239872 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:54.676663 140210684239872 attention.py:418] Single window, no scan.
I0123 17:37:54.676697 140210684239872 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:54.676728 140210684239872 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.676787 140210684239872 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.677337 140210684239872 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.677410 140210684239872 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.677773 140210684239872 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.678452 140210684239872 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.680908 140210684239872 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.681521 140210684239872 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.681595 140210684239872 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:54.681628 140210684239872 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:54.681694 140210684239872 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.681823 140210684239872 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:54.681931 140210684239872 nn_components.py:325] mlp: activation = None
I0123 17:37:54.681968 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.683892 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.683982 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.686372 140210684239872 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.686448 140210684239872 transformer_base.py:443] tbase: final FFN
I0123 17:37:54.686556 140210684239872 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:54.688735 140210684239872 nn_components.py:329] mlp: final activation = None
I0123 17:37:54.690567 140210684239872 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.690659 140210684239872 nn_components.py:261] mlp: residual
I0123 17:37:54.690946 140210684239872 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:54.691029 140210684239872 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:37:54.693844 140210684239872 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:38:09.315342 140210684239872 alphageometry.py:566] LM output (score=-2.199203): "p : C l m p 21 D l p m p 22 ;"
I0123 17:38:09.315489 140210684239872 alphageometry.py:567] Translation: "p = on_line p l m, on_bline p m l"

I0123 17:38:09.315532 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l ? coll m o n"
I0123 17:38:09.315696 140210684239872 graph.py:498] 
I0123 17:38:09.315753 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l ? coll m o n
I0123 17:38:14.393142 140210684239872 ddar.py:60] Depth 1/1000 time = 5.052680492401123
I0123 17:38:26.498906 140210684239872 ddar.py:60] Depth 2/1000 time = 12.105573415756226
I0123 17:38:43.856796 140210684239872 ddar.py:60] Depth 3/1000 time = 17.35767912864685
I0123 17:39:04.792713 140210684239872 ddar.py:60] Depth 4/1000 time = 20.93570351600647
I0123 17:39:25.695631 140210684239872 ddar.py:60] Depth 5/1000 time = 20.90211033821106
I0123 17:39:48.881632 140210684239872 ddar.py:60] Depth 6/1000 time = 22.85504150390625
I0123 17:40:12.411593 140210684239872 ddar.py:60] Depth 7/1000 time = 23.24878430366516
I0123 17:40:12.411945 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:40:12.412071 140210684239872 alphageometry.py:566] LM output (score=-2.479882): "p : C l n p 21 D l p n p 22 ;"
I0123 17:40:12.412108 140210684239872 alphageometry.py:567] Translation: "p = on_line p l n, on_bline p n l"

I0123 17:40:12.412156 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l n, on_bline p n l ? coll m o n"
I0123 17:40:12.412339 140210684239872 graph.py:498] 
I0123 17:40:12.412398 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l n, on_bline p n l ? coll m o n
I0123 17:40:17.565459 140210684239872 ddar.py:60] Depth 1/1000 time = 5.119319677352905
I0123 17:40:29.595555 140210684239872 ddar.py:60] Depth 2/1000 time = 12.029915571212769
I0123 17:40:47.162177 140210684239872 ddar.py:60] Depth 3/1000 time = 17.566396951675415
I0123 17:41:08.211050 140210684239872 ddar.py:60] Depth 4/1000 time = 21.04862070083618
I0123 17:41:29.232814 140210684239872 ddar.py:60] Depth 5/1000 time = 21.02085518836975
I0123 17:41:52.367616 140210684239872 ddar.py:60] Depth 6/1000 time = 22.82246208190918
I0123 17:42:16.205631 140210684239872 ddar.py:60] Depth 7/1000 time = 23.551990270614624
I0123 17:42:16.205872 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:42:16.205960 140210684239872 alphageometry.py:566] LM output (score=-2.493752): "p : P a e l p 21 P a l e p 22 ;"
I0123 17:42:16.205995 140210684239872 alphageometry.py:567] Translation: "p = on_pline p l a e, on_pline p e a l"

I0123 17:42:16.206030 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p l a e, on_pline p e a l ? coll m o n"
I0123 17:42:16.206193 140210684239872 graph.py:498] 
I0123 17:42:16.206252 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p l a e, on_pline p e a l ? coll m o n
I0123 17:42:22.796468 140210684239872 ddar.py:60] Depth 1/1000 time = 6.5602357387542725
I0123 17:42:36.666775 140210684239872 ddar.py:60] Depth 2/1000 time = 13.870026588439941
I0123 17:42:56.326055 140210684239872 ddar.py:60] Depth 3/1000 time = 19.659077882766724
I0123 17:43:19.042165 140210684239872 ddar.py:60] Depth 4/1000 time = 22.715881824493408
I0123 17:43:42.138627 140210684239872 ddar.py:60] Depth 5/1000 time = 23.095699548721313
I0123 17:44:07.054641 140210684239872 ddar.py:60] Depth 6/1000 time = 24.58424425125122
I0123 17:44:32.683266 140210684239872 ddar.py:60] Depth 7/1000 time = 25.34412932395935
I0123 17:44:32.683625 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:44:32.683746 140210684239872 alphageometry.py:566] LM output (score=-2.547256): "p : C h l p 21 D h p l p 22 ;"
I0123 17:44:32.683782 140210684239872 alphageometry.py:567] Translation: "p = on_line p h l, on_bline p l h"

I0123 17:44:32.683831 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p h l, on_bline p l h ? coll m o n"
I0123 17:44:32.684015 140210684239872 graph.py:498] 
I0123 17:44:32.684072 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p h l, on_bline p l h ? coll m o n
I0123 17:44:38.084540 140210684239872 ddar.py:60] Depth 1/1000 time = 5.356889486312866
I0123 17:44:50.554997 140210684239872 ddar.py:60] Depth 2/1000 time = 12.470231294631958
I0123 17:45:07.999531 140210684239872 ddar.py:60] Depth 3/1000 time = 17.444222688674927
I0123 17:45:30.772977 140210684239872 ddar.py:60] Depth 4/1000 time = 22.773215532302856
I0123 17:45:53.458645 140210684239872 ddar.py:60] Depth 5/1000 time = 22.685410261154175
I0123 17:46:16.434321 140210684239872 ddar.py:60] Depth 6/1000 time = 22.974790811538696
I0123 17:46:40.994114 140210684239872 ddar.py:60] Depth 7/1000 time = 24.252474546432495
I0123 17:47:06.139180 140210684239872 ddar.py:60] Depth 8/1000 time = 24.860193252563477
I0123 17:47:06.139551 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:47:06.139672 140210684239872 alphageometry.py:566] LM output (score=-2.697185): "p : P a b c p 21 P a p b c 22 ;"
I0123 17:47:06.139707 140210684239872 alphageometry.py:567] Translation: "p = on_pline p c a b, on_pline p a b c"

I0123 17:47:06.139757 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a b, on_pline p a b c ? coll m o n"
I0123 17:47:06.139939 140210684239872 graph.py:498] 
I0123 17:47:06.139997 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a b, on_pline p a b c ? coll m o n
I0123 17:47:11.822995 140210684239872 ddar.py:60] Depth 1/1000 time = 5.651650905609131
I0123 17:47:24.019636 140210684239872 ddar.py:60] Depth 2/1000 time = 12.196390151977539
I0123 17:47:41.598366 140210684239872 ddar.py:60] Depth 3/1000 time = 17.57835817337036
I0123 17:48:02.948595 140210684239872 ddar.py:60] Depth 4/1000 time = 21.349863529205322
I0123 17:48:23.993961 140210684239872 ddar.py:60] Depth 5/1000 time = 21.04454231262207
I0123 17:48:47.196718 140210684239872 ddar.py:60] Depth 6/1000 time = 22.87757921218872
I0123 17:49:10.618868 140210684239872 ddar.py:60] Depth 7/1000 time = 23.16255235671997
I0123 17:49:10.619220 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:49:10.619339 140210684239872 alphageometry.py:566] LM output (score=-2.764605): "p : P a b c p 21 T a b a p 22 ;"
I0123 17:49:10.619374 140210684239872 alphageometry.py:567] Translation: "p = on_pline p c a b, on_tline p a a b"

I0123 17:49:10.619421 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a b, on_tline p a a b ? coll m o n"
I0123 17:49:10.619605 140210684239872 graph.py:498] 
I0123 17:49:10.619662 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a b, on_tline p a a b ? coll m o n
I0123 17:49:15.982878 140210684239872 ddar.py:60] Depth 1/1000 time = 5.333166837692261
I0123 17:49:28.517251 140210684239872 ddar.py:60] Depth 2/1000 time = 12.534133672714233
I0123 17:49:46.780232 140210684239872 ddar.py:60] Depth 3/1000 time = 18.262669801712036
I0123 17:50:08.478122 140210684239872 ddar.py:60] Depth 4/1000 time = 21.697667360305786
I0123 17:50:30.232948 140210684239872 ddar.py:60] Depth 5/1000 time = 21.754101037979126
I0123 17:50:54.036276 140210684239872 ddar.py:60] Depth 6/1000 time = 23.49803614616394
I0123 17:51:18.128933 140210684239872 ddar.py:60] Depth 7/1000 time = 23.808462381362915
I0123 17:51:18.129297 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:51:18.129421 140210684239872 alphageometry.py:566] LM output (score=-2.772523): "p : P e f h p 21 ;"
I0123 17:51:18.129457 140210684239872 alphageometry.py:567] Translation: "p = on_pline p h e f"

I0123 17:51:18.129504 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p h e f ? coll m o n"
I0123 17:51:18.129691 140210684239872 graph.py:498] 
I0123 17:51:18.129751 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p h e f ? coll m o n
I0123 17:51:23.785221 140210684239872 ddar.py:60] Depth 1/1000 time = 5.624941349029541
I0123 17:51:35.967081 140210684239872 ddar.py:60] Depth 2/1000 time = 12.181687593460083
I0123 17:51:53.495996 140210684239872 ddar.py:60] Depth 3/1000 time = 17.52870202064514
I0123 17:52:14.255871 140210684239872 ddar.py:60] Depth 4/1000 time = 20.759628295898438
I0123 17:52:34.997485 140210684239872 ddar.py:60] Depth 5/1000 time = 20.74074125289917
I0123 17:52:57.661226 140210684239872 ddar.py:60] Depth 6/1000 time = 22.362433433532715
I0123 17:53:20.877150 140210684239872 ddar.py:60] Depth 7/1000 time = 22.923423290252686
I0123 17:53:20.877362 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:53:20.877455 140210684239872 alphageometry.py:566] LM output (score=-2.857673): "p : P a e k p 21 P a k e p 22 ;"
I0123 17:53:20.877494 140210684239872 alphageometry.py:567] Translation: "p = on_pline p k a e, on_pline p e a k"

I0123 17:53:20.877548 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p k a e, on_pline p e a k ? coll m o n"
I0123 17:53:20.877719 140210684239872 graph.py:498] 
I0123 17:53:20.877779 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p k a e, on_pline p e a k ? coll m o n
I0123 17:53:27.811895 140210684239872 ddar.py:60] Depth 1/1000 time = 6.903751373291016
I0123 17:53:42.253171 140210684239872 ddar.py:60] Depth 2/1000 time = 14.441084384918213
I0123 17:54:04.922100 140210684239872 ddar.py:60] Depth 3/1000 time = 22.668708324432373
I0123 17:54:31.117190 140210684239872 ddar.py:60] Depth 4/1000 time = 26.194804191589355
I0123 17:54:57.701040 140210684239872 ddar.py:60] Depth 5/1000 time = 26.582906007766724
I0123 17:55:26.322040 140210684239872 ddar.py:60] Depth 6/1000 time = 28.30387020111084
I0123 17:55:55.948572 140210684239872 ddar.py:60] Depth 7/1000 time = 29.297011137008667
I0123 17:55:55.948938 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:55:55.949061 140210684239872 alphageometry.py:566] LM output (score=-2.859631): "p : P a b c p 21 ;"
I0123 17:55:55.949098 140210684239872 alphageometry.py:567] Translation: "p = on_pline p c a b"

I0123 17:55:55.949147 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a b ? coll m o n"
I0123 17:55:55.949328 140210684239872 graph.py:498] 
I0123 17:55:55.949385 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a b ? coll m o n
I0123 17:56:01.456966 140210684239872 ddar.py:60] Depth 1/1000 time = 5.4812915325164795
I0123 17:56:13.421278 140210684239872 ddar.py:60] Depth 2/1000 time = 11.96413779258728
I0123 17:56:30.729923 140210684239872 ddar.py:60] Depth 3/1000 time = 17.30841374397278
I0123 17:56:51.365692 140210684239872 ddar.py:60] Depth 4/1000 time = 20.635525941848755
I0123 17:57:11.869886 140210684239872 ddar.py:60] Depth 5/1000 time = 20.503439903259277
I0123 17:57:34.918710 140210684239872 ddar.py:60] Depth 6/1000 time = 22.731319904327393
I0123 17:57:58.027580 140210684239872 ddar.py:60] Depth 7/1000 time = 22.84092688560486
I0123 17:57:58.027802 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:57:58.027891 140210684239872 alphageometry.py:566] LM output (score=-2.890653): "p : P b e k p 21 ;"
I0123 17:57:58.027927 140210684239872 alphageometry.py:567] Translation: "p = on_pline p k b e"

I0123 17:57:58.027961 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p k b e ? coll m o n"
I0123 17:57:58.028129 140210684239872 graph.py:498] 
I0123 17:57:58.028187 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p k b e ? coll m o n
I0123 17:58:03.254310 140210684239872 ddar.py:60] Depth 1/1000 time = 5.200253963470459
I0123 17:58:15.473778 140210684239872 ddar.py:60] Depth 2/1000 time = 12.219188213348389
I0123 17:58:32.965952 140210684239872 ddar.py:60] Depth 3/1000 time = 17.49195957183838
I0123 17:58:53.553920 140210684239872 ddar.py:60] Depth 4/1000 time = 20.58769464492798
I0123 17:59:14.490692 140210684239872 ddar.py:60] Depth 5/1000 time = 20.935899257659912
I0123 17:59:37.397862 140210684239872 ddar.py:60] Depth 6/1000 time = 22.606732845306396
I0123 18:00:00.723631 140210684239872 ddar.py:60] Depth 7/1000 time = 23.04567837715149
I0123 18:00:00.723996 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:00:00.724117 140210684239872 alphageometry.py:566] LM output (score=-2.925906): "p : P a e k p 21 P a p e k 22 ;"
I0123 18:00:00.724153 140210684239872 alphageometry.py:567] Translation: "p = on_pline p k a e, on_pline p a e k"

I0123 18:00:00.724200 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p k a e, on_pline p a e k ? coll m o n"
I0123 18:00:00.724387 140210684239872 graph.py:498] 
I0123 18:00:00.724444 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p k a e, on_pline p a e k ? coll m o n
I0123 18:00:06.537741 140210684239872 ddar.py:60] Depth 1/1000 time = 5.782238245010376
I0123 18:00:20.026379 140210684239872 ddar.py:60] Depth 2/1000 time = 13.488428831100464
I0123 18:00:39.052676 140210684239872 ddar.py:60] Depth 3/1000 time = 19.026073694229126
I0123 18:01:01.168515 140210684239872 ddar.py:60] Depth 4/1000 time = 22.11555528640747
I0123 18:01:23.705383 140210684239872 ddar.py:60] Depth 5/1000 time = 22.535821676254272
I0123 18:01:48.220395 140210684239872 ddar.py:60] Depth 6/1000 time = 24.19647192955017
I0123 18:02:13.562412 140210684239872 ddar.py:60] Depth 7/1000 time = 25.036425590515137
I0123 18:02:13.562657 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:02:13.562749 140210684239872 alphageometry.py:566] LM output (score=-2.930677): "p : P e g e p 21 ;"
I0123 18:02:13.562784 140210684239872 alphageometry.py:567] Translation: "ERROR: Invalid predicate P e g e p"

I0123 18:02:13.562820 140210684239872 alphageometry.py:566] LM output (score=-2.932788): "p : P b e c p 21 ;"
I0123 18:02:13.562846 140210684239872 alphageometry.py:567] Translation: "p = on_pline p c b e"

I0123 18:02:13.562874 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c b e ? coll m o n"
I0123 18:02:13.563033 140210684239872 graph.py:498] 
I0123 18:02:13.563090 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c b e ? coll m o n
I0123 18:02:18.978892 140210684239872 ddar.py:60] Depth 1/1000 time = 5.39105749130249
I0123 18:02:30.977368 140210684239872 ddar.py:60] Depth 2/1000 time = 11.998140811920166
I0123 18:02:48.116757 140210684239872 ddar.py:60] Depth 3/1000 time = 17.139126300811768
I0123 18:03:08.828012 140210684239872 ddar.py:60] Depth 4/1000 time = 20.710857152938843
I0123 18:03:29.516288 140210684239872 ddar.py:60] Depth 5/1000 time = 20.687371969223022
I0123 18:03:52.608254 140210684239872 ddar.py:60] Depth 6/1000 time = 22.796694040298462
I0123 18:04:15.742707 140210684239872 ddar.py:60] Depth 7/1000 time = 22.85377550125122
I0123 18:04:15.742928 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:04:15.743016 140210684239872 alphageometry.py:566] LM output (score=-2.987006): "p : P a e c p 21 P a p c e 22 ;"
I0123 18:04:15.743051 140210684239872 alphageometry.py:567] Translation: "p = on_pline p c a e, on_pline p a c e"

I0123 18:04:15.743086 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a e, on_pline p a c e ? coll m o n"
I0123 18:04:15.743253 140210684239872 graph.py:498] 
I0123 18:04:15.743311 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a e, on_pline p a c e ? coll m o n
I0123 18:04:21.370212 140210684239872 ddar.py:60] Depth 1/1000 time = 5.596051454544067
I0123 18:04:33.889651 140210684239872 ddar.py:60] Depth 2/1000 time = 12.51918363571167
I0123 18:04:52.155153 140210684239872 ddar.py:60] Depth 3/1000 time = 18.265101194381714
I0123 18:05:14.049446 140210684239872 ddar.py:60] Depth 4/1000 time = 21.893908500671387
I0123 18:05:35.562133 140210684239872 ddar.py:60] Depth 5/1000 time = 21.511804580688477
I0123 18:05:59.130552 140210684239872 ddar.py:60] Depth 6/1000 time = 23.26354742050171
I0123 18:06:23.256000 140210684239872 ddar.py:60] Depth 7/1000 time = 23.84378957748413
I0123 18:06:23.256367 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:06:23.256489 140210684239872 alphageometry.py:566] LM output (score=-3.039947): "p : P a e k p 21 ;"
I0123 18:06:23.256524 140210684239872 alphageometry.py:567] Translation: "p = on_pline p k a e"

I0123 18:06:23.256573 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p k a e ? coll m o n"
I0123 18:06:23.256763 140210684239872 graph.py:498] 
I0123 18:06:23.256820 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p k a e ? coll m o n
I0123 18:06:28.910932 140210684239872 ddar.py:60] Depth 1/1000 time = 5.615111351013184
I0123 18:06:40.657258 140210684239872 ddar.py:60] Depth 2/1000 time = 11.746065855026245
I0123 18:06:58.176921 140210684239872 ddar.py:60] Depth 3/1000 time = 17.519291639328003
I0123 18:07:18.685025 140210684239872 ddar.py:60] Depth 4/1000 time = 20.50779438018799
I0123 18:07:39.569502 140210684239872 ddar.py:60] Depth 5/1000 time = 20.88378667831421
I0123 18:08:01.772464 140210684239872 ddar.py:60] Depth 6/1000 time = 21.902012825012207
I0123 18:08:25.099947 140210684239872 ddar.py:60] Depth 7/1000 time = 23.04594349861145
I0123 18:08:25.100297 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:08:25.100412 140210684239872 alphageometry.py:566] LM output (score=-3.063112): "p : D b p c d 21 P b p c d 22 ;"
I0123 18:08:25.100447 140210684239872 alphageometry.py:567] Translation: "p = eqdistance p b c d, on_pline p b c d"

I0123 18:08:25.100493 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = eqdistance p b c d, on_pline p b c d ? coll m o n"
I0123 18:08:25.100684 140210684239872 graph.py:498] 
I0123 18:08:25.100740 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = eqdistance p b c d, on_pline p b c d ? coll m o n
I0123 18:08:30.524058 140210684239872 ddar.py:60] Depth 1/1000 time = 5.3914735317230225
I0123 18:08:43.225336 140210684239872 ddar.py:60] Depth 2/1000 time = 12.701092720031738
I0123 18:09:01.771098 140210684239872 ddar.py:60] Depth 3/1000 time = 18.545541524887085
I0123 18:09:23.173867 140210684239872 ddar.py:60] Depth 4/1000 time = 21.40253973007202
I0123 18:09:44.936774 140210684239872 ddar.py:60] Depth 5/1000 time = 21.762213945388794
I0123 18:10:08.269595 140210684239872 ddar.py:60] Depth 6/1000 time = 23.024173498153687
I0123 18:10:32.249518 140210684239872 ddar.py:60] Depth 7/1000 time = 23.69653296470642
I0123 18:10:32.249838 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:10:32.249945 140210684239872 alphageometry.py:566] LM output (score=-3.071681): "p : P e i h p 21 ;"
I0123 18:10:32.249981 140210684239872 alphageometry.py:567] Translation: "p = on_pline p h e i"

I0123 18:10:32.250024 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p h e i ? coll m o n"
I0123 18:10:32.250200 140210684239872 graph.py:498] 
I0123 18:10:32.250256 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p h e i ? coll m o n
I0123 18:10:38.264171 140210684239872 ddar.py:60] Depth 1/1000 time = 5.988194227218628
I0123 18:10:50.151879 140210684239872 ddar.py:60] Depth 2/1000 time = 11.887534379959106
I0123 18:11:07.714898 140210684239872 ddar.py:60] Depth 3/1000 time = 17.562813997268677
I0123 18:11:28.885695 140210684239872 ddar.py:60] Depth 4/1000 time = 21.170551776885986
I0123 18:11:49.765575 140210684239872 ddar.py:60] Depth 5/1000 time = 20.879048347473145
I0123 18:12:12.793326 140210684239872 ddar.py:60] Depth 6/1000 time = 22.724406957626343
I0123 18:12:35.841610 140210684239872 ddar.py:60] Depth 7/1000 time = 22.757630109786987
I0123 18:12:35.841869 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:12:35.841972 140210684239872 alphageometry.py:566] LM output (score=-3.129571): "p : P a e l p 21 P a p e l 22 ;"
I0123 18:12:35.842010 140210684239872 alphageometry.py:567] Translation: "p = on_pline p l a e, on_pline p a e l"

I0123 18:12:35.842050 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p l a e, on_pline p a e l ? coll m o n"
I0123 18:12:35.842232 140210684239872 graph.py:498] 
I0123 18:12:35.842295 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p l a e, on_pline p a e l ? coll m o n
I0123 18:12:41.957187 140210684239872 ddar.py:60] Depth 1/1000 time = 6.082487106323242
I0123 18:13:03.345494 140210684239872 ddar.py:60] Depth 2/1000 time = 21.38797903060913
I0123 18:13:35.273337 140210684239872 ddar.py:60] Depth 3/1000 time = 31.927493572235107
I0123 18:14:12.288623 140210684239872 ddar.py:60] Depth 4/1000 time = 37.01498222351074
I0123 18:14:49.439340 140210684239872 ddar.py:60] Depth 5/1000 time = 37.14970016479492
I0123 18:15:29.065087 140210684239872 ddar.py:60] Depth 6/1000 time = 39.250489950180054
I0123 18:16:10.300379 140210684239872 ddar.py:60] Depth 7/1000 time = 40.74450659751892
I0123 18:16:10.300763 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:16:10.300888 140210684239872 alphageometry.py:566] LM output (score=-3.140636): "p : P a d b p 21 ;"
I0123 18:16:10.300924 140210684239872 alphageometry.py:567] Translation: "p = on_pline p b a d"

I0123 18:16:10.300973 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p b a d ? coll m o n"
I0123 18:16:10.301168 140210684239872 graph.py:498] 
I0123 18:16:10.301226 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p b a d ? coll m o n
I0123 18:16:15.853030 140210684239872 ddar.py:60] Depth 1/1000 time = 5.526046514511108
I0123 18:16:27.553790 140210684239872 ddar.py:60] Depth 2/1000 time = 11.700484275817871
I0123 18:16:44.973899 140210684239872 ddar.py:60] Depth 3/1000 time = 17.419848442077637
I0123 18:17:05.440489 140210684239872 ddar.py:60] Depth 4/1000 time = 20.466185808181763
I0123 18:17:26.543268 140210684239872 ddar.py:60] Depth 5/1000 time = 21.101918935775757
I0123 18:17:48.900416 140210684239872 ddar.py:60] Depth 6/1000 time = 22.05107092857361
I0123 18:18:11.890847 140210684239872 ddar.py:60] Depth 7/1000 time = 22.71176528930664
I0123 18:18:11.891199 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:18:11.891327 140210684239872 alphageometry.py:566] LM output (score=-3.214561): "p : D a b c p 21 ;"
I0123 18:18:11.891366 140210684239872 alphageometry.py:567] Translation: "p = eqdistance p c a b"

I0123 18:18:11.891417 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = eqdistance p c a b ? coll m o n"
I0123 18:18:11.891616 140210684239872 graph.py:498] 
I0123 18:18:11.891676 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = eqdistance p c a b ? coll m o n
I0123 18:18:17.438120 140210684239872 ddar.py:60] Depth 1/1000 time = 5.512326717376709
I0123 18:18:29.322753 140210684239872 ddar.py:60] Depth 2/1000 time = 11.884373664855957
I0123 18:18:46.403958 140210684239872 ddar.py:60] Depth 3/1000 time = 17.080825567245483
I0123 18:19:07.045701 140210684239872 ddar.py:60] Depth 4/1000 time = 20.641361236572266
I0123 18:19:27.825491 140210684239872 ddar.py:60] Depth 5/1000 time = 20.77900195121765
I0123 18:19:50.451314 140210684239872 ddar.py:60] Depth 6/1000 time = 22.32796812057495
I0123 18:20:13.595539 140210684239872 ddar.py:60] Depth 7/1000 time = 22.86957812309265
I0123 18:20:13.595916 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:20:13.596051 140210684239872 alphageometry.py:566] LM output (score=-3.293625): "p : D b p c g 21 P b p c g 22 ;"
I0123 18:20:13.596096 140210684239872 alphageometry.py:567] Translation: "p = eqdistance p b c g, on_pline p b c g"

I0123 18:20:13.596152 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = eqdistance p b c g, on_pline p b c g ? coll m o n"
I0123 18:20:13.596350 140210684239872 graph.py:498] 
I0123 18:20:13.596411 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = eqdistance p b c g, on_pline p b c g ? coll m o n
I0123 18:20:19.765683 140210684239872 ddar.py:60] Depth 1/1000 time = 6.140543699264526
I0123 18:20:35.376468 140210684239872 ddar.py:60] Depth 2/1000 time = 15.61052680015564
I0123 18:20:57.203883 140210684239872 ddar.py:60] Depth 3/1000 time = 21.827054500579834
I0123 18:21:21.988192 140210684239872 ddar.py:60] Depth 4/1000 time = 24.783998012542725
I0123 18:21:47.485157 140210684239872 ddar.py:60] Depth 5/1000 time = 25.496022701263428
I0123 18:22:14.696317 140210684239872 ddar.py:60] Depth 6/1000 time = 26.889028549194336
I0123 18:22:42.995970 140210684239872 ddar.py:60] Depth 7/1000 time = 28.01728320121765
I0123 18:22:42.996324 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:22:42.996444 140210684239872 alphageometry.py:566] LM output (score=-3.296333): "p : P f g h p 21 ;"
I0123 18:22:42.996483 140210684239872 alphageometry.py:567] Translation: "p = on_pline p h f g"

I0123 18:22:42.996532 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p h f g ? coll m o n"
I0123 18:22:42.996718 140210684239872 graph.py:498] 
I0123 18:22:42.996779 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p h f g ? coll m o n
I0123 18:22:48.970525 140210684239872 ddar.py:60] Depth 1/1000 time = 5.948480606079102
I0123 18:23:00.861816 140210684239872 ddar.py:60] Depth 2/1000 time = 11.891097068786621
I0123 18:23:18.610024 140210684239872 ddar.py:60] Depth 3/1000 time = 17.747983932495117
I0123 18:23:39.793152 140210684239872 ddar.py:60] Depth 4/1000 time = 21.182877779006958
I0123 18:24:00.367449 140210684239872 ddar.py:60] Depth 5/1000 time = 20.573482990264893
I0123 18:24:23.560707 140210684239872 ddar.py:60] Depth 6/1000 time = 22.871673345565796
I0123 18:24:47.365128 140210684239872 ddar.py:60] Depth 7/1000 time = 23.520039796829224
I0123 18:24:47.365359 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:24:47.365446 140210684239872 alphageometry.py:566] LM output (score=-3.302458): "p : P a e c p 21 ;"
I0123 18:24:47.365483 140210684239872 alphageometry.py:567] Translation: "p = on_pline p c a e"

I0123 18:24:47.365521 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a e ? coll m o n"
I0123 18:24:47.365690 140210684239872 graph.py:498] 
I0123 18:24:47.365754 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p c a e ? coll m o n
I0123 18:24:52.698028 140210684239872 ddar.py:60] Depth 1/1000 time = 5.306762933731079
I0123 18:25:04.877898 140210684239872 ddar.py:60] Depth 2/1000 time = 12.179654359817505
I0123 18:25:22.374063 140210684239872 ddar.py:60] Depth 3/1000 time = 17.4959077835083
I0123 18:25:42.716714 140210684239872 ddar.py:60] Depth 4/1000 time = 20.342238426208496
I0123 18:26:03.552905 140210684239872 ddar.py:60] Depth 5/1000 time = 20.835275411605835
I0123 18:26:25.812615 140210684239872 ddar.py:60] Depth 6/1000 time = 21.94542121887207
I0123 18:26:49.216722 140210684239872 ddar.py:60] Depth 7/1000 time = 23.136555433273315
I0123 18:26:49.217108 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:26:49.217248 140210684239872 alphageometry.py:566] LM output (score=-3.305499): "p : P a e d p 21 ;"
I0123 18:26:49.217292 140210684239872 alphageometry.py:567] Translation: "p = on_pline p d a e"

I0123 18:26:49.217346 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p d a e ? coll m o n"
I0123 18:26:49.217540 140210684239872 graph.py:498] 
I0123 18:26:49.217602 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p d a e ? coll m o n
I0123 18:26:54.627526 140210684239872 ddar.py:60] Depth 1/1000 time = 5.382925748825073
I0123 18:27:06.963343 140210684239872 ddar.py:60] Depth 2/1000 time = 12.335594654083252
I0123 18:27:24.609330 140210684239872 ddar.py:60] Depth 3/1000 time = 17.645678520202637
I0123 18:27:45.073288 140210684239872 ddar.py:60] Depth 4/1000 time = 20.463541507720947
I0123 18:28:06.163139 140210684239872 ddar.py:60] Depth 5/1000 time = 21.088916540145874
I0123 18:28:28.677530 140210684239872 ddar.py:60] Depth 6/1000 time = 22.197277069091797
I0123 18:28:52.235231 140210684239872 ddar.py:60] Depth 7/1000 time = 23.289603233337402
I0123 18:28:52.235644 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:28:52.235790 140210684239872 alphageometry.py:566] LM output (score=-3.306075): "p : P c g e p 21 ;"
I0123 18:28:52.235830 140210684239872 alphageometry.py:567] Translation: "p = on_pline p e c g"

I0123 18:28:52.235882 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p e c g ? coll m o n"
I0123 18:28:52.236100 140210684239872 graph.py:498] 
I0123 18:28:52.236162 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p e c g ? coll m o n
I0123 18:28:57.964363 140210684239872 ddar.py:60] Depth 1/1000 time = 5.69959831237793
I0123 18:29:10.664205 140210684239872 ddar.py:60] Depth 2/1000 time = 12.699615955352783
I0123 18:29:28.706092 140210684239872 ddar.py:60] Depth 3/1000 time = 18.041598558425903
I0123 18:29:49.801095 140210684239872 ddar.py:60] Depth 4/1000 time = 21.094790935516357
I0123 18:30:11.010535 140210684239872 ddar.py:60] Depth 5/1000 time = 21.208712816238403
I0123 18:30:33.932622 140210684239872 ddar.py:60] Depth 6/1000 time = 22.59285044670105
I0123 18:30:57.985482 140210684239872 ddar.py:60] Depth 7/1000 time = 23.784417152404785
I0123 18:30:57.985834 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:30:57.985959 140210684239872 alphageometry.py:566] LM output (score=-3.306480): "p : P b e d p 21 ;"
I0123 18:30:57.986000 140210684239872 alphageometry.py:567] Translation: "p = on_pline p d b e"

I0123 18:30:57.986050 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p d b e ? coll m o n"
I0123 18:30:57.986240 140210684239872 graph.py:498] 
I0123 18:30:57.986300 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p d b e ? coll m o n
I0123 18:31:03.520358 140210684239872 ddar.py:60] Depth 1/1000 time = 5.507835865020752
I0123 18:31:15.440109 140210684239872 ddar.py:60] Depth 2/1000 time = 11.91956877708435
I0123 18:31:33.277086 140210684239872 ddar.py:60] Depth 3/1000 time = 17.836777448654175
I0123 18:31:54.228137 140210684239872 ddar.py:60] Depth 4/1000 time = 20.950831174850464
I0123 18:32:15.076082 140210684239872 ddar.py:60] Depth 5/1000 time = 20.847211837768555
I0123 18:32:37.797519 140210684239872 ddar.py:60] Depth 6/1000 time = 22.424260139465332
I0123 18:33:01.101048 140210684239872 ddar.py:60] Depth 7/1000 time = 23.019911289215088
I0123 18:33:01.101271 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:33:01.101369 140210684239872 alphageometry.py:566] LM output (score=-3.330333): "p : T a c c p 21 ;"
I0123 18:33:01.101408 140210684239872 alphageometry.py:567] Translation: "p = on_tline p c a c"

I0123 18:33:01.101447 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_tline p c a c ? coll m o n"
I0123 18:33:01.101615 140210684239872 graph.py:498] 
I0123 18:33:01.101680 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_tline p c a c ? coll m o n
I0123 18:33:06.533783 140210684239872 ddar.py:60] Depth 1/1000 time = 5.40697979927063
I0123 18:33:18.709691 140210684239872 ddar.py:60] Depth 2/1000 time = 12.175625085830688
I0123 18:33:36.402989 140210684239872 ddar.py:60] Depth 3/1000 time = 17.693036317825317
I0123 18:33:57.813865 140210684239872 ddar.py:60] Depth 4/1000 time = 21.41046953201294
I0123 18:34:19.301362 140210684239872 ddar.py:60] Depth 5/1000 time = 21.486572980880737
I0123 18:34:42.692616 140210684239872 ddar.py:60] Depth 6/1000 time = 23.084275007247925
I0123 18:35:06.754812 140210684239872 ddar.py:60] Depth 7/1000 time = 23.78306245803833
I0123 18:35:06.755046 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:35:06.755145 140210684239872 alphageometry.py:566] LM output (score=-3.334128): "p : P e f e p 21 ;"
I0123 18:35:06.755183 140210684239872 alphageometry.py:567] Translation: "ERROR: Invalid predicate P e f e p"

I0123 18:35:06.755220 140210684239872 alphageometry.py:566] LM output (score=-3.336909): "p : P c d g p 21 P c p d g 22 ;"
I0123 18:35:06.755247 140210684239872 alphageometry.py:567] Translation: "p = on_pline p g c d, on_pline p c d g"

I0123 18:35:06.755277 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p g c d, on_pline p c d g ? coll m o n"
I0123 18:35:06.755445 140210684239872 graph.py:498] 
I0123 18:35:06.755503 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p g c d, on_pline p c d g ? coll m o n
I0123 18:35:12.844553 140210684239872 ddar.py:60] Depth 1/1000 time = 6.057669162750244
I0123 18:35:25.934129 140210684239872 ddar.py:60] Depth 2/1000 time = 13.089243173599243
I0123 18:35:44.519247 140210684239872 ddar.py:60] Depth 3/1000 time = 18.584905862808228
I0123 18:36:06.796825 140210684239872 ddar.py:60] Depth 4/1000 time = 22.277302026748657
I0123 18:36:29.052260 140210684239872 ddar.py:60] Depth 5/1000 time = 22.254539728164673
I0123 18:36:53.326832 140210684239872 ddar.py:60] Depth 6/1000 time = 23.95082688331604
I0123 18:37:18.183426 140210684239872 ddar.py:60] Depth 7/1000 time = 24.555736780166626
I0123 18:37:18.183778 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:37:18.183897 140210684239872 alphageometry.py:566] LM output (score=-3.340648): "p : P d g e p 21 ;"
I0123 18:37:18.183936 140210684239872 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2570, in add_clause
    raise DepCheckFailError(
graph.DepCheckFailError: ncoll e d g
"

I0123 18:37:18.183984 140210684239872 alphageometry.py:566] LM output (score=-3.353781): "p : P e f k p 21 ;"
I0123 18:37:18.184013 140210684239872 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2570, in add_clause
    raise DepCheckFailError(
graph.DepCheckFailError: ncoll k e f
"

I0123 18:37:18.184043 140210684239872 alphageometry.py:566] LM output (score=-3.362827): "p : P b e l p 21 ;"
I0123 18:37:18.184069 140210684239872 alphageometry.py:567] Translation: "p = on_pline p l b e"

I0123 18:37:18.184115 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p l b e ? coll m o n"
I0123 18:37:18.184305 140210684239872 graph.py:498] 
I0123 18:37:18.184365 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_pline p l b e ? coll m o n
I0123 18:37:23.898398 140210684239872 ddar.py:60] Depth 1/1000 time = 5.6886842250823975
I0123 18:37:35.906221 140210684239872 ddar.py:60] Depth 2/1000 time = 12.007651567459106
I0123 18:37:53.157129 140210684239872 ddar.py:60] Depth 3/1000 time = 17.250694513320923
I0123 18:38:13.458444 140210684239872 ddar.py:60] Depth 4/1000 time = 20.30109167098999
I0123 18:38:34.383617 140210684239872 ddar.py:60] Depth 5/1000 time = 20.924432277679443
I0123 18:38:57.239752 140210684239872 ddar.py:60] Depth 6/1000 time = 22.558613777160645
I0123 18:39:19.973523 140210684239872 ddar.py:60] Depth 7/1000 time = 22.453592538833618
I0123 18:39:19.973770 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:39:19.973887 140210684239872 alphageometry.py:540] Depth 1. There are 28 nodes to expand:
I0123 18:39:19.973931 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : C l m p 21 D l p m p 22 ; x00
I0123 18:39:19.973966 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : C l n p 21 D l p n p 22 ; x00
I0123 18:39:19.973998 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a e l p 21 P a l e p 22 ; x00
I0123 18:39:19.974029 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : C h l p 21 D h p l p 22 ; x00
I0123 18:39:19.974060 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a b c p 21 P a p b c 22 ; x00
I0123 18:39:19.974088 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a b c p 21 T a b a p 22 ; x00
I0123 18:39:19.974129 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P e f h p 21 ; x00
I0123 18:39:19.974160 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a e k p 21 P a k e p 22 ; x00
I0123 18:39:19.974189 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a b c p 21 ; x00
I0123 18:39:19.974220 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P b e k p 21 ; x00
I0123 18:39:19.974249 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a e k p 21 P a p e k 22 ; x00
I0123 18:39:19.974280 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P b e c p 21 ; x00
I0123 18:39:19.974310 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a e c p 21 P a p c e 22 ; x00
I0123 18:39:19.974528 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a e k p 21 ; x00
I0123 18:39:19.974556 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : D b p c d 21 P b p c d 22 ; x00
I0123 18:39:19.974590 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P e i h p 21 ; x00
I0123 18:39:19.974620 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a e l p 21 P a p e l 22 ; x00
I0123 18:39:19.974651 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a d b p 21 ; x00
I0123 18:39:19.974682 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : D a b c p 21 ; x00
I0123 18:39:19.974713 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : D b p c g 21 P b p c g 22 ; x00
I0123 18:39:19.974741 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P f g h p 21 ; x00
I0123 18:39:19.974770 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a e c p 21 ; x00
I0123 18:39:19.974798 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P a e d p 21 ; x00
I0123 18:39:19.974826 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P c g e p 21 ; x00
I0123 18:39:19.974858 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P b e d p 21 ; x00
I0123 18:39:19.974886 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : T a c c p 21 ; x00
I0123 18:39:19.974912 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P c d g p 21 P c p d g 22 ; x00
I0123 18:39:19.974936 140210684239872 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : P b e l p 21 ; x00
I0123 18:39:19.974965 140210684239872 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : C a c d 00 ; e : C a b e 01 ; f : C d e f 02 ; g : C d e g 03 T c g d e 04 ; h : C c g h 05 D c g g h 06 ; i : C d e i 07 T b i d e 08 ; j : C b i j 09 D b i i j 10 ; k : C d e k 11 T a k d e 12 ; l : C a k l 13 D a k k l 14 ; m : C a b m 15 C f h m 16 ; n : C a c n 17 C f j n 18 ; o : C b c o 19 C f l o 20 ? C m o n {F1} x00 p : C l m p 21 D l p m p 22 ; x00
I0123 18:39:28.714351 140210684239872 alphageometry.py:566] LM output (score=-0.806930): "q : C k m q 23 D k q m q 24 ;"
I0123 18:39:28.714609 140210684239872 alphageometry.py:567] Translation: "q = on_line q k m, on_bline q m k"

I0123 18:39:28.714663 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q k m, on_bline q m k ? coll m o n"
I0123 18:39:28.714851 140210684239872 graph.py:498] 
I0123 18:39:28.714912 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q k m, on_bline q m k ? coll m o n
I0123 18:39:34.912572 140210684239872 ddar.py:60] Depth 1/1000 time = 6.15739631652832
I0123 18:39:50.452680 140210684239872 ddar.py:60] Depth 2/1000 time = 15.539915323257446
I0123 18:40:11.279464 140210684239872 ddar.py:60] Depth 3/1000 time = 20.826570510864258
I0123 18:40:37.071251 140210684239872 ddar.py:60] Depth 4/1000 time = 25.791462898254395
I0123 18:41:02.100493 140210684239872 ddar.py:60] Depth 5/1000 time = 25.02798318862915
I0123 18:41:29.490458 140210684239872 ddar.py:60] Depth 6/1000 time = 27.035351991653442
I0123 18:41:58.529668 140210684239872 ddar.py:60] Depth 7/1000 time = 28.740373611450195
I0123 18:41:58.529930 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:41:58.530021 140210684239872 alphageometry.py:566] LM output (score=-1.118968): "q : C h m q 23 D h q m q 24 ;"
I0123 18:41:58.530059 140210684239872 alphageometry.py:567] Translation: "q = on_line q h m, on_bline q m h"

I0123 18:41:58.530096 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q h m, on_bline q m h ? coll m o n"
I0123 18:41:58.530291 140210684239872 graph.py:498] 
I0123 18:41:58.530354 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q h m, on_bline q m h ? coll m o n
I0123 18:42:04.943354 140210684239872 ddar.py:60] Depth 1/1000 time = 6.3681535720825195
I0123 18:42:18.967504 140210684239872 ddar.py:60] Depth 2/1000 time = 14.023930549621582
I0123 18:42:39.478798 140210684239872 ddar.py:60] Depth 3/1000 time = 20.511033058166504
I0123 18:43:04.091391 140210684239872 ddar.py:60] Depth 4/1000 time = 24.61225128173828
I0123 18:43:28.030561 140210684239872 ddar.py:60] Depth 5/1000 time = 23.938740491867065
I0123 18:43:52.628790 140210684239872 ddar.py:60] Depth 6/1000 time = 24.597010374069214
I0123 18:44:18.844517 140210684239872 ddar.py:60] Depth 7/1000 time = 25.874525785446167
I0123 18:44:46.269244 140210684239872 ddar.py:60] Depth 8/1000 time = 27.12789797782898
I0123 18:44:46.269658 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:44:46.269750 140210684239872 alphageometry.py:566] LM output (score=-1.128288): "q : C b l q 23 D b q l q 24 ;"
I0123 18:44:46.269785 140210684239872 alphageometry.py:567] Translation: "q = on_line q b l, on_bline q l b"

I0123 18:44:46.269837 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q b l, on_bline q l b ? coll m o n"
I0123 18:44:46.270053 140210684239872 graph.py:498] 
I0123 18:44:46.270111 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q b l, on_bline q l b ? coll m o n
I0123 18:44:52.612665 140210684239872 ddar.py:60] Depth 1/1000 time = 6.305222034454346
I0123 18:45:07.030400 140210684239872 ddar.py:60] Depth 2/1000 time = 14.41752815246582
I0123 18:45:27.964672 140210684239872 ddar.py:60] Depth 3/1000 time = 20.93402934074402
I0123 18:45:52.383352 140210684239872 ddar.py:60] Depth 4/1000 time = 24.41837239265442
I0123 18:46:17.461068 140210684239872 ddar.py:60] Depth 5/1000 time = 25.076395750045776
I0123 18:46:43.924632 140210684239872 ddar.py:60] Depth 6/1000 time = 26.141260147094727
I0123 18:47:11.840881 140210684239872 ddar.py:60] Depth 7/1000 time = 27.59590172767639
I0123 18:47:11.841285 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:47:11.841380 140210684239872 alphageometry.py:566] LM output (score=-1.300415): "q : C l p q 23 D l q p q 24 ;"
I0123 18:47:11.841414 140210684239872 alphageometry.py:567] Translation: "q = on_line q l p, on_bline q p l"

I0123 18:47:11.841480 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q l p, on_bline q p l ? coll m o n"
I0123 18:47:11.841701 140210684239872 graph.py:498] 
I0123 18:47:11.841762 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q l p, on_bline q p l ? coll m o n
I0123 18:47:18.347663 140210684239872 ddar.py:60] Depth 1/1000 time = 6.465698480606079
I0123 18:47:32.397119 140210684239872 ddar.py:60] Depth 2/1000 time = 14.049252033233643
I0123 18:47:52.810097 140210684239872 ddar.py:60] Depth 3/1000 time = 20.41273546218872
I0123 18:48:16.391534 140210684239872 ddar.py:60] Depth 4/1000 time = 23.58118176460266
I0123 18:48:40.069444 140210684239872 ddar.py:60] Depth 5/1000 time = 23.675928115844727
I0123 18:49:05.832946 140210684239872 ddar.py:60] Depth 6/1000 time = 25.420770406723022
I0123 18:49:32.271217 140210684239872 ddar.py:60] Depth 7/1000 time = 26.146106481552124
I0123 18:49:32.271615 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:49:32.271714 140210684239872 alphageometry.py:566] LM output (score=-1.389284): "q : C f m q 23 D f q m q 24 ;"
I0123 18:49:32.271751 140210684239872 alphageometry.py:567] Translation: "q = on_line q f m, on_bline q m f"

I0123 18:49:32.271806 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q f m, on_bline q m f ? coll m o n"
I0123 18:49:32.272026 140210684239872 graph.py:498] 
I0123 18:49:32.272087 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q f m, on_bline q m f ? coll m o n
I0123 18:49:38.938016 140210684239872 ddar.py:60] Depth 1/1000 time = 6.628248929977417
I0123 18:49:52.957285 140210684239872 ddar.py:60] Depth 2/1000 time = 14.019005298614502
I0123 18:50:13.484863 140210684239872 ddar.py:60] Depth 3/1000 time = 20.52716612815857
I0123 18:50:37.051105 140210684239872 ddar.py:60] Depth 4/1000 time = 23.565826892852783
I0123 18:51:00.644200 140210684239872 ddar.py:60] Depth 5/1000 time = 23.5919029712677
I0123 18:51:26.416669 140210684239872 ddar.py:60] Depth 6/1000 time = 25.431732892990112
I0123 18:51:52.657394 140210684239872 ddar.py:60] Depth 7/1000 time = 25.954090118408203
I0123 18:51:52.657623 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:51:52.657683 140210684239872 alphageometry.py:566] LM output (score=-1.403187): "q : C j m q 23 D j q m q 24 ;"
I0123 18:51:52.657718 140210684239872 alphageometry.py:567] Translation: "q = on_line q j m, on_bline q m j"

I0123 18:51:52.657761 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q j m, on_bline q m j ? coll m o n"
I0123 18:51:52.657955 140210684239872 graph.py:498] 
I0123 18:51:52.658016 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q j m, on_bline q m j ? coll m o n
I0123 18:51:58.372683 140210684239872 ddar.py:60] Depth 1/1000 time = 5.673779249191284
I0123 18:52:12.779180 140210684239872 ddar.py:60] Depth 2/1000 time = 14.406301498413086
I0123 18:52:32.093135 140210684239872 ddar.py:60] Depth 3/1000 time = 19.31363868713379
I0123 18:52:56.338232 140210684239872 ddar.py:60] Depth 4/1000 time = 24.244697093963623
I0123 18:53:20.644055 140210684239872 ddar.py:60] Depth 5/1000 time = 24.305501222610474
I0123 18:53:45.067174 140210684239872 ddar.py:60] Depth 6/1000 time = 24.4218590259552
I0123 18:54:10.545564 140210684239872 ddar.py:60] Depth 7/1000 time = 25.134239435195923
I0123 18:54:37.546715 140210684239872 ddar.py:60] Depth 8/1000 time = 26.705639123916626
I0123 18:54:37.547133 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:54:37.547226 140210684239872 alphageometry.py:566] LM output (score=-1.440993): "q : C h l q 23 D h q l q 24 ;"
I0123 18:54:37.547262 140210684239872 alphageometry.py:567] Translation: "q = on_line q h l, on_bline q l h"

I0123 18:54:37.547316 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q h l, on_bline q l h ? coll m o n"
I0123 18:54:37.547526 140210684239872 graph.py:498] 
I0123 18:54:37.547587 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q h l, on_bline q l h ? coll m o n
I0123 18:54:43.375477 140210684239872 ddar.py:60] Depth 1/1000 time = 5.790261268615723
I0123 18:54:57.851927 140210684239872 ddar.py:60] Depth 2/1000 time = 14.476233005523682
I0123 18:55:17.115622 140210684239872 ddar.py:60] Depth 3/1000 time = 19.263387441635132
I0123 18:55:43.119803 140210684239872 ddar.py:60] Depth 4/1000 time = 26.003798961639404
I0123 18:56:08.318980 140210684239872 ddar.py:60] Depth 5/1000 time = 25.19886803627014
I0123 18:56:33.564514 140210684239872 ddar.py:60] Depth 6/1000 time = 25.244215965270996
I0123 18:57:01.083996 140210684239872 ddar.py:60] Depth 7/1000 time = 27.163745164871216
I0123 18:57:29.755908 140210684239872 ddar.py:60] Depth 8/1000 time = 28.370574235916138
I0123 18:57:29.756286 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:57:29.756377 140210684239872 alphageometry.py:566] LM output (score=-1.761452): "q : C e l q 23 D e q l q 24 ;"
I0123 18:57:29.756410 140210684239872 alphageometry.py:567] Translation: "q = on_line q e l, on_bline q l e"

I0123 18:57:29.756462 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q e l, on_bline q l e ? coll m o n"
I0123 18:57:29.756661 140210684239872 graph.py:498] 
I0123 18:57:29.756720 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q e l, on_bline q l e ? coll m o n
I0123 18:57:35.660450 140210684239872 ddar.py:60] Depth 1/1000 time = 5.866991996765137
I0123 18:58:03.797347 140210684239872 ddar.py:60] Depth 2/1000 time = 28.136651039123535
I0123 18:58:39.875749 140210684239872 ddar.py:60] Depth 3/1000 time = 36.07797574996948
I0123 18:59:20.556279 140210684239872 ddar.py:60] Depth 4/1000 time = 40.68011403083801
I0123 19:00:04.138551 140210684239872 ddar.py:60] Depth 5/1000 time = 43.58113217353821
I0123 19:00:47.394464 140210684239872 ddar.py:60] Depth 6/1000 time = 42.91238737106323
I0123 19:01:32.146981 140210684239872 ddar.py:60] Depth 7/1000 time = 44.46124887466431
I0123 19:01:32.147355 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:01:32.147445 140210684239872 alphageometry.py:566] LM output (score=-1.781557): "q : C i m q 23 D i q m q 24 ;"
I0123 19:01:32.147480 140210684239872 alphageometry.py:567] Translation: "q = on_line q i m, on_bline q m i"

I0123 19:01:32.147530 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q i m, on_bline q m i ? coll m o n"
I0123 19:01:32.147736 140210684239872 graph.py:498] 
I0123 19:01:32.147797 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q i m, on_bline q m i ? coll m o n
I0123 19:01:37.259203 140210684239872 ddar.py:60] Depth 1/1000 time = 5.0744524002075195
I0123 19:01:50.586762 140210684239872 ddar.py:60] Depth 2/1000 time = 13.327343940734863
I0123 19:02:10.320224 140210684239872 ddar.py:60] Depth 3/1000 time = 19.733142852783203
I0123 19:02:33.312313 140210684239872 ddar.py:60] Depth 4/1000 time = 22.99165415763855
I0123 19:02:56.344809 140210684239872 ddar.py:60] Depth 5/1000 time = 23.03127646446228
I0123 19:03:20.510292 140210684239872 ddar.py:60] Depth 6/1000 time = 23.85163903236389
I0123 19:03:47.247479 140210684239872 ddar.py:60] Depth 7/1000 time = 26.43993330001831
I0123 19:03:47.247761 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:03:47.247837 140210684239872 alphageometry.py:566] LM output (score=-1.795744): "q : C j l q 23 D j q l q 24 ;"
I0123 19:03:47.247875 140210684239872 alphageometry.py:567] Translation: "q = on_line q j l, on_bline q l j"

I0123 19:03:47.247916 140210684239872 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q j l, on_bline q l j ? coll m o n"
I0123 19:03:47.248107 140210684239872 graph.py:498] 
I0123 19:03:47.248172 140210684239872 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g c d e; h = mirror h c g; i = foot i b d e; j = mirror j b i; k = foot k a d e; l = mirror l a k; m = on_line m h f, on_line m b a; n = on_line n j f, on_line n c a; o = on_line o l f, on_line o c b; p = on_line p l m, on_bline p m l; q = on_line q j l, on_bline q l j ? coll m o n
I0123 19:03:53.225701 140210684239872 ddar.py:60] Depth 1/1000 time = 5.936020135879517
I0123 19:04:07.111247 140210684239872 ddar.py:60] Depth 2/1000 time = 13.885244607925415
I0123 19:04:26.900200 140210684239872 ddar.py:60] Depth 3/1000 time = 19.788732767105103
I0123 19:04:52.848509 140210684239872 ddar.py:60] Depth 4/1000 time = 25.947992086410522
I0123 19:05:18.817050 140210684239872 ddar.py:60] Depth 5/1000 time = 25.968092918395996
I0123 19:05:44.904147 140210684239872 ddar.py:60] Depth 6/1000 time = 26.085774898529053
I0123 19:06:13.010025 140210684239872 ddar.py:60] Depth 7/1000 time = 27.752378225326538
I0123 19:06:13.309234 140210684239872 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:06:13.309326 140210684239872 alphageometry.py:585] Timeout.
