I0123 15:32:19.952841 140537667182592 inference_utils.py:69] Parsing gin configuration.
I0123 15:32:19.954360 140537667182592 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 15:32:19.954596 140537667182592 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 15:32:19.954631 140537667182592 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 15:32:19.954660 140537667182592 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 15:32:19.954687 140537667182592 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 15:32:19.954713 140537667182592 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 15:32:19.954739 140537667182592 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 15:32:19.954765 140537667182592 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 15:32:19.954790 140537667182592 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 15:32:19.954815 140537667182592 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 15:32:19.954842 140537667182592 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 15:32:19.954890 140537667182592 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 15:32:19.955026 140537667182592 resource_reader.py:55] Path not found: base_htrans.gin
I0123 15:32:19.955234 140537667182592 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 15:32:19.955339 140537667182592 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 15:32:19.961528 140537667182592 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 15:32:19.961662 140537667182592 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 15:32:19.961982 140537667182592 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 15:32:19.962086 140537667182592 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 15:32:19.962357 140537667182592 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 15:32:19.962455 140537667182592 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 15:32:19.962855 140537667182592 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 15:32:19.962954 140537667182592 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 15:32:19.966627 140537667182592 training_loop.py:334] ==== Training loop: initializing model ====
I0123 15:32:20.069868 140537667182592 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 15:32:20.070572 140537667182592 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 15:32:20.077224 140537667182592 training_loop.py:335] Process 0 of 1
I0123 15:32:20.077277 140537667182592 training_loop.py:336] Local device count = 1
I0123 15:32:20.077315 140537667182592 training_loop.py:337] Number of replicas = 1
I0123 15:32:20.077345 140537667182592 training_loop.py:339] Using random number seed 42
I0123 15:32:20.555678 140537667182592 training_loop.py:359] Initializing the model.
I0123 15:32:20.954569 140537667182592 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:20.954827 140537667182592 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 15:32:20.954932 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955067 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955152 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955238 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955316 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955391 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955465 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955538 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955611 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955687 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955771 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955852 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:20.955893 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:20.955938 140537667182592 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:32:20.956053 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:20.956092 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:20.956123 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:20.958073 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:20.963438 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:20.973994 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:20.974271 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:20.978590 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:20.989034 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:20.989091 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:20.989128 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:20.989161 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:20.989223 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:20.990410 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:20.990489 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:20.991184 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:20.993627 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:20.999778 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.001025 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.001109 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.001145 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.001206 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.001334 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.001679 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.001729 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.003626 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.003726 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.006626 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.006712 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.007146 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.017354 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.026003 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.026103 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.026397 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.026479 140537667182592 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:32:21.026588 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.026627 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.026658 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.028576 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.030970 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.036539 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.036806 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.039410 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.043216 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.043272 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.043307 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.043338 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.043398 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.043964 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.044039 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.044396 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.045157 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.047610 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.048276 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.048353 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.048387 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.048444 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.048570 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.048897 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.048941 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.050833 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.050927 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.053434 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.053514 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.054517 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.056959 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.058848 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.058957 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.059257 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.059340 140537667182592 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:32:21.059452 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.059491 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.059522 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.061440 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.063791 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.069709 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.069977 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.072617 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.076449 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.076506 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.076541 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.076573 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.076635 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.077197 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.077272 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.077628 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.078391 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.080912 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.081530 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.081606 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.081646 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.081709 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.081838 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.082155 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.082197 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.084082 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.084175 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.086685 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.086771 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.087209 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.089455 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.091353 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.091448 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.091738 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.091818 140537667182592 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:32:21.091928 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.091966 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.091997 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.093898 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.096274 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.101825 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.102086 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.104732 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.108442 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.108497 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.108531 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.108562 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.108622 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.109166 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.109241 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.109595 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.110364 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.112880 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.113495 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.113572 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.113607 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.113672 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.113815 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.114138 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.114181 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.116081 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.116279 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.118903 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.118993 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.119424 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.121662 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.123603 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.123697 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.123989 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.124070 140537667182592 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:32:21.124180 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.124217 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.124247 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.126075 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.128419 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.133932 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.134194 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.136831 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.140513 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.140568 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.140604 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.140634 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.140696 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.141610 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.141695 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.142058 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.142819 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.145248 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.145867 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.145943 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.145978 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.146036 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.146164 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.146485 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.146526 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.148377 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.148470 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.150959 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.151039 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.151463 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.153738 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.155603 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.155698 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.155981 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.156061 140537667182592 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:32:21.156170 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.156208 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.156237 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.158045 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.160429 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.165882 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.166134 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.168670 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.172443 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.172502 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.172537 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.172567 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.172630 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.173176 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.173252 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.173602 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.174381 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.176816 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.177433 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.177508 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.177542 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.177601 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.177733 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.178050 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.178093 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.180006 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.180099 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.182555 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.182635 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.183058 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.185325 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.187231 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.187327 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.187613 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.187696 140537667182592 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:32:21.187806 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.187845 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.187875 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.189744 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.192072 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.197578 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.197843 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.200423 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.204141 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.204196 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.204231 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.204262 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.204323 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.204877 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.204952 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.205306 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.206074 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.208470 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.209130 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.209209 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.209244 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.209301 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.209425 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.209750 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.209794 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.211643 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.211735 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.214158 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.214241 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.215025 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.217326 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.219212 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.219312 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.219604 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.219690 140537667182592 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:32:21.219799 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.219838 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.219868 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.362345 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.365232 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.371079 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.371372 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.374129 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.378026 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.378084 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.378122 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.378153 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.378220 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.378845 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.378924 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.379285 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.380064 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.382658 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.383301 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.383379 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.383414 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.383474 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.383599 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.383930 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.383973 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.385886 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.385980 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.388510 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.388590 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.389027 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.391372 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.393380 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.393489 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.393791 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.393882 140537667182592 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:32:21.394013 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.394054 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.394084 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.395984 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.398444 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.404087 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.404344 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.407006 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.410825 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.410887 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.410922 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.410953 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.411014 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.411629 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.411717 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.412092 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.412866 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.415376 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.416022 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.416099 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.416133 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.416189 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.416312 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.416637 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.416681 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.418570 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.418666 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.421254 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.421333 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.421774 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.424128 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.426042 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.426138 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.426425 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.426512 140537667182592 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:32:21.426624 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.426663 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.426698 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.428578 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.431016 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.436930 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.437192 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.439853 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.443611 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.443669 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.443711 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.443747 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.443819 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.444379 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.444459 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.444811 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.445564 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.448063 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.448668 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.448745 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.448780 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.448836 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.448957 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.449276 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.449319 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.451289 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.451385 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.453855 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.453935 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.454358 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.456701 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.458572 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.458675 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.458995 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.459081 140537667182592 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:32:21.459192 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.459230 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.459260 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.461254 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.463585 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.469106 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.469368 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.471951 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.475681 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.475738 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.475773 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.475805 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.475866 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.476423 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.476498 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.476845 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.477617 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.480063 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.480727 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.480805 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.480839 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.480897 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.481025 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.481345 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.481388 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.483260 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.483352 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.486090 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.486169 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.486649 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.488866 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.490770 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.490865 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.491159 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.491240 140537667182592 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:32:21.491358 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.491397 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.491428 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.493305 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.495640 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.501153 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.501413 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.503989 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:21.507741 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.507797 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.507833 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.507863 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.507924 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.508486 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.508563 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.508919 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.509688 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.512495 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.513119 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.513197 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.513232 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.513291 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.513419 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.513752 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.513796 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.515662 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.515755 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.518265 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.518346 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.518778 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.520998 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.522871 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.522967 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.523248 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.523532 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.523602 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.523667 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.523724 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.523778 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.523832 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.523885 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.523936 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.523987 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.524038 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.524089 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.524140 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:21.524176 140537667182592 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:32:21.527652 140537667182592 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:21.575323 140537667182592 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.575409 140537667182592 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:32:21.575464 140537667182592 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:32:21.575571 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.575608 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.575639 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.575700 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.578088 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.583506 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.583760 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.586369 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.602795 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.602852 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.602887 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.602918 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.602981 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.604100 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.604180 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.604882 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.606854 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.611536 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.612828 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.612913 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.612949 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.613009 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.613140 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.613254 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.613293 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.615193 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.615289 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.617712 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.617800 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.617916 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.620128 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.622077 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.622174 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.622465 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.622546 140537667182592 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:32:21.622657 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.622696 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.622726 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.622790 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.625032 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.630475 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.630728 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.633380 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.646402 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.646458 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.646492 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.646522 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.646584 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.647137 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.647213 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.647574 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.648260 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.650745 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.651360 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.651437 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.651475 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.651533 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.651657 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.651765 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.651803 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.653740 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.653839 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.656228 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.656307 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.656416 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.658618 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.660528 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.660622 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.660912 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.660994 140537667182592 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:32:21.661104 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.661143 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.661174 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.661237 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.663474 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.668860 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.669119 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.671918 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.684489 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.684545 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.684580 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.684610 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.684671 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.685228 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.685305 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.685673 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.686359 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.688797 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.689412 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.689488 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.689522 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.689593 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.689725 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.689835 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.689872 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.691771 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.691866 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.694268 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.694349 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.694456 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.696655 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.698559 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.698656 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.698943 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.699025 140537667182592 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:32:21.699135 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.699173 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.699203 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.699265 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.701491 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.706904 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.707158 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.709806 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.727599 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.727680 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.727717 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.727749 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.727826 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.728430 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.728506 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.728869 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.729578 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.732112 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.732744 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.732821 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.732855 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.732918 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.733050 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.733164 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.733201 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.735217 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.735313 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.737762 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.737842 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.737951 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.740190 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.742065 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.742161 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.742452 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.742535 140537667182592 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:32:21.742649 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.742690 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.742721 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.742789 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.745390 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.750863 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.751126 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.754134 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.766843 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.766900 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.766935 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.766965 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.767027 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.767587 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.767663 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.768018 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.768716 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.771251 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.771887 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.771965 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.772000 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.772060 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.772192 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.772303 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.772342 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.774234 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.774329 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.776735 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.776814 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.776926 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.779201 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.781076 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.781172 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.781460 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.781541 140537667182592 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:32:21.781656 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.781696 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.781727 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.781792 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.784037 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.789484 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.789757 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.792434 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.805051 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.805108 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.805143 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.805173 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.805234 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.805799 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.805879 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.806231 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.806921 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.809363 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.810016 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.810098 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.810133 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.810192 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.810324 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.810444 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.810483 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.812424 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.812518 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.814919 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.815000 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.815108 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.817309 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.819144 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.819240 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.819524 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.819604 140537667182592 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:32:21.819716 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.819754 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.819786 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.819849 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.822077 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.827717 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.827971 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.830557 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.843137 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.843194 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.843229 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.843260 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.843326 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.843887 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.843963 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.844321 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.845012 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.847486 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.848474 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.848552 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.848591 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.848650 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.848781 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.848892 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.848936 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.850827 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.850920 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.853289 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.853368 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.853476 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.855731 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.857631 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.857734 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.858022 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.858105 140537667182592 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:32:21.858214 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.858253 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.858285 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.858350 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.860581 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.866046 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.866313 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.868941 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.881546 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.881603 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.881638 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.881680 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.881742 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.882341 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.882417 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.882769 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.883457 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.885925 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.886551 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.886628 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.886662 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.886720 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.886847 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.886957 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.887000 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.888879 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.888974 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.891419 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.891500 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.891617 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.893793 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.895632 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.895727 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.896011 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.896092 140537667182592 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:32:21.896201 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.896239 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.896270 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.896333 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.898554 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.904011 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.904268 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.906865 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.919456 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.919512 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.919548 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.919578 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.919640 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.920203 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.920279 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.920635 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.921333 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.923808 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.924484 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.924562 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.924598 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.924655 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.924788 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.924898 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.924936 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.926831 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.926927 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.929312 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.929394 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.929502 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.931711 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.933625 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.933730 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.934018 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.934099 140537667182592 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:32:21.934210 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.934249 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.934279 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.934342 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.936567 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.941963 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.942219 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.944888 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.957824 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.957882 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.957917 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.957946 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.958007 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.958609 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.958685 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.959041 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.959723 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.962167 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.962784 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.962860 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:21.962895 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:21.962952 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.963264 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:21.963378 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:21.963418 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.965290 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.965389 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.967852 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.967931 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:21.968038 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:21.970235 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:21.972078 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.972172 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:21.972459 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.972540 140537667182592 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:32:21.972649 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:21.972688 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:21.972718 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:21.972779 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.974997 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:21.980459 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.980715 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:21.983332 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:21.995893 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:21.995951 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:21.995985 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:21.996015 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.996076 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.996625 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.996701 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.997056 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:21.997758 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.000225 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.000885 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.000962 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.000996 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.001053 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.001199 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.001310 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.001349 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.003241 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.003342 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.005754 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.005837 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.005946 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.008135 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.010047 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.010143 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.010430 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.010514 140537667182592 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:32:22.010625 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.010664 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.010695 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.010759 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.012972 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.018337 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.018597 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.021258 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.033779 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.033836 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.033871 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.033902 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.033964 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.034511 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.034587 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.034943 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.035667 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.038135 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.038743 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.038821 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.038856 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.038915 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.039044 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.039154 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.039196 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.041050 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.041144 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.043557 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.043638 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.043747 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.046003 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.047841 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.047936 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.048220 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.048310 140537667182592 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:32:22.051146 140537667182592 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:22.106050 140537667182592 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.106137 140537667182592 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:32:22.106192 140537667182592 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:32:22.106297 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.106335 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.106365 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.106428 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.109074 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.114404 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.114659 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.117208 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.129497 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.129552 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.129586 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.129616 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.129686 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.130237 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.130313 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.130663 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.131337 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.133807 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.134415 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.134491 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.134525 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.134584 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.134711 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.134828 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.134866 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.136699 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.136793 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.139165 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.139245 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.139353 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.141570 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.143420 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.143518 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.143805 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.143889 140537667182592 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:32:22.143997 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.144037 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.144068 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.144134 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.146362 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.151679 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.151934 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.154561 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.166806 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.166862 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.166898 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.166927 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.166988 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.167537 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.167613 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.167964 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.168637 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.171109 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.171718 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.171795 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.171830 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.171889 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.172015 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.172122 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.172165 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.174004 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.174099 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.176461 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.176541 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.176649 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.178876 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.180698 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.180793 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.181255 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.181338 140537667182592 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:32:22.181447 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.181486 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.181517 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.181580 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.183790 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.189119 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.189563 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.192179 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.204410 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.204467 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.204503 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.204535 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.204597 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.205143 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.205220 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.205570 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.206254 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.208728 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.209338 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.209416 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.209452 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.209511 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.209647 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.209760 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.209799 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.211633 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.211728 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.214114 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.214196 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.214303 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.216984 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.218834 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.218930 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.219216 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.219299 140537667182592 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:32:22.219408 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.219448 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.219478 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.219541 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.221756 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.227078 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.227335 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.229952 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.242201 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.242256 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.242298 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.242338 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.242400 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.242947 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.243023 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.243376 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.244062 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.246566 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.247178 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.247254 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.247289 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.247347 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.247473 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.247579 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.247618 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.249466 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.249558 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.251947 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.252026 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.252131 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.254391 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.256225 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.256319 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.256601 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.256681 140537667182592 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:32:22.256788 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.256825 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.256855 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.256916 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.259101 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.264401 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.264652 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.267286 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.279572 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.279628 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.279666 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.279696 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.279757 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.280301 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.280375 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.280725 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.281399 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.283897 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.284514 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.284589 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.284623 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.284679 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.284806 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.284913 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.284949 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.286821 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.286920 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.289288 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.289366 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.289472 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.291797 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.293637 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.293740 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.294025 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.294106 140537667182592 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:32:22.294214 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.294251 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.294281 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.294343 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.296570 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.301914 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.302167 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.304831 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.317178 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.317233 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.317267 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.317296 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.317356 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.317909 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.317985 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.318339 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.319024 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.321533 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.322169 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.322246 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.322279 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.322336 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.322460 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.322569 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.322606 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.324463 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.324560 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.326949 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.327029 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.327136 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.329802 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.331629 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.331724 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.332010 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.332090 140537667182592 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:32:22.332198 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.332236 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.332266 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.332328 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.334545 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.339865 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.340117 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.342782 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.355085 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.355141 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.355175 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.355204 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.355264 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.355812 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.355891 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.356245 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.356923 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.359432 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.360048 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.360123 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.360157 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.360215 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.360340 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.360448 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.360488 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.362339 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.362433 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.364796 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.364873 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.364985 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.367261 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.369088 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.369180 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.369462 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.369542 140537667182592 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:32:22.369657 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.369696 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.369725 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.369787 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.371980 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.377352 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.377609 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.380254 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.392806 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.392860 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.392894 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.392924 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.392985 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.393648 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.393724 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.394074 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.394747 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.397244 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.397865 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.397942 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.397976 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.398033 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.398156 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.398268 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.398304 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.400140 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.400233 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.402598 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.402686 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.402797 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.405040 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.406872 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.406968 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.407252 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.407332 140537667182592 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:32:22.407438 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.407475 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.407505 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.407566 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.409795 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.415157 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.415412 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.418063 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.430466 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.430521 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.430555 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.430584 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.430643 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.431196 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.431270 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.431627 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.432312 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.434832 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.435444 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.435519 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.435553 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.435611 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.435736 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.435847 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.435885 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.437733 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.437825 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.440209 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.440293 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.440403 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.443059 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.444898 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.444992 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.445277 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.445358 140537667182592 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:32:22.445466 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.445503 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.445533 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.445595 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.447820 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.453189 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.453441 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.456086 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.468473 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.468526 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.468560 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.468589 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.468649 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.469207 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.469282 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.469631 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.470327 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.472821 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.473425 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.473502 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.473536 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.473592 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.473724 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.473833 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.473870 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.476129 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.476222 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.478557 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.478635 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.478749 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.480967 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.482790 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.482884 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.483165 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.483246 140537667182592 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:32:22.483354 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.483391 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.483420 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.483482 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.485692 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.491003 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.491256 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.493967 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.506257 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.506312 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.506347 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.506377 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.506440 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.506993 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.507067 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.507416 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.508096 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.510597 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.511214 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.511290 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.511324 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.511382 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.511505 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.511611 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.511648 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.513471 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.513562 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.515903 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.515982 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.516086 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.518333 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.520179 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.520272 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.520554 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.520634 140537667182592 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:32:22.520742 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:22.520779 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:22.520809 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:22.520871 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.523082 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:22.528400 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.528653 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:22.531311 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:22.543657 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:22.543711 140537667182592 attention.py:418] Single window, no scan.
I0123 15:32:22.543746 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:22.543776 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.543835 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.544384 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.544458 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.544808 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.545492 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.548018 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.548626 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.548702 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:22.548737 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:22.548793 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.548919 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:22.549026 140537667182592 nn_components.py:325] mlp: activation = None
I0123 15:32:22.549063 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.550905 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.550997 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.553329 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.553407 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 15:32:22.553514 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:22.556135 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 15:32:22.557981 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.558076 140537667182592 nn_components.py:261] mlp: residual
I0123 15:32:22.558359 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:22.558444 140537667182592 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:32:22.561205 140537667182592 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:26.908060 140537667182592 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 15:32:27.436239 140537667182592 training_loop.py:409] No working directory specified.
I0123 15:32:27.436362 140537667182592 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 15:32:27.437165 140537667182592 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 15:32:30.819862 140537667182592 training_loop.py:447] Only restoring trainable parameters.
I0123 15:32:30.820499 140537667182592 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 15:32:30.820581 140537667182592 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.820631 140537667182592 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.820676 140537667182592 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.820719 140537667182592 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.820761 140537667182592 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.820801 140537667182592 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.820840 140537667182592 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.820878 140537667182592 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.820915 140537667182592 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.820952 140537667182592 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.820990 140537667182592 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.821026 140537667182592 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.821063 140537667182592 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.821099 140537667182592 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821135 140537667182592 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.821172 140537667182592 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821210 140537667182592 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821247 140537667182592 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.821283 140537667182592 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.821334 140537667182592 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821372 140537667182592 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.821408 140537667182592 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.821445 140537667182592 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.821481 140537667182592 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821516 140537667182592 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.821553 140537667182592 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821589 140537667182592 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821625 140537667182592 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.821671 140537667182592 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.821708 140537667182592 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821744 140537667182592 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.821782 140537667182592 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.821818 140537667182592 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.821857 140537667182592 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821895 140537667182592 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.821932 140537667182592 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.821969 140537667182592 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822005 140537667182592 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.822041 140537667182592 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.822077 140537667182592 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822113 140537667182592 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.822148 140537667182592 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.822185 140537667182592 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.822221 140537667182592 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822256 140537667182592 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.822298 140537667182592 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822336 140537667182592 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822373 140537667182592 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.822408 140537667182592 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.822444 140537667182592 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822479 140537667182592 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.822515 140537667182592 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.822550 140537667182592 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.822585 140537667182592 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822620 140537667182592 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.822655 140537667182592 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822690 140537667182592 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822725 140537667182592 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.822760 140537667182592 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.822796 140537667182592 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822831 140537667182592 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.822866 140537667182592 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.822902 140537667182592 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.822938 140537667182592 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.822974 140537667182592 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.823010 140537667182592 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823045 140537667182592 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823080 140537667182592 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.823116 140537667182592 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.823151 140537667182592 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823187 140537667182592 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.823224 140537667182592 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.823264 140537667182592 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.823301 140537667182592 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823336 140537667182592 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.823371 140537667182592 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823405 140537667182592 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823441 140537667182592 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.823476 140537667182592 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.823511 140537667182592 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823547 140537667182592 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.823582 140537667182592 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.823618 140537667182592 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.823654 140537667182592 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823690 140537667182592 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.823725 140537667182592 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823761 140537667182592 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823796 140537667182592 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.823832 140537667182592 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.823868 140537667182592 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.823904 140537667182592 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.823940 140537667182592 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.823977 140537667182592 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.824012 140537667182592 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824048 140537667182592 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.824083 140537667182592 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824119 140537667182592 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824155 140537667182592 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.824191 140537667182592 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.824232 140537667182592 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824269 140537667182592 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.824304 140537667182592 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.824341 140537667182592 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.824377 140537667182592 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824413 140537667182592 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.824449 140537667182592 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824484 140537667182592 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824520 140537667182592 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.824555 140537667182592 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.824590 140537667182592 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824626 140537667182592 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.824661 140537667182592 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:30.824695 140537667182592 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:30.824731 140537667182592 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824766 140537667182592 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.824802 140537667182592 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824837 140537667182592 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824872 140537667182592 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:30.824907 140537667182592 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:30.824942 140537667182592 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:30.824977 140537667182592 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:30.825005 140537667182592 training_loop.py:725] Total parameters: 152072288
I0123 15:32:30.825237 140537667182592 training_loop.py:739] Total state size: 0
I0123 15:32:30.849442 140537667182592 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 15:32:30.849697 140537667182592 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 15:32:30.850037 140537667182592 training_loop.py:652] Compiling mode beam_search with jit.
I0123 15:32:30.850375 140537667182592 training_loop.py:89] registering functions: dict_keys([])
I0123 15:32:30.866949 140537667182592 graph.py:499] a b c = triangle a b c; d = midpoint d b a; e = midpoint e c a; f = midpoint f c b; g = on_circle g e c, on_bline g c a; h = on_circle h d a, on_bline h a b; i = on_circle i f b, on_bline i b c; j = mirror j b i; k = mirror k c i; l = mirror l a g; m = mirror m c g; n = mirror n b h; o = mirror o a h; p = on_line p c o, on_line p b m; q = on_line q b m, on_line q a j; r = on_line r a j, on_line r c o; s = on_line s c n, on_line s a k; t = on_line t c n, on_line t b l; u = on_line u b l, on_line u a k ? contri p r q s u t
I0123 15:32:41.210466 140537667182592 ddar.py:60] Depth 1/1000 time = 10.193580865859985
I0123 15:33:02.539127 140537667182592 ddar.py:60] Depth 2/1000 time = 21.328383922576904
I0123 15:34:02.835752 140537667182592 ddar.py:60] Depth 3/1000 time = 60.29620957374573
I0123 15:36:10.264984 140537667182592 ddar.py:60] Depth 4/1000 time = 127.42846775054932
I0123 15:39:13.973391 140537667182592 ddar.py:60] Depth 5/1000 time = 183.70744037628174
I0123 15:42:26.373690 140537667182592 ddar.py:60] Depth 6/1000 time = 192.39967894554138
I0123 15:45:45.260723 140537667182592 ddar.py:60] Depth 7/1000 time = 198.8864643573761
I0123 15:49:02.629027 140537667182592 ddar.py:60] Depth 8/1000 time = 197.3677716255188
I0123 15:52:25.309893 140537667182592 ddar.py:60] Depth 9/1000 time = 202.67818546295166
I0123 15:55:44.353987 140537667182592 ddar.py:60] Depth 10/1000 time = 199.01990365982056
I0123 15:59:11.013980 140537667182592 ddar.py:60] Depth 11/1000 time = 204.14759755134583
I0123 16:02:44.813011 140537667182592 ddar.py:60] Depth 12/1000 time = 212.26970219612122
I0123 16:06:14.671592 140537667182592 ddar.py:60] Depth 13/1000 time = 209.77379059791565
I0123 16:06:14.672342 140537667182592 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:06:14.672770 140537667182592 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 16:06:14.672812 140537667182592 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a b d 00 D a d b d 01 ; e : C a c e 02 D a e c e 03 ; f : C b c f 04 D b f c f 05 ; g : D a g c g 06 D c e e g 07 ^ c a c g a g a c 08 ; h : D a d d h 09 D a h b h 10 ^ b a b h a h a b 11 ; i : D b f f i 12 D b i c i 13 ^ c b c i b i b c 14 ; j : C b i j 15 D b i i j 16 ; k : C c i k 17 D c i i k 18 ; l : C a g l 19 D a g g l 20 ; m : C c g m 21 D c g g m 22 ; n : C b h n 23 D b h h n 24 ; o : C a h o 25 D a h h o 26 ; p : C b m p 27 C c o p 28 ; q : C a j q 29 C b m q 30 ; r : C a j r 31 C c o r 32 ; s : C a k s 33 C c n s 34 ; t : C b l t 35 C c n t 36 ; u : C a k u 37 C b l u 38 ? = p r q s u t {F1} x00
I0123 16:06:14.672845 140537667182592 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : C a b d 00 D a d b d 01 ; e : C a c e 02 D a e c e 03 ; f : C b c f 04 D b f c f 05 ; g : D a g c g 06 D c e e g 07 ^ c a c g a g a c 08 ; h : D a d d h 09 D a h b h 10 ^ b a b h a h a b 11 ; i : D b f f i 12 D b i c i 13 ^ c b c i b i b c 14 ; j : C b i j 15 D b i i j 16 ; k : C c i k 17 D c i i k 18 ; l : C a g l 19 D a g g l 20 ; m : C c g m 21 D c g g m 22 ; n : C b h n 23 D b h h n 24 ; o : C a h o 25 D a h h o 26 ; p : C b m p 27 C c o p 28 ; q : C a j q 29 C b m q 30 ; r : C a j r 31 C c o r 32 ; s : C a k s 33 C c n s 34 ; t : C b l t 35 C c n t 36 ; u : C a k u 37 C b l u 38 ? = p r q s u t {F1} x00
I0123 16:06:14.824844 140537667182592 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.825056 140537667182592 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 16:06:14.825161 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825237 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825308 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825376 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825454 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825522 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825590 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825664 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825734 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825801 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825869 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825935 140537667182592 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:06:14.825976 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:14.826023 140537667182592 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:06:14.826132 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:14.826171 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:14.826200 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:14.828091 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.830633 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:14.836713 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.836988 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:14.839591 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:14.843587 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:14.843645 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:14.843683 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:14.843715 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.843777 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.844420 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.844495 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.844861 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.845648 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.848206 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.848830 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.848907 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:14.848941 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:14.849003 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.849133 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:14.849465 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:14.849509 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.851509 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.851614 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.854092 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.854172 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:14.854601 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:14.856965 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.858909 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.859005 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.859302 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.859387 140537667182592 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:06:14.859500 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:14.859540 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:14.859572 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:14.861526 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.863962 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:14.869683 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.869949 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:14.872683 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:14.876446 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:14.876500 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:14.876535 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:14.876566 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.876628 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.877186 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.877262 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.877617 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.878421 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.880914 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.881576 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.881660 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:14.881697 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:14.881756 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.881885 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:14.882212 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:14.882254 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.884326 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.884419 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.886958 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.887041 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:14.887487 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:14.889889 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.891875 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.891970 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.892261 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.892343 140537667182592 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:06:14.892452 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:14.892489 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:14.892519 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:14.894346 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.896732 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:14.902528 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.902796 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:14.905417 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:14.909172 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:14.909226 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:14.909262 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:14.909293 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.909356 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.909977 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.910057 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.910426 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.911246 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.913720 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.914353 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.914432 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:14.914469 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:14.914529 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.914662 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:14.915001 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:14.915044 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.917059 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.917152 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.919626 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.919711 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:14.920135 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:14.922436 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.924396 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.924490 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.924778 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.924859 140537667182592 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:06:14.924968 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:14.925005 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:14.925036 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:14.926968 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.929288 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:14.934988 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.935247 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:14.937798 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:14.941606 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:14.941668 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:14.941704 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:14.941736 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.941798 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.942347 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.942423 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.942775 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.943548 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.945995 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.946641 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.946721 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:14.946757 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:14.946817 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.946950 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:14.947726 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:14.947770 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.949707 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.949801 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.952250 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.952335 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:14.952764 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:14.955033 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.957010 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.957104 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.957397 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.957480 140537667182592 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:06:14.957589 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:14.957628 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:14.957672 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:14.959502 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.961821 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:14.967510 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.967767 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:14.970318 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:14.973976 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:14.974030 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:14.974065 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:14.974096 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.974208 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.974764 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.974839 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.975192 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.975949 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.978411 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.979038 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.979117 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:14.979152 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:14.979215 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.979364 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:14.979699 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:14.979742 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.981929 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.982022 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.984471 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.984549 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:14.984974 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:14.987242 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:14.989183 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.989279 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:14.989570 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.989682 140537667182592 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:06:14.989794 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:14.989832 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:14.989862 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:14.991768 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.994081 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:14.999740 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:14.999997 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.002598 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:15.006254 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.006309 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.006344 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.006375 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.006436 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.006986 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.007060 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.007443 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.008202 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.010668 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.011347 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.011425 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.011460 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.011520 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.011668 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.011989 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.012032 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.013963 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.014056 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.016520 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.016600 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.017030 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.019377 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.021285 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.021378 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.021689 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.021773 140537667182592 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:06:15.021881 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.021919 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.021950 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.023742 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.026075 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.031800 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.032056 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.034647 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:15.038321 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.038375 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.038411 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.038443 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.038556 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.039117 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.039194 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.039581 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.040351 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.042807 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.043432 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.043509 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.043544 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.043604 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.043731 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.044050 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.044090 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.046080 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.046174 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.048840 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.048920 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.049353 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.051758 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.053687 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.053787 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.054076 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.054159 140537667182592 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:06:15.054269 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.054308 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.054340 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.056565 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.058892 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.064471 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.064723 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.067343 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:15.071025 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.071080 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.071115 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.071146 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.071207 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.071764 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.071839 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.072192 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.072951 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.075434 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.076101 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.076178 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.076214 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.076274 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.076402 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.076720 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.076762 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.078703 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.078798 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.081259 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.081339 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.081777 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.084113 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.086024 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.086128 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.086415 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.086496 140537667182592 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:06:15.086605 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.086643 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.086674 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.088458 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.090763 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.096402 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.096659 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.099202 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:15.102845 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.102900 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.102935 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.102966 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.103028 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.103641 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.103718 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.104074 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.104826 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.107282 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.107892 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.107968 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.108003 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.108061 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.108186 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.108504 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.108545 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.110477 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.110570 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.113096 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.113176 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.113602 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.115935 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.117960 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.118059 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.118358 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.118445 140537667182592 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:06:15.118558 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.118597 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.118628 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.120418 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.122830 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.128419 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.128674 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.131325 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:15.134970 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.135025 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.135061 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.135093 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.135207 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.135768 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.135843 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.136204 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.136973 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.139455 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.140073 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.140150 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.140185 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.140245 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.140375 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.140689 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.140731 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.142693 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.142787 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.145317 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.145395 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.145830 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.148072 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.149969 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.150063 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.150350 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.150437 140537667182592 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:06:15.150743 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.150782 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.150813 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.152599 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.155016 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.160619 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.160876 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.163416 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:15.167078 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.167134 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.167169 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.167201 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.167317 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.167881 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.167958 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.168314 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.169063 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.171517 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.172134 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.172212 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.172247 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.172305 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.172431 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.172746 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.172786 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.174693 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.174787 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.177680 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.177761 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.178183 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.180441 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.182366 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.182461 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.182753 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.182837 140537667182592 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:06:15.182954 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.182992 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.183023 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.184799 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.187173 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.192733 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.192987 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.195545 140537667182592 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:06:15.199196 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.199251 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.199286 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.199318 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.199432 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.199986 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.200062 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.200419 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.201188 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.203639 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.204252 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.204328 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.204363 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.204422 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.204550 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.204869 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.204911 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.206826 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.206920 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.209430 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.209509 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.209936 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.212193 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.214119 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.214215 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.214507 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.214753 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.214821 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.214884 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.214941 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.214994 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.215046 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.215098 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.215149 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.215201 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.215254 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.215305 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.215356 140537667182592 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:06:15.215391 140537667182592 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:06:15.218322 140537667182592 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:06:15.263328 140537667182592 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.263413 140537667182592 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:06:15.263467 140537667182592 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:06:15.263571 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.263609 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.263641 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.263704 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.266070 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.271449 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.271709 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.274271 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.287309 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.287366 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.287402 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.287434 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.287495 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.288069 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.288146 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.288513 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.289219 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.291786 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.292417 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.292500 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.292536 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.292597 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.292727 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.292836 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.292874 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.294754 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.294848 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.297274 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.297356 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.297467 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.299724 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.301572 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.301672 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.301966 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.302048 140537667182592 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:06:15.302157 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.302195 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.302225 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.302289 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.304533 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.309920 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.310175 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.312814 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.325309 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.325364 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.325399 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.325429 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.325491 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.326050 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.326126 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.326478 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.327209 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.329647 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.330262 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.330343 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.330378 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.330436 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.330563 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.330672 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.330711 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.332565 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.332657 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.335062 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.335141 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.335252 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.337491 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.339338 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.339433 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.339721 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.339804 140537667182592 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:06:15.339915 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.339952 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.339982 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.340046 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.342294 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.347666 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.347924 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.350567 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.363394 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.363451 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.363486 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.363516 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.363578 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.364131 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.364206 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.364567 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.365309 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.367945 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.368559 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.368636 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.368676 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.368736 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.368865 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.368974 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.369011 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.370865 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.370959 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.373351 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.373429 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.373537 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.375781 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.377622 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.377728 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.378018 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.378099 140537667182592 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:06:15.378207 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.378245 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.378275 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.378338 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.380560 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.385937 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.386198 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.388866 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.401295 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.401351 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.401386 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.401417 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.401478 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.402036 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.402113 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.402470 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.403158 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.405669 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.406284 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.406363 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.406398 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.406463 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.406592 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.406702 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.406739 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.408583 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.408677 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.411065 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.411145 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.411252 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.413499 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.415371 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.415466 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.415757 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.415838 140537667182592 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:06:15.415946 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.415984 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.416015 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.416079 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.418346 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.423753 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.424013 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.426673 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.439143 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.439200 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.439235 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.439265 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.439327 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.439880 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.439955 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.440313 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.441000 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.443521 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.444142 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.444219 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.444256 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.444315 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.444456 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.444568 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.444606 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.446484 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.446578 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.449000 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.449079 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.449187 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.451476 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.453327 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.453422 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.453722 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.453805 140537667182592 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:06:15.453916 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.453955 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.453986 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.454050 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.456276 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.461739 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.462000 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.464664 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.477855 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.477911 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.477947 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.477978 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.478041 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.478597 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.478673 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.479030 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.479722 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.482253 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.482871 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.482947 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.482981 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.483041 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.483177 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.483287 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.483325 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.485183 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.485276 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.487696 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.487776 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.487886 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.490177 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.492037 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.492131 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.492421 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.492502 140537667182592 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:06:15.492609 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.492647 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.492677 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.492742 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.494975 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.500378 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.500638 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.503316 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.515706 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.515761 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.515798 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.515827 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.515889 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.516443 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.516520 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.516878 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.517563 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.520076 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.520689 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.520765 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.520799 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.520857 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.520984 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.521099 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.521137 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.522994 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.523088 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.525471 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.525549 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.525665 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.527914 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.529759 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.529854 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.530144 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.530224 140537667182592 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:06:15.530333 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.530372 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.530403 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.530466 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.532696 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.538110 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.538371 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.541014 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.553460 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.553516 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.553551 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.553583 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.553653 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.554211 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.554287 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.554645 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.555323 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.557832 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.558443 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.558520 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.558554 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.558613 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.558739 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.558855 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.558894 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.560739 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.560831 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.563239 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.563319 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.563426 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.565679 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.567533 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.567628 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.567920 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.568003 140537667182592 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:06:15.568114 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.568153 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.568184 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.568247 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.570484 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.576106 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.576365 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.579024 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.591895 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.591951 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.591986 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.592017 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.592079 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.592635 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.592711 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.593069 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.593779 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.596299 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.597106 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.597181 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.597216 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.597275 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.597404 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.597512 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.597557 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.599446 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.599540 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.602306 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.602386 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.602497 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.604943 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.606811 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.606908 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.607202 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.607285 140537667182592 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:06:15.607395 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.607434 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.607465 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.607528 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.609775 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.615183 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.615449 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.618124 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.630790 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.630846 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.630882 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.630913 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.630978 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.631542 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.631618 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.631977 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.632670 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.635200 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.635818 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.635895 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.635929 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.635987 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.636114 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.636225 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.636264 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.638149 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.638243 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.640671 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.640751 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.640861 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.643148 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.645005 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.645098 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.645388 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.645469 140537667182592 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:06:15.645578 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.645615 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.645653 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.645720 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.647961 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.653370 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.653628 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.656362 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.668985 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.669041 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.669076 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.669107 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.669170 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.669754 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.669830 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.670189 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.670887 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.673437 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.674060 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.674137 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.674171 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.674230 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.674359 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.674470 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.674508 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.676375 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.676477 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.678905 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.678986 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.679095 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.681348 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.683209 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.683305 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.683597 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.683678 140537667182592 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:06:15.683789 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.683826 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.683857 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.683921 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.686166 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.691617 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.691877 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.694552 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.707826 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.707881 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.707916 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.707947 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.708009 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.708567 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.708643 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.709000 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.709699 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.712246 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.712872 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.712949 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.712985 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.713044 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.713177 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.713287 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.713325 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.715198 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.715300 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.717731 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.717810 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.717919 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.720196 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.722066 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.722163 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.722456 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.722543 140537667182592 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:06:15.725389 140537667182592 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:06:15.776083 140537667182592 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.776171 140537667182592 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:06:15.776225 140537667182592 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:06:15.776330 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.776368 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.776398 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.776462 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.778784 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.784336 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.784596 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.787201 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.799847 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.799903 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.799938 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.799970 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.800034 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.800597 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.800673 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.801033 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.801717 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.804349 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.804959 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.805036 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.805071 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.805130 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.805265 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.805376 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.805413 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.807358 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.807452 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.809977 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.810058 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.810168 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.812374 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.814239 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.814335 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.814632 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.814714 140537667182592 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:06:15.814823 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.814862 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.814893 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.814956 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.817212 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.822886 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.823148 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.826046 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.838660 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.838715 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.838751 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.838783 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.838845 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.839404 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.839481 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.839843 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.840528 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.842956 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.843566 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.843643 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.843677 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.843737 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.843863 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.843979 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.844017 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.845950 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.846044 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.848458 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.848538 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.848646 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.850839 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.852693 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.852787 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.853077 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.853159 140537667182592 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:06:15.853271 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.853309 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.853340 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.853403 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.855644 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.861138 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.861400 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.863996 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.876471 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.876525 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.876561 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.876591 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.876653 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.877202 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.877278 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.877631 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.878315 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.880727 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.881337 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.881413 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.881448 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.881507 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.881633 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.881753 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.881796 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.884189 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.884283 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.886689 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.886768 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.886878 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.889041 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.890886 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.890981 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.891271 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.891352 140537667182592 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:06:15.891460 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.891498 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.891530 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.891593 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.893830 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.899313 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.899568 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.902159 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.914818 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.914873 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.914908 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.914938 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.915000 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.915554 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.915630 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.915987 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.916677 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.919119 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.919735 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.919813 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.919848 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.919906 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.920034 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.920145 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.920190 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.922141 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.922235 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.924630 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.924709 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.924818 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.927016 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.928852 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.928946 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.929235 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.929318 140537667182592 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:06:15.929428 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.929466 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.929498 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.929562 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.931797 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.937268 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.937529 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.940121 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.952509 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.952564 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.952600 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.952630 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.952692 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.953242 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.953317 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.953681 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.954360 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.956788 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.957397 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.957474 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.957509 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.957568 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.957702 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.957812 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.957849 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.959771 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.959863 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.962260 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.962339 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:15.962448 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:15.964642 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.966502 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.966597 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:15.966888 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.966970 140537667182592 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:06:15.967077 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:15.967115 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:15.967146 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:15.967209 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.969437 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:15.974869 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.975126 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:15.977716 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:15.990145 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:15.990200 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:15.990236 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:15.990267 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.990328 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.990877 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.990954 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.991309 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.991987 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.994430 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.995048 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.995125 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:15.995160 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:15.995219 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.995345 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:15.995455 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:15.995493 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:15.997881 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:15.997982 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.000368 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.000447 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:16.000557 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:16.002810 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.004813 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.004908 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.005201 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.005284 140537667182592 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:06:16.005396 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:16.005435 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:16.005466 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:16.005531 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.007764 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:16.013245 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.013506 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:16.016076 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:16.028526 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:16.028581 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:16.028617 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:16.028649 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.028712 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.029269 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.029346 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.029716 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.030403 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.032842 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.033463 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.033539 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:16.033573 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:16.033632 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.033768 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:16.033878 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:16.033916 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.035861 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.035961 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.038368 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.038448 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:16.038558 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:16.040760 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.042603 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.042698 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.042991 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.043073 140537667182592 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:06:16.043184 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:16.043222 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:16.043252 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:16.043315 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.045539 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:16.051013 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.051270 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:16.053852 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:16.066368 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:16.066422 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:16.066458 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:16.066488 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.066550 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.067104 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.067179 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.067536 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.068217 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.070651 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.071264 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.071339 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:16.071374 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:16.071432 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.071556 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:16.071663 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:16.071701 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.073615 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.073714 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.076093 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.076171 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:16.076280 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:16.078507 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.080359 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.080453 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.080745 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.080825 140537667182592 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:06:16.080934 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:16.080971 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:16.081001 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:16.081064 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.083303 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:16.088753 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.089013 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:16.091590 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:16.104037 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:16.104092 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:16.104128 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:16.104159 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.104218 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.104768 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.104843 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.105199 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.105909 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.108361 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.108972 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.109047 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:16.109082 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:16.109141 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.109267 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:16.109375 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:16.109413 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.111763 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.111857 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.114254 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.114339 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:16.114449 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:16.116637 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.118499 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.118595 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.118889 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.118970 140537667182592 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:06:16.119081 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:16.119120 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:16.119152 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:16.119215 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.121440 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:16.127233 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.127497 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:16.130079 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:16.142661 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:16.142718 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:16.142753 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:16.142784 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.142845 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.143403 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.143479 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.143835 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.144517 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.146958 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.147569 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.147644 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:16.147678 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:16.147736 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.147862 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:16.147970 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:16.148008 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.150015 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.150108 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.152516 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.152599 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:16.152708 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:16.154907 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.156763 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.156857 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.157151 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.157231 140537667182592 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:06:16.157342 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:16.157380 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:16.157410 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:16.157474 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.159732 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:16.165214 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.165472 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:16.168063 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:16.180675 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:16.180734 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:16.180769 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:16.180800 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.180860 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.181410 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.181484 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.181849 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.182543 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.184988 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.185601 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.185686 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:16.185722 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:16.185781 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.185909 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:16.186020 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:16.186058 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.188009 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.188101 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.190521 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.190600 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:16.190717 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:16.192911 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.194763 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.194857 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.195150 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.195230 140537667182592 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:06:16.195339 140537667182592 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:06:16.195377 140537667182592 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:06:16.195408 140537667182592 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:06:16.195471 140537667182592 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.197700 140537667182592 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:06:16.203204 140537667182592 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.203464 140537667182592 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:06:16.206041 140537667182592 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:06:16.218462 140537667182592 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:06:16.218518 140537667182592 attention.py:418] Single window, no scan.
I0123 16:06:16.218554 140537667182592 transformer_layer.py:389] tlayer: self-attention.
I0123 16:06:16.218585 140537667182592 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.218645 140537667182592 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.219198 140537667182592 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.219272 140537667182592 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.219625 140537667182592 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.220301 140537667182592 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.222738 140537667182592 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.223349 140537667182592 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.223424 140537667182592 transformer_layer.py:468] tlayer: End windows.
I0123 16:06:16.223458 140537667182592 transformer_layer.py:472] tlayer: final FFN.
I0123 16:06:16.223516 140537667182592 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.223643 140537667182592 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:06:16.223752 140537667182592 nn_components.py:325] mlp: activation = None
I0123 16:06:16.223789 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.226129 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.226222 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.228738 140537667182592 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.228815 140537667182592 transformer_base.py:443] tbase: final FFN
I0123 16:06:16.228924 140537667182592 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:06:16.231208 140537667182592 nn_components.py:329] mlp: final activation = None
I0123 16:06:16.233060 140537667182592 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.233153 140537667182592 nn_components.py:261] mlp: residual
I0123 16:06:16.233445 140537667182592 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:16.233529 140537667182592 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:06:16.236380 140537667182592 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:06:30.723382 140537667182592 alphageometry.py:566] LM output (score=-2.153080): "t : T g m g t 39 ;"
I0123 16:06:30.723555 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.723597 140537667182592 alphageometry.py:566] LM output (score=-2.537251): "s : T m p t s 39 ;"
I0123 16:06:30.723626 140537667182592 alphageometry.py:567] Translation: "ERROR: point s already exists."

I0123 16:06:30.723654 140537667182592 alphageometry.py:566] LM output (score=-2.557280): "t : T m p t s 39 ;"
I0123 16:06:30.723680 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.723708 140537667182592 alphageometry.py:566] LM output (score=-2.633213): "t : P e g l t 39 ;"
I0123 16:06:30.723732 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.723755 140537667182592 alphageometry.py:566] LM output (score=-2.637987): "t : T b n c t 39 ;"
I0123 16:06:30.723778 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.723801 140537667182592 alphageometry.py:566] LM output (score=-2.805837): "t : P l t p t 39 ;"
I0123 16:06:30.723824 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.723849 140537667182592 alphageometry.py:566] LM output (score=-2.840524): "t : T m p t v 39 ;"
I0123 16:06:30.723874 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.723898 140537667182592 alphageometry.py:566] LM output (score=-2.920953): "t : P e s l t 39 ;"
I0123 16:06:30.723920 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.723944 140537667182592 alphageometry.py:566] LM output (score=-3.060893): "t : T l t p t 39 ;"
I0123 16:06:30.723967 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.723989 140537667182592 alphageometry.py:566] LM output (score=-3.138719): "t : T m p t r 39 ;"
I0123 16:06:30.724012 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724034 140537667182592 alphageometry.py:566] LM output (score=-3.150755): "t : P f t l t 39 ;"
I0123 16:06:30.724055 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724077 140537667182592 alphageometry.py:566] LM output (score=-3.182929): "t : T m t o t 39 ;"
I0123 16:06:30.724099 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724121 140537667182592 alphageometry.py:566] LM output (score=-3.249215): "t : T m t t s 39 ;"
I0123 16:06:30.724142 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724164 140537667182592 alphageometry.py:566] LM output (score=-3.261976): "t : P e f l t 39 ;"
I0123 16:06:30.724185 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724207 140537667182592 alphageometry.py:566] LM output (score=-3.355684): "t : P e g n t 39 ;"
I0123 16:06:30.724227 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724249 140537667182592 alphageometry.py:566] LM output (score=-3.360883): "t : P e g k t 39 ;"
I0123 16:06:30.724282 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724307 140537667182592 alphageometry.py:566] LM output (score=-3.377354): "t : T g t h o 39 ;"
I0123 16:06:30.724330 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724353 140537667182592 alphageometry.py:566] LM output (score=-3.413115): "t : P j t s t 39 ;"
I0123 16:06:30.724376 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724399 140537667182592 alphageometry.py:566] LM output (score=-3.416912): "t : P f t l m 39 ;"
I0123 16:06:30.724421 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724444 140537667182592 alphageometry.py:566] LM output (score=-3.423531): "t : P l t n t 39 ;"
I0123 16:06:30.724466 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724488 140537667182592 alphageometry.py:566] LM output (score=-3.424966): "t : P j t r s 39 ;"
I0123 16:06:30.724510 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724533 140537667182592 alphageometry.py:566] LM output (score=-3.480494): "t : P g t l m 39 ;"
I0123 16:06:30.724555 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724578 140537667182592 alphageometry.py:566] LM output (score=-3.541559): "t : P f t n s 39 ;"
I0123 16:06:30.724601 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724623 140537667182592 alphageometry.py:566] LM output (score=-3.589885): "t : T f t n t 39 ;"
I0123 16:06:30.724645 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724667 140537667182592 alphageometry.py:566] LM output (score=-3.610296): "t : P f t n t 39 ;"
I0123 16:06:30.724689 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724711 140537667182592 alphageometry.py:566] LM output (score=-3.660648): "t : P e t j q 39 ;"
I0123 16:06:30.724734 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724757 140537667182592 alphageometry.py:566] LM output (score=-3.686990): "t : P g t n t 39 ;"
I0123 16:06:30.724780 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724802 140537667182592 alphageometry.py:566] LM output (score=-3.691740): "s : T m t o s 39 ;"
I0123 16:06:30.724826 140537667182592 alphageometry.py:567] Translation: "ERROR: point s already exists."

I0123 16:06:30.724850 140537667182592 alphageometry.py:566] LM output (score=-3.713062): "t : P k t l t 39 ;"
I0123 16:06:30.724874 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724898 140537667182592 alphageometry.py:566] LM output (score=-3.863229): "s : T m p t s 46 ;"
I0123 16:06:30.724921 140537667182592 alphageometry.py:567] Translation: "ERROR: point s already exists."

I0123 16:06:30.724944 140537667182592 alphageometry.py:566] LM output (score=-3.934725): "t : P d t f p 39 ;"
I0123 16:06:30.724967 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.724990 140537667182592 alphageometry.py:566] LM output (score=-3.935044): "t : D b t c t 39 ;"
I0123 16:06:30.725013 140537667182592 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 16:06:30.725056 140537667182592 alphageometry.py:540] Depth 1. There are 0 nodes to expand:
I0123 16:06:30.725086 140537667182592 alphageometry.py:540] Depth 2. There are 0 nodes to expand:
I0123 16:06:30.725113 140537667182592 alphageometry.py:540] Depth 3. There are 0 nodes to expand:
I0123 16:06:30.725139 140537667182592 alphageometry.py:540] Depth 4. There are 0 nodes to expand:
I0123 16:06:30.725163 140537667182592 alphageometry.py:540] Depth 5. There are 0 nodes to expand:
I0123 16:06:30.725193 140537667182592 alphageometry.py:540] Depth 6. There are 0 nodes to expand:
I0123 16:06:30.725222 140537667182592 alphageometry.py:540] Depth 7. There are 0 nodes to expand:
I0123 16:06:30.725248 140537667182592 alphageometry.py:540] Depth 8. There are 0 nodes to expand:
I0123 16:06:30.725273 140537667182592 alphageometry.py:540] Depth 9. There are 0 nodes to expand:
I0123 16:06:30.725298 140537667182592 alphageometry.py:540] Depth 10. There are 0 nodes to expand:
I0123 16:06:30.725322 140537667182592 alphageometry.py:540] Depth 11. There are 0 nodes to expand:
I0123 16:06:30.725347 140537667182592 alphageometry.py:540] Depth 12. There are 0 nodes to expand:
I0123 16:06:30.725371 140537667182592 alphageometry.py:540] Depth 13. There are 0 nodes to expand:
I0123 16:06:30.725395 140537667182592 alphageometry.py:540] Depth 14. There are 0 nodes to expand:
I0123 16:06:30.725420 140537667182592 alphageometry.py:540] Depth 15. There are 0 nodes to expand:
