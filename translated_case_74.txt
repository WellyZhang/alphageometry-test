I0124 00:28:19.252490 140530918526976 inference_utils.py:69] Parsing gin configuration.
I0124 00:28:19.252588 140530918526976 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0124 00:28:19.252799 140530918526976 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0124 00:28:19.252834 140530918526976 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0124 00:28:19.252866 140530918526976 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0124 00:28:19.252895 140530918526976 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0124 00:28:19.252923 140530918526976 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0124 00:28:19.252950 140530918526976 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0124 00:28:19.252976 140530918526976 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0124 00:28:19.253004 140530918526976 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0124 00:28:19.253030 140530918526976 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0124 00:28:19.253056 140530918526976 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0124 00:28:19.253105 140530918526976 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0124 00:28:19.253243 140530918526976 resource_reader.py:55] Path not found: base_htrans.gin
I0124 00:28:19.253446 140530918526976 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0124 00:28:19.253556 140530918526976 resource_reader.py:55] Path not found: trainer_configuration.gin
I0124 00:28:19.259932 140530918526976 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0124 00:28:19.260061 140530918526976 resource_reader.py:55] Path not found: size/medium_150M.gin
I0124 00:28:19.260384 140530918526976 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0124 00:28:19.260493 140530918526976 resource_reader.py:55] Path not found: options/positions_t5.gin
I0124 00:28:19.260777 140530918526976 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0124 00:28:19.260881 140530918526976 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0124 00:28:19.261291 140530918526976 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0124 00:28:19.261394 140530918526976 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0124 00:28:19.265106 140530918526976 training_loop.py:334] ==== Training loop: initializing model ====
I0124 00:28:19.375715 140530918526976 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0124 00:28:19.376459 140530918526976 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0124 00:28:19.383400 140530918526976 training_loop.py:335] Process 0 of 1
I0124 00:28:19.383460 140530918526976 training_loop.py:336] Local device count = 1
I0124 00:28:19.383502 140530918526976 training_loop.py:337] Number of replicas = 1
I0124 00:28:19.383536 140530918526976 training_loop.py:339] Using random number seed 42
I0124 00:28:19.857130 140530918526976 training_loop.py:359] Initializing the model.
I0124 00:28:20.285379 140530918526976 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.285700 140530918526976 decoder_stack.py:316] dstack: scanning over 1 windows.
I0124 00:28:20.285809 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.285888 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.285967 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286050 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286122 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286191 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286260 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286327 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286395 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286464 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286530 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286597 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0124 00:28:20.286636 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.286683 140530918526976 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0124 00:28:20.286799 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.286841 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.286873 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.288846 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.294066 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.304584 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.304867 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.309151 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.319598 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.319662 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.319701 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.319734 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.319797 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.320963 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.321043 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.321746 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.324168 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.329756 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.331468 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.331551 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.331587 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.331648 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.331777 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.332112 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.332164 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.334063 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.334167 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.336976 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.337059 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.337547 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.347497 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.356094 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.356195 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.356491 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.356576 140530918526976 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0124 00:28:20.356687 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.356728 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.356760 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.358582 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.361002 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.367277 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.367608 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.370235 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.374083 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.374144 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.374182 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.374215 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.374275 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.374853 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.374931 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.375285 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.376041 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.378473 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.379093 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.379172 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.379208 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.379265 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.379391 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.379716 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.379761 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.381688 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.381786 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.384245 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.384330 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.384757 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.387086 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.388963 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.389059 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.389343 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.389427 140530918526976 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0124 00:28:20.389539 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.389579 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.389611 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.391513 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.393833 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.399715 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.399976 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.402596 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.406399 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.406457 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.406492 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.406522 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.406583 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.407138 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.407215 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.407578 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.408325 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.410770 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.411431 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.411510 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.411546 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.411603 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.411729 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.412051 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.412096 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.413992 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.414088 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.416534 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.416621 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.417099 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.419362 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.421259 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.421355 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.421649 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.421734 140530918526976 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0124 00:28:20.421844 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.421886 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.421918 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.423783 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.426143 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.431714 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.431984 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.434593 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.438374 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.438433 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.438469 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.438501 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.438562 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.439116 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.439194 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.439555 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.440325 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.442854 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.443488 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.443569 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.443605 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.443665 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.443800 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.444135 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.444182 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.446086 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.446182 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.448716 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.448807 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.449242 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.451516 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.453413 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.453512 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.453817 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.453902 140530918526976 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0124 00:28:20.454015 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.454056 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.454088 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.455981 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.458389 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.463967 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.464226 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.466895 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.470627 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.470685 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.470721 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.470751 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.470816 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.471379 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.471458 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.471812 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.472575 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.475413 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.476031 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.476114 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.476150 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.476208 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.476345 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.476672 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.476718 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.478600 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.478695 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.481205 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.481286 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.481726 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.483989 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.485947 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.486044 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.486332 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.486416 140530918526976 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0124 00:28:20.486526 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.486566 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.486598 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.488416 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.490777 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.496316 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.496576 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.499242 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.502957 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.503015 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.503051 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.503082 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.503144 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.503738 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.503818 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.504176 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.504955 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.507412 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.508033 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.508113 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.508148 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.508207 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.508337 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.508656 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.508702 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.510586 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.510682 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.513178 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.513262 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.513714 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.516002 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.517909 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.518008 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.518298 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.518382 140530918526976 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0124 00:28:20.518494 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.518537 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.518568 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.520363 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.522791 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.528325 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.528589 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.531187 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.534973 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.535032 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.535069 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.535102 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.535166 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.535722 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.535804 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.536162 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.536937 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.539377 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.540003 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.540086 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.540121 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.540179 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.540308 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.540625 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.540670 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.542623 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.542721 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.545205 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.545287 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.545720 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.548329 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.550216 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.550323 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.550611 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.550695 140530918526976 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0124 00:28:20.550806 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.550846 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.550878 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.693192 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.696345 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.702285 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.702594 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.705315 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.709321 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.709384 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.709423 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.709457 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.709525 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.710155 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.710237 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.710616 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.711415 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.714017 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.714678 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.714759 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.714796 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.714859 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.714990 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.715336 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.715383 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.717310 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.717408 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.720030 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.720114 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.720557 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.722928 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.724872 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.724982 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.725286 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.725373 140530918526976 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0124 00:28:20.725488 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.725529 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.725564 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.727555 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.729966 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.735695 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.735968 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.738675 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.742553 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.742613 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.742651 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.742684 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.742752 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.743325 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.743405 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.743781 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.744562 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.747151 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.747779 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.747861 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.747899 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.747958 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.748091 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.748416 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.748463 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.750396 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.750496 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.753038 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.753120 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.753553 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.755869 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.758034 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.758137 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.758432 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.758525 140530918526976 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0124 00:28:20.758642 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.758684 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.758717 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.760576 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.763041 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.768656 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.768930 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.771983 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.775811 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.775870 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.775912 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.775945 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.776010 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.776620 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.776711 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.777078 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.777868 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.780334 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.780962 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.781048 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.781085 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.781148 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.781279 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.781604 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.781656 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.783586 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.783683 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.786248 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.786334 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.786805 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.789167 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.791146 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.791251 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.791576 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.791669 140530918526976 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0124 00:28:20.791786 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.791827 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.791861 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.793720 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.796202 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.801850 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.802122 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.804796 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.808676 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.808737 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.808775 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.808809 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.808873 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.809459 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.809539 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.809924 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.810713 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.813204 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.813847 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.813929 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.813966 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.814026 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.814156 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.814480 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.814527 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.816490 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.816588 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.819403 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.819487 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.819922 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.822289 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.824197 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.824299 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.824591 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.824676 140530918526976 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0124 00:28:20.824796 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.824839 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.824873 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.826807 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.829191 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.834834 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.835103 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.837748 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:20.841552 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.841611 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.841656 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.841692 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.841758 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.842329 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.842409 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.842772 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.843543 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.845994 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.846975 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.847057 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.847095 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.847156 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.847286 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.847615 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.847662 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.849570 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.849677 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.852174 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.852258 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.852746 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.855002 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.856917 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.857014 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.857311 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.857599 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.857680 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.857751 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.857813 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.857870 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.857928 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.857984 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.858039 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.858093 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.858148 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.858206 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.858261 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0124 00:28:20.858301 140530918526976 decoder_stack.py:344] dstack: Final layernorm.
I0124 00:28:20.861840 140530918526976 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:20.909997 140530918526976 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.910089 140530918526976 decoder_stack.py:333] dstack: autoregressive generator.
I0124 00:28:20.910146 140530918526976 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0124 00:28:20.910251 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.910291 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.910324 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.910387 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.912810 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.918325 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.918591 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.921210 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:20.937927 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.937989 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.938028 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.938060 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.938128 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.939252 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.939334 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.940044 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.942057 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.946807 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.948132 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.948222 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.948261 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.948323 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.948457 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.948570 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.948612 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.950548 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.950647 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.953088 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.953171 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.953283 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.955562 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.957520 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.957620 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.957918 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.958004 140530918526976 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0124 00:28:20.958116 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.958157 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.958191 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.958255 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.960505 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:20.965985 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.966252 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:20.968942 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:20.982278 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:20.982338 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:20.982377 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:20.982409 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.982472 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.983034 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.983117 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.983472 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.984170 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.986659 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.987280 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.987360 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:20.987403 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:20.987465 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.987598 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:20.987711 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:20.987753 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.989696 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.989794 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.992191 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.992272 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:20.992383 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:20.994637 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:20.996589 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.996689 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:20.996977 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.997062 140530918526976 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0124 00:28:20.997172 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:20.997213 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:20.997246 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:20.997312 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:20.999563 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.005058 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.005324 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.008018 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.025700 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.025788 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.025829 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.025864 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.025942 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.026560 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.026643 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.027016 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.027739 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.030319 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.030956 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.031038 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.031074 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.031147 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.031278 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.031392 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.031433 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.033506 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.033603 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.036107 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.036191 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.036307 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.038619 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.040584 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.040682 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.040968 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.041056 140530918526976 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0124 00:28:21.041173 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.041217 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.041249 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.041318 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.043611 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.049149 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.049417 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.052175 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.065216 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.065279 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.065317 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.065350 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.065417 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.065978 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.066059 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.066416 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.067116 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.069583 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.070219 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.070299 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.070337 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.070397 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.070537 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.070653 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.070695 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.072659 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.072756 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.075181 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.075263 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.075374 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.077607 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.079502 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.079601 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.079887 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.079972 140530918526976 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0124 00:28:21.080082 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.080123 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.080157 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.080222 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.082823 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.088356 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.088626 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.091260 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.104147 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.104207 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.104246 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.104280 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.104344 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.104906 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.104987 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.105347 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.106064 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.108614 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.109241 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.109322 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.109359 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.109420 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.109556 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.109682 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.109727 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.111630 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.111727 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.114144 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.114226 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.114337 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.116643 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.118550 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.118648 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.118936 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.119022 140530918526976 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0124 00:28:21.119134 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.119175 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.119207 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.119273 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.121526 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.127022 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.127285 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.129985 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.142940 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.143001 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.143039 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.143072 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.143135 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.143694 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.143773 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.144131 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.144840 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.147333 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.147963 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.148048 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.148086 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.148150 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.148285 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.148406 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.148450 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.150419 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.150517 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.152935 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.153017 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.153129 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.155389 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.157270 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.157369 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.157665 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.157752 140530918526976 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0124 00:28:21.157864 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.157906 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.157938 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.158002 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.160259 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.165844 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.166112 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.168728 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.181685 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.181747 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.181787 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.181819 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.181887 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.182445 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.182528 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.182895 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.183590 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.186115 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.187149 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.187236 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.187273 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.187335 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.187471 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.187588 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.187638 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.189590 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.189696 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.192131 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.192214 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.192334 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.194618 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.196601 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.196701 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.196993 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.197080 140530918526976 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0124 00:28:21.197193 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.197236 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.197268 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.197333 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.199604 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.205109 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.205385 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.208063 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.220914 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.220975 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.221014 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.221047 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.221111 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.221730 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.221811 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.222171 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.222872 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.225359 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.226007 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.226089 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.226127 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.226187 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.226322 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.226434 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.226482 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.228381 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.228479 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.230962 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.231046 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.231158 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.233386 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.235302 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.235401 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.235689 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.235776 140530918526976 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0124 00:28:21.235887 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.235930 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.235963 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.236027 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.238289 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.243846 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.244117 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.246779 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.259708 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.259769 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.259808 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.259841 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.259908 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.260476 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.260556 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.260914 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.261615 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.264109 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.264787 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.264868 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.264906 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.264972 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.265108 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.265222 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.265264 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.267194 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.267292 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.269705 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.269788 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.269899 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.272135 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.274107 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.274206 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.274492 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.274577 140530918526976 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0124 00:28:21.274690 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.274731 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.274764 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.274828 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.277074 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.282575 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.282840 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.285536 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.298753 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.298814 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.298856 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.298889 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.298954 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.299562 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.299643 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.299999 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.300701 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.303223 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.303855 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.303937 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.303973 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.304033 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.304162 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.304272 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.304314 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.306226 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.306331 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.308809 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.308892 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.309006 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.311267 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.313162 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.313261 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.313547 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.313632 140530918526976 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0124 00:28:21.313754 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.313796 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.313829 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.313893 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.316151 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.321722 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.321989 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.324657 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.337558 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.337618 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.337663 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.337698 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.337761 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.338315 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.338395 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.338749 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.339439 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.341922 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.342597 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.342679 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.342716 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.342776 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.342913 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.343028 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.343070 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.344980 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.345086 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.347522 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.347609 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.347720 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.349945 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.351909 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.352009 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.352296 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.352381 140530918526976 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0124 00:28:21.352494 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.352537 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.352571 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.352635 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.354898 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.360365 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.360632 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.363352 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.376156 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.376216 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.376254 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.376287 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.376350 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.376906 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.376985 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.377338 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.378090 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.380573 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.381204 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.381285 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.381323 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.381383 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.381515 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.381626 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.381676 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.383573 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.383668 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.386118 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.386202 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.386313 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.388606 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.390520 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.390619 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.390906 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.390999 140530918526976 decoder_stack.py:344] dstack: Final layernorm.
I0124 00:28:21.393919 140530918526976 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:21.450472 140530918526976 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.450570 140530918526976 decoder_stack.py:333] dstack: autoregressive generator.
I0124 00:28:21.450627 140530918526976 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0124 00:28:21.450735 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.450775 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.450808 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.450873 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.453550 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.458978 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.459246 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.461852 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.474371 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.474431 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.474470 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.474502 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.474564 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.475118 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.475199 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.475557 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.476240 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.478743 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.479362 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.479442 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.479479 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.479538 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.479669 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.479790 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.479833 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.481684 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.481787 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.484180 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.484261 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.484374 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.486653 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.488532 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.488630 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.488918 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.489003 140530918526976 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0124 00:28:21.489114 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.489155 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.489189 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.489254 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.491557 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.496958 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.497223 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.499868 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.512276 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.512342 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.512380 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.512413 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.512476 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.513033 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.513113 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.513470 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.514163 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.516664 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.517284 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.517366 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.517404 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.517465 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.517596 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.517715 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.517765 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.519621 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.519720 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.522126 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.522210 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.522322 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.524575 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.526447 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.526546 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.526832 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.526918 140530918526976 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0124 00:28:21.527027 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.527067 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.527100 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.527163 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.529405 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.534844 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.535109 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.537800 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.550293 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.550354 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.550392 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.550425 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.550488 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.551041 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.551121 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.551473 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.552152 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.554657 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.555273 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.555353 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.555390 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.555450 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.555579 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.555690 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.555733 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.557565 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.557669 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.560048 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.560132 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.560242 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.562949 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.564817 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.564916 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.565202 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.565287 140530918526976 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0124 00:28:21.565398 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.565439 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.565473 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.565537 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.567776 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.573135 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.573400 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.576075 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.588656 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.588716 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.588755 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.588798 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.588864 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.589423 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.589503 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.589865 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.590560 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.593093 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.593732 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.593815 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.593851 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.593911 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.594038 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.594148 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.594189 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.596072 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.596168 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.598567 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.598649 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.598759 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.601057 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.602941 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.603038 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.603323 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.603408 140530918526976 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0124 00:28:21.603516 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.603556 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.603587 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.603650 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.605887 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.611320 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.611585 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.614287 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.627023 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.627081 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.627118 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.627150 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.627215 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.627768 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.627846 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.628195 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.628887 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.631424 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.632057 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.632138 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.632174 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.632235 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.632367 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.632479 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.632519 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.634420 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.634524 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.636918 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.636999 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.637110 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.639418 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.641287 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.641386 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.641679 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.641769 140530918526976 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0124 00:28:21.641879 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.641919 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.641952 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.642015 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.644257 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.649675 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.649939 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.652614 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.665383 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.665442 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.665480 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.665513 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.665576 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.666148 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.666229 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.666591 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.667288 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.669844 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.670466 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.670545 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.670581 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.670639 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.670769 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.670881 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.670922 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.672796 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.672900 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.675316 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.675398 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.675508 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.678223 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.680108 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.680205 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.680493 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.680576 140530918526976 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0124 00:28:21.680686 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.680725 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.680757 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.680820 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.683109 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.688557 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.688822 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.691531 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.704261 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.704320 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.704358 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.704389 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.704451 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.705018 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.705097 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.705454 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.706147 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.708662 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.709295 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.709374 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.709410 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.709471 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.709602 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.709722 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.709765 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.711637 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.711733 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.714196 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.714279 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.714390 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.716705 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.718608 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.718706 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.718990 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.719073 140530918526976 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0124 00:28:21.719183 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.719222 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.719254 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.719316 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.721540 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.726998 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.727256 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.729956 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.742643 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.742703 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.742739 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.742772 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.742836 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.743400 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.743479 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.743832 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.744521 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.747063 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.747692 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.747772 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.747808 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.747869 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.747997 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.748111 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.748151 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.750038 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.750133 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.752512 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.752601 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.752712 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.754997 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.756855 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.756952 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.757235 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.757317 140530918526976 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0124 00:28:21.757425 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.757465 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.757497 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.757559 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.759789 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.765200 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.765463 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.768150 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.780753 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.780812 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.780853 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.780886 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.780948 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.781501 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.781579 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.781944 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.782633 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.785150 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.785773 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.785854 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.785889 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.785947 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.786075 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.786186 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.786226 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.788095 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.788190 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.790596 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.790687 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.790799 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.793450 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.795319 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.795418 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.795701 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.795787 140530918526976 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0124 00:28:21.795896 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.795936 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.795967 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.796029 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.798247 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.803650 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.803914 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.806605 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.819307 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.819367 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.819404 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.819437 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.819499 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.820055 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.820133 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.820485 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.821172 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.823688 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.824312 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.824392 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.824427 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.824485 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.824616 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.824727 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.824768 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.827211 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.827310 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.829699 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.829781 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.829900 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.832142 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.834003 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.834100 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.834383 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.834466 140530918526976 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0124 00:28:21.834572 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.834613 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.834644 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.834707 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.836915 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.842309 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.842575 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.845229 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.857881 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.857939 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.857977 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.858009 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.858072 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.858626 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.858704 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.859060 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.859748 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.862319 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.862940 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.863020 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.863056 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.863115 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.863243 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.863351 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.863391 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.865269 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.865364 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.867823 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.867905 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.868014 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.870318 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.872162 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.872258 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.872541 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.872624 140530918526976 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0124 00:28:21.872731 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:21.872771 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:21.872802 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:21.872866 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.875089 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:21.880650 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.880915 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:21.883595 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:21.896564 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:21.896622 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:21.896659 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:21.896691 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.896754 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.897309 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.897389 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.897756 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.898480 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.901041 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.901669 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.901750 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:21.901798 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:21.901867 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.902003 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:21.902112 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:21.902151 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.904091 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.904186 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.906566 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.906648 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:21.906757 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:21.909429 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:21.911361 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.911479 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:21.911772 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:21.911858 140530918526976 decoder_stack.py:344] dstack: Final layernorm.
I0124 00:28:21.914689 140530918526976 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:26.343532 140530918526976 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0124 00:28:26.912065 140530918526976 training_loop.py:409] No working directory specified.
I0124 00:28:26.912189 140530918526976 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0124 00:28:26.912954 140530918526976 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0124 00:28:30.271970 140530918526976 training_loop.py:447] Only restoring trainable parameters.
I0124 00:28:30.272626 140530918526976 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0124 00:28:30.272714 140530918526976 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.272767 140530918526976 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.272814 140530918526976 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.272857 140530918526976 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.272899 140530918526976 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.272939 140530918526976 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.272981 140530918526976 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273020 140530918526976 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.273059 140530918526976 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.273098 140530918526976 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273136 140530918526976 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.273175 140530918526976 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.273213 140530918526976 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.273251 140530918526976 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273289 140530918526976 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.273329 140530918526976 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273367 140530918526976 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273405 140530918526976 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.273442 140530918526976 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.273498 140530918526976 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273540 140530918526976 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.273579 140530918526976 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.273616 140530918526976 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.273666 140530918526976 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273708 140530918526976 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.273745 140530918526976 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273781 140530918526976 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273817 140530918526976 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.273854 140530918526976 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.273891 140530918526976 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.273928 140530918526976 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.273966 140530918526976 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.274003 140530918526976 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.274040 140530918526976 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274079 140530918526976 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.274117 140530918526976 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274154 140530918526976 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274191 140530918526976 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.274227 140530918526976 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.274264 140530918526976 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274301 140530918526976 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.274337 140530918526976 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.274373 140530918526976 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.274409 140530918526976 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274446 140530918526976 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.274490 140530918526976 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274528 140530918526976 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274564 140530918526976 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.274601 140530918526976 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.274639 140530918526976 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274676 140530918526976 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.274712 140530918526976 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.274748 140530918526976 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.274785 140530918526976 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274822 140530918526976 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.274859 140530918526976 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274895 140530918526976 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.274933 140530918526976 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.274969 140530918526976 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.275006 140530918526976 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275042 140530918526976 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.275079 140530918526976 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.275114 140530918526976 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.275150 140530918526976 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275188 140530918526976 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.275225 140530918526976 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275261 140530918526976 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275298 140530918526976 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.275334 140530918526976 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.275371 140530918526976 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275409 140530918526976 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.275447 140530918526976 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.275490 140530918526976 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.275531 140530918526976 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275568 140530918526976 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.275604 140530918526976 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275641 140530918526976 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275678 140530918526976 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.275715 140530918526976 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.275752 140530918526976 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275788 140530918526976 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.275825 140530918526976 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.275861 140530918526976 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.275898 140530918526976 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.275934 140530918526976 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.275971 140530918526976 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276007 140530918526976 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276043 140530918526976 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.276079 140530918526976 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.276116 140530918526976 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276152 140530918526976 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.276188 140530918526976 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.276223 140530918526976 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.276260 140530918526976 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276296 140530918526976 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.276333 140530918526976 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276369 140530918526976 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276405 140530918526976 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.276442 140530918526976 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.276485 140530918526976 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276524 140530918526976 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.276562 140530918526976 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.276598 140530918526976 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.276636 140530918526976 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276673 140530918526976 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.276709 140530918526976 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276746 140530918526976 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276781 140530918526976 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.276818 140530918526976 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.276855 140530918526976 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.276891 140530918526976 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.276927 140530918526976 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0124 00:28:30.276964 140530918526976 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0124 00:28:30.277001 140530918526976 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.277038 140530918526976 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.277074 140530918526976 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.277110 140530918526976 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.277148 140530918526976 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0124 00:28:30.277186 140530918526976 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0124 00:28:30.277222 140530918526976 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0124 00:28:30.277259 140530918526976 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0124 00:28:30.277287 140530918526976 training_loop.py:725] Total parameters: 152072288
I0124 00:28:30.277522 140530918526976 training_loop.py:739] Total state size: 0
I0124 00:28:30.304560 140530918526976 training_loop.py:492] Training loop: creating task for mode beam_search
I0124 00:28:30.304783 140530918526976 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0124 00:28:30.305224 140530918526976 training_loop.py:652] Compiling mode beam_search with jit.
I0124 00:28:30.305581 140530918526976 training_loop.py:89] registering functions: dict_keys([])
I0124 00:28:30.323131 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a ? coll i c h
I0124 00:28:30.480562 140530918526976 ddar.py:60] Depth 1/1000 time = 0.1421043872833252
I0124 00:28:31.084940 140530918526976 ddar.py:60] Depth 2/1000 time = 0.6042635440826416
I0124 00:28:32.306280 140530918526976 ddar.py:60] Depth 3/1000 time = 1.2211687564849854
I0124 00:28:33.484892 140530918526976 ddar.py:60] Depth 4/1000 time = 1.1784327030181885
I0124 00:28:34.818462 140530918526976 ddar.py:60] Depth 5/1000 time = 1.3332269191741943
I0124 00:28:34.828732 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:28:34.828829 140530918526976 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0124 00:28:34.828869 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00
I0124 00:28:34.828903 140530918526976 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00
I0124 00:28:34.951225 140530918526976 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.951418 140530918526976 decoder_stack.py:316] dstack: scanning over 1 windows.
I0124 00:28:34.951534 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.951626 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.951699 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.951770 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.951838 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.951906 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.951973 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.952039 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.952105 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.952171 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.952237 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.952302 140530918526976 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0124 00:28:34.952341 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:34.952386 140530918526976 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0124 00:28:34.952491 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:34.952531 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:34.952562 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:34.954421 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.957013 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:34.962745 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.963020 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:34.965639 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:34.970020 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:34.970080 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:34.970118 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:34.970150 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.970214 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.970849 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.970928 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.971282 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.972067 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.974547 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.975258 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.975338 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:34.975374 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:34.975432 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.975577 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:34.975911 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:34.975955 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:34.977863 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.977960 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:34.980450 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.980530 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:34.980947 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:34.983341 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:34.985297 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.985395 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:34.985690 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.985773 140530918526976 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0124 00:28:34.985882 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:34.985921 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:34.985952 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:34.987791 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.990081 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:34.995866 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:34.996127 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:34.998695 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.002423 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.002484 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.002522 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.002553 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.002616 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.003235 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.003314 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.003672 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.004443 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.006886 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.007515 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.007597 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.007633 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.007692 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.007820 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.008138 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.008183 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.010149 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.010246 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.012677 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.012760 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.013186 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.015484 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.017399 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.017495 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.017790 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.017879 140530918526976 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0124 00:28:35.017989 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.018028 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.018061 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.019932 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.022238 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.027771 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.028026 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.030586 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.034326 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.034394 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.034433 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.034465 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.034528 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.035086 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.035165 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.035520 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.036284 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.038708 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.039326 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.039404 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.039439 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.039498 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.039625 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.039991 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.040038 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.041949 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.042044 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.044484 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.044566 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.044989 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.047276 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.049262 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.049359 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.049653 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.049738 140530918526976 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0124 00:28:35.049848 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.049888 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.049921 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.051695 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.053988 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.059620 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.059878 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.062427 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.066104 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.066163 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.066199 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.066239 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.066303 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.066916 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.066994 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.067347 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.068109 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.070529 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.071151 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.071230 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.071266 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.071325 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.071453 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.071780 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.071826 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.074174 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.074272 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.076699 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.076781 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.077201 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.079482 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.081396 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.081494 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.081792 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.081879 140530918526976 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0124 00:28:35.081990 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.082029 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.082062 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.083934 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.086248 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.091765 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.092025 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.094632 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.098320 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.098378 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.098415 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.098446 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.098516 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.099081 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.099161 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.099514 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.100271 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.102722 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.103396 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.103477 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.103513 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.103571 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.103698 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.104013 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.104059 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.105959 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.106055 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.108458 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.108540 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.108961 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.111276 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.113169 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.113266 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.113552 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.113634 140530918526976 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0124 00:28:35.113762 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.113803 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.113836 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.115602 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.117910 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.123534 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.123795 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.126359 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.130019 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.130077 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.130114 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.130145 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.130265 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.130820 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.130898 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.131249 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.131998 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.134421 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.135040 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.135122 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.135157 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.135215 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.135363 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.135690 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.135736 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.137730 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.137826 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.140275 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.140356 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.140778 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.143047 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.144958 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.145056 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.145346 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.145430 140530918526976 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0124 00:28:35.145540 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.145581 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.145612 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.147459 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.149787 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.155384 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.155648 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.158250 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.161934 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.161993 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.162029 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.162060 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.162122 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.162688 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.162768 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.163120 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.163876 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.166323 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.166997 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.167078 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.167115 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.167173 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.167304 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.167623 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.167669 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.169589 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.169693 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.172122 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.172204 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.172621 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.174944 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.176848 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.176946 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.177234 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.177317 140530918526976 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0124 00:28:35.177426 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.177466 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.177499 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.179288 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.181577 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.187615 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.187878 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.190459 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.194115 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.194174 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.194211 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.194242 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.194353 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.194912 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.194998 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.195361 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.196132 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.198597 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.199218 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.199297 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.199332 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.199390 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.199518 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.199836 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.199882 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.201861 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.201958 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.204390 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.204472 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.204897 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.207152 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.209037 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.209135 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.209421 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.209503 140530918526976 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0124 00:28:35.209612 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.209658 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.209693 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.211539 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.213858 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.219441 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.219702 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.222301 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.225991 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.226049 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.226085 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.226116 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.226179 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.226736 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.226828 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.227191 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.227964 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.230406 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.231029 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.231109 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.231145 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.231204 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.231331 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.231701 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.231748 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.233651 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.233752 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.236184 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.236265 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.236687 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.238923 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.240890 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.240988 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.241278 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.241362 140530918526976 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0124 00:28:35.241472 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.241511 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.241544 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.243334 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.245620 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.251358 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.251618 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.254192 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.257871 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.257929 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.257965 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.257997 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.258060 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.258673 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.258754 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.259118 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.259880 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.262329 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.262950 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.263031 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.263067 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.263126 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.263252 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.263573 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.263620 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.265524 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.265619 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.268120 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.268202 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.268629 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.270902 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.272814 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.272912 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.273199 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.273283 140530918526976 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0124 00:28:35.273392 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.273432 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.273464 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.275269 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.277699 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.283297 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.283558 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.286138 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.289806 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.289864 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.289900 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.289931 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.290045 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.290605 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.290685 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.291039 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.291808 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.294250 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.294872 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.294951 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.294986 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.295045 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.295174 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.295494 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.295541 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.297895 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.297992 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.300428 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.300510 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.300936 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.303216 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.305121 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.305218 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.305506 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.305591 140530918526976 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0124 00:28:35.305707 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.305750 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.305783 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.307654 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.309970 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.315575 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.315838 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.318421 140530918526976 transformer_layer.py:213] tlayer: windowed attention.
I0124 00:28:35.322187 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.322245 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.322281 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.322314 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.322376 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.322930 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.323009 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.323368 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.324143 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.326573 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.327191 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.327271 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.327306 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.327364 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.327492 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.327815 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.327861 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.329844 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.329941 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.332364 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.332448 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.332872 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.335132 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.337030 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.337128 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.337416 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.337677 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.337750 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.337808 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.337864 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.337918 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.337972 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.338023 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.338076 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.338129 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.338182 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.338234 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.338286 140530918526976 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0124 00:28:35.338324 140530918526976 decoder_stack.py:344] dstack: Final layernorm.
I0124 00:28:35.341207 140530918526976 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0124 00:28:35.386041 140530918526976 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.386130 140530918526976 decoder_stack.py:333] dstack: autoregressive generator.
I0124 00:28:35.386185 140530918526976 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0124 00:28:35.386289 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.386336 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.386368 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.386430 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.388757 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.394124 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.394389 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.396931 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.409658 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.409718 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.409753 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.409786 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.409848 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.410404 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.410484 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.410838 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.411523 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.414038 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.414663 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.414742 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.414778 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.414839 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.414971 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.415080 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.415120 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.416971 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.417066 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.419473 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.419557 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.419667 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.421922 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.423771 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.423869 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.424156 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.424240 140530918526976 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0124 00:28:35.424348 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.424388 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.424427 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.424493 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.426745 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.432147 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.432414 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.435092 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.447632 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.447694 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.447730 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.447761 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.447823 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.448434 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.448515 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.448882 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.449568 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.452036 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.452657 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.452737 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.452773 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.452831 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.452959 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.453066 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.453104 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.454951 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.455046 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.457475 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.457556 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.457672 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.459862 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.461724 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.461821 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.462110 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.462193 140530918526976 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0124 00:28:35.462301 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.462341 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.462381 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.462445 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.464669 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.470155 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.470419 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.472984 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.485909 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.485967 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.486003 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.486034 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.486095 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.486643 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.486722 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.487079 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.487770 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.490212 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.490831 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.490910 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.490945 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.491003 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.491132 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.491243 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.491282 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.493192 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.493290 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.495696 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.495779 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.495889 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.498087 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.499930 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.500027 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.500316 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.500400 140530918526976 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0124 00:28:35.500509 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.500548 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.500581 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.500653 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.502950 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.508347 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.508616 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.511198 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.523773 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.523833 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.523869 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.523901 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.523963 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.524518 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.524598 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.524953 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.525648 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.528140 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.528754 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.528834 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.528870 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.528929 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.529058 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.529168 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.529208 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.531062 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.531158 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.533541 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.533623 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.533741 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.535994 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.537853 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.537951 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.538239 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.538334 140530918526976 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0124 00:28:35.538451 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.538490 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.538523 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.538585 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.540884 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.546305 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.546572 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.549209 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.561714 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.561773 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.561809 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.561841 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.561903 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.562516 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.562595 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.562952 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.563643 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.566056 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.566673 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.566752 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.566788 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.566845 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.566974 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.567084 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.567123 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.568959 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.569054 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.571501 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.571584 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.571695 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.573882 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.575723 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.575819 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.576107 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.576188 140530918526976 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0124 00:28:35.576297 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.576339 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.576371 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.576432 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.578668 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.584508 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.584774 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.587370 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.599895 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.599954 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.599991 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.600023 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.600085 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.600641 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.600720 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.601075 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.601824 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.604259 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.604877 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.604957 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.604993 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.605051 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.605180 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.605289 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.605329 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.607232 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.607329 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.609710 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.609796 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.609906 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.612074 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.613924 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.614021 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.614311 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.614395 140530918526976 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0124 00:28:35.614505 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.614545 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.614577 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.614639 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.616927 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.622360 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.622624 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.625174 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.637681 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.637740 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.637777 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.637809 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.637872 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.638426 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.638505 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.638861 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.639543 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.642018 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.642637 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.642717 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.642751 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.642809 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.642938 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.643046 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.643086 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.644924 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.645019 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.647401 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.647483 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.647592 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.649854 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.651702 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.651799 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.652086 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.652169 140530918526976 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0124 00:28:35.652276 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.652315 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.652347 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.652409 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.654633 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.660022 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.660293 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.662929 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.675431 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.675491 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.675528 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.675558 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.675620 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.676234 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.676314 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.676681 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.677365 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.679833 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.680453 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.680532 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.680567 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.680625 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.680752 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.680859 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.680898 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.682751 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.682846 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.685653 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.685739 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.685849 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.688032 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.689895 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.689992 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.690279 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.690360 140530918526976 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0124 00:28:35.690468 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.690507 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.690541 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.690602 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.692806 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.698307 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.698577 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.701110 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.713575 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.713634 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.713679 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.713715 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.713777 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.714331 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.714409 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.714766 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.715453 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.717965 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.718586 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.718665 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.718700 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.718759 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.718888 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.718996 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.719035 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.720947 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.721042 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.723428 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.723511 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.723623 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.725806 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.727653 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.727749 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.728038 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.728121 140530918526976 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0124 00:28:35.728229 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.728269 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.728301 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.728363 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.730572 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.735988 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.736263 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.738814 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.751229 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.751289 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.751326 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.751358 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.751420 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.751973 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.752053 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.752406 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.753090 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.755501 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.756116 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.756195 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.756231 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.756288 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.756417 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.756528 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.756567 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.758499 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.758596 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.760978 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.761059 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.761169 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.763371 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.765223 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.765321 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.765614 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.765706 140530918526976 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0124 00:28:35.765817 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.765856 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.765887 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.765949 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.768159 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.773618 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.773893 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.776469 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.789009 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.789069 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.789105 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.789136 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.789199 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.789790 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.789870 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.790225 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.790936 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.793352 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.793977 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.794059 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.794094 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.794152 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.794280 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.794389 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.794427 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.796716 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.796812 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.799216 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.799298 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.799409 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.801579 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.803435 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.803535 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.803824 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.803907 140530918526976 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0124 00:28:35.804016 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.804055 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.804087 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.804149 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.806351 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.811774 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.812039 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.814606 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.827147 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.827206 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.827243 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.827275 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.827338 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.827890 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.827968 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.828323 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.829010 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.831440 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.832049 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.832128 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.832164 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.832221 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.832349 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.832458 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.832496 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.834404 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.834500 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.836861 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.836942 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.837052 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.839243 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.841076 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.841171 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.841458 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.841543 140530918526976 decoder_stack.py:344] dstack: Final layernorm.
I0124 00:28:35.844367 140530918526976 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0124 00:28:35.894687 140530918526976 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.894780 140530918526976 decoder_stack.py:333] dstack: autoregressive generator.
I0124 00:28:35.894838 140530918526976 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0124 00:28:35.894944 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.894984 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.895014 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.895077 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.897370 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.902880 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.903143 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.905714 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.918390 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.918449 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.918488 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.918520 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.918583 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.919149 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.919227 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.919588 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.920273 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.922740 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.923359 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.923440 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.923476 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.923536 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.923667 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.923778 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.923818 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.925755 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.925850 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.928243 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.928324 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.928436 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.930649 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.932499 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.932597 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.932888 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.932969 140530918526976 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0124 00:28:35.933077 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.933116 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.933148 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.933210 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.935432 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.940935 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.941203 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.943784 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.956245 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.956304 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.956342 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.956373 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.956437 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.956994 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.957072 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.957428 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.958123 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.960529 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.961147 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.961225 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.961261 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.961320 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.961448 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.961556 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.961595 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.963503 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.963600 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.965983 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.966065 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:35.966174 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:35.968339 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:35.970194 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.970293 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:35.970584 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.970669 140530918526976 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0124 00:28:35.970777 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:35.970816 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:35.970848 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:35.970910 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.973108 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:35.978526 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.978801 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:35.981338 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:35.994389 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:35.994448 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:35.994485 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:35.994517 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.994580 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.995130 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.995209 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.995562 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.996237 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.998638 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.999253 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.999334 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:35.999369 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:35.999427 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:35.999555 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:35.999664 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:35.999703 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.001596 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.001700 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.004084 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.004165 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.004276 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.006464 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.008310 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.008407 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.008693 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.008776 140530918526976 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0124 00:28:36.008884 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:36.008924 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:36.008956 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:36.009018 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.011237 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:36.016678 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.016955 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:36.019525 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:36.031955 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:36.032015 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:36.032052 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:36.032084 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.032147 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.032700 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.032779 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.033131 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.033823 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.036227 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.036844 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.036923 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:36.036959 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:36.037016 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.037144 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:36.037252 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:36.037290 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.039200 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.039297 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.041691 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.041774 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.041884 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.044056 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.045916 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.046014 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.046302 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.046386 140530918526976 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0124 00:28:36.046494 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:36.046534 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:36.046567 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:36.046628 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.048842 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:36.054302 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.054567 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:36.057142 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:36.069540 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:36.069600 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:36.069636 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:36.069678 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.069741 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.070296 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.070376 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.070733 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.071412 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.073827 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.074445 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.074525 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:36.074561 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:36.074618 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.074745 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:36.074854 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:36.074893 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.076792 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.076887 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.079260 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.079342 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.079452 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.081618 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.083466 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.083563 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.083849 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.083932 140530918526976 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0124 00:28:36.084040 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:36.084079 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:36.084111 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:36.084174 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.086387 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:36.091803 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.092069 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:36.094632 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:36.107556 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:36.107614 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:36.107651 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:36.107683 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.107747 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.108303 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.108383 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.108738 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.109428 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.111874 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.112489 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.112569 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:36.112604 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:36.112663 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.112790 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:36.112899 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:36.112938 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.114856 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.114952 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.117329 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.117412 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.117523 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.119713 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.121551 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.121654 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.121945 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.122029 140530918526976 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0124 00:28:36.122138 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:36.122177 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:36.122209 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:36.122272 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.124460 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:36.129897 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.130160 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:36.132711 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:36.145121 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:36.145181 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:36.145218 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:36.145251 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.145313 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.145879 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.145959 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.146312 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.146993 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.149403 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.150030 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.150111 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:36.150146 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:36.150204 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.150331 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:36.150439 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:36.150477 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.152378 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.152472 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.154842 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.154924 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.155034 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.157205 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.159052 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.159151 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.159439 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.159524 140530918526976 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0124 00:28:36.159633 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:36.159673 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:36.159705 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:36.159767 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.161989 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:36.167381 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.167644 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:36.170193 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:36.182609 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:36.182678 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:36.182715 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:36.182747 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.182810 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.183364 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.183443 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.183796 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.184477 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.186909 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.187524 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.187603 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:36.187638 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:36.187696 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.187825 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:36.187934 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:36.187973 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.189897 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.189992 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.192365 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.192445 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.192555 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.194736 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.196562 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.196657 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.196944 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.197024 140530918526976 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0124 00:28:36.197132 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:36.197171 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:36.197203 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:36.197266 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.199477 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:36.204890 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.205153 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:36.207711 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:36.220634 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:36.220692 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:36.220737 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:36.220771 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.220834 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.221397 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.221476 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.221844 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.222530 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.224951 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.225563 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.225647 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:36.225684 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:36.225749 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.225876 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:36.225985 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:36.226025 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.227943 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.228037 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.230410 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.230490 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.230600 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.232782 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.234640 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.234735 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.235022 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.235102 140530918526976 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0124 00:28:36.235209 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:36.235249 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:36.235281 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:36.235344 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.237549 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:36.243055 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.243320 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:36.245888 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:36.258386 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:36.258445 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:36.258488 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:36.258522 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.258584 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.259140 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.259219 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.259577 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.260255 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.262679 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.263298 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.263376 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:36.263411 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:36.263468 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.263595 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:36.263703 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:36.263742 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.265666 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.265760 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.268132 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.268211 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.268321 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.270497 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.272338 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.272432 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.272716 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.272797 140530918526976 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0124 00:28:36.272906 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:36.272945 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:36.272977 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:36.273039 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.275260 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:36.280704 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.280967 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:36.283547 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:36.296044 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:36.296104 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:36.296142 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:36.296183 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.296246 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.296800 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.296878 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.297230 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.297926 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.300358 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.300974 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.301053 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:36.301089 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:36.301147 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.301274 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:36.301384 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:36.301423 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.303338 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.303432 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.305804 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.305884 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.305993 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.308166 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.310020 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.310116 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.310403 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.310484 140530918526976 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0124 00:28:36.310592 140530918526976 transformer_layer.py:154] tlayer: recurrent = False
I0124 00:28:36.310632 140530918526976 transformer_layer.py:155] tlayer: compute_importance = False
I0124 00:28:36.310664 140530918526976 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0124 00:28:36.310726 140530918526976 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.312924 140530918526976 transformer_base.py:161] kvq: pre_attn dropout.
I0124 00:28:36.318392 140530918526976 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.318658 140530918526976 transformer_base.py:194] kvq: normalize keys, queries.
I0124 00:28:36.321217 140530918526976 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0124 00:28:36.334120 140530918526976 transformer_layer.py:299] tlayer: num_windows = 1.
I0124 00:28:36.334181 140530918526976 attention.py:418] Single window, no scan.
I0124 00:28:36.334218 140530918526976 transformer_layer.py:389] tlayer: self-attention.
I0124 00:28:36.334250 140530918526976 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.334323 140530918526976 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.334886 140530918526976 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.334964 140530918526976 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.335318 140530918526976 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.336003 140530918526976 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.338435 140530918526976 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.339056 140530918526976 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.339134 140530918526976 transformer_layer.py:468] tlayer: End windows.
I0124 00:28:36.339171 140530918526976 transformer_layer.py:472] tlayer: final FFN.
I0124 00:28:36.339228 140530918526976 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.339355 140530918526976 transformer_base.py:410] tbase: post-attention MLP.
I0124 00:28:36.339466 140530918526976 nn_components.py:325] mlp: activation = None
I0124 00:28:36.339506 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.341432 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.341526 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.343910 140530918526976 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.343991 140530918526976 transformer_base.py:443] tbase: final FFN
I0124 00:28:36.344101 140530918526976 nn_components.py:320] mlp: hidden 4096, relu
I0124 00:28:36.346295 140530918526976 nn_components.py:329] mlp: final activation = None
I0124 00:28:36.348144 140530918526976 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.348239 140530918526976 nn_components.py:261] mlp: residual
I0124 00:28:36.348528 140530918526976 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:36.348616 140530918526976 decoder_stack.py:344] dstack: Final layernorm.
I0124 00:28:36.351443 140530918526976 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0124 00:28:48.545851 140530918526976 alphageometry.py:566] LM output (score=-0.655544): "c : T a b a c 12 ;"
I0124 00:28:48.546135 140530918526976 alphageometry.py:567] Translation: "ERROR: point c already exists."

I0124 00:28:48.546187 140530918526976 alphageometry.py:566] LM output (score=-1.624107): "e : D d e d f 12 ;"
I0124 00:28:48.546219 140530918526976 alphageometry.py:567] Translation: "ERROR: point e already exists."

I0124 00:28:48.546247 140530918526976 alphageometry.py:566] LM output (score=-1.721739): "j : C a d j 12 D a j d j 13 ;"
I0124 00:28:48.546274 140530918526976 alphageometry.py:567] Translation: "j = on_line j a d, on_bline j d a"

I0124 00:28:48.546308 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a ? coll i c h"
I0124 00:28:48.546434 140530918526976 graph.py:498] 
I0124 00:28:48.546488 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a ? coll i c h
I0124 00:28:48.768464 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2083902359008789
I0124 00:28:49.912474 140530918526976 ddar.py:60] Depth 2/1000 time = 1.1438992023468018
I0124 00:28:52.144011 140530918526976 ddar.py:60] Depth 3/1000 time = 2.231356143951416
I0124 00:28:54.550512 140530918526976 ddar.py:60] Depth 4/1000 time = 2.4063220024108887
I0124 00:28:57.137306 140530918526976 ddar.py:60] Depth 5/1000 time = 2.5863358974456787
I0124 00:28:57.154163 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:28:57.154273 140530918526976 alphageometry.py:566] LM output (score=-1.828553): "j : C a h j 12 D a j h j 13 ;"
I0124 00:28:57.154314 140530918526976 alphageometry.py:567] Translation: "j = on_line j a h, on_bline j h a"

I0124 00:28:57.154352 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a h, on_bline j h a ? coll i c h"
I0124 00:28:57.154471 140530918526976 graph.py:498] 
I0124 00:28:57.154520 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a h, on_bline j h a ? coll i c h
I0124 00:28:57.374234 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2058262825012207
I0124 00:28:58.246337 140530918526976 ddar.py:60] Depth 2/1000 time = 0.8719868659973145
I0124 00:28:59.715727 140530918526976 ddar.py:60] Depth 3/1000 time = 1.4692072868347168
I0124 00:29:01.509028 140530918526976 ddar.py:60] Depth 4/1000 time = 1.793109655380249
I0124 00:29:03.320030 140530918526976 ddar.py:60] Depth 5/1000 time = 1.8106927871704102
I0124 00:29:03.332890 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:29:03.332987 140530918526976 alphageometry.py:566] LM output (score=-1.983870): "j : C a c j 12 D a j c j 13 ;"
I0124 00:29:03.333027 140530918526976 alphageometry.py:567] Translation: "j = on_line j a c, on_bline j c a"

I0124 00:29:03.333064 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a c, on_bline j c a ? coll i c h"
I0124 00:29:03.333185 140530918526976 graph.py:498] 
I0124 00:29:03.333233 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a c, on_bline j c a ? coll i c h
I0124 00:29:03.578726 140530918526976 ddar.py:60] Depth 1/1000 time = 0.23144960403442383
I0124 00:29:04.702231 140530918526976 ddar.py:60] Depth 2/1000 time = 1.123384714126587
I0124 00:29:06.546879 140530918526976 ddar.py:60] Depth 3/1000 time = 1.8444631099700928
I0124 00:29:08.365176 140530918526976 ddar.py:60] Depth 4/1000 time = 1.8181049823760986
I0124 00:29:10.384645 140530918526976 ddar.py:60] Depth 5/1000 time = 2.019103527069092
I0124 00:29:10.397882 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:29:10.397984 140530918526976 alphageometry.py:566] LM output (score=-1.985175): "c : T a b b c 12 ;"
I0124 00:29:10.398024 140530918526976 alphageometry.py:567] Translation: "ERROR: point c already exists."

I0124 00:29:10.398065 140530918526976 alphageometry.py:566] LM output (score=-1.997548): "j : C b d j 12 D b j d j 13 ;"
I0124 00:29:10.398094 140530918526976 alphageometry.py:567] Translation: "j = on_line j b d, on_bline j d b"

I0124 00:29:10.398128 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b d, on_bline j d b ? coll i c h"
I0124 00:29:10.398256 140530918526976 graph.py:498] 
I0124 00:29:10.398325 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b d, on_bline j d b ? coll i c h
I0124 00:29:10.631739 140530918526976 ddar.py:60] Depth 1/1000 time = 0.21977758407592773
I0124 00:29:11.942413 140530918526976 ddar.py:60] Depth 2/1000 time = 1.310490369796753
I0124 00:29:14.243508 140530918526976 ddar.py:60] Depth 3/1000 time = 2.300891399383545
I0124 00:29:16.550842 140530918526976 ddar.py:60] Depth 4/1000 time = 2.3071534633636475
I0124 00:29:18.853987 140530918526976 ddar.py:60] Depth 5/1000 time = 2.302746295928955
I0124 00:29:18.871427 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:29:18.871531 140530918526976 alphageometry.py:566] LM output (score=-2.029124): "d : D a b a d 12 D a b b d 13 ;"
I0124 00:29:18.871573 140530918526976 alphageometry.py:567] Translation: "ERROR: point d already exists."

I0124 00:29:18.871610 140530918526976 alphageometry.py:566] LM output (score=-2.050446): "j : C a e j 12 D a j e j 13 ;"
I0124 00:29:18.871640 140530918526976 alphageometry.py:567] Translation: "j = on_line j a e, on_bline j e a"

I0124 00:29:18.871673 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a e, on_bline j e a ? coll i c h"
I0124 00:29:18.871812 140530918526976 graph.py:498] 
I0124 00:29:18.871869 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a e, on_bline j e a ? coll i c h
I0124 00:29:19.121140 140530918526976 ddar.py:60] Depth 1/1000 time = 0.23446249961853027
I0124 00:29:20.295392 140530918526976 ddar.py:60] Depth 2/1000 time = 1.1741347312927246
I0124 00:29:22.170802 140530918526976 ddar.py:60] Depth 3/1000 time = 1.8752200603485107
I0124 00:29:24.038356 140530918526976 ddar.py:60] Depth 4/1000 time = 1.867379903793335
I0124 00:29:25.892710 140530918526976 ddar.py:60] Depth 5/1000 time = 1.8540222644805908
I0124 00:29:25.904989 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:29:25.905080 140530918526976 alphageometry.py:566] LM output (score=-2.078509): "j : C b h j 12 D b j h j 13 ;"
I0124 00:29:25.905119 140530918526976 alphageometry.py:567] Translation: "j = on_line j b h, on_bline j h b"

I0124 00:29:25.905157 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b h, on_bline j h b ? coll i c h"
I0124 00:29:25.905280 140530918526976 graph.py:498] 
I0124 00:29:25.905328 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b h, on_bline j h b ? coll i c h
I0124 00:29:26.123413 140530918526976 ddar.py:60] Depth 1/1000 time = 0.20416665077209473
I0124 00:29:26.998791 140530918526976 ddar.py:60] Depth 2/1000 time = 0.8752584457397461
I0124 00:29:28.627685 140530918526976 ddar.py:60] Depth 3/1000 time = 1.6287095546722412
I0124 00:29:30.236694 140530918526976 ddar.py:60] Depth 4/1000 time = 1.608820915222168
I0124 00:29:32.092519 140530918526976 ddar.py:60] Depth 5/1000 time = 1.8554437160491943
I0124 00:29:32.105206 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:29:32.105312 140530918526976 alphageometry.py:566] LM output (score=-2.298107): "j : C b c j 12 D b j c j 13 ;"
I0124 00:29:32.105351 140530918526976 alphageometry.py:567] Translation: "j = on_line j b c, on_bline j c b"

I0124 00:29:32.105393 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b c, on_bline j c b ? coll i c h"
I0124 00:29:32.105539 140530918526976 graph.py:498] 
I0124 00:29:32.105597 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b c, on_bline j c b ? coll i c h
I0124 00:29:32.560005 140530918526976 ddar.py:60] Depth 1/1000 time = 0.4400479793548584
I0124 00:29:33.727994 140530918526976 ddar.py:60] Depth 2/1000 time = 1.1678063869476318
I0124 00:29:35.406216 140530918526976 ddar.py:60] Depth 3/1000 time = 1.6780383586883545
I0124 00:29:37.303033 140530918526976 ddar.py:60] Depth 4/1000 time = 1.89662766456604
I0124 00:29:39.426857 140530918526976 ddar.py:60] Depth 5/1000 time = 2.1234683990478516
I0124 00:29:39.440247 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:29:39.440333 140530918526976 alphageometry.py:566] LM output (score=-2.335266): "c : D d c d e 12 ;"
I0124 00:29:39.440371 140530918526976 alphageometry.py:567] Translation: "ERROR: point c already exists."

I0124 00:29:39.440405 140530918526976 alphageometry.py:566] LM output (score=-2.588451): "f : D d f d e 12 ;"
I0124 00:29:39.440433 140530918526976 alphageometry.py:567] Translation: "ERROR: point f already exists."

I0124 00:29:39.440459 140530918526976 alphageometry.py:566] LM output (score=-2.594143): "j : C b e j 12 D b j e j 13 ;"
I0124 00:29:39.440485 140530918526976 alphageometry.py:567] Translation: "j = on_line j b e, on_bline j e b"

I0124 00:29:39.440514 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b e, on_bline j e b ? coll i c h"
I0124 00:29:39.440633 140530918526976 graph.py:498] 
I0124 00:29:39.440680 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b e, on_bline j e b ? coll i c h
I0124 00:29:39.648733 140530918526976 ddar.py:60] Depth 1/1000 time = 0.19420623779296875
I0124 00:29:40.526663 140530918526976 ddar.py:60] Depth 2/1000 time = 0.8778173923492432
I0124 00:29:41.938245 140530918526976 ddar.py:60] Depth 3/1000 time = 1.4113965034484863
I0124 00:29:43.620187 140530918526976 ddar.py:60] Depth 4/1000 time = 1.681748628616333
I0124 00:29:45.097057 140530918526976 ddar.py:60] Depth 5/1000 time = 1.4765233993530273
I0124 00:29:45.114350 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:29:45.114564 140530918526976 alphageometry.py:566] LM output (score=-2.647455): "j : C d g j 12 D d g d j 13 ;"
I0124 00:29:45.114603 140530918526976 alphageometry.py:567] Translation: "j = on_line j d g, on_circle j d g"

I0124 00:29:45.114658 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j d g, on_circle j d g ? coll i c h"
I0124 00:29:45.114804 140530918526976 graph.py:498] 
I0124 00:29:45.114859 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j d g, on_circle j d g ? coll i c h
I0124 00:29:45.615349 140530918526976 ddar.py:60] Depth 1/1000 time = 0.48666810989379883
I0124 00:29:46.952833 140530918526976 ddar.py:60] Depth 2/1000 time = 1.337284803390503
I0124 00:29:50.216904 140530918526976 ddar.py:60] Depth 3/1000 time = 3.2638626098632812
I0124 00:29:53.002468 140530918526976 ddar.py:60] Depth 4/1000 time = 2.785362958908081
I0124 00:29:55.777665 140530918526976 ddar.py:60] Depth 5/1000 time = 2.7748138904571533
I0124 00:29:55.794854 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:29:55.794966 140530918526976 alphageometry.py:566] LM output (score=-2.669453): "e : T a c d e 12 ;"
I0124 00:29:55.795007 140530918526976 alphageometry.py:567] Translation: "ERROR: point e already exists."

I0124 00:29:55.795043 140530918526976 alphageometry.py:566] LM output (score=-2.713159): "f : D d e d f 12 ;"
I0124 00:29:55.795070 140530918526976 alphageometry.py:567] Translation: "ERROR: point f already exists."

I0124 00:29:55.795099 140530918526976 alphageometry.py:566] LM output (score=-2.722369): "e : T a e b c 12 ;"
I0124 00:29:55.795125 140530918526976 alphageometry.py:567] Translation: "ERROR: point e already exists."

I0124 00:29:55.795151 140530918526976 alphageometry.py:566] LM output (score=-2.822086): "j : C c d j 12 D c j d j 13 ;"
I0124 00:29:55.795177 140530918526976 alphageometry.py:567] Translation: "j = on_line j c d, on_bline j d c"

I0124 00:29:55.795207 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j c d, on_bline j d c ? coll i c h"
I0124 00:29:55.795326 140530918526976 graph.py:498] 
I0124 00:29:55.795372 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j c d, on_bline j d c ? coll i c h
I0124 00:29:56.021591 140530918526976 ddar.py:60] Depth 1/1000 time = 0.21187090873718262
I0124 00:29:57.089961 140530918526976 ddar.py:60] Depth 2/1000 time = 1.0682499408721924
I0124 00:29:58.888836 140530918526976 ddar.py:60] Depth 3/1000 time = 1.798691987991333
I0124 00:30:01.053421 140530918526976 ddar.py:60] Depth 4/1000 time = 2.164412260055542
I0124 00:30:03.216876 140530918526976 ddar.py:60] Depth 5/1000 time = 2.163058280944824
I0124 00:30:03.232414 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:30:03.232504 140530918526976 alphageometry.py:566] LM output (score=-2.857819): "j : C a g j 12 D a g a j 13 ;"
I0124 00:30:03.232543 140530918526976 alphageometry.py:567] Translation: "j = on_line j a g, on_circle j a g"

I0124 00:30:03.232583 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a g, on_circle j a g ? coll i c h"
I0124 00:30:03.232706 140530918526976 graph.py:498] 
I0124 00:30:03.232756 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a g, on_circle j a g ? coll i c h
I0124 00:30:03.725978 140530918526976 ddar.py:60] Depth 1/1000 time = 0.4782264232635498
I0124 00:30:05.043297 140530918526976 ddar.py:60] Depth 2/1000 time = 1.3171215057373047
I0124 00:30:07.351120 140530918526976 ddar.py:60] Depth 3/1000 time = 2.3076188564300537
I0124 00:30:09.777816 140530918526976 ddar.py:60] Depth 4/1000 time = 2.4264988899230957
I0124 00:30:12.204545 140530918526976 ddar.py:60] Depth 5/1000 time = 2.426309108734131
I0124 00:30:12.218744 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:30:12.218832 140530918526976 alphageometry.py:566] LM output (score=-2.866847): "e : T a b d e 12 ;"
I0124 00:30:12.218870 140530918526976 alphageometry.py:567] Translation: "ERROR: point e already exists."

I0124 00:30:12.218905 140530918526976 alphageometry.py:566] LM output (score=-2.867888): "j : C a e j 12 D a e a j 13 ;"
I0124 00:30:12.218932 140530918526976 alphageometry.py:567] Translation: "j = on_line j a e, on_circle j a e"

I0124 00:30:12.218962 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a e, on_circle j a e ? coll i c h"
I0124 00:30:12.219104 140530918526976 graph.py:498] 
I0124 00:30:12.219160 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a e, on_circle j a e ? coll i c h
I0124 00:30:12.459243 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2256929874420166
I0124 00:30:13.648682 140530918526976 ddar.py:60] Depth 2/1000 time = 1.1893267631530762
I0124 00:30:15.511647 140530918526976 ddar.py:60] Depth 3/1000 time = 1.862774133682251
I0124 00:30:17.764475 140530918526976 ddar.py:60] Depth 4/1000 time = 2.2526464462280273
I0124 00:30:19.812507 140530918526976 ddar.py:60] Depth 5/1000 time = 2.047598361968994
I0124 00:30:19.827076 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:30:19.827159 140530918526976 alphageometry.py:566] LM output (score=-2.923811): "j : C a g j 12 D a j g j 13 ;"
I0124 00:30:19.827196 140530918526976 alphageometry.py:567] Translation: "j = on_line j a g, on_bline j g a"

I0124 00:30:19.827234 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a g, on_bline j g a ? coll i c h"
I0124 00:30:19.827352 140530918526976 graph.py:498] 
I0124 00:30:19.827401 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a g, on_bline j g a ? coll i c h
I0124 00:30:20.104840 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2617485523223877
I0124 00:30:21.540707 140530918526976 ddar.py:60] Depth 2/1000 time = 1.4357516765594482
I0124 00:30:23.257894 140530918526976 ddar.py:60] Depth 3/1000 time = 1.7170026302337646
I0124 00:30:25.375364 140530918526976 ddar.py:60] Depth 4/1000 time = 2.117277145385742
I0124 00:30:27.513135 140530918526976 ddar.py:60] Depth 5/1000 time = 2.1372883319854736
I0124 00:30:27.523704 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:30:27.523820 140530918526976 alphageometry.py:566] LM output (score=-2.944155): "b : D d e d b 12 ;"
I0124 00:30:27.523859 140530918526976 alphageometry.py:567] Translation: "ERROR: point b already exists."

I0124 00:30:27.523896 140530918526976 alphageometry.py:566] LM output (score=-2.946215): "a : D d e d a 12 ;"
I0124 00:30:27.523923 140530918526976 alphageometry.py:567] Translation: "ERROR: point a already exists."

I0124 00:30:27.523951 140530918526976 alphageometry.py:566] LM output (score=-2.966521): "d : D a d c d 12 D b d c d 13 ;"
I0124 00:30:27.523977 140530918526976 alphageometry.py:567] Translation: "ERROR: point d already exists."

I0124 00:30:27.524004 140530918526976 alphageometry.py:566] LM output (score=-2.981534): "j : C b g j 12 D b j g j 13 ;"
I0124 00:30:27.524029 140530918526976 alphageometry.py:567] Translation: "j = on_line j b g, on_bline j g b"

I0124 00:30:27.524058 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b g, on_bline j g b ? coll i c h"
I0124 00:30:27.524182 140530918526976 graph.py:498] 
I0124 00:30:27.524235 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j b g, on_bline j g b ? coll i c h
I0124 00:30:27.801949 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2617683410644531
I0124 00:30:29.252284 140530918526976 ddar.py:60] Depth 2/1000 time = 1.4502170085906982
I0124 00:30:30.985610 140530918526976 ddar.py:60] Depth 3/1000 time = 1.7331442832946777
I0124 00:30:33.119350 140530918526976 ddar.py:60] Depth 4/1000 time = 2.133542776107788
I0124 00:30:35.260789 140530918526976 ddar.py:60] Depth 5/1000 time = 2.1409852504730225
I0124 00:30:35.271343 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:30:35.271430 140530918526976 alphageometry.py:566] LM output (score=-3.017223): "j : C a g j 12 D a g g j 13 ;"
I0124 00:30:35.271466 140530918526976 alphageometry.py:567] Translation: "j = on_line j a g, on_circle j g a"

I0124 00:30:35.271502 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a g, on_circle j g a ? coll i c h"
I0124 00:30:35.271623 140530918526976 graph.py:498] 
I0124 00:30:35.271670 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a g, on_circle j g a ? coll i c h
I0124 00:30:35.573602 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2859368324279785
I0124 00:30:37.097379 140530918526976 ddar.py:60] Depth 2/1000 time = 1.5236458778381348
I0124 00:30:39.503538 140530918526976 ddar.py:60] Depth 3/1000 time = 2.405974864959717
I0124 00:30:42.109010 140530918526976 ddar.py:60] Depth 4/1000 time = 2.605285882949829
I0124 00:30:44.751107 140530918526976 ddar.py:60] Depth 5/1000 time = 2.6415629386901855
I0124 00:30:44.766326 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:30:44.766408 140530918526976 alphageometry.py:566] LM output (score=-3.024947): "d : D a d c d 12 T a c b d 13 ;"
I0124 00:30:44.766445 140530918526976 alphageometry.py:567] Translation: "ERROR: point d already exists."

I0124 00:30:44.766479 140530918526976 alphageometry.py:566] LM output (score=-3.053724): "a : T a b a c 12 ;"
I0124 00:30:44.766506 140530918526976 alphageometry.py:567] Translation: "ERROR: point a already exists."

I0124 00:30:44.766534 140530918526976 alphageometry.py:566] LM output (score=-3.085726): "e : D a e c e 12 ;"
I0124 00:30:44.766560 140530918526976 alphageometry.py:567] Translation: "ERROR: point e already exists."

I0124 00:30:44.766586 140530918526976 alphageometry.py:566] LM output (score=-3.094523): "e : D d f d e 12 ;"
I0124 00:30:44.766611 140530918526976 alphageometry.py:567] Translation: "ERROR: point e already exists."

I0124 00:30:44.766646 140530918526976 alphageometry.py:540] Depth 1. There are 15 nodes to expand:
I0124 00:30:44.766674 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a d j 12 D a j d j 13 ; x00
I0124 00:30:44.766701 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a h j 12 D a j h j 13 ; x00
I0124 00:30:44.766726 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a c j 12 D a j c j 13 ; x00
I0124 00:30:44.766751 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C b d j 12 D b j d j 13 ; x00
I0124 00:30:44.766775 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a e j 12 D a j e j 13 ; x00
I0124 00:30:44.766814 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C b h j 12 D b j h j 13 ; x00
I0124 00:30:44.766842 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C b c j 12 D b j c j 13 ; x00
I0124 00:30:44.766867 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C b e j 12 D b j e j 13 ; x00
I0124 00:30:44.766893 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C d g j 12 D d g d j 13 ; x00
I0124 00:30:44.766917 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C c d j 12 D c j d j 13 ; x00
I0124 00:30:44.766941 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a g j 12 D a g a j 13 ; x00
I0124 00:30:44.766964 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a e j 12 D a e a j 13 ; x00
I0124 00:30:44.766988 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a g j 12 D a j g j 13 ; x00
I0124 00:30:44.767012 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C b g j 12 D b j g j 13 ; x00
I0124 00:30:44.767035 140530918526976 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a g j 12 D a g g j 13 ; x00
I0124 00:30:44.767061 140530918526976 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a d j 12 D a j d j 13 ; x00
I0124 00:30:52.482478 140530918526976 alphageometry.py:566] LM output (score=-0.019550): "k : C b d k 14 D b k d k 15 ;"
I0124 00:30:52.482782 140530918526976 alphageometry.py:567] Translation: "k = on_line k b d, on_bline k d b"

I0124 00:30:52.482850 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b d, on_bline k d b ? coll i c h"
I0124 00:30:52.483010 140530918526976 graph.py:498] 
I0124 00:30:52.483068 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b d, on_bline k d b ? coll i c h
I0124 00:30:52.819601 140530918526976 ddar.py:60] Depth 1/1000 time = 0.31529664993286133
I0124 00:30:55.403993 140530918526976 ddar.py:60] Depth 2/1000 time = 2.5842678546905518
I0124 00:31:00.969042 140530918526976 ddar.py:60] Depth 3/1000 time = 5.564868927001953
I0124 00:31:07.140695 140530918526976 ddar.py:60] Depth 4/1000 time = 6.171459913253784
I0124 00:31:13.638890 140530918526976 ddar.py:60] Depth 5/1000 time = 6.49799919128418
I0124 00:31:19.878430 140530918526976 ddar.py:60] Depth 6/1000 time = 6.239022254943848
I0124 00:31:19.915794 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:31:19.915859 140530918526976 alphageometry.py:566] LM output (score=-2.550761): "k : C d b k 14 D d k b k 15 ;"
I0124 00:31:19.915895 140530918526976 alphageometry.py:567] Translation: "k = on_line k d b, on_bline k b d"

I0124 00:31:19.915933 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k d b, on_bline k b d ? coll i c h"
I0124 00:31:19.916064 140530918526976 graph.py:498] 
I0124 00:31:19.916114 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k d b, on_bline k b d ? coll i c h
I0124 00:31:20.246707 140530918526976 ddar.py:60] Depth 1/1000 time = 0.31146931648254395
I0124 00:31:22.206286 140530918526976 ddar.py:60] Depth 2/1000 time = 1.959463119506836
I0124 00:31:27.654839 140530918526976 ddar.py:60] Depth 3/1000 time = 5.448369264602661
I0124 00:31:33.304847 140530918526976 ddar.py:60] Depth 4/1000 time = 5.649756908416748
I0124 00:31:39.310353 140530918526976 ddar.py:60] Depth 5/1000 time = 6.005183935165405
I0124 00:31:44.992573 140530918526976 ddar.py:60] Depth 6/1000 time = 5.6817381381988525
I0124 00:31:45.028353 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:31:45.028436 140530918526976 alphageometry.py:566] LM output (score=-3.000587): "k : C f g k 14 D f k g k 15 ;"
I0124 00:31:45.028475 140530918526976 alphageometry.py:567] Translation: "k = on_line k f g, on_bline k g f"

I0124 00:31:45.028512 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f g, on_bline k g f ? coll i c h"
I0124 00:31:45.028651 140530918526976 graph.py:498] 
I0124 00:31:45.028710 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f g, on_bline k g f ? coll i c h
I0124 00:31:45.339471 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2904336452484131
I0124 00:31:46.574061 140530918526976 ddar.py:60] Depth 2/1000 time = 1.2344741821289062
I0124 00:31:49.610829 140530918526976 ddar.py:60] Depth 3/1000 time = 3.036587953567505
I0124 00:31:53.297672 140530918526976 ddar.py:60] Depth 4/1000 time = 3.6866447925567627
I0124 00:31:57.173860 140530918526976 ddar.py:60] Depth 5/1000 time = 3.8759593963623047
I0124 00:32:00.753029 140530918526976 ddar.py:60] Depth 6/1000 time = 3.578890800476074
I0124 00:32:04.578599 140530918526976 ddar.py:60] Depth 7/1000 time = 3.825145959854126
I0124 00:32:08.392351 140530918526976 ddar.py:60] Depth 8/1000 time = 3.789501190185547
I0124 00:32:12.540951 140530918526976 ddar.py:60] Depth 9/1000 time = 4.142345428466797
I0124 00:32:12.541148 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:32:12.541218 140530918526976 alphageometry.py:566] LM output (score=-3.065319): "k : C f e k 14 D f k e k 15 ;"
I0124 00:32:12.541253 140530918526976 alphageometry.py:567] Translation: "k = on_line k f e, on_bline k e f"

I0124 00:32:12.541288 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f e, on_bline k e f ? coll i c h"
I0124 00:32:12.541429 140530918526976 graph.py:498] 
I0124 00:32:12.541490 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f e, on_bline k e f ? coll i c h
I0124 00:32:12.881214 140530918526976 ddar.py:60] Depth 1/1000 time = 0.31931138038635254
I0124 00:32:14.309951 140530918526976 ddar.py:60] Depth 2/1000 time = 1.428619146347046
I0124 00:32:18.220736 140530918526976 ddar.py:60] Depth 3/1000 time = 3.9105780124664307
I0124 00:32:22.109048 140530918526976 ddar.py:60] Depth 4/1000 time = 3.888098955154419
I0124 00:32:26.412610 140530918526976 ddar.py:60] Depth 5/1000 time = 4.303375720977783
I0124 00:32:30.742470 140530918526976 ddar.py:60] Depth 6/1000 time = 4.329658031463623
I0124 00:32:35.091385 140530918526976 ddar.py:60] Depth 7/1000 time = 4.348446369171143
I0124 00:32:39.534061 140530918526976 ddar.py:60] Depth 8/1000 time = 4.41011905670166
I0124 00:32:39.534368 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:32:39.534443 140530918526976 alphageometry.py:566] LM output (score=-3.129256): "k : C a c k 14 D a k c k 15 ;"
I0124 00:32:39.534480 140530918526976 alphageometry.py:567] Translation: "k = on_line k a c, on_bline k c a"

I0124 00:32:39.534516 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a c, on_bline k c a ? coll i c h"
I0124 00:32:39.534654 140530918526976 graph.py:498] 
I0124 00:32:39.534713 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a c, on_bline k c a ? coll i c h
I0124 00:32:39.884661 140530918526976 ddar.py:60] Depth 1/1000 time = 0.32976531982421875
I0124 00:32:42.303887 140530918526976 ddar.py:60] Depth 2/1000 time = 2.4191043376922607
I0124 00:32:45.391106 140530918526976 ddar.py:60] Depth 3/1000 time = 3.0870277881622314
I0124 00:32:49.048611 140530918526976 ddar.py:60] Depth 4/1000 time = 3.6573173999786377
I0124 00:32:53.042071 140530918526976 ddar.py:60] Depth 5/1000 time = 3.9929778575897217
I0124 00:32:53.067437 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:32:53.067515 140530918526976 alphageometry.py:566] LM output (score=-3.156514): "k : C f h k 14 D f k h k 15 ;"
I0124 00:32:53.067553 140530918526976 alphageometry.py:567] Translation: "k = on_line k f h, on_bline k h f"

I0124 00:32:53.067591 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f h, on_bline k h f ? coll i c h"
I0124 00:32:53.067726 140530918526976 graph.py:498] 
I0124 00:32:53.067778 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f h, on_bline k h f ? coll i c h
I0124 00:32:53.409438 140530918526976 ddar.py:60] Depth 1/1000 time = 0.3225247859954834
I0124 00:32:54.868296 140530918526976 ddar.py:60] Depth 2/1000 time = 1.4586408138275146
I0124 00:32:57.821555 140530918526976 ddar.py:60] Depth 3/1000 time = 2.953064203262329
I0124 00:33:01.116971 140530918526976 ddar.py:60] Depth 4/1000 time = 3.2952210903167725
I0124 00:33:04.088316 140530918526976 ddar.py:60] Depth 5/1000 time = 2.970902919769287
I0124 00:33:04.107161 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:33:04.107232 140530918526976 alphageometry.py:566] LM output (score=-3.260020): "k : C b e k 14 D b k e k 15 ;"
I0124 00:33:04.107269 140530918526976 alphageometry.py:567] Translation: "k = on_line k b e, on_bline k e b"

I0124 00:33:04.107306 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b e, on_bline k e b ? coll i c h"
I0124 00:33:04.107445 140530918526976 graph.py:498] 
I0124 00:33:04.107502 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b e, on_bline k e b ? coll i c h
I0124 00:33:04.415458 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2882273197174072
I0124 00:33:06.315983 140530918526976 ddar.py:60] Depth 2/1000 time = 1.9004077911376953
I0124 00:33:09.172507 140530918526976 ddar.py:60] Depth 3/1000 time = 2.8563437461853027
I0124 00:33:12.170277 140530918526976 ddar.py:60] Depth 4/1000 time = 2.997574806213379
I0124 00:33:15.186255 140530918526976 ddar.py:60] Depth 5/1000 time = 3.0153894424438477
I0124 00:33:15.208367 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:33:15.208430 140530918526976 alphageometry.py:566] LM output (score=-3.321943): "k : C e g k 14 D e k g k 15 ;"
I0124 00:33:15.208465 140530918526976 alphageometry.py:567] Translation: "k = on_line k e g, on_bline k g e"

I0124 00:33:15.208503 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k e g, on_bline k g e ? coll i c h"
I0124 00:33:15.208640 140530918526976 graph.py:498] 
I0124 00:33:15.208694 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k e g, on_bline k g e ? coll i c h
I0124 00:33:15.512888 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2850191593170166
I0124 00:33:17.063540 140530918526976 ddar.py:60] Depth 2/1000 time = 1.5505366325378418
I0124 00:33:22.404041 140530918526976 ddar.py:60] Depth 3/1000 time = 5.3403236865997314
I0124 00:33:27.390297 140530918526976 ddar.py:60] Depth 4/1000 time = 4.986062526702881
I0124 00:33:32.180302 140530918526976 ddar.py:60] Depth 5/1000 time = 4.789755344390869
I0124 00:33:37.660753 140530918526976 ddar.py:60] Depth 6/1000 time = 5.480128049850464
I0124 00:33:42.447375 140530918526976 ddar.py:60] Depth 7/1000 time = 4.786149501800537
I0124 00:33:47.957018 140530918526976 ddar.py:60] Depth 8/1000 time = 5.475085258483887
I0124 00:33:47.957229 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:33:47.957286 140530918526976 alphageometry.py:566] LM output (score=-3.327647): "k : C b h k 14 D b k h k 15 ;"
I0124 00:33:47.957319 140530918526976 alphageometry.py:567] Translation: "k = on_line k b h, on_bline k h b"

I0124 00:33:47.957354 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b h, on_bline k h b ? coll i c h"
I0124 00:33:47.957514 140530918526976 graph.py:498] 
I0124 00:33:47.957576 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b h, on_bline k h b ? coll i c h
I0124 00:33:48.277310 140530918526976 ddar.py:60] Depth 1/1000 time = 0.29999709129333496
I0124 00:33:49.515176 140530918526976 ddar.py:60] Depth 2/1000 time = 1.2377450466156006
I0124 00:33:52.375615 140530918526976 ddar.py:60] Depth 3/1000 time = 2.860236167907715
I0124 00:33:55.499000 140530918526976 ddar.py:60] Depth 4/1000 time = 3.123162269592285
I0124 00:33:58.618060 140530918526976 ddar.py:60] Depth 5/1000 time = 3.118575096130371
I0124 00:34:01.757110 140530918526976 ddar.py:60] Depth 6/1000 time = 3.1181397438049316
I0124 00:34:01.759189 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:34:01.759257 140530918526976 alphageometry.py:566] LM output (score=-3.401299): "k : C c d k 14 D c k d k 15 ;"
I0124 00:34:01.759295 140530918526976 alphageometry.py:567] Translation: "k = on_line k c d, on_bline k d c"

I0124 00:34:01.759333 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k c d, on_bline k d c ? coll i c h"
I0124 00:34:01.759476 140530918526976 graph.py:498] 
I0124 00:34:01.759539 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k c d, on_bline k d c ? coll i c h
I0124 00:34:02.378533 140530918526976 ddar.py:60] Depth 1/1000 time = 0.5990297794342041
I0124 00:34:04.451195 140530918526976 ddar.py:60] Depth 2/1000 time = 2.0724706649780273
I0124 00:34:07.942009 140530918526976 ddar.py:60] Depth 3/1000 time = 3.490626335144043
I0124 00:34:12.179056 140530918526976 ddar.py:60] Depth 4/1000 time = 4.236847400665283
I0124 00:34:16.072212 140530918526976 ddar.py:60] Depth 5/1000 time = 3.8929316997528076
I0124 00:34:19.912507 140530918526976 ddar.py:60] Depth 6/1000 time = 3.8398845195770264
I0124 00:34:19.944686 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:34:19.944752 140530918526976 alphageometry.py:566] LM output (score=-3.527621): "k : C a h k 14 D a k h k 15 ;"
I0124 00:34:19.944788 140530918526976 alphageometry.py:567] Translation: "k = on_line k a h, on_bline k h a"

I0124 00:34:19.944823 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a h, on_bline k h a ? coll i c h"
I0124 00:34:19.944967 140530918526976 graph.py:498] 
I0124 00:34:19.945023 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a h, on_bline k h a ? coll i c h
I0124 00:34:20.261095 140530918526976 ddar.py:60] Depth 1/1000 time = 0.29591822624206543
I0124 00:34:22.370879 140530918526976 ddar.py:60] Depth 2/1000 time = 2.1096644401550293
I0124 00:34:25.550568 140530918526976 ddar.py:60] Depth 3/1000 time = 3.179492235183716
I0124 00:34:29.051239 140530918526976 ddar.py:60] Depth 4/1000 time = 3.5004191398620605
I0124 00:34:32.569277 140530918526976 ddar.py:60] Depth 5/1000 time = 3.517378330230713
I0124 00:34:32.594750 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:34:32.594832 140530918526976 alphageometry.py:566] LM output (score=-3.558835): "k : C a f k 14 D a k f k 15 ;"
I0124 00:34:32.594892 140530918526976 alphageometry.py:567] Translation: "k = on_line k a f, on_bline k f a"

I0124 00:34:32.594936 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a f, on_bline k f a ? coll i c h"
I0124 00:34:32.595082 140530918526976 graph.py:498] 
I0124 00:34:32.595140 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a f, on_bline k f a ? coll i c h
I0124 00:34:32.901660 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2870147228240967
I0124 00:34:34.584237 140530918526976 ddar.py:60] Depth 2/1000 time = 1.6824569702148438
I0124 00:34:37.412088 140530918526976 ddar.py:60] Depth 3/1000 time = 2.827653169631958
I0124 00:34:40.579838 140530918526976 ddar.py:60] Depth 4/1000 time = 3.167560577392578
I0124 00:34:43.725198 140530918526976 ddar.py:60] Depth 5/1000 time = 3.1449007987976074
I0124 00:34:43.750215 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:34:43.750294 140530918526976 alphageometry.py:566] LM output (score=-3.690590): "k : D a k f k 14 D d k f k 15 ;"
I0124 00:34:43.750333 140530918526976 alphageometry.py:567] Translation: "k = on_bline k f a, on_bline k f d"

I0124 00:34:43.750372 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_bline k f a, on_bline k f d ? coll i c h"
I0124 00:34:43.750502 140530918526976 graph.py:498] 
I0124 00:34:43.750551 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_bline k f a, on_bline k f d ? coll i c h
I0124 00:34:44.620518 140530918526976 ddar.py:60] Depth 1/1000 time = 0.8452095985412598
I0124 00:34:46.573310 140530918526976 ddar.py:60] Depth 2/1000 time = 1.9526004791259766
I0124 00:34:49.966292 140530918526976 ddar.py:60] Depth 3/1000 time = 3.3927695751190186
I0124 00:34:53.556686 140530918526976 ddar.py:60] Depth 4/1000 time = 3.590165138244629
I0124 00:34:57.118843 140530918526976 ddar.py:60] Depth 5/1000 time = 3.56156849861145
I0124 00:35:01.088764 140530918526976 ddar.py:60] Depth 6/1000 time = 3.959134817123413
I0124 00:35:04.802661 140530918526976 ddar.py:60] Depth 7/1000 time = 3.693084478378296
I0124 00:35:04.804564 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:35:04.804636 140530918526976 alphageometry.py:566] LM output (score=-3.754802): "k : C g e k 14 D g k e k 15 ;"
I0124 00:35:04.804673 140530918526976 alphageometry.py:567] Translation: "k = on_line k g e, on_bline k e g"

I0124 00:35:04.804710 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k g e, on_bline k e g ? coll i c h"
I0124 00:35:04.804847 140530918526976 graph.py:498] 
I0124 00:35:04.804913 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k g e, on_bline k e g ? coll i c h
I0124 00:35:05.116364 140530918526976 ddar.py:60] Depth 1/1000 time = 0.29161953926086426
I0124 00:35:06.439477 140530918526976 ddar.py:60] Depth 2/1000 time = 1.3229975700378418
I0124 00:35:11.900578 140530918526976 ddar.py:60] Depth 3/1000 time = 5.460896253585815
I0124 00:35:17.023569 140530918526976 ddar.py:60] Depth 4/1000 time = 5.122784376144409
I0124 00:35:22.322692 140530918526976 ddar.py:60] Depth 5/1000 time = 5.29892635345459
I0124 00:35:27.685668 140530918526976 ddar.py:60] Depth 6/1000 time = 5.362706899642944
I0124 00:35:32.672186 140530918526976 ddar.py:60] Depth 7/1000 time = 4.985887050628662
I0124 00:35:37.992811 140530918526976 ddar.py:60] Depth 8/1000 time = 5.285982131958008
I0124 00:35:37.993023 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:35:37.993082 140530918526976 alphageometry.py:566] LM output (score=-3.756426): "k : C b f k 14 D b k f k 15 ;"
I0124 00:35:37.993117 140530918526976 alphageometry.py:567] Translation: "k = on_line k b f, on_bline k f b"

I0124 00:35:37.993153 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b f, on_bline k f b ? coll i c h"
I0124 00:35:37.993294 140530918526976 graph.py:498] 
I0124 00:35:37.993354 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b f, on_bline k f b ? coll i c h
I0124 00:35:38.344633 140530918526976 ddar.py:60] Depth 1/1000 time = 0.3314211368560791
I0124 00:35:40.464822 140530918526976 ddar.py:60] Depth 2/1000 time = 2.1200640201568604
I0124 00:35:43.428552 140530918526976 ddar.py:60] Depth 3/1000 time = 2.9635496139526367
I0124 00:35:46.997879 140530918526976 ddar.py:60] Depth 4/1000 time = 3.5691142082214355
I0124 00:35:50.171947 140530918526976 ddar.py:60] Depth 5/1000 time = 3.1734371185302734
I0124 00:35:50.192534 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:35:50.192611 140530918526976 alphageometry.py:566] LM output (score=-3.915950): "k : C b j k 14 D b k j k 15 ;"
I0124 00:35:50.192648 140530918526976 alphageometry.py:567] Translation: "k = on_line k b j, on_bline k j b"

I0124 00:35:50.192684 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b j, on_bline k j b ? coll i c h"
I0124 00:35:50.192830 140530918526976 graph.py:498] 
I0124 00:35:50.192887 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k b j, on_bline k j b ? coll i c h
I0124 00:35:50.487918 140530918526976 ddar.py:60] Depth 1/1000 time = 0.27564144134521484
I0124 00:35:51.991528 140530918526976 ddar.py:60] Depth 2/1000 time = 1.5034899711608887
I0124 00:35:54.769711 140530918526976 ddar.py:60] Depth 3/1000 time = 2.777998924255371
I0124 00:35:57.948998 140530918526976 ddar.py:60] Depth 4/1000 time = 3.179108142852783
I0124 00:36:00.732807 140530918526976 ddar.py:60] Depth 5/1000 time = 2.7833614349365234
I0124 00:36:00.755310 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:36:00.755379 140530918526976 alphageometry.py:566] LM output (score=-3.964231): "f : C b d f 14 D b f d f 15 ;"
I0124 00:36:00.755415 140530918526976 alphageometry.py:567] Translation: "ERROR: point f already exists."

I0124 00:36:00.755450 140530918526976 alphageometry.py:566] LM output (score=-3.968360): "k : C c f k 14 D c f c k 15 ;"
I0124 00:36:00.755477 140530918526976 alphageometry.py:567] Translation: "k = on_line k c f, on_circle k c f"

I0124 00:36:00.755508 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k c f, on_circle k c f ? coll i c h"
I0124 00:36:00.755658 140530918526976 graph.py:498] 
I0124 00:36:00.755708 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k c f, on_circle k c f ? coll i c h
I0124 00:36:01.099761 140530918526976 ddar.py:60] Depth 1/1000 time = 0.324932336807251
I0124 00:36:03.135159 140530918526976 ddar.py:60] Depth 2/1000 time = 2.035271644592285
I0124 00:36:06.341485 140530918526976 ddar.py:60] Depth 3/1000 time = 3.206113576889038
I0124 00:36:10.180477 140530918526976 ddar.py:60] Depth 4/1000 time = 3.8387656211853027
I0124 00:36:13.640053 140530918526976 ddar.py:60] Depth 5/1000 time = 3.459155321121216
I0124 00:36:13.665891 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:36:13.665952 140530918526976 alphageometry.py:566] LM output (score=-3.998840): "k : C c h k 14 D c k h k 15 ;"
I0124 00:36:13.665989 140530918526976 alphageometry.py:567] Translation: "k = on_line k c h, on_bline k h c"

I0124 00:36:13.666025 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k c h, on_bline k h c ? coll i c h"
I0124 00:36:13.666164 140530918526976 graph.py:498] 
I0124 00:36:13.666222 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k c h, on_bline k h c ? coll i c h
I0124 00:36:13.975402 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2899792194366455
I0124 00:36:15.461382 140530918526976 ddar.py:60] Depth 2/1000 time = 1.4858644008636475
I0124 00:36:17.811722 140530918526976 ddar.py:60] Depth 3/1000 time = 2.350154161453247
I0124 00:36:20.719210 140530918526976 ddar.py:60] Depth 4/1000 time = 2.9072976112365723
I0124 00:36:23.245757 140530918526976 ddar.py:60] Depth 5/1000 time = 2.526069164276123
I0124 00:36:23.265167 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:36:23.265248 140530918526976 alphageometry.py:566] LM output (score=-4.004039): "k : C e f k 14 D e k f k 15 ;"
I0124 00:36:23.265285 140530918526976 alphageometry.py:567] Translation: "k = on_line k e f, on_bline k f e"

I0124 00:36:23.265332 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k e f, on_bline k f e ? coll i c h"
I0124 00:36:23.265482 140530918526976 graph.py:498] 
I0124 00:36:23.265538 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k e f, on_bline k f e ? coll i c h
I0124 00:36:23.604554 140530918526976 ddar.py:60] Depth 1/1000 time = 0.31975579261779785
I0124 00:36:25.636730 140530918526976 ddar.py:60] Depth 2/1000 time = 2.0320518016815186
I0124 00:36:29.705716 140530918526976 ddar.py:60] Depth 3/1000 time = 4.06879734992981
I0124 00:36:33.952897 140530918526976 ddar.py:60] Depth 4/1000 time = 4.246997833251953
I0124 00:36:38.386479 140530918526976 ddar.py:60] Depth 5/1000 time = 4.433393955230713
I0124 00:36:42.880469 140530918526976 ddar.py:60] Depth 6/1000 time = 4.493730306625366
I0124 00:36:47.381731 140530918526976 ddar.py:60] Depth 7/1000 time = 4.500650882720947
I0124 00:36:51.883925 140530918526976 ddar.py:60] Depth 8/1000 time = 4.469517230987549
I0124 00:36:51.884133 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:36:51.884199 140530918526976 alphageometry.py:566] LM output (score=-4.033875): "k : C a e k 14 D a k e k 15 ;"
I0124 00:36:51.884252 140530918526976 alphageometry.py:567] Translation: "k = on_line k a e, on_bline k e a"

I0124 00:36:51.884292 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a e, on_bline k e a ? coll i c h"
I0124 00:36:51.884435 140530918526976 graph.py:498] 
I0124 00:36:51.884496 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a e, on_bline k e a ? coll i c h
I0124 00:36:52.242374 140530918526976 ddar.py:60] Depth 1/1000 time = 0.33751749992370605
I0124 00:36:54.472753 140530918526976 ddar.py:60] Depth 2/1000 time = 2.230255126953125
I0124 00:36:57.756699 140530918526976 ddar.py:60] Depth 3/1000 time = 3.283756732940674
I0124 00:37:01.891564 140530918526976 ddar.py:60] Depth 4/1000 time = 4.134614706039429
I0124 00:37:05.566810 140530918526976 ddar.py:60] Depth 5/1000 time = 3.674645185470581
I0124 00:37:05.590361 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:37:05.590430 140530918526976 alphageometry.py:566] LM output (score=-4.046449): "k : C d f k 14 D d k f k 15 ;"
I0124 00:37:05.590466 140530918526976 alphageometry.py:567] Translation: "k = on_line k d f, on_bline k f d"

I0124 00:37:05.590502 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k d f, on_bline k f d ? coll i c h"
I0124 00:37:05.590639 140530918526976 graph.py:498] 
I0124 00:37:05.590697 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k d f, on_bline k f d ? coll i c h
I0124 00:37:05.904281 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2944514751434326
I0124 00:37:07.497155 140530918526976 ddar.py:60] Depth 2/1000 time = 1.5927584171295166
I0124 00:37:10.335256 140530918526976 ddar.py:60] Depth 3/1000 time = 2.8379013538360596
I0124 00:37:13.428089 140530918526976 ddar.py:60] Depth 4/1000 time = 3.092648506164551
I0124 00:37:16.537849 140530918526976 ddar.py:60] Depth 5/1000 time = 3.1092560291290283
I0124 00:37:16.558508 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:37:16.558585 140530918526976 alphageometry.py:566] LM output (score=-4.065685): "k : C g f k 14 D g k f k 15 ;"
I0124 00:37:16.558624 140530918526976 alphageometry.py:567] Translation: "k = on_line k g f, on_bline k f g"

I0124 00:37:16.558663 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k g f, on_bline k f g ? coll i c h"
I0124 00:37:16.558795 140530918526976 graph.py:498] 
I0124 00:37:16.558846 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k g f, on_bline k f g ? coll i c h
I0124 00:37:16.867403 140530918526976 ddar.py:60] Depth 1/1000 time = 0.289581298828125
I0124 00:37:18.417560 140530918526976 ddar.py:60] Depth 2/1000 time = 1.5500283241271973
I0124 00:37:21.313166 140530918526976 ddar.py:60] Depth 3/1000 time = 2.8953616619110107
I0124 00:37:25.384861 140530918526976 ddar.py:60] Depth 4/1000 time = 4.071404933929443
I0124 00:37:29.177766 140530918526976 ddar.py:60] Depth 5/1000 time = 3.7927255630493164
I0124 00:37:33.402892 140530918526976 ddar.py:60] Depth 6/1000 time = 4.22492241859436
I0124 00:37:37.224909 140530918526976 ddar.py:60] Depth 7/1000 time = 3.8215548992156982
I0124 00:37:41.144863 140530918526976 ddar.py:60] Depth 8/1000 time = 3.8922922611236572
I0124 00:37:45.003352 140530918526976 ddar.py:60] Depth 9/1000 time = 3.852205276489258
I0124 00:37:45.003558 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:37:45.003627 140530918526976 alphageometry.py:566] LM output (score=-4.093217): "k : C f j k 14 D f j j k 15 ;"
I0124 00:37:45.003665 140530918526976 alphageometry.py:567] Translation: "k = on_line k f j, on_circle k j f"

I0124 00:37:45.003702 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f j, on_circle k j f ? coll i c h"
I0124 00:37:45.003847 140530918526976 graph.py:498] 
I0124 00:37:45.003908 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f j, on_circle k j f ? coll i c h
I0124 00:37:45.345878 140530918526976 ddar.py:60] Depth 1/1000 time = 0.3221557140350342
I0124 00:37:47.111693 140530918526976 ddar.py:60] Depth 2/1000 time = 1.7656960487365723
I0124 00:37:49.910055 140530918526976 ddar.py:60] Depth 3/1000 time = 2.7981858253479004
I0124 00:37:53.302734 140530918526976 ddar.py:60] Depth 4/1000 time = 3.3924925327301025
I0124 00:37:56.689673 140530918526976 ddar.py:60] Depth 5/1000 time = 3.3864378929138184
I0124 00:37:56.711949 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:37:56.712028 140530918526976 alphageometry.py:566] LM output (score=-4.137887): "k : C d f k 14 D d f d k 15 ;"
I0124 00:37:56.712067 140530918526976 alphageometry.py:567] Translation: "k = on_line k d f, on_circle k d f"

I0124 00:37:56.712104 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k d f, on_circle k d f ? coll i c h"
I0124 00:37:56.712239 140530918526976 graph.py:498] 
I0124 00:37:56.712289 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k d f, on_circle k d f ? coll i c h
I0124 00:37:57.018565 140530918526976 ddar.py:60] Depth 1/1000 time = 0.28764986991882324
I0124 00:37:59.277832 140530918526976 ddar.py:60] Depth 2/1000 time = 2.2591443061828613
I0124 00:38:02.670349 140530918526976 ddar.py:60] Depth 3/1000 time = 3.392308473587036
I0124 00:38:06.891940 140530918526976 ddar.py:60] Depth 4/1000 time = 4.221403360366821
I0124 00:38:10.257067 140530918526976 ddar.py:60] Depth 5/1000 time = 3.3649284839630127
I0124 00:38:14.479752 140530918526976 ddar.py:60] Depth 6/1000 time = 4.222215890884399
I0124 00:38:14.510136 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:38:14.510214 140530918526976 alphageometry.py:566] LM output (score=-4.145591): "k : C f g k 14 D g j j k 15 ;"
I0124 00:38:14.510254 140530918526976 alphageometry.py:567] Translation: "k = on_line k f g, on_circle k j g"

I0124 00:38:14.510292 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f g, on_circle k j g ? coll i c h"
I0124 00:38:14.510434 140530918526976 graph.py:498] 
I0124 00:38:14.510494 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f g, on_circle k j g ? coll i c h
I0124 00:38:14.812191 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2829573154449463
I0124 00:38:16.259322 140530918526976 ddar.py:60] Depth 2/1000 time = 1.447011947631836
I0124 00:38:21.943836 140530918526976 ddar.py:60] Depth 3/1000 time = 5.684308290481567
I0124 00:38:28.127750 140530918526976 ddar.py:60] Depth 4/1000 time = 6.18369197845459
I0124 00:38:35.038170 140530918526976 ddar.py:60] Depth 5/1000 time = 6.910224914550781
I0124 00:38:41.974472 140530918526976 ddar.py:60] Depth 6/1000 time = 6.935729265213013
I0124 00:38:42.029203 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:38:42.029271 140530918526976 alphageometry.py:566] LM output (score=-4.173689): "k : C g d k 14 D g d d k 15 ;"
I0124 00:38:42.029308 140530918526976 alphageometry.py:567] Translation: "k = on_line k g d, on_circle k d g"

I0124 00:38:42.029345 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k g d, on_circle k d g ? coll i c h"
I0124 00:38:42.029480 140530918526976 graph.py:498] 
I0124 00:38:42.029534 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k g d, on_circle k d g ? coll i c h
I0124 00:38:42.393975 140530918526976 ddar.py:60] Depth 1/1000 time = 0.3450808525085449
I0124 00:38:44.659436 140530918526976 ddar.py:60] Depth 2/1000 time = 2.265338897705078
I0124 00:38:49.607513 140530918526976 ddar.py:60] Depth 3/1000 time = 4.947890520095825
I0124 00:38:54.469355 140530918526976 ddar.py:60] Depth 4/1000 time = 4.861649751663208
I0124 00:38:58.914448 140530918526976 ddar.py:60] Depth 5/1000 time = 4.44487452507019
I0124 00:39:03.749025 140530918526976 ddar.py:60] Depth 6/1000 time = 4.834087371826172
I0124 00:39:03.779369 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:39:03.779437 140530918526976 alphageometry.py:566] LM output (score=-4.178633): "k : C d g k 14 D d g d k 15 ;"
I0124 00:39:03.779473 140530918526976 alphageometry.py:567] Translation: "k = on_line k d g, on_circle k d g"

I0124 00:39:03.779510 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k d g, on_circle k d g ? coll i c h"
I0124 00:39:03.779657 140530918526976 graph.py:498] 
I0124 00:39:03.779715 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k d g, on_circle k d g ? coll i c h
I0124 00:39:04.146288 140530918526976 ddar.py:60] Depth 1/1000 time = 0.3472750186920166
I0124 00:39:06.447274 140530918526976 ddar.py:60] Depth 2/1000 time = 2.300858974456787
I0124 00:39:10.895583 140530918526976 ddar.py:60] Depth 3/1000 time = 4.448124885559082
I0124 00:39:15.766917 140530918526976 ddar.py:60] Depth 4/1000 time = 4.871075391769409
I0124 00:39:20.631975 140530918526976 ddar.py:60] Depth 5/1000 time = 4.864729881286621
I0124 00:39:25.476018 140530918526976 ddar.py:60] Depth 6/1000 time = 4.84347939491272
I0124 00:39:25.507299 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:39:25.507381 140530918526976 alphageometry.py:566] LM output (score=-4.199586): "k : C f d k 14 D f k d k 15 ;"
I0124 00:39:25.507419 140530918526976 alphageometry.py:567] Translation: "k = on_line k f d, on_bline k d f"

I0124 00:39:25.507455 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f d, on_bline k d f ? coll i c h"
I0124 00:39:25.507624 140530918526976 graph.py:498] 
I0124 00:39:25.507685 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k f d, on_bline k d f ? coll i c h
I0124 00:39:25.819487 140530918526976 ddar.py:60] Depth 1/1000 time = 0.2921593189239502
I0124 00:39:27.542071 140530918526976 ddar.py:60] Depth 2/1000 time = 1.7224571704864502
I0124 00:39:30.082756 140530918526976 ddar.py:60] Depth 3/1000 time = 2.540499687194824
I0124 00:39:33.335723 140530918526976 ddar.py:60] Depth 4/1000 time = 3.252737283706665
I0124 00:39:36.608707 140530918526976 ddar.py:60] Depth 5/1000 time = 3.272404909133911
I0124 00:39:36.629350 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:39:36.629418 140530918526976 alphageometry.py:566] LM output (score=-4.205588): "k : C g d k 14 D g k d k 15 ;"
I0124 00:39:36.629454 140530918526976 alphageometry.py:567] Translation: "k = on_line k g d, on_bline k d g"

I0124 00:39:36.629493 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k g d, on_bline k d g ? coll i c h"
I0124 00:39:36.629634 140530918526976 graph.py:498] 
I0124 00:39:36.629699 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k g d, on_bline k d g ? coll i c h
I0124 00:39:37.002183 140530918526976 ddar.py:60] Depth 1/1000 time = 0.35331249237060547
I0124 00:39:39.434930 140530918526976 ddar.py:60] Depth 2/1000 time = 2.4326250553131104
I0124 00:39:43.286453 140530918526976 ddar.py:60] Depth 3/1000 time = 3.8513290882110596
I0124 00:39:47.011133 140530918526976 ddar.py:60] Depth 4/1000 time = 3.7244725227355957
I0124 00:39:51.209194 140530918526976 ddar.py:60] Depth 5/1000 time = 4.197613477706909
I0124 00:39:51.231079 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:39:51.231158 140530918526976 alphageometry.py:566] LM output (score=-4.246047): "k : C a g k 14 D a g g k 15 ;"
I0124 00:39:51.231195 140530918526976 alphageometry.py:567] Translation: "k = on_line k a g, on_circle k g a"

I0124 00:39:51.231233 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a g, on_circle k g a ? coll i c h"
I0124 00:39:51.231367 140530918526976 graph.py:498] 
I0124 00:39:51.231417 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a g, on_circle k g a ? coll i c h
I0124 00:39:51.654459 140530918526976 ddar.py:60] Depth 1/1000 time = 0.401961088180542
I0124 00:39:54.392512 140530918526976 ddar.py:60] Depth 2/1000 time = 2.7379097938537598
I0124 00:39:58.668246 140530918526976 ddar.py:60] Depth 3/1000 time = 4.275513172149658
I0124 00:40:03.395069 140530918526976 ddar.py:60] Depth 4/1000 time = 4.726642370223999
I0124 00:40:08.106427 140530918526976 ddar.py:60] Depth 5/1000 time = 4.710808753967285
I0124 00:40:08.131426 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:40:08.131487 140530918526976 alphageometry.py:566] LM output (score=-4.318646): "k : C a g k 14 D a g a k 15 ;"
I0124 00:40:08.131545 140530918526976 alphageometry.py:567] Translation: "k = on_line k a g, on_circle k a g"

I0124 00:40:08.131585 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a g, on_circle k a g ? coll i c h"
I0124 00:40:08.131728 140530918526976 graph.py:498] 
I0124 00:40:08.131787 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a d, on_bline j d a; k = on_line k a g, on_circle k a g ? coll i c h
I0124 00:40:08.939902 140530918526976 ddar.py:60] Depth 1/1000 time = 0.7874128818511963
I0124 00:40:10.819930 140530918526976 ddar.py:60] Depth 2/1000 time = 1.8798298835754395
I0124 00:40:15.066311 140530918526976 ddar.py:60] Depth 3/1000 time = 4.2461771965026855
I0124 00:40:19.576801 140530918526976 ddar.py:60] Depth 4/1000 time = 4.510281801223755
I0124 00:40:24.105987 140530918526976 ddar.py:60] Depth 5/1000 time = 4.528990983963013
I0124 00:40:28.654301 140530918526976 ddar.py:60] Depth 6/1000 time = 4.547825813293457
I0124 00:40:28.678348 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:40:28.678432 140530918526976 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C d g h 08 C e f h 09 ; i : C a b i 10 D a i b i 11 ? C i c h {F1} x00 j : C a h j 12 D a j h j 13 ; x00
I0124 00:40:35.004850 140530918526976 alphageometry.py:566] LM output (score=-0.387000): "k : C b h k 14 D b k h k 15 ;"
I0124 00:40:35.005194 140530918526976 alphageometry.py:567] Translation: "k = on_line k b h, on_bline k h b"

I0124 00:40:35.005263 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a h, on_bline j h a; k = on_line k b h, on_bline k h b ? coll i c h"
I0124 00:40:35.005410 140530918526976 graph.py:498] 
I0124 00:40:35.005468 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a h, on_bline j h a; k = on_line k b h, on_bline k h b ? coll i c h
I0124 00:40:35.318532 140530918526976 ddar.py:60] Depth 1/1000 time = 0.293912410736084
I0124 00:40:37.002970 140530918526976 ddar.py:60] Depth 2/1000 time = 1.6843018531799316
I0124 00:40:40.044275 140530918526976 ddar.py:60] Depth 3/1000 time = 3.0411150455474854
I0124 00:40:43.324831 140530918526976 ddar.py:60] Depth 4/1000 time = 3.2803666591644287
I0124 00:40:47.083215 140530918526976 ddar.py:60] Depth 5/1000 time = 3.7578823566436768
I0124 00:40:47.102747 140530918526976 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:40:47.102822 140530918526976 alphageometry.py:566] LM output (score=-1.153097): "k : C d h k 14 D d k h k 15 ;"
I0124 00:40:47.102860 140530918526976 alphageometry.py:567] Translation: "k = on_line k d h, on_bline k h d"

I0124 00:40:47.102899 140530918526976 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a h, on_bline j h a; k = on_line k d h, on_bline k h d ? coll i c h"
I0124 00:40:47.103033 140530918526976 graph.py:498] 
I0124 00:40:47.103085 140530918526976 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = on_line h g d, on_line h e f; i = midpoint i b a; j = on_line j a h, on_bline j h a; k = on_line k d h, on_bline k h d ? coll i c h
I0124 00:40:47.469918 140530918526976 ddar.py:60] Depth 1/1000 time = 0.3482227325439453
I0124 00:40:49.530468 140530918526976 ddar.py:60] Depth 2/1000 time = 2.0604135990142822
I0124 00:40:51.833080 140530918526976 ddar.py:60] Depth 3/1000 time = 2.3024163246154785
I0124 00:40:54.740719 140530918526976 ddar.py:60] Depth 4/1000 time = 2.907453775405884
I0124 00:40:57.666734 140530918526976 ddar.py:60] Depth 5/1000 time = 2.9255974292755127
