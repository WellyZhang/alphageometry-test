I0123 11:01:22.963788 140212176609280 inference_utils.py:69] Parsing gin configuration.
I0123 11:01:22.963956 140212176609280 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:01:22.964205 140212176609280 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:01:22.964245 140212176609280 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:01:22.964276 140212176609280 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:01:22.964304 140212176609280 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:01:22.964331 140212176609280 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:01:22.964360 140212176609280 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:01:22.964387 140212176609280 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:01:22.964413 140212176609280 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:01:22.964439 140212176609280 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:01:22.964465 140212176609280 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:01:22.964524 140212176609280 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:01:22.964703 140212176609280 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:01:22.964964 140212176609280 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:01:22.965086 140212176609280 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:01:22.971562 140212176609280 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:01:22.971703 140212176609280 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:01:22.972030 140212176609280 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:01:22.972142 140212176609280 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:01:22.972426 140212176609280 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:01:22.972534 140212176609280 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:01:22.972944 140212176609280 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:01:22.973052 140212176609280 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:01:22.976814 140212176609280 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:01:23.082892 140212176609280 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:01:23.084067 140212176609280 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:01:23.094301 140212176609280 training_loop.py:335] Process 0 of 1
I0123 11:01:23.094404 140212176609280 training_loop.py:336] Local device count = 1
I0123 11:01:23.094476 140212176609280 training_loop.py:337] Number of replicas = 1
I0123 11:01:23.094533 140212176609280 training_loop.py:339] Using random number seed 42
I0123 11:01:23.603858 140212176609280 training_loop.py:359] Initializing the model.
I0123 11:01:24.005706 140212176609280 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.007360 140212176609280 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:01:24.007487 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.007575 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.007658 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.007756 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.007836 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.007912 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.007990 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.008065 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.008167 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.008256 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.008332 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.008405 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:24.008449 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.008501 140212176609280 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:24.008642 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.008690 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.008723 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.011041 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.016911 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.028554 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.028861 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.033543 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.047514 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.047597 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.047653 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.047704 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.047793 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.049863 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.049989 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.050893 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.054614 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.063432 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.065020 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.065115 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.065156 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.065225 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.065365 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.065765 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.065823 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.067861 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.067979 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.071018 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.071109 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.071646 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.082776 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.092432 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.092545 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.092862 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.092957 140212176609280 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:24.093076 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.093120 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.093153 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.095251 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.097953 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.104247 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.104525 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.107388 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.111562 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.111625 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.111665 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.111699 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.111765 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.112514 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.112600 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.113001 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.113843 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.116572 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.117249 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.117339 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.117394 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.117462 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.117601 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.118033 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.118088 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.120421 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.120553 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.123279 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.123368 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.123835 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.126319 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.128335 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.128447 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.128760 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.128850 140212176609280 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:24.128977 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.129020 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.129054 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.131510 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.134789 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.144654 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.145116 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.149679 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.155814 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.155883 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.155923 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.155964 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.156046 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.156681 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.156767 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.157163 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.158039 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.160790 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.161518 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.161607 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.161660 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.161731 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.161887 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.162257 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.162310 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.164847 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.164987 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.167996 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.168099 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.168653 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.171318 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.173525 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.173660 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.173999 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.174103 140212176609280 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:24.174234 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.174277 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.174311 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.176483 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.179220 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.185732 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.186093 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.189251 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.193947 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.194020 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.194065 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.194101 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.194186 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.194944 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.195036 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.195513 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.196458 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.199561 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.200412 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.200523 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.200569 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.200644 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.200840 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.201269 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.201326 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.203732 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.203877 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.207186 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.207300 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.207886 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.211083 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.213478 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.213608 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.214006 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.214116 140212176609280 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:24.214249 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.214297 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.214332 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.216725 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.219622 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.227517 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.227914 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.232710 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.238072 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.238147 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.238195 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.238244 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.238326 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.239084 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.239188 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.239659 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.240675 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.244516 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.245536 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.245669 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.245728 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.245829 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.246039 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.246595 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.246664 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.249776 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.249925 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.254115 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.254246 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.254976 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.258902 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.262128 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.262291 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.262760 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.262892 140212176609280 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:24.263067 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.263136 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.263188 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.266544 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.270757 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.281608 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.282107 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.287239 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.294245 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.294496 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.294701 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.294925 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.295201 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.296462 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.296730 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.297499 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.299043 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.303386 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.304505 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.304755 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.304945 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.305191 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.305545 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.306251 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.306458 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.309767 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.310055 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.314574 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.314851 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.315778 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.319747 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.323244 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.323547 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.324176 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.324443 140212176609280 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:24.324793 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.325014 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.325223 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.328799 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.332853 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.342479 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.342909 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.347043 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.353326 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.353409 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.353466 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.353518 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.353614 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.354579 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.354689 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.355260 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.356478 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.360551 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.361525 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.361627 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.361693 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.361781 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.361963 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.362467 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.362528 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.366181 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.366313 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.370129 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.370236 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.370850 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.526086 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.528659 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.528841 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.529184 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.529286 140212176609280 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:24.529412 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.529457 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.529493 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.531693 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.534347 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.540390 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.540684 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.543551 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.547755 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.547819 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.547861 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.547895 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.547967 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.548633 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.548716 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.549105 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.549960 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.552732 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.553414 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.553498 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.553540 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.553607 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.553764 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.554125 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.554174 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.556199 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.556301 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.558985 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.559072 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.559593 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.562015 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.564011 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.564126 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.564435 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.564523 140212176609280 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:24.564640 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.564683 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.564717 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.566703 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.569203 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.575085 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.575371 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.578170 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.582106 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.582165 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.582203 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.582236 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.582304 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.582914 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.582996 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.583378 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.584199 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.586855 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.587511 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.587593 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.587632 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.587694 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.587833 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.588181 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.588227 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.590220 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.590334 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.592983 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.593072 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.593526 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.595926 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.597922 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.598026 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.598340 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.598435 140212176609280 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:24.598555 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.598598 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.598630 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.600614 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.603086 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.609273 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.609565 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.612364 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.616276 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.616336 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.616374 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.616407 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.616472 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.617066 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.617148 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.617532 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.618421 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.621020 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.621695 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.621780 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.621817 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.621880 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.622020 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.622357 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.622404 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.624379 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.624483 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.627135 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.627222 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.627669 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.630039 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.632069 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.632173 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.632477 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.632573 140212176609280 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:24.632692 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.632735 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.632768 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.634704 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.637213 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.642991 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.643265 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.646011 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.649921 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.649983 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.650020 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.650053 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.650162 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.650750 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.650833 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.651212 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.652034 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.654619 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.655264 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.655347 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.655384 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.655452 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.655586 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.655921 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.655967 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.657990 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.658091 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.660858 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.660943 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.661381 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.663778 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.665676 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.665775 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.666065 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.666151 140212176609280 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:24.666275 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.666319 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.666351 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.668221 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.670629 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.676317 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.676572 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.679174 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:24.683341 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.683404 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.683442 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.683474 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.683538 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.684115 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.684194 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.684551 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.685328 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.687896 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.688521 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.688600 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.688637 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.688698 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.688831 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.689154 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.689198 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.691139 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.691240 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.693702 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.693785 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.694230 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.696520 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.698410 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.698513 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.698801 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.699083 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699155 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699223 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699283 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699338 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699392 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699446 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699498 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699550 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699602 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699655 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699707 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:24.699744 140212176609280 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:24.703213 140212176609280 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:24.750795 140212176609280 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.750886 140212176609280 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:24.750944 140212176609280 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:24.751053 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.751094 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.751132 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.751198 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.753686 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.759190 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.759465 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.762128 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:24.778567 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.778627 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.778664 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.778695 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.778758 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.779892 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.779976 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.780689 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.782708 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.787452 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.788769 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.788861 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.788898 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.788959 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.789099 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.789222 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.789263 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.791185 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.791283 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.793727 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.793810 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.793920 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.796168 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.798154 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.798255 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.798557 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.798641 140212176609280 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:24.798751 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.798792 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.798825 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.798894 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.801170 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.806612 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.806885 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.809549 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:24.822556 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.822614 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.822650 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.822681 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.822744 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.823315 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.823394 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.823760 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.824452 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.826929 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.827549 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.827627 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.827668 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.827729 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.827866 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.827984 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.828032 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.829963 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.830060 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.832447 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.832527 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.832634 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.834845 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.836748 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.836847 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.837146 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.837229 140212176609280 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:24.837338 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.837378 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.837409 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.837472 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.839725 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.845124 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.845396 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.848067 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:24.860669 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.860730 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.860766 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.860798 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.860860 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.861423 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.861501 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.861859 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.862581 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.865080 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.865729 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.865809 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.865845 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.865912 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.866042 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.866160 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.866206 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.868116 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.868212 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.870638 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.870724 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.870833 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.873021 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.874952 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.875052 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.875338 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.875421 140212176609280 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:24.875530 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.875572 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.875603 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.875667 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.877900 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.883322 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.883587 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.886245 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:24.898963 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.899023 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.899061 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.899094 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.899157 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.899732 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.899811 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.900177 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.900881 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.903383 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.904021 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.904102 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.904139 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.904200 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.904346 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.904463 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.904505 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.906749 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.906850 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.909278 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.909362 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.909474 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.911734 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.913620 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.913730 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.914020 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.914103 140212176609280 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:24.914213 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.914254 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.914287 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.914353 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.916663 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.922178 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.922451 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.925069 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:24.937837 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.937896 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.937934 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.937968 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.938035 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.938599 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.938678 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.939047 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.939757 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.942311 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.942945 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.943025 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.943061 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.943122 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.943260 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.943378 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.943419 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.945306 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.945404 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.947845 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.947928 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.948038 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.950345 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.952224 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.952322 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.952609 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.952692 140212176609280 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:24.952803 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.952845 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.952877 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.952943 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.955214 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.960664 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.960932 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:24.963593 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:24.976202 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:24.976260 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:24.976296 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:24.976327 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.976389 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.976955 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.977036 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.977399 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.978105 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.980565 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.981184 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.981261 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:24.981296 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:24.981356 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.981490 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:24.981610 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:24.981661 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.983580 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.983677 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.986065 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.986148 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:24.986258 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:24.988470 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:24.990328 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.990427 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:24.990715 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.990799 140212176609280 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:24.990910 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:24.990950 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:24.990983 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:24.991047 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.993272 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:24.998769 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:24.999031 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.001595 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.014618 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.014677 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.014716 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.014750 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.014812 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.015387 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.015468 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.015823 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.016508 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.018987 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.019666 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.019746 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.019783 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.019843 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.019979 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.020093 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.020141 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.022031 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.022129 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.024513 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.024599 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.024709 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.026906 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.028825 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.028924 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.029208 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.029290 140212176609280 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:25.029400 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.029441 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.029473 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.029536 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.031763 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.037086 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.037357 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.039978 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.052512 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.052571 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.052608 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.052639 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.052700 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.053306 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.053384 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.053748 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.054443 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.056906 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.057537 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.057617 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.057664 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.057729 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.057861 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.057974 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.058022 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.059919 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.060018 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.062478 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.062561 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.062669 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.064855 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.066709 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.066808 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.067097 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.067181 140212176609280 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:25.067291 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.067332 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.067364 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.067428 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.069680 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.075142 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.075410 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.078021 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.090577 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.090636 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.090673 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.090704 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.090765 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.091336 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.091416 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.091778 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.092468 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.094927 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.095612 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.095693 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.095729 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.095789 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.095918 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.096031 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.096072 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.097967 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.098066 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.100453 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.100535 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.100646 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.102853 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.104782 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.104882 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.105170 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.105255 140212176609280 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:25.105366 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.105409 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.105440 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.105503 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.107730 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.113117 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.113377 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.116388 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.128873 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.128933 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.128969 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.129001 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.129064 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.129689 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.129769 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.130125 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.130833 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.133298 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.133925 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.134006 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.134043 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.134103 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.134239 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.134353 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.134394 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.136258 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.136365 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.138806 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.138890 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.139001 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.141193 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.143046 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.143146 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.143434 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.143519 140212176609280 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:25.143630 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.143672 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.143703 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.143767 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.146022 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.151557 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.151828 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.154495 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.167455 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.167513 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.167551 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.167583 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.167648 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.168213 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.168295 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.168657 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.169347 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.171850 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.172514 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.172594 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.172630 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.172691 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.172825 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.172937 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.172979 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.174894 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.175002 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.177434 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.177516 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.177637 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.179869 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.181808 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.181910 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.182202 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.182287 140212176609280 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:25.182400 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.182442 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.182476 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.182542 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.184804 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.190445 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.190712 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.193326 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.205897 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.205955 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.205992 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.206024 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.206087 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.206653 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.206731 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.207090 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.207769 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.210307 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.210920 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.210998 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.211033 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.211093 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.211226 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.211341 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.211381 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.213240 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.213336 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.215749 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.215835 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.215945 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.218554 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.220394 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.220493 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.220779 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.220867 140212176609280 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:25.223701 140212176609280 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:25.278248 140212176609280 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.278338 140212176609280 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:25.278395 140212176609280 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:25.278502 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.278543 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.278580 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.278645 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.280989 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.286370 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.286641 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.289500 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.301912 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.301972 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.302009 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.302042 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.302106 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.302678 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.302756 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.303123 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.303810 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.306325 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.306955 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.307035 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.307071 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.307135 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.307265 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.307390 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.307434 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.309294 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.309391 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.311792 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.311876 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.311986 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.314237 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.316088 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.316187 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.316482 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.316565 140212176609280 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:25.316673 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.316714 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.316749 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.316815 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.319062 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.324455 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.324734 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.327421 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.339838 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.339899 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.339941 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.339974 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.340036 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.340611 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.340692 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.341062 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.341765 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.344276 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.344903 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.344983 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.345020 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.345085 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.345218 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.345334 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.345385 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.347260 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.347357 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.349768 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.349853 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.349966 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.352227 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.354072 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.354171 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.354466 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.354549 140212176609280 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:25.354659 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.354700 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.354736 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.354800 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.357023 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.362335 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.362604 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.365250 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.377680 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.377739 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.377777 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.377810 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.377876 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.378455 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.378535 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.378915 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.379611 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.382554 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.383187 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.383267 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.383305 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.383366 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.383501 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.383624 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.383664 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.385527 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.385625 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.388041 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.388122 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.388233 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.390469 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.392296 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.392395 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.392684 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.392768 140212176609280 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:25.392878 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.392919 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.392952 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.393018 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.395260 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.400581 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.400842 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.403484 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.415949 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.416007 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.416045 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.416094 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.416159 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.416724 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.416800 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.417167 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.417864 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.420381 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.420994 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.421072 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.421108 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.421168 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.421296 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.421406 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.421447 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.423317 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.423414 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.425802 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.425884 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.425994 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.428271 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.430135 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.430233 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.430521 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.430604 140212176609280 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:25.430711 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.430752 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.430782 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.430844 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.433072 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.438463 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.438724 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.441368 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.453970 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.454028 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.454064 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.454095 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.454157 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.454721 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.454802 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.455160 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.455847 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.458376 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.458996 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.459077 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.459113 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.459175 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.459302 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.459411 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.459451 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.461322 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.461425 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.463844 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.463926 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.464035 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.466321 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.468172 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.468269 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.468554 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.468635 140212176609280 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:25.468744 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.468784 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.468814 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.468877 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.471110 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.476502 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.476764 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.479434 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.491986 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.492044 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.492080 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.492112 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.492173 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.492731 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.492808 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.493168 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.493849 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.496765 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.497389 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.497467 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.497502 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.497562 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.497695 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.497807 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.497847 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.499720 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.499822 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.502210 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.502292 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.502406 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.504674 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.506548 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.506647 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.506933 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.507015 140212176609280 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:25.507123 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.507163 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.507193 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.507256 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.509484 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.514876 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.515138 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.517827 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.530394 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.530454 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.530490 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.530521 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.530582 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.531157 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.531235 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.531588 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.532280 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.534813 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.535447 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.535526 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.535561 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.535621 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.535748 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.535863 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.535902 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.537784 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.537881 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.540293 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.540372 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.540481 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.542766 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.544613 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.544709 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.544995 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.545077 140212176609280 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:25.545185 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.545226 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.545256 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.545320 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.547538 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.552963 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.553227 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.555908 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.568527 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.568585 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.568622 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.568654 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.568719 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.569282 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.569359 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.569725 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.570415 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.572941 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.573564 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.573655 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.573693 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.573761 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.573891 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.574000 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.574040 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.575914 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.576010 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.578447 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.578537 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.578650 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.580936 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.582801 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.582900 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.583189 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.583271 140212176609280 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:25.583380 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.583421 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.583453 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.583517 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.585776 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.591160 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.591423 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.594095 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.606684 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.606742 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.606778 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.606808 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.606870 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.607440 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.607517 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.607882 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.608570 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.611478 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.612111 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.612190 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.612224 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.612283 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.612410 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.612520 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.612558 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.614436 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.614532 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.616915 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.617002 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.617114 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.619393 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.621245 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.621342 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.621630 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.621720 140212176609280 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:25.621829 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.621868 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.621899 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.621962 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.624189 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.629603 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.629876 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.632548 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.645190 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.645247 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.645284 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.645315 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.645377 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.645955 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.646032 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.646389 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.647079 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.649620 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.650262 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.650340 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.650375 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.650434 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.650561 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.650675 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.650714 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.652878 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.652975 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.655362 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.655443 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.655560 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.657814 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.659676 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.659772 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.660061 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.660142 140212176609280 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:25.660251 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.660291 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.660322 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.660385 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.662613 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.667988 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.668248 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.670946 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.683525 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.683583 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.683619 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.683650 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.683712 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.684289 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.684367 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.684732 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.685420 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.687994 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.688626 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.688703 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.688739 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.688799 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.688931 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.689042 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.689082 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.690966 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.691061 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.693438 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.693519 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.693629 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.695923 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.697786 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.697884 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.698174 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.698256 140212176609280 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:25.698365 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:25.698404 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:25.698435 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:25.698497 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.700727 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:25.706135 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.706394 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:25.709046 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:25.721687 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:25.721743 140212176609280 attention.py:418] Single window, no scan.
I0123 11:01:25.721778 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:25.721809 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.721872 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.722447 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.722525 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.722882 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.723569 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.726469 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.727096 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.727174 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:25.727209 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:25.727267 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.727395 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:25.727512 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:01:25.727553 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.729419 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.729515 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.731906 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.731987 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:01:25.732099 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:25.734381 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:01:25.736238 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.736335 140212176609280 nn_components.py:261] mlp: residual
I0123 11:01:25.736622 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:25.736708 140212176609280 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:25.739527 140212176609280 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:30.225021 140212176609280 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:01:30.769517 140212176609280 training_loop.py:409] No working directory specified.
I0123 11:01:30.769755 140212176609280 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:01:30.770982 140212176609280 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:01:33.980943 140212176609280 training_loop.py:447] Only restoring trainable parameters.
I0123 11:01:33.981693 140212176609280 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:01:33.981785 140212176609280 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.981840 140212176609280 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.981886 140212176609280 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.981929 140212176609280 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.981971 140212176609280 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.982010 140212176609280 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982051 140212176609280 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982090 140212176609280 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.982128 140212176609280 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.982167 140212176609280 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982209 140212176609280 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.982250 140212176609280 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.982289 140212176609280 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.982333 140212176609280 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982375 140212176609280 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.982414 140212176609280 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982453 140212176609280 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982493 140212176609280 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.982532 140212176609280 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.982599 140212176609280 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982645 140212176609280 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.982684 140212176609280 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.982724 140212176609280 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.982764 140212176609280 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982804 140212176609280 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.982844 140212176609280 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982883 140212176609280 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.982923 140212176609280 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.982963 140212176609280 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.983001 140212176609280 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.983041 140212176609280 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.983080 140212176609280 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.983120 140212176609280 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.983159 140212176609280 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.983199 140212176609280 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.983238 140212176609280 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.983279 140212176609280 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.983318 140212176609280 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.983358 140212176609280 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.983398 140212176609280 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.983441 140212176609280 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.983484 140212176609280 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.983524 140212176609280 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.983565 140212176609280 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.983605 140212176609280 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.983655 140212176609280 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.983698 140212176609280 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.983737 140212176609280 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.983778 140212176609280 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.983817 140212176609280 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.983857 140212176609280 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.983896 140212176609280 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.983933 140212176609280 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.983970 140212176609280 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984008 140212176609280 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.984047 140212176609280 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984085 140212176609280 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984122 140212176609280 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.984160 140212176609280 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.984197 140212176609280 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984235 140212176609280 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.984272 140212176609280 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.984311 140212176609280 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.984348 140212176609280 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984385 140212176609280 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.984424 140212176609280 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984461 140212176609280 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984497 140212176609280 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.984534 140212176609280 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.984573 140212176609280 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984611 140212176609280 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.984647 140212176609280 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.984694 140212176609280 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.984734 140212176609280 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984771 140212176609280 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.984809 140212176609280 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984849 140212176609280 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.984887 140212176609280 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.984925 140212176609280 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.984963 140212176609280 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985001 140212176609280 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.985039 140212176609280 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.985075 140212176609280 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.985114 140212176609280 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985152 140212176609280 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.985189 140212176609280 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985228 140212176609280 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985265 140212176609280 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.985302 140212176609280 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.985339 140212176609280 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985377 140212176609280 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.985414 140212176609280 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.985451 140212176609280 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.985487 140212176609280 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985527 140212176609280 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.985563 140212176609280 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985599 140212176609280 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985638 140212176609280 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.985690 140212176609280 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.985736 140212176609280 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985777 140212176609280 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.985814 140212176609280 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.985852 140212176609280 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.985890 140212176609280 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.985929 140212176609280 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.985967 140212176609280 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.986003 140212176609280 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.986039 140212176609280 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.986078 140212176609280 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.986114 140212176609280 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.986151 140212176609280 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.986190 140212176609280 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:33.986227 140212176609280 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:33.986265 140212176609280 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.986303 140212176609280 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.986342 140212176609280 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.986381 140212176609280 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.986418 140212176609280 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:33.986458 140212176609280 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:33.986497 140212176609280 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:33.986535 140212176609280 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:33.986565 140212176609280 training_loop.py:725] Total parameters: 152072288
I0123 11:01:33.986881 140212176609280 training_loop.py:739] Total state size: 0
I0123 11:01:34.010009 140212176609280 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:01:34.010367 140212176609280 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:01:34.011055 140212176609280 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:01:34.011442 140212176609280 training_loop.py:89] registering functions: dict_keys([])
I0123 11:01:34.032537 140212176609280 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d b; f = lc_tangent f e d, on_line f b d; g = on_line g b a, on_line g f e; h = on_line h c b, on_line h f e; i = on_line i c a, on_line i f e; j = foot j e b a; k = mirror k e j; l = foot l e c b; m = mirror m e l; n = foot n e c a; o = mirror o e n; p = on_line p i o, on_line p g k; q = on_line q g k, on_line q h m; r = on_line r i o, on_line r h m; s = circle s p q r; t = foot t e c d; u = mirror u e t; v = on_circle v d u, on_line v r u ? coll v d s
I0123 11:01:45.312024 140212176609280 ddar.py:60] Depth 1/1000 time = 11.153725385665894
I0123 11:02:13.037364 140212176609280 ddar.py:60] Depth 2/1000 time = 27.725071668624878
I0123 11:03:08.655367 140212176609280 ddar.py:60] Depth 3/1000 time = 55.6174840927124
I0123 11:03:59.834675 140212176609280 ddar.py:60] Depth 4/1000 time = 51.178704500198364
I0123 11:04:57.061746 140212176609280 ddar.py:60] Depth 5/1000 time = 57.22634696960449
I0123 11:06:01.401012 140212176609280 ddar.py:60] Depth 6/1000 time = 64.33873081207275
I0123 11:07:05.323213 140212176609280 ddar.py:60] Depth 7/1000 time = 63.92151618003845
I0123 11:08:13.929735 140212176609280 ddar.py:60] Depth 8/1000 time = 68.60591721534729
I0123 11:09:25.073194 140212176609280 ddar.py:60] Depth 9/1000 time = 71.14295220375061
I0123 11:10:36.285325 140212176609280 ddar.py:60] Depth 10/1000 time = 71.21167731285095
I0123 11:11:47.731403 140212176609280 ddar.py:60] Depth 11/1000 time = 71.44436597824097
I0123 11:12:57.353079 140212176609280 ddar.py:60] Depth 12/1000 time = 69.62072110176086
I0123 11:14:09.018151 140212176609280 ddar.py:60] Depth 13/1000 time = 71.66441988945007
I0123 11:15:19.717944 140212176609280 ddar.py:60] Depth 14/1000 time = 70.69914674758911
I0123 11:16:32.098575 140212176609280 ddar.py:60] Depth 15/1000 time = 71.54273843765259
I0123 11:17:43.445670 140212176609280 ddar.py:60] Depth 16/1000 time = 71.16268348693848
I0123 11:18:53.916589 140212176609280 ddar.py:60] Depth 17/1000 time = 70.4498028755188
I0123 11:18:53.917268 140212176609280 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:18:53.917397 140212176609280 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 11:18:53.917436 140212176609280 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D b d d e 02 ; f : C b d f 03 T d e e f 04 ; g : C a b g 05 C e f g 06 ; h : C b c h 07 C e f h 08 ; i : C a c i 09 C e f i 10 ; j : C a b j 11 T a b e j 12 ; k : C e j k 13 D e j j k 14 ; l : C b c l 15 T b c e l 16 ; m : C e l m 17 D e l l m 18 ; n : C a c n 19 T a c e n 20 ; o : C e n o 21 D e n n o 22 ; p : C g k p 23 C i o p 24 ; q : C g k q 25 C h m q 26 ; r : C h m r 27 C i o r 28 ; s : D p s q s 29 D q s r s 30 ; t : C c d t 31 T c d e t 32 ; u : C e t u 33 D e t t u 34 ; v : C r u v 35 D d u d v 36 ? C v d s {F1} x00
I0123 11:18:53.917467 140212176609280 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D b d d e 02 ; f : C b d f 03 T d e e f 04 ; g : C a b g 05 C e f g 06 ; h : C b c h 07 C e f h 08 ; i : C a c i 09 C e f i 10 ; j : C a b j 11 T a b e j 12 ; k : C e j k 13 D e j j k 14 ; l : C b c l 15 T b c e l 16 ; m : C e l m 17 D e l l m 18 ; n : C a c n 19 T a c e n 20 ; o : C e n o 21 D e n n o 22 ; p : C g k p 23 C i o p 24 ; q : C g k q 25 C h m q 26 ; r : C h m r 27 C i o r 28 ; s : D p s q s 29 D q s r s 30 ; t : C c d t 31 T c d e t 32 ; u : C e t u 33 D e t t u 34 ; v : C r u v 35 D d u d v 36 ? C v d s {F1} x00
I0123 11:18:54.076048 140212176609280 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.076267 140212176609280 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:18:54.076370 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.076443 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.076524 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.076589 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.076653 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.076715 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.076777 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.076838 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.076900 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.076963 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.077026 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.077087 140212176609280 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:18:54.077127 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.077171 140212176609280 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:18:54.077277 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.077317 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.077345 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.079315 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.081831 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.087681 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.087960 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.090608 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.094592 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.094647 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.094683 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.094714 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.094776 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.095421 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.095496 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.095848 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.096602 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.099147 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.099771 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.099848 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.099882 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.099944 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.100070 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.100393 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.100442 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.102371 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.102463 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.104901 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.104978 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.105404 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.107906 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.109835 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.109928 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.110219 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.110299 140212176609280 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:18:54.110406 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.110443 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.110472 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.112250 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.114554 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.120167 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.120425 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.122999 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.126667 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.126721 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.126755 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.126784 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.126845 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.127445 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.127519 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.127870 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.128618 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.131042 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.131652 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.131726 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.131759 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.131817 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.131943 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.132260 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.132301 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.134262 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.134357 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.136799 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.136878 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.137300 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.139531 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.141413 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.141506 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.141806 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.141886 140212176609280 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:18:54.141992 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.142030 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.142059 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.143910 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.146207 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.151714 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.151970 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.154526 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.158227 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.158281 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.158314 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.158344 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.158404 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.158952 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.159025 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.159380 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.160132 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.162737 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.163348 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.163424 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.163457 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.163514 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.163638 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.164000 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.164041 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.165931 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.166042 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.168485 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.168562 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.168982 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.171216 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.173503 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.173597 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.173893 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.173973 140212176609280 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:18:54.174080 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.174117 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.174145 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.175916 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.178204 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.183730 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.183982 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.186495 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.190100 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.190154 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.190186 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.190214 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.190274 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.190867 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.190942 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.191290 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.192031 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.194455 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.195059 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.195133 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.195165 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.195221 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.195346 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.195655 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.195695 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.197624 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.197728 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.200152 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.200230 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.200646 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.202887 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.204753 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.204847 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.205131 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.205209 140212176609280 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:18:54.205314 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.205351 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.205381 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.207228 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.209530 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.215063 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.215318 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.217844 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.221513 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.221564 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.221598 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.221626 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.221695 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.222241 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.222314 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.222664 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.223412 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.225823 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.226424 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.226499 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.226531 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.226588 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.226713 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.227073 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.227114 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.228987 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.229079 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.231496 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.231580 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.231993 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.234247 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.236202 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.236295 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.236580 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.236659 140212176609280 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:18:54.236765 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.236803 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.236832 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.238606 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.240898 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.246532 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.246787 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.249316 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.252913 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.252966 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.253000 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.253030 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.253091 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.253692 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.253768 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.254120 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.254875 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.257290 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.257972 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.258059 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.258094 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.258154 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.258303 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.258631 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.258674 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.260622 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.260714 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.263300 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.263383 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.263800 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.266039 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.267924 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.268015 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.268298 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.268377 140212176609280 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:18:54.268483 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.268520 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.268550 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.270407 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.272688 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.278209 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.278462 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.280987 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.284998 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.285052 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.285085 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.285115 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.285175 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.285727 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.285802 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.286151 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.286894 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.289287 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.289913 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.289989 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.290022 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.290079 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.290204 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.290565 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.290607 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.292499 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.292590 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.295004 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.295083 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.295501 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.297722 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.299669 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.299762 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.300045 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.300124 140212176609280 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:18:54.300231 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.300268 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.300297 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.302076 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.304368 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.309880 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.310133 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.312652 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.316259 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.316313 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.316347 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.316376 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.316437 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.317041 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.317115 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.317466 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.318224 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.320680 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.321287 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.321362 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.321395 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.321452 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.321575 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.321893 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.321935 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.323870 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.323961 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.326401 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.326481 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.326899 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.329130 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.331014 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.331108 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.331393 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.331472 140212176609280 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:18:54.331577 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.331614 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.331643 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.333466 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.335769 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.341298 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.341554 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.344076 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.347747 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.347800 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.347833 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.347862 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.347922 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.348469 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.348544 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.348895 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.349652 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.352076 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.352677 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.352751 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.352784 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.352841 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.352964 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.353276 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.353316 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.355249 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.355341 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.357746 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.357823 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.358242 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.360475 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.362379 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.362480 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.362772 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.362852 140212176609280 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:18:54.362958 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.362995 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.363025 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.364875 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.367166 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.372712 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.372967 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.375483 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.379174 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.379227 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.379260 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.379288 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.379348 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.379888 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.379960 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.380308 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.381051 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.383474 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.384076 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.384150 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.384184 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.384241 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.384367 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.384686 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.384728 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.386687 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.386781 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.389199 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.389276 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.389702 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.391935 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.393932 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.394033 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.394322 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.394401 140212176609280 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:18:54.394506 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.394544 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.394573 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.396749 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.399060 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.404587 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.404842 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.407422 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.411131 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.411185 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.411218 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.411247 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.411308 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.411851 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.411926 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.412277 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.413014 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.415426 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.416034 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.416109 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.416143 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.416200 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.416326 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.416638 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.416679 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.418627 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.418720 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.421138 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.421215 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.421629 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.423854 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.425758 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.425852 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.426145 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.426224 140212176609280 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:18:54.426328 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.426365 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.426394 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.428222 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.430507 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.436001 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.436257 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.438802 140212176609280 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:18:54.442495 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.442548 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.442582 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.442611 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.442671 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.443221 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.443294 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.443643 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.444410 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.446858 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.447466 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.447541 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.447575 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.447633 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.447759 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.448073 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.448114 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.450066 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.450160 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.452601 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.452679 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.453094 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.455321 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.457194 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.457287 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.457572 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.457834 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.457902 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.457957 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458009 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458060 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458111 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458163 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458213 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458262 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458311 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458361 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458410 140212176609280 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:18:54.458444 140212176609280 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:18:54.461340 140212176609280 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:18:54.506204 140212176609280 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.506288 140212176609280 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:18:54.506339 140212176609280 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:18:54.506440 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.506477 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.506506 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.506566 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.508894 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.514312 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.514574 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.517145 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.530071 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.530128 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.530163 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.530192 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.530253 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.530814 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.530889 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.531250 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.531948 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.534512 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.535129 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.535204 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.535238 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.535296 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.535422 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.535529 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.535566 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.537389 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.537482 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.539890 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.539970 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.540077 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.542314 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.544151 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.544244 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.544534 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.544613 140212176609280 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:18:54.544720 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.544757 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.544787 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.544850 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.547064 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.552401 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.552659 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.555298 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.567675 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.567729 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.567763 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.567793 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.567855 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.568405 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.568479 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.568831 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.569564 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.572019 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.572629 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.572704 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.572737 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.572795 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.572921 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.573027 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.573063 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.574894 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.574986 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.577351 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.577428 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.577534 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.580181 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.582018 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.582112 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.582400 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.582478 140212176609280 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:18:54.582584 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.582621 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.582650 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.582710 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.584903 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.590204 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.590458 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.593058 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.605345 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.605400 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.605434 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.605464 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.605524 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.606081 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.606157 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.606508 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.607236 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.609660 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.610277 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.610354 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.610388 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.610446 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.610573 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.610681 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.610717 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.612555 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.612647 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.615036 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.615114 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.615221 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.617471 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.619309 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.619402 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.619690 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.619769 140212176609280 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:18:54.619877 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.619914 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.619944 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.620006 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.622224 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.627563 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.627823 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.630661 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.643000 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.643056 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.643090 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.643119 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.643179 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.643727 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.643802 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.644153 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.644879 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.647307 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.647905 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.647984 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.648019 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.648078 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.648206 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.648314 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.648351 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.650196 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.650289 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.652672 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.652749 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.652856 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.655110 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.656958 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.657052 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.657339 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.657419 140212176609280 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:18:54.657525 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.657562 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.657592 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.657662 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.659865 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.665167 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.665424 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.668054 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.680351 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.680406 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.680440 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.680469 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.680529 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.681075 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.681149 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.681499 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.682246 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.684677 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.685283 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.685363 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.685397 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.685455 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.685580 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.685695 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.685734 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.687551 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.687643 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.690008 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.690085 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.690190 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.692862 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.694701 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.694796 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.695085 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.695163 140212176609280 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:18:54.695269 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.695306 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.695334 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.695395 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.697595 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.702992 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.703250 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.705899 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.718239 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.718293 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.718327 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.718358 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.718419 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.718965 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.719038 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.719385 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.720103 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.722507 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.723114 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.723187 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.723226 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.723287 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.723413 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.723522 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.723559 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.725406 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.725498 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.727894 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.727973 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.728079 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.730328 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.732169 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.732262 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.732550 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.732628 140212176609280 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:18:54.732734 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.732772 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.732801 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.732861 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.735073 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.740441 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.740699 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.743324 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.755669 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.755724 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.755759 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.755789 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.755849 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.756388 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.756462 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.756813 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.757544 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.759964 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.760567 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.760642 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.760674 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.760739 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.760866 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.760971 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.761008 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.762831 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.762924 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.765289 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.765365 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.765471 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.767687 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.769502 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.769593 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.769888 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.769968 140212176609280 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:18:54.770073 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.770110 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.770139 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.770199 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.772370 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.777671 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.777931 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.780533 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.792827 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.792881 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.792915 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.792944 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.793004 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.793547 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.793621 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.793977 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.794706 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.797099 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.797705 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.797780 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.797814 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.797871 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.798006 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.798115 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.798151 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.799965 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.800057 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.802433 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.802512 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.802619 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.805300 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.807135 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.807229 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.807518 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.807598 140212176609280 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:18:54.807704 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.807741 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.807770 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.807830 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.810039 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.815358 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.815614 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.818246 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.830495 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.830549 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.830581 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.830610 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.830670 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.831214 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.831287 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.831633 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.832299 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.834766 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.835367 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.835442 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.835475 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.835532 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.835669 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.835778 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.835815 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.837634 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.837734 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.840114 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.840192 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.840299 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.842526 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.844354 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.844447 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.844732 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.844811 140212176609280 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:18:54.844918 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.844954 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.844983 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.845044 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.847225 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.852562 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.852820 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.855455 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.868461 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.868516 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.868550 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.868580 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.868642 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.869187 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.869261 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.869613 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.870313 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.872834 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.873436 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.873511 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.873545 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.873603 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.873737 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.873851 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.873888 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.875729 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.875822 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.878232 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.878312 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.878421 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.880656 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.882495 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.882589 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.882876 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.882956 140212176609280 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:18:54.883063 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.883101 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.883131 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.883194 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.885406 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.890884 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.891139 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.893780 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.906176 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.906230 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.906265 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.906295 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.906356 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.906903 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.906976 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.907329 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.907997 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.910485 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.911086 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.911163 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.911197 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.911253 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.911377 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.911490 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.911527 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.913357 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.913449 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.915839 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.915918 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.916025 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.918689 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.920531 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.920625 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.920916 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.920995 140212176609280 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:18:54.921102 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:54.921139 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:54.921169 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:54.921231 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.923464 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:54.928805 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.929065 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:54.931748 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:54.944251 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:54.944306 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:54.944339 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:54.944368 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.944429 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.944972 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.945046 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.945401 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.946081 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.948736 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.949342 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.949417 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:54.949451 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:54.949509 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.949636 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:54.949753 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:54.949796 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.951641 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.951733 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.954131 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.954209 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:54.954314 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:54.956559 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:54.958412 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.958506 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:54.958796 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:54.958881 140212176609280 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:18:54.961701 140212176609280 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:18:55.011691 140212176609280 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.011776 140212176609280 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:18:55.011828 140212176609280 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:18:55.011932 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.011969 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.011998 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.012061 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.014347 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.019797 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.020056 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.022640 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.035125 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.035180 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.035215 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.035244 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.035305 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.035857 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.035933 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.036283 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.036962 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.039381 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.039987 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.040061 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.040101 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.040161 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.040287 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.040392 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.040430 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.042451 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.042545 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.044938 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.045015 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.045123 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.047322 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.049171 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.049266 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.049555 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.049635 140212176609280 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:18:55.049755 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.049792 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.049822 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.049884 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.052114 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.057558 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.057832 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.060403 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.072820 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.072875 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.072909 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.072940 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.073000 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.073545 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.073620 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.073983 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.074651 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.077065 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.077682 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.077759 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.077793 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.077859 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.077986 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.078093 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.078130 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.080029 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.080122 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.082512 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.082591 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.082699 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.084858 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.086733 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.086829 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.087121 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.087201 140212176609280 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:18:55.087307 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.087345 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.087375 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.087438 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.089836 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.095277 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.095537 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.098115 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.111012 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.111066 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.111100 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.111131 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.111192 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.111736 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.111809 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.112160 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.112831 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.115237 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.115840 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.115916 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.115950 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.116008 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.116140 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.116247 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.116284 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.118190 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.118282 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.120645 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.120723 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.120830 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.123018 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.124845 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.124938 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.125225 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.125305 140212176609280 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:18:55.125411 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.125448 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.125477 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.125541 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.127765 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.133166 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.133423 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.135976 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.148358 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.148412 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.148447 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.148477 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.148537 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.149082 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.149156 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.149509 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.150189 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.152605 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.153208 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.153284 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.153318 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.153378 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.153512 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.153619 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.153664 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.155575 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.155668 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.158047 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.158125 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.158232 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.160410 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.162233 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.162328 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.162615 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.162695 140212176609280 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:18:55.162802 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.162840 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.162870 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.162931 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.165158 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.170592 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.170852 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.173398 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.185706 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.185761 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.185796 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.185826 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.185888 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.186434 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.186507 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.186856 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.187522 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.189918 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.190524 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.190599 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.190633 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.190691 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.190816 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.190930 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.190967 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.193042 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.193135 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.195655 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.195734 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.195841 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.198004 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.199822 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.199917 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.200206 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.200287 140212176609280 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:18:55.200395 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.200432 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.200463 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.200525 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.202754 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.208149 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.208412 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.210982 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.223829 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.223884 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.223919 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.223949 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.224009 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.224550 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.224624 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.224978 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.225659 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.228083 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.228681 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.228754 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.228787 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.228846 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.228971 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.229086 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.229124 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.231034 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.231128 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.233486 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.233564 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.233678 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.235844 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.237671 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.237765 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.238052 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.238132 140212176609280 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:18:55.238239 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.238276 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.238306 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.238368 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.240569 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.245951 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.246207 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.248742 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.261010 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.261065 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.261100 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.261130 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.261191 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.261750 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.261826 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.262183 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.262851 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.265280 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.265896 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.265973 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.266006 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.266065 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.266190 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.266297 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.266341 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.268241 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.268334 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.270724 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.270804 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.270912 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.273077 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.274916 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.275012 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.275298 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.275377 140212176609280 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:18:55.275484 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.275521 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.275553 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.275615 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.277840 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.283393 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.283650 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.286185 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.298465 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.298520 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.298555 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.298586 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.298646 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.299194 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.299269 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.299624 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.300298 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.302735 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.303340 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.303414 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.303448 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.303506 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.303632 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.303739 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.303776 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.305674 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.305766 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.308149 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.308226 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.308334 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.310521 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.312336 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.312428 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.312714 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.312793 140212176609280 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:18:55.312899 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.312936 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.312966 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.313028 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.315223 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.320612 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.320870 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.323413 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.336188 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.336242 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.336277 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.336307 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.336367 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.336909 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.336982 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.337335 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.338017 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.340417 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.341015 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.341089 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.341123 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.341181 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.341305 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.341411 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.341447 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.343360 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.343457 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.345846 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.345924 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.346031 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.348206 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.350045 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.350139 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.350428 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.350506 140212176609280 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:18:55.350613 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.350651 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.350681 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.350744 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.352952 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.358404 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.358661 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.361219 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.373487 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.373543 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.373576 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.373606 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.373676 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.374217 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.374290 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.374635 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.375297 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.377704 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.378308 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.378381 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.378415 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.378472 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.378595 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.378703 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.378740 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.380813 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.380909 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.383264 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.383342 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.383450 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.385601 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.387432 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.387524 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.387810 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.387889 140212176609280 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:18:55.387995 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.388034 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.388064 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.388127 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.390412 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.395782 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.396039 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.398588 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.410911 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.410966 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.411001 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.411031 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.411089 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.411627 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.411699 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.412050 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.412721 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.415136 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.415737 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.415811 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.415845 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.415904 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.416027 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.416135 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.416171 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.418063 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.418155 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.420523 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.420600 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.420708 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.422871 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.424695 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.424787 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.425072 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.425150 140212176609280 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:18:55.425256 140212176609280 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:18:55.425293 140212176609280 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:18:55.425323 140212176609280 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:18:55.425384 140212176609280 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.427588 140212176609280 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:18:55.432949 140212176609280 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.433207 140212176609280 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:18:55.435742 140212176609280 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:18:55.448449 140212176609280 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:18:55.448502 140212176609280 attention.py:418] Single window, no scan.
I0123 11:18:55.448536 140212176609280 transformer_layer.py:389] tlayer: self-attention.
I0123 11:18:55.448565 140212176609280 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.448623 140212176609280 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.449162 140212176609280 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.449235 140212176609280 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.449583 140212176609280 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.450256 140212176609280 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.452664 140212176609280 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.453266 140212176609280 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.453339 140212176609280 transformer_layer.py:468] tlayer: End windows.
I0123 11:18:55.453372 140212176609280 transformer_layer.py:472] tlayer: final FFN.
I0123 11:18:55.453430 140212176609280 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.453555 140212176609280 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:18:55.453669 140212176609280 nn_components.py:325] mlp: activation = None
I0123 11:18:55.453707 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.455611 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.455703 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.458078 140212176609280 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.458160 140212176609280 transformer_base.py:443] tbase: final FFN
I0123 11:18:55.458269 140212176609280 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:18:55.460426 140212176609280 nn_components.py:329] mlp: final activation = None
I0123 11:18:55.462260 140212176609280 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.462353 140212176609280 nn_components.py:261] mlp: residual
I0123 11:18:55.462639 140212176609280 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:18:55.462722 140212176609280 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:18:55.465540 140212176609280 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:11.405159 140212176609280 alphageometry.py:566] LM output (score=-1.934037): "t : C m s t 37 D m t s t 38 ;"
I0123 11:19:11.405380 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405422 140212176609280 alphageometry.py:566] LM output (score=-1.954484): "t : P c m l t 37 ;"
I0123 11:19:11.405451 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405478 140212176609280 alphageometry.py:566] LM output (score=-2.150451): "t : P f q p t 37 ;"
I0123 11:19:11.405503 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405528 140212176609280 alphageometry.py:566] LM output (score=-2.255949): "t : T m q t v 37 ;"
I0123 11:19:11.405551 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405575 140212176609280 alphageometry.py:566] LM output (score=-2.305621): "t : P f q r t 37 ;"
I0123 11:19:11.405599 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405623 140212176609280 alphageometry.py:566] LM output (score=-2.326902): "t : P f q l t 37 ;"
I0123 11:19:11.405652 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405678 140212176609280 alphageometry.py:566] LM output (score=-2.404785): "t : P c h c t 37 ;"
I0123 11:19:11.405700 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405722 140212176609280 alphageometry.py:566] LM output (score=-2.497958): "t : P c i l t 37 ;"
I0123 11:19:11.405745 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405770 140212176609280 alphageometry.py:566] LM output (score=-2.537084): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.405795 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405819 140212176609280 alphageometry.py:566] LM output (score=-2.537339): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.405842 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405865 140212176609280 alphageometry.py:566] LM output (score=-2.538100): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.405888 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405910 140212176609280 alphageometry.py:566] LM output (score=-2.539196): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.405952 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.405976 140212176609280 alphageometry.py:566] LM output (score=-2.541138): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406000 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406023 140212176609280 alphageometry.py:566] LM output (score=-2.541149): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406044 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406066 140212176609280 alphageometry.py:566] LM output (score=-2.541518): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406088 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406110 140212176609280 alphageometry.py:566] LM output (score=-2.542403): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406131 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406153 140212176609280 alphageometry.py:566] LM output (score=-2.542450): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406174 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406195 140212176609280 alphageometry.py:566] LM output (score=-2.542625): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406215 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406237 140212176609280 alphageometry.py:566] LM output (score=-2.542683): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406259 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406281 140212176609280 alphageometry.py:566] LM output (score=-2.543391): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406302 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406323 140212176609280 alphageometry.py:566] LM output (score=-2.544394): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406348 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406371 140212176609280 alphageometry.py:566] LM output (score=-2.544990): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406393 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406415 140212176609280 alphageometry.py:566] LM output (score=-2.545688): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406437 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406459 140212176609280 alphageometry.py:566] LM output (score=-2.546452): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406481 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406503 140212176609280 alphageometry.py:566] LM output (score=-2.546786): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406526 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406548 140212176609280 alphageometry.py:566] LM output (score=-2.549262): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406570 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406594 140212176609280 alphageometry.py:566] LM output (score=-2.549423): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406617 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406639 140212176609280 alphageometry.py:566] LM output (score=-2.550549): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406661 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406683 140212176609280 alphageometry.py:566] LM output (score=-2.552817): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406705 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406728 140212176609280 alphageometry.py:566] LM output (score=-2.556721): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406753 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406776 140212176609280 alphageometry.py:566] LM output (score=-2.558722): "t : P d t d t 37 T g u t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406799 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406821 140212176609280 alphageometry.py:566] LM output (score=-2.559006): "t : C m r t 37 D m t t v 38 ? C d s s {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 11:19:11.406844 140212176609280 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 11:19:11.406874 140212176609280 alphageometry.py:540] Depth 1. There are 0 nodes to expand:
I0123 11:19:11.406905 140212176609280 alphageometry.py:540] Depth 2. There are 0 nodes to expand:
I0123 11:19:11.406931 140212176609280 alphageometry.py:540] Depth 3. There are 0 nodes to expand:
I0123 11:19:11.406956 140212176609280 alphageometry.py:540] Depth 4. There are 0 nodes to expand:
I0123 11:19:11.406981 140212176609280 alphageometry.py:540] Depth 5. There are 0 nodes to expand:
I0123 11:19:11.407004 140212176609280 alphageometry.py:540] Depth 6. There are 0 nodes to expand:
I0123 11:19:11.407027 140212176609280 alphageometry.py:540] Depth 7. There are 0 nodes to expand:
I0123 11:19:11.407051 140212176609280 alphageometry.py:540] Depth 8. There are 0 nodes to expand:
I0123 11:19:11.407074 140212176609280 alphageometry.py:540] Depth 9. There are 0 nodes to expand:
I0123 11:19:11.407097 140212176609280 alphageometry.py:540] Depth 10. There are 0 nodes to expand:
I0123 11:19:11.407120 140212176609280 alphageometry.py:540] Depth 11. There are 0 nodes to expand:
I0123 11:19:11.407142 140212176609280 alphageometry.py:540] Depth 12. There are 0 nodes to expand:
I0123 11:19:11.407164 140212176609280 alphageometry.py:540] Depth 13. There are 0 nodes to expand:
I0123 11:19:11.407187 140212176609280 alphageometry.py:540] Depth 14. There are 0 nodes to expand:
I0123 11:19:11.407209 140212176609280 alphageometry.py:540] Depth 15. There are 0 nodes to expand:
