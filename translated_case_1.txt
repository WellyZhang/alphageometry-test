I0123 11:21:22.727374 140230058647552 inference_utils.py:69] Parsing gin configuration.
I0123 11:21:22.727483 140230058647552 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:21:22.727700 140230058647552 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:21:22.727733 140230058647552 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:21:22.727761 140230058647552 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:21:22.727788 140230058647552 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:21:22.727815 140230058647552 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:21:22.727841 140230058647552 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:21:22.727866 140230058647552 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:21:22.727891 140230058647552 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:21:22.727916 140230058647552 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:21:22.727940 140230058647552 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:21:22.727985 140230058647552 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:21:22.728123 140230058647552 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:21:22.728336 140230058647552 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:21:22.728435 140230058647552 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:21:22.734685 140230058647552 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:21:22.734803 140230058647552 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:21:22.735123 140230058647552 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:21:22.735227 140230058647552 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:21:22.735510 140230058647552 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:21:22.735611 140230058647552 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:21:22.736014 140230058647552 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:21:22.736114 140230058647552 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:21:22.739791 140230058647552 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:21:22.841626 140230058647552 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:21:22.842352 140230058647552 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:21:22.849086 140230058647552 training_loop.py:335] Process 0 of 1
I0123 11:21:22.849141 140230058647552 training_loop.py:336] Local device count = 1
I0123 11:21:22.849182 140230058647552 training_loop.py:337] Number of replicas = 1
I0123 11:21:22.849214 140230058647552 training_loop.py:339] Using random number seed 42
I0123 11:21:23.317965 140230058647552 training_loop.py:359] Initializing the model.
I0123 11:21:23.731242 140230058647552 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.731530 140230058647552 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:21:23.731641 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.731723 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.731803 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.731888 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.731963 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.732036 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.732109 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.732181 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.732252 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.732323 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.732393 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.732463 140230058647552 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:21:23.732503 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:23.732548 140230058647552 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:21:23.732665 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:23.732705 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:23.732736 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:23.734839 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.740270 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:23.751708 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.751999 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:23.756557 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:23.767603 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:23.767664 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:23.767704 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:23.767738 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.767804 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.769067 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.769149 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.769896 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.772461 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.778840 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.780205 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.780293 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:23.780330 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:23.780392 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.780526 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:23.780877 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:23.780925 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.782932 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.783041 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.786052 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.786138 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:23.786669 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:23.797229 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.806369 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.806474 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.806785 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.806871 140230058647552 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:21:23.806989 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:23.807030 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:23.807062 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:23.809003 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.811618 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:23.817453 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.817736 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:23.820472 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:23.824474 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:23.824532 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:23.824570 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:23.824602 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.824666 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.825279 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.825361 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.825746 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.826548 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.829128 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.829798 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.829880 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:23.829917 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:23.829977 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.830110 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:23.830472 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:23.830517 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.832562 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.832661 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.835266 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.835351 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:23.835802 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:23.838220 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.840211 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.840314 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.840618 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.840703 140230058647552 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:21:23.840818 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:23.840860 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:23.840893 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:23.843388 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.846005 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:23.851739 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.852007 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:23.854699 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:23.858882 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:23.858950 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:23.858999 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:23.859042 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.859119 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.859742 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.859823 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.860198 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.861183 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.863973 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.864647 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.864726 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:23.864764 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:23.864823 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.864956 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:23.865278 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:23.865322 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.867323 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.867423 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.870076 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.870169 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:23.870681 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:23.873023 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.875020 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.875122 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.875432 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.875517 140230058647552 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:21:23.875629 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:23.875669 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:23.875699 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:23.877634 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.880125 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:23.885962 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.886250 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:23.888970 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:23.892905 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:23.892961 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:23.892998 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:23.893028 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.893090 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.893663 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.893744 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.894114 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.894926 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.897501 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.898154 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.898237 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:23.898273 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:23.898334 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.898485 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:23.898818 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:23.898863 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.900804 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.900900 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.903545 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.903633 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:23.904069 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:23.906411 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.908381 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.908480 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.908771 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.908854 140230058647552 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:21:23.908966 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:23.909006 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:23.909037 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:23.911030 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.913451 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:23.919847 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.920174 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:23.923329 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:23.927263 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:23.927322 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:23.927358 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:23.927390 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.927455 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.928052 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.928130 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.928493 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.929276 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.931898 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.932536 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.932615 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:23.932650 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:23.932709 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.932849 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:23.933183 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:23.933227 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.935206 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.935305 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.937880 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.937965 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:23.938431 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:23.940797 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.942809 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.942908 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.943213 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.943299 140230058647552 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:21:23.943413 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:23.943452 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:23.943484 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:23.945347 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.947776 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:23.953485 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.953758 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:23.956462 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:23.960257 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:23.960313 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:23.960353 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:23.960386 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.960450 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.961061 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.961140 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.961508 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.962311 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.965194 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.965829 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.965909 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:23.965943 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:23.966001 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.966137 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:23.966463 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:23.966507 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.968418 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.968517 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.971096 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.971177 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:23.971608 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:23.973946 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:23.975877 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.975974 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:23.976264 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.976346 140230058647552 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:21:23.976457 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:23.976497 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:23.976528 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:23.978399 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.980837 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:23.986499 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.986763 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:23.989451 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:23.993294 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:23.993353 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:23.993389 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:23.993420 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.993484 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.994059 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.994140 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.994503 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.995305 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.997879 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.998533 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.998617 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:23.998653 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:23.998711 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:23.998844 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:23.999171 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:23.999214 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.001493 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.001590 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.004121 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.004204 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.004640 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.151302 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.153556 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.153719 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.154044 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.154135 140230058647552 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:21:24.154254 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.154296 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.154329 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.156441 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.159033 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.164904 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.165180 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.167905 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:24.172234 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.172291 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.172329 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.172360 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.172423 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.173032 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.173109 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.173476 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.174282 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.176898 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.177539 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.177621 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.177665 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.177727 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.177859 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.178189 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.178233 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.180180 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.180275 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.182845 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.182934 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.183436 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.185740 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.187671 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.187773 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.188066 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.188153 140230058647552 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:21:24.188265 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.188305 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.188335 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.190260 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.192619 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.198353 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.198619 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.201318 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:24.205150 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.205206 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.205242 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.205273 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.205336 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.205917 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.205996 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.206367 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.207148 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.209708 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.210336 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.210415 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.210450 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.210509 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.210637 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.210958 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.211002 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.212925 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.213020 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.215586 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.215668 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.216102 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.218397 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.220332 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.220429 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.220719 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.220808 140230058647552 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:21:24.220921 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.220961 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.220992 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.222913 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.225277 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.231248 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.231509 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.234219 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:24.237984 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.238041 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.238077 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.238108 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.238170 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.238733 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.238813 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.239169 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.239985 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.242447 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.243083 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.243162 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.243198 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.243257 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.243385 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.243708 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.243752 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.245648 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.245744 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.248273 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.248353 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.248784 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.251091 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.253056 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.253153 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.253445 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.253532 140230058647552 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:21:24.253651 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.253692 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.253726 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.255581 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.258019 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.263573 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.263837 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.266502 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:24.270445 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.270504 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.270540 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.270570 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.270675 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.271245 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.271322 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.271675 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.272578 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.275052 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.275681 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.275759 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.275794 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.275851 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.275986 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.276309 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.276351 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.278312 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.278414 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.281165 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.281245 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.281688 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.283999 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.285908 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.286004 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.286292 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.286375 140230058647552 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:21:24.286492 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.286532 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.286563 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.288392 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.290802 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.296409 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.296672 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.299296 140230058647552 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:21:24.303468 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.303525 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.303562 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.303593 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.303662 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.304240 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.304318 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.304678 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.305453 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.307940 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.308565 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.308643 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.308678 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.308737 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.308871 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.309205 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.309249 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.311225 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.311320 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.313839 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.313922 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.314356 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.316677 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.318591 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.318688 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.318989 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.319278 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319349 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319415 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319473 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319529 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319584 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319638 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319692 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319746 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319799 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319853 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319906 140230058647552 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:21:24.319943 140230058647552 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:21:24.323506 140230058647552 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:21:24.371951 140230058647552 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.372037 140230058647552 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:21:24.372092 140230058647552 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:21:24.372198 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.372236 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.372266 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.372328 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.374953 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.380517 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.380779 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.383456 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.400240 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.400297 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.400334 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.400365 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.400429 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.401583 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.401669 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.402380 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.404403 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.409167 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.410496 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.410583 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.410619 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.410680 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.410811 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.410922 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.410962 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.412890 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.412985 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.415426 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.415508 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.415618 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.417867 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.419866 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.419963 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.420253 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.420336 140230058647552 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:21:24.420446 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.420485 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.420516 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.420580 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.422867 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.428419 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.428684 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.431404 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.444700 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.444757 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.444794 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.444826 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.444889 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.445458 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.445536 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.445909 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.446614 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.449155 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.449791 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.449874 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.449914 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.449975 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.450110 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.450222 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.450261 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.452199 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.452295 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.454719 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.454800 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.454908 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.457136 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.459084 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.459181 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.459467 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.459549 140230058647552 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:21:24.459660 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.459700 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.459733 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.459796 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.462066 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.467541 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.467799 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.470502 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.483773 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.483832 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.483868 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.483898 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.483961 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.484531 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.484609 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.484965 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.485672 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.488141 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.488770 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.488848 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.488883 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.488947 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.489078 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.489188 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.489228 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.491176 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.491274 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.493720 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.493802 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.493912 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.496159 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.498120 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.498218 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.498506 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.498588 140230058647552 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:21:24.498699 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.498739 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.498770 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.498834 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.501080 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.506557 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.506819 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.509492 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.522386 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.522442 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.522478 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.522510 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.522572 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.523128 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.523206 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.523559 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.524261 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.526768 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.527394 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.527472 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.527507 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.527567 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.527710 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.527823 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.527862 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.530107 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.530205 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.532616 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.532697 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.532808 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.535063 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.536952 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.537049 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.537339 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.537423 140230058647552 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:21:24.537536 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.537575 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.537606 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.537676 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.539977 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.545514 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.545792 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.548429 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.561506 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.561564 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.561599 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.561630 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.561701 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.562275 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.562361 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.562742 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.563472 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.566070 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.566736 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.566819 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.566856 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.566917 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.567069 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.567188 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.567229 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.569162 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.569257 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.571738 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.571823 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.571938 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.574337 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.576392 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.576489 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.576776 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.576857 140230058647552 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:21:24.576968 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.577008 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.577039 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.577101 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.583528 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.589187 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.589470 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.592279 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.605419 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.605478 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.605515 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.605545 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.605612 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.606225 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.606302 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.606673 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.607379 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.609948 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.610588 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.610665 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.610700 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.610763 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.610897 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.611017 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.611056 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.613014 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.613111 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.615540 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.615622 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.615740 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.618048 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.619958 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.620054 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.620340 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.620424 140230058647552 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:21:24.620537 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.620580 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.620611 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.620674 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.622954 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.628620 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.628879 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.631517 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.644867 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.644923 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.644958 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.644989 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.645052 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.645616 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.645701 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.646063 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.646760 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.649269 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.649966 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.650047 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.650082 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.650141 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.650276 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.650389 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.650434 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.652368 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.652464 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.654913 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.654995 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.655106 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.657358 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.659321 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.659419 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.659708 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.659790 140230058647552 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:21:24.659900 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.659939 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.659969 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.660032 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.662279 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.667802 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.668075 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.670776 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.683978 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.684034 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.684069 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.684098 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.684160 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.684768 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.684846 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.685204 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.685901 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.688384 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.689020 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.689100 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.689134 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.689197 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.689342 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.689456 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.689500 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.691404 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.691500 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.693983 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.694064 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.694174 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.696404 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.698297 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.698394 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.698679 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.698763 140230058647552 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:21:24.698874 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.698914 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.698945 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.699008 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.701237 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.706756 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.707020 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.709649 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.722668 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.722724 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.722759 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.722790 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.722853 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.723419 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.723498 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.723863 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.724560 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.727089 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.727772 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.727850 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.727885 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.727944 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.728076 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.728188 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.728226 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.730139 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.730237 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.732630 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.732709 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.732820 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.735064 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.737021 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.737118 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.737408 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.737492 140230058647552 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:21:24.737613 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.737659 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.737692 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.737757 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.740006 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.745475 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.745746 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.748760 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.761631 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.761696 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.761739 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.761775 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.761838 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.762458 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.762536 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.762898 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.763599 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.766089 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.766723 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.766800 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.766835 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.766891 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.767018 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.767127 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.767166 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.769072 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.769173 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.771674 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.771759 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.771870 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.774124 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.776003 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.776100 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.776388 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.776472 140230058647552 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:21:24.776585 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.776625 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.776657 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.776721 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.779019 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.784613 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.784880 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.787581 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.800614 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.800671 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.800707 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.800737 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.800799 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.801361 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.801439 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.801808 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.802518 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.805030 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.805717 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.805797 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.805832 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.805892 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.806027 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.806139 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.806178 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.808096 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.808199 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.810673 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.810754 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.810863 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.813091 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.815042 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.815140 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.815426 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.815510 140230058647552 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:21:24.815622 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.815662 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.815693 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.815756 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.818005 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.823431 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.823693 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.826333 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.839249 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.839305 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.839339 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.839370 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.839432 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.840000 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.840077 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.840437 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.841138 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.843700 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.844326 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.844405 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.844439 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.844498 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.844630 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.844744 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.844783 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.846677 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.846772 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.849182 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.849262 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.849370 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.852017 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.853922 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.854019 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.854305 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.854396 140230058647552 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:21:24.857274 140230058647552 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:21:24.915472 140230058647552 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.915563 140230058647552 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:21:24.915619 140230058647552 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:21:24.915726 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.915765 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.915794 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.915857 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.918240 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.923765 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.924037 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.926647 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.939434 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.939493 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.939528 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.939559 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.939621 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.940182 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.940260 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.940621 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.941302 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.943855 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.944478 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.944556 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.944591 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.944649 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.944779 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.944895 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.944935 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.946821 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.946923 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.949343 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.949424 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.949534 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.951808 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.953684 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.953782 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.954066 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.954149 140230058647552 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:21:24.954263 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.954302 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.954333 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.954396 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.956647 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:24.962039 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.962298 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:24.965124 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:24.977749 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:24.977805 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:24.977841 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:24.977872 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.977934 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.978505 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.978584 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.978942 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.979644 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.982172 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.982797 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.982875 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:24.982910 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:24.982970 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.983100 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:24.983210 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:24.983254 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.985102 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.985197 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.987571 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.987652 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:24.987761 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:24.990007 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:24.991839 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.991936 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:24.992221 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.992303 140230058647552 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:21:24.992412 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:24.992451 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:24.992481 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:24.992543 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:24.994782 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.000158 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.000418 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.003054 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.015552 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.015609 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.015644 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.015674 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.015736 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.016299 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.016376 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.016733 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.017419 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.020366 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.020995 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.021075 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.021110 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.021169 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.021299 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.021410 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.021449 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.023319 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.023416 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.025812 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.025894 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.026004 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.028301 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.030199 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.030296 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.030583 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.030667 140230058647552 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:21:25.030778 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:25.030817 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:25.030847 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:25.030911 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.033131 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.038523 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.038784 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.041442 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.054105 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.054162 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.054200 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.054247 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.054312 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.054878 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.054954 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.055315 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.056007 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.058546 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.059173 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.059249 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.059282 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.059341 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.059471 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.059581 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.059623 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.061489 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.061583 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.063969 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.064049 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.064162 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.066453 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.068500 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.068595 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.068878 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.068960 140230058647552 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:21:25.069067 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:25.069104 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:25.069133 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:25.069195 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.071409 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.076809 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.077071 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.079762 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.092428 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.092483 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.092516 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.092545 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.092608 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.093171 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.093248 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.093609 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.094314 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.096843 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.097467 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.097544 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.097577 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.097633 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.097769 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.097881 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.097919 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.099778 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.099877 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.102302 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.102382 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.102491 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.104797 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.106679 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.106775 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.107062 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.107143 140230058647552 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:21:25.107252 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:25.107290 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:25.107320 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:25.107379 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.109610 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.115033 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.115295 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.117982 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.130741 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.130795 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.130829 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.130857 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.130918 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.131475 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.131551 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.131908 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.132596 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.135553 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.136183 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.136260 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.136294 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.136350 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.136475 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.136583 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.136620 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.138496 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.138596 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.140986 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.141065 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.141175 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.143464 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.145442 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.145539 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.145830 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.145913 140230058647552 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:21:25.146021 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:25.146059 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:25.146089 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:25.146149 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.148371 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.153807 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.154068 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.156735 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.169404 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.169459 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.169492 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.169522 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.169584 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.170156 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.170232 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.170590 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.171280 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.173838 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.174479 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.174556 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.174590 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.174646 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.174772 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.174879 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.174917 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.176780 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.176872 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.179260 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.179344 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.179451 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.181723 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.183579 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.183673 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.183955 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.184036 140230058647552 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:21:25.184144 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:25.184182 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:25.184211 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:25.184272 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.186504 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.191905 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.192168 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.194864 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.207526 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.207580 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.207614 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.207643 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.207702 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.208264 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.208339 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.208692 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.209380 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.211924 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.212558 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.212636 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.212669 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.212727 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.212854 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.212960 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.212998 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.214866 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.214960 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.217351 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.217435 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.217545 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.219857 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.221739 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.221834 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.222119 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.222200 140230058647552 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:21:25.222308 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:25.222347 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:25.222376 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:25.222437 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.224659 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.230077 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.230337 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.233001 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.245887 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.245943 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.245978 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.246009 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.246073 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.246665 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.246744 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.247105 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.247782 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.250705 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.251333 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.251413 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.251449 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.251508 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.251646 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.251760 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.251799 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.253706 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.253799 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.256183 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.256266 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.256375 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.258662 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.260523 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.260618 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.260905 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.260988 140230058647552 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:21:25.261097 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:25.261134 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:25.261163 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:25.261225 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.263460 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.268889 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.269148 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.271830 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.284523 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.284578 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.284612 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.284641 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.284705 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.285265 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.285341 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.285703 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.286410 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.288973 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.289599 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.289683 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.289717 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.289774 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.289901 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.290008 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.290046 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.292398 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.292492 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.294924 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.295006 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.295124 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.297373 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.299263 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.299363 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.299656 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.299740 140230058647552 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:21:25.299852 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:25.299890 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:25.299920 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:25.299983 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.302193 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.307543 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.307797 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.310456 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.323000 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.323054 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.323088 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.323117 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.323179 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.323743 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.323820 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.324177 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.324862 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.327376 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.328007 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.328084 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.328117 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.328173 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.328304 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.328412 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.328449 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.330312 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.330404 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.332787 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.332866 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.332975 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.335265 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.337116 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.337211 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.337493 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.337576 140230058647552 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:21:25.337690 140230058647552 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:21:25.337729 140230058647552 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:21:25.337759 140230058647552 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:21:25.337820 140230058647552 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.340032 140230058647552 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:21:25.345656 140230058647552 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.345914 140230058647552 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:21:25.348633 140230058647552 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:21:25.361233 140230058647552 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:21:25.361288 140230058647552 attention.py:418] Single window, no scan.
I0123 11:21:25.361322 140230058647552 transformer_layer.py:389] tlayer: self-attention.
I0123 11:21:25.361351 140230058647552 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.361414 140230058647552 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.361987 140230058647552 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.362063 140230058647552 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.362425 140230058647552 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.363114 140230058647552 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.366027 140230058647552 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.366667 140230058647552 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.366744 140230058647552 transformer_layer.py:468] tlayer: End windows.
I0123 11:21:25.366778 140230058647552 transformer_layer.py:472] tlayer: final FFN.
I0123 11:21:25.366834 140230058647552 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.366961 140230058647552 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:21:25.367069 140230058647552 nn_components.py:325] mlp: activation = None
I0123 11:21:25.367108 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.368984 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.369077 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.371444 140230058647552 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.371523 140230058647552 transformer_base.py:443] tbase: final FFN
I0123 11:21:25.371629 140230058647552 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:21:25.373904 140230058647552 nn_components.py:329] mlp: final activation = None
I0123 11:21:25.375761 140230058647552 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.375856 140230058647552 nn_components.py:261] mlp: residual
I0123 11:21:25.376135 140230058647552 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:25.376221 140230058647552 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:21:25.379050 140230058647552 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:21:29.781602 140230058647552 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:21:30.319378 140230058647552 training_loop.py:409] No working directory specified.
I0123 11:21:30.319501 140230058647552 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:21:30.320253 140230058647552 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:21:33.414362 140230058647552 training_loop.py:447] Only restoring trainable parameters.
I0123 11:21:33.415115 140230058647552 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:21:33.415174 140230058647552 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.415220 140230058647552 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.415262 140230058647552 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.415302 140230058647552 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.415342 140230058647552 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.415380 140230058647552 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.415418 140230058647552 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.415455 140230058647552 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.415492 140230058647552 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.415530 140230058647552 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.415567 140230058647552 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.415605 140230058647552 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.415642 140230058647552 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.415679 140230058647552 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.415717 140230058647552 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.415754 140230058647552 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.415791 140230058647552 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.415827 140230058647552 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.415863 140230058647552 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.415913 140230058647552 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.415951 140230058647552 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.415988 140230058647552 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.416025 140230058647552 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.416062 140230058647552 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416098 140230058647552 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.416134 140230058647552 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416169 140230058647552 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416205 140230058647552 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.416240 140230058647552 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.416275 140230058647552 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416311 140230058647552 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.416347 140230058647552 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.416382 140230058647552 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.416418 140230058647552 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416454 140230058647552 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.416490 140230058647552 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416526 140230058647552 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416561 140230058647552 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.416597 140230058647552 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.416632 140230058647552 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416668 140230058647552 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.416703 140230058647552 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.416739 140230058647552 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.416774 140230058647552 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416809 140230058647552 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.416851 140230058647552 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416889 140230058647552 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.416924 140230058647552 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.416960 140230058647552 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.416995 140230058647552 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417030 140230058647552 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.417065 140230058647552 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.417101 140230058647552 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.417136 140230058647552 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417172 140230058647552 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.417209 140230058647552 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417245 140230058647552 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417281 140230058647552 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.417317 140230058647552 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.417352 140230058647552 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417387 140230058647552 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.417422 140230058647552 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.417458 140230058647552 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.417494 140230058647552 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417529 140230058647552 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.417563 140230058647552 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417598 140230058647552 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417633 140230058647552 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.417677 140230058647552 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.417713 140230058647552 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417750 140230058647552 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.417785 140230058647552 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.417826 140230058647552 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.417863 140230058647552 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417900 140230058647552 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.417935 140230058647552 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.417971 140230058647552 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418005 140230058647552 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.418040 140230058647552 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.418075 140230058647552 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418110 140230058647552 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.418145 140230058647552 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.418180 140230058647552 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.418215 140230058647552 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418250 140230058647552 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.418284 140230058647552 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418318 140230058647552 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418353 140230058647552 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.418388 140230058647552 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.418423 140230058647552 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418458 140230058647552 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.418493 140230058647552 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.418528 140230058647552 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.418563 140230058647552 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418598 140230058647552 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.418634 140230058647552 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418669 140230058647552 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418705 140230058647552 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.418740 140230058647552 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.418782 140230058647552 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418820 140230058647552 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.418864 140230058647552 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.418900 140230058647552 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.418936 140230058647552 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.418972 140230058647552 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.419008 140230058647552 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.419043 140230058647552 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.419078 140230058647552 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.419113 140230058647552 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.419147 140230058647552 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.419183 140230058647552 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.419218 140230058647552 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:21:33.419253 140230058647552 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:21:33.419289 140230058647552 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.419324 140230058647552 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.419359 140230058647552 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.419395 140230058647552 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.419431 140230058647552 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:21:33.419466 140230058647552 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:21:33.419502 140230058647552 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:21:33.419538 140230058647552 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:21:33.419566 140230058647552 training_loop.py:725] Total parameters: 152072288
I0123 11:21:33.419799 140230058647552 training_loop.py:739] Total state size: 0
I0123 11:21:33.440367 140230058647552 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:21:33.440666 140230058647552 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:21:33.441189 140230058647552 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:21:33.441521 140230058647552 training_loop.py:89] registering functions: dict_keys([])
I0123 11:21:33.457208 140230058647552 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = angle_bisector e b c a, on_line e b a; f = on_circle f d c, on_line f e c; g = midpoint g b c; h = lc_tangent h g c, on_line h f c; i = midpoint i a c; j = lc_tangent j i c, on_line j f c ? eqangle h g h f j f j i
I0123 11:21:34.581612 140230058647552 ddar.py:60] Depth 1/1000 time = 1.09489107131958
I0123 11:21:34.584600 140230058647552 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J : Points
DB = DA [00]
DA = DC [01]
BCE = ECA [02]
C,E,F are collinear [03]
GB = GC [04]
C,G,B are collinear [05]
GH  GC [06]
C,H,F are collinear [07]
IA = IC [08]
C,I,A are collinear [09]
C,J,F are collinear [10]
IJ  IC [11]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. DB = DA [00] & DA = DC [01]   DC = DB [12]
002. DC = DB [12] & GB = GC [04]   CB  DG [13]
003. DA = DC [01] & IA = IC [08]   AC  DI [14]
004. C,I,A are collinear [09] & G,B,C are collinear [05] & GH  GC [06] & IJ  IC [11]   JIC = CGH [15]
005. C,J,F are collinear [10] & C,E,F are collinear [03] & C,I,A are collinear [09] & C,G,B are collinear [05] & C,H,F are collinear [07] & ECA = BCE [02]   JCI = GCH [16]
006. JIC = CGH [15] & JCI = GCH [16] (Similar Triangles)  IJC = CHG [17]
007. CB  DG [13] & GH  GC [06] & C,G,B are collinear [05] & C,H,F are collinear [07] & C,E,F are collinear [03] & C,J,F are collinear [10] & AC  DI [14] & IJ  IC [11] & C,I,A are collinear [09] & IJC = CHG [17]   GHF = FJI
==========================

