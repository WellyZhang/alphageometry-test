I0123 11:59:07.832903 140665016369152 inference_utils.py:69] Parsing gin configuration.
I0123 11:59:07.833002 140665016369152 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:59:07.833215 140665016369152 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:59:07.833251 140665016369152 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:59:07.833282 140665016369152 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:59:07.833311 140665016369152 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:59:07.833340 140665016369152 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:59:07.833367 140665016369152 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:59:07.833393 140665016369152 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:59:07.833420 140665016369152 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:59:07.833446 140665016369152 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:59:07.833472 140665016369152 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:59:07.833519 140665016369152 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:59:07.833660 140665016369152 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:59:07.833867 140665016369152 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:59:07.833971 140665016369152 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:59:07.840322 140665016369152 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:59:07.840447 140665016369152 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:59:07.840775 140665016369152 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:59:07.840882 140665016369152 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:59:07.841165 140665016369152 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:59:07.841265 140665016369152 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:59:07.841682 140665016369152 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:59:07.841784 140665016369152 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:59:07.845527 140665016369152 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:59:07.948705 140665016369152 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:59:07.949484 140665016369152 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:59:07.956272 140665016369152 training_loop.py:335] Process 0 of 1
I0123 11:59:07.956329 140665016369152 training_loop.py:336] Local device count = 1
I0123 11:59:07.956372 140665016369152 training_loop.py:337] Number of replicas = 1
I0123 11:59:07.956405 140665016369152 training_loop.py:339] Using random number seed 42
I0123 11:59:08.426548 140665016369152 training_loop.py:359] Initializing the model.
I0123 11:59:08.802763 140665016369152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.803039 140665016369152 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:59:08.803147 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803229 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803306 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803388 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803460 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803529 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803599 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803666 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803734 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803803 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803871 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803939 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:08.803978 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:08.804023 140665016369152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:59:08.804136 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:08.804175 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:08.804204 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:08.806182 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.811451 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:08.821995 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.822269 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:08.826594 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:08.837186 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:08.837245 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:08.837284 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:08.837318 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.837382 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.838582 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.838662 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.839372 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.841864 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.847602 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.849326 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.849406 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:08.849442 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:08.849504 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.849644 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:08.849989 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:08.850036 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:08.851946 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.852048 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:08.854896 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.854980 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:08.855476 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:08.865770 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:08.874735 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.874841 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:08.875153 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.875236 140665016369152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:59:08.875349 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:08.875390 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:08.875423 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:08.877301 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.879871 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:08.885835 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.886105 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:08.888770 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:08.892674 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:08.892730 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:08.892767 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:08.892798 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.892860 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.893435 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.893513 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.893885 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.894692 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.897153 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.897776 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.897858 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:08.897894 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:08.897957 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.898089 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:08.898422 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:08.898467 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:08.900436 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.900531 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:08.903113 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.903198 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:08.903628 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:08.905998 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:08.907974 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.908075 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:08.908365 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.908447 140665016369152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:59:08.908560 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:08.908601 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:08.908634 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:08.910599 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.913067 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:08.919208 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.919483 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:08.922185 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:08.926136 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:08.926196 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:08.926234 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:08.926268 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.926334 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.926923 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.927003 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.927364 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.928132 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.930667 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.931364 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.931443 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:08.931480 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:08.931542 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.931672 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:08.932003 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:08.932048 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:08.933990 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.934091 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:08.936672 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.936759 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:08.937250 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:08.939610 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:08.941557 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.941661 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:08.941963 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.942048 140665016369152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:59:08.942165 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:08.942207 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:08.942241 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:08.944218 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.946651 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:08.952412 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.952679 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:08.955336 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:08.959192 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:08.959249 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:08.959286 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:08.959319 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.959382 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.959949 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.960026 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.960385 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.961163 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.963734 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.964363 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.964441 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:08.964477 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:08.964539 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.964674 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:08.964998 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:08.965042 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:08.966990 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.967090 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:08.969674 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.969760 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:08.970191 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:08.972468 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:08.974416 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.974512 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:08.974803 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.974885 140665016369152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:59:08.974997 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:08.975037 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:08.975069 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:08.977006 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.979437 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:08.985258 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.985530 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:08.988261 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:08.992088 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:08.992144 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:08.992181 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:08.992213 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.992276 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.992849 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.992929 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.993295 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.994088 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.996965 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.997598 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.997684 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:08.997720 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:08.997782 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:08.997923 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:08.998250 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:08.998293 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.000205 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.000299 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.002885 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.002972 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.003408 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.005727 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.007715 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.007811 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.008097 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.008180 140665016369152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:59:09.008292 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.008332 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.008364 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.010250 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.012665 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.018389 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.018657 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.021369 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:09.025193 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.025250 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.025288 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.025321 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.025384 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.026002 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.026085 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.026446 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.027233 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.029741 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.030372 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.030449 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.030485 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.030546 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.030680 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.031007 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.031052 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.032982 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.033077 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.035649 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.035730 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.036180 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.038547 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.040507 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.040608 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.040901 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.040984 140665016369152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:59:09.041100 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.041141 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.041174 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.043725 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.046340 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.052111 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.052384 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.055064 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:09.058968 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.059025 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.059062 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.059095 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.059157 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.059739 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.059819 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.060179 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.060958 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.063450 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.064078 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.064156 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.064193 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.064254 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.064387 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.064716 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.064761 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.066774 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.066871 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.069386 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.069469 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.069924 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.072613 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.074561 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.074664 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.074960 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.075045 140665016369152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:59:09.075158 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.075199 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.075232 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.213904 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.217028 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.222986 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.223293 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.226018 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:09.230013 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.230073 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.230112 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.230145 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.230216 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.230839 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.230917 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.231287 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.232082 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.234680 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.235330 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.235409 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.235445 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.235510 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.235641 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.235983 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.236027 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.238006 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.238104 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.240710 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.240792 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.241245 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.243634 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.245604 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.246783 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.247090 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.247178 140665016369152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:59:09.247294 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.247335 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.247369 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.249369 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.251810 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.257616 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.257895 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.260646 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:09.264536 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.264594 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.264632 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.264666 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.264731 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.265305 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.265387 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.265761 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.266543 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.269119 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.269775 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.269856 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.269893 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.269954 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.270087 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.270417 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.270462 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.272398 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.272494 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.275093 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.275176 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.275621 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.277968 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.279988 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.280085 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.280378 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.280470 140665016369152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:59:09.280586 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.280627 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.280660 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.282573 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.285247 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.290887 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.291158 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.294237 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:09.298063 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.298120 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.298158 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.298191 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.298254 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.298857 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.298939 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.299304 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.300074 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.302569 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.303199 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.303276 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.303312 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.303372 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.303500 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.303822 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.303865 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.305798 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.305894 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.308439 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.308519 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.308946 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.311312 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.313264 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.313360 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.313655 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.313750 140665016369152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:59:09.313866 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.313906 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.313939 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.315808 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.318283 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.323959 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.324228 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.326886 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:09.330701 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.330757 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.330794 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.330827 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.330890 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.331461 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.331541 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.331910 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.332693 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.335180 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.335817 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.335896 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.335932 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.335994 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.336125 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.336448 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.336492 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.338473 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.338569 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.341362 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.341442 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.341872 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.344222 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.346152 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.346250 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.346546 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.346629 140665016369152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:59:09.346749 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.346791 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.346824 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.348742 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.351143 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.356780 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.357045 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.359686 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:09.363518 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.363574 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.363611 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.363644 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.363707 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.364274 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.364351 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.364711 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.365483 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.367964 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.368945 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.369027 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.369065 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.369128 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.369257 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.369584 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.369628 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.371563 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.371660 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.374174 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.374256 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.374746 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.377010 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.378929 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.379025 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.379315 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.379602 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.379677 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.379747 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.379807 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.379863 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.379919 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.379972 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.380026 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.380081 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.380134 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.380188 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.380243 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:09.380282 140665016369152 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:59:09.383802 140665016369152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:09.431439 140665016369152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.431524 140665016369152 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:59:09.431579 140665016369152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:59:09.431682 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.431721 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.431751 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.431816 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.434245 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.439733 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.439992 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.442589 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.459129 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.459185 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.459226 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.459258 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.459321 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.460451 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.460530 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.461224 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.463227 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.467919 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.469225 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.469311 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.469347 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.469408 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.469538 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.469652 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.469697 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.471608 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.471704 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.474118 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.474199 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.474319 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.476543 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.478508 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.478607 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.478893 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.478976 140665016369152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:59:09.479086 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.479125 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.479156 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.479221 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.481461 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.486952 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.487211 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.489876 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.503032 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.503089 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.503127 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.503157 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.503219 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.503779 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.503857 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.504209 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.504898 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.507349 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.507963 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.508039 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.508085 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.508146 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.508276 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.508384 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.508422 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.510346 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.510441 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.512816 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.512896 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.513009 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.515220 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.517142 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.517237 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.517517 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.517598 140665016369152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:59:09.517713 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.517754 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.517786 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.517850 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.520052 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.525456 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.525726 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.528374 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.541080 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.541137 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.541173 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.541204 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.541268 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.541828 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.541905 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.542253 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.542940 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.545359 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.545986 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.546064 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.546100 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.546165 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.546295 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.546403 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.546447 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.548359 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.548454 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.550867 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.550948 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.551055 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.553270 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.555215 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.555311 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.555597 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.555679 140665016369152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:59:09.555788 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.555827 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.555860 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.555924 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.558145 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.563567 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.563828 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.566471 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.579354 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.579411 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.579448 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.579479 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.579542 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.580101 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.580176 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.580528 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.581207 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.583639 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.584250 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.584325 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.584359 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.584418 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.584554 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.584664 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.584703 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.586671 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.586765 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.589141 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.589221 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.589327 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.591534 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.593366 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.593461 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.593753 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.593836 140665016369152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:59:09.593945 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.593984 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.594015 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.594079 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.596634 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.602115 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.602385 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.604980 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.617830 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.617887 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.617924 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.617955 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.618018 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.618575 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.618653 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.619009 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.619693 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.622224 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.622858 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.622936 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.622972 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.623032 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.623167 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.623279 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.623317 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.625181 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.625274 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.627660 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.627741 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.627853 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.630129 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.631984 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.632080 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.632361 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.632443 140665016369152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:59:09.632553 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.632592 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.632623 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.632689 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.634923 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.640433 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.640702 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.643417 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.656115 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.656171 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.656207 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.656237 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.656302 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.656856 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.656932 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.657280 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.657969 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.660404 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.661015 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.661093 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.661129 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.661189 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.661322 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.661439 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.661479 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.663425 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.663521 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.665895 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.665976 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.666083 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.668280 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.670140 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.670236 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.670521 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.670603 140665016369152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:59:09.670713 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.670753 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.670785 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.670850 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.673076 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.678539 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.678799 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.681365 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.694120 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.694176 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.694212 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.694242 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.694303 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.694847 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.694924 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.695276 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.695966 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.703915 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.705049 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.705131 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.705166 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.705237 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.705379 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.705506 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.705551 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.707575 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.707671 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.710163 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.710244 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.710355 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.712610 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.714546 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.714643 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.714923 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.715009 140665016369152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:59:09.715123 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.715165 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.715196 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.715264 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.717487 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.722938 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.723213 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.726074 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.739124 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.739181 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.739218 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.739250 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.739312 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.739923 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.740001 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.740362 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.741063 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.743541 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.744168 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.744246 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.744281 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.744341 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.744478 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.744598 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.744643 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.746551 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.746647 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.749103 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.749183 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.749292 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.751503 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.753365 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.753461 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.753752 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.753836 140665016369152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:59:09.753945 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.753984 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.754015 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.754080 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.756328 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.761857 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.762118 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.764720 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.777520 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.777576 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.777613 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.777651 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.777717 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.778287 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.778363 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.778717 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.779395 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.781869 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.782544 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.782623 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.782659 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.782720 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.782852 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.782961 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.782999 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.784886 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.784980 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.787382 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.787464 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.787571 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.789804 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.791746 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.791842 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.792128 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.792211 140665016369152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:59:09.792320 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.792361 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.792392 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.792457 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.794705 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.800163 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.800423 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.803090 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.816241 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.816297 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.816332 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.816363 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.816426 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.817043 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.817122 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.817473 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.818175 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.820646 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.821271 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.821348 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.821382 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.821439 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.821566 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.821678 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.821719 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.823598 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.823698 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.826122 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.826202 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.826478 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.828699 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.830564 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.830661 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.830944 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.831027 140665016369152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:59:09.831138 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.831177 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.831208 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.831272 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.833509 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.839018 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.839280 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.841889 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.854636 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.854692 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.854728 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.854759 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.854822 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.855379 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.855456 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.855807 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.856491 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.858921 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.859610 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.859692 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.859728 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.859789 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.859925 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.860038 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.860078 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.862055 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.862159 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.864592 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.864671 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.864778 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.867013 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.868921 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.869016 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.869295 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.869375 140665016369152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:59:09.869483 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.869523 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.869554 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.869619 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.871894 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.877350 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.877608 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.880371 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.893248 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.893304 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.893340 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.893371 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.893433 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.894017 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.894097 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.894464 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.895214 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.897675 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.898316 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.898397 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.898434 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.898495 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.898633 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.898746 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.898786 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.900658 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.900752 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.903217 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.903298 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:09.903407 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:09.905704 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.907627 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.907723 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:09.908007 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.908097 140665016369152 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:59:09.910997 140665016369152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:09.967277 140665016369152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.967364 140665016369152 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:59:09.967418 140665016369152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:59:09.967521 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:09.967560 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:09.967591 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:09.967655 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.970293 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:09.975640 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.975897 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:09.978430 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:09.990887 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:09.990945 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:09.990982 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:09.991015 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.991077 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.991646 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.991724 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.992085 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.992765 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.995276 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.995897 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.995975 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:09.996011 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:09.996073 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.996205 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:09.996322 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:09.996362 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:09.998229 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:09.998324 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.000715 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.000796 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.000907 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.003181 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.005047 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.005144 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.005428 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.005510 140665016369152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:59:10.005620 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.005665 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.005698 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.005765 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.007999 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.013402 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.013672 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.016320 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.028865 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.028923 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.028960 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.028992 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.029054 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.029610 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.029696 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.030050 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.030736 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.033253 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.033884 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.033964 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.034001 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.034065 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.034196 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.034306 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.034353 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.036232 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.036327 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.038710 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.038791 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.038901 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.041181 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.043064 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.043164 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.043445 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.043526 140665016369152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:59:10.043634 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.043674 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.043707 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.043774 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.046163 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.051598 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.051864 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.054522 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.067719 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.067776 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.067813 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.067845 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.067909 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.068469 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.068548 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.068903 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.069592 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.072107 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.072730 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.072810 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.072847 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.072910 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.073042 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.073151 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.073190 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.075054 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.075152 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.077547 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.077628 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.077747 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.080454 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.082322 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.082421 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.082706 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.082790 140665016369152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:59:10.082900 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.082939 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.082971 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.083038 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.085266 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.090678 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.090942 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.093589 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.106296 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.106353 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.106392 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.106436 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.106503 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.107065 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.107141 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.107504 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.108193 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.110733 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.111358 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.111436 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.111471 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.111533 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.111662 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.111771 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.111816 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.113715 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.113809 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.116177 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.116255 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.116367 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.118676 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.120539 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.120634 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.120918 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.120998 140665016369152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:59:10.121105 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.121142 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.121172 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.121237 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.123464 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.128884 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.129141 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.131810 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.144544 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.144598 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.144634 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.144665 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.144725 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.145290 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.145364 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.145725 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.146415 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.148923 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.149547 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.149623 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.149667 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.149730 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.149864 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.149975 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.150013 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.151900 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.151999 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.154422 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.154502 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.154611 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.156902 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.158832 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.158931 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.159227 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.159311 140665016369152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:59:10.159422 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.159462 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.159494 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.159561 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.161814 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.167388 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.167652 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.170344 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.183151 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.183206 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.183242 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.183272 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.183334 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.183894 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.183971 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.184331 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.185020 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.187537 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.188161 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.188241 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.188277 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.188337 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.188466 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.188575 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.188613 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.190508 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.190607 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.192989 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.193068 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.193176 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.195879 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.197767 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.197863 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.198144 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.198225 140665016369152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:59:10.198333 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.198371 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.198402 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.198468 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.200701 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.206177 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.206436 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.209117 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.221932 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.221987 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.222023 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.222054 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.222115 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.222686 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.222762 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.223119 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.223810 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.226315 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.226947 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.227023 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.227057 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.227117 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.227246 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.227355 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.227392 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.229261 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.229354 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.231744 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.231823 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.231931 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.234230 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.236103 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.236199 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.236479 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.236561 140665016369152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:59:10.236668 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.236706 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.236737 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.236802 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.239029 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.244465 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.244729 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.247405 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.260229 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.260284 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.260320 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.260351 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.260411 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.260978 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.261055 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.261416 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.262131 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.264676 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.265311 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.265389 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.265425 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.265486 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.265617 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.265734 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.265772 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.267937 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.268031 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.270433 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.270518 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.270626 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.272914 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.274785 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.274879 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.275159 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.275243 140665016369152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:59:10.275350 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.275388 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.275418 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.275483 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.277729 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.283181 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.283446 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.286127 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.298881 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.298936 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.298972 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.299003 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.299064 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.299628 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.299705 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.300068 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.300767 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.303320 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.303955 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.304033 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.304068 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.304129 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.304259 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.304367 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.304405 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.306298 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.306392 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.308774 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.308859 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.308969 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.311677 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.313573 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.313673 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.313959 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.314039 140665016369152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:59:10.314145 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.314183 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.314213 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.314276 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.316506 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.321952 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.322213 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.324874 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.337748 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.337801 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.337837 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.337868 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.337930 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.338495 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.338571 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.338930 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.339623 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.342180 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.342808 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.342886 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.342921 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.342981 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.343113 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.343221 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.343259 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.345658 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.345753 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.348130 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.348213 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.348329 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.350577 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.352433 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.352527 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.352804 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.352885 140665016369152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:59:10.352990 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.353028 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.353058 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.353121 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.355340 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.360736 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.360997 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.363678 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.376444 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.376503 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.376539 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.376569 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.376632 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.377189 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.377263 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.377618 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.378313 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.380815 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.381444 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.381521 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.381555 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.381614 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.381750 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.381857 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.381895 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.383748 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.383840 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.386215 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.386295 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.386401 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.388690 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.390564 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.390660 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.390937 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.391018 140665016369152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:59:10.391124 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:10.391162 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:10.391193 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:10.391257 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.393487 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:10.398902 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.399163 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:10.401845 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:10.414582 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:10.414637 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:10.414672 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:10.414703 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.414766 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.415325 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.415402 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.415755 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.416450 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.418981 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.419611 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.419687 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:10.419722 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:10.419782 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.419912 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:10.420020 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:10.420058 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.421956 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.422050 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.424442 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.424521 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:10.424629 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:10.427305 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:10.429172 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.429267 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:10.429547 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:10.429630 140665016369152 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:59:10.432440 140665016369152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:14.907170 140665016369152 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:59:15.455916 140665016369152 training_loop.py:409] No working directory specified.
I0123 11:59:15.456041 140665016369152 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:59:15.456837 140665016369152 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:59:18.400471 140665016369152 training_loop.py:447] Only restoring trainable parameters.
I0123 11:59:18.401176 140665016369152 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:59:18.401236 140665016369152 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.401282 140665016369152 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.401324 140665016369152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.401365 140665016369152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.401404 140665016369152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.401441 140665016369152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.401479 140665016369152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.401516 140665016369152 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.401553 140665016369152 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.401589 140665016369152 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.401626 140665016369152 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.401674 140665016369152 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.401713 140665016369152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.401752 140665016369152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.401796 140665016369152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.401834 140665016369152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.401870 140665016369152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.401907 140665016369152 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.401944 140665016369152 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.401993 140665016369152 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402031 140665016369152 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.402068 140665016369152 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.402105 140665016369152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.402142 140665016369152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402178 140665016369152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.402214 140665016369152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402251 140665016369152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402288 140665016369152 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.402325 140665016369152 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.402361 140665016369152 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402397 140665016369152 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.402433 140665016369152 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.402469 140665016369152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.402504 140665016369152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402541 140665016369152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.402577 140665016369152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402614 140665016369152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402651 140665016369152 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.402687 140665016369152 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.402724 140665016369152 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402760 140665016369152 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.402797 140665016369152 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.402833 140665016369152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.402868 140665016369152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402904 140665016369152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.402945 140665016369152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.402983 140665016369152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403019 140665016369152 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.403055 140665016369152 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.403090 140665016369152 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403125 140665016369152 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.403161 140665016369152 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.403196 140665016369152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.403232 140665016369152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403267 140665016369152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.403303 140665016369152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403338 140665016369152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403374 140665016369152 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.403410 140665016369152 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.403445 140665016369152 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403481 140665016369152 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.403516 140665016369152 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.403552 140665016369152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.403588 140665016369152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403624 140665016369152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.403660 140665016369152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403695 140665016369152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403731 140665016369152 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.403766 140665016369152 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.403802 140665016369152 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403837 140665016369152 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.403872 140665016369152 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.403912 140665016369152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.403949 140665016369152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.403985 140665016369152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.404021 140665016369152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404057 140665016369152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404092 140665016369152 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.404128 140665016369152 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.404166 140665016369152 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404204 140665016369152 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.404242 140665016369152 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.404277 140665016369152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.404313 140665016369152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404348 140665016369152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.404383 140665016369152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404419 140665016369152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404455 140665016369152 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.404489 140665016369152 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.404525 140665016369152 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404560 140665016369152 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.404597 140665016369152 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.404632 140665016369152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.404667 140665016369152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404704 140665016369152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.404741 140665016369152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404775 140665016369152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404810 140665016369152 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.404846 140665016369152 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.404886 140665016369152 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.404923 140665016369152 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.404959 140665016369152 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.404994 140665016369152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.405029 140665016369152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.405065 140665016369152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.405100 140665016369152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.405134 140665016369152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.405169 140665016369152 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.405204 140665016369152 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.405240 140665016369152 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.405275 140665016369152 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.405310 140665016369152 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:18.405346 140665016369152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:18.405381 140665016369152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.405416 140665016369152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.405451 140665016369152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.405487 140665016369152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.405521 140665016369152 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:18.405556 140665016369152 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:18.405592 140665016369152 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:18.405628 140665016369152 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:18.405664 140665016369152 training_loop.py:725] Total parameters: 152072288
I0123 11:59:18.405885 140665016369152 training_loop.py:739] Total state size: 0
I0123 11:59:18.426959 140665016369152 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:59:18.427219 140665016369152 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:59:18.427560 140665016369152 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:59:18.427875 140665016369152 training_loop.py:89] registering functions: dict_keys([])
I0123 11:59:18.444132 140665016369152 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d b a; g = on_circle g d f, on_line g d f; h = on_line h c g, on_line h b a; i = on_circle i d e, on_line i d e; j = on_line j c h, on_line j b i ? cong c g h j
I0123 11:59:18.713142 140665016369152 ddar.py:60] Depth 1/1000 time = 0.25371885299682617
I0123 11:59:20.050268 140665016369152 ddar.py:60] Depth 2/1000 time = 1.3370318412780762
I0123 11:59:21.997308 140665016369152 ddar.py:60] Depth 3/1000 time = 1.9468779563903809
I0123 11:59:23.954396 140665016369152 ddar.py:60] Depth 4/1000 time = 1.9569144248962402
I0123 11:59:25.910825 140665016369152 ddar.py:60] Depth 5/1000 time = 1.9560942649841309
I0123 11:59:25.917059 140665016369152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:59:25.917134 140665016369152 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 11:59:25.917170 140665016369152 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C d f g 06 D d f d g 07 ; h : C a b h 08 C c g h 09 ; i : C d e i 10 D d e d i 11 ; j : C b i j 12 C c h j 13 ? D c g h j {F1} x00
I0123 11:59:25.917202 140665016369152 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C d f g 06 D d f d g 07 ; h : C a b h 08 C c g h 09 ; i : C d e i 10 D d e d i 11 ; j : C b i j 12 C c h j 13 ? D c g h j {F1} x00
I0123 11:59:26.041478 140665016369152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.041651 140665016369152 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:59:26.041756 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.041833 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.041906 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.041976 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.042044 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.042112 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.042181 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.042249 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.042318 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.042386 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.042454 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.042521 140665016369152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:59:26.042560 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.042605 140665016369152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:59:26.042712 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.042751 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.042782 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.044615 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.047043 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.052734 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.053013 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.055581 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.059438 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.059494 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.059530 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.059562 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.059623 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.060221 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.060297 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.060650 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.061404 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.063877 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.064498 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.064575 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.064609 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.064668 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.064796 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.065115 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.065157 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.067132 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.067226 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.069669 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.069748 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.070171 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.072455 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.074366 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.074461 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.074744 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.074823 140665016369152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:59:26.074930 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.074968 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.074998 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.076837 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.079130 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.084681 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.084940 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.087597 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.091253 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.091308 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.091343 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.091374 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.091434 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.091986 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.092061 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.092408 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.093158 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.095562 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.096230 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.096307 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.096342 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.096402 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.096531 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.096846 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.096888 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.098783 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.098876 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.101302 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.101380 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.101807 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.104532 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.106456 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.106552 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.106837 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.106917 140665016369152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:59:26.107025 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.107063 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.107094 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.108883 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.111198 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.116905 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.117166 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.119755 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.123462 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.123517 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.123553 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.123585 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.123649 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.124262 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.124339 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.124696 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.125469 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.127919 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.128540 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.128617 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.128653 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.128711 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.128841 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.129160 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.129203 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.131198 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.131292 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.133741 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.133820 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.134242 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.136842 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.138750 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.138846 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.139131 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.139211 140665016369152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:59:26.139318 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.139358 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.139389 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.141253 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.143574 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.149178 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.149436 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.151994 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.155726 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.155786 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.155823 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.155855 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.155917 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.156476 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.156552 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.156904 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.157665 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.160073 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.160686 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.160761 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.160796 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.160855 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.160984 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.161354 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.161397 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.163292 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.163386 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.165811 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.165889 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.166315 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.168545 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.170518 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.170614 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.170898 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.170979 140665016369152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:59:26.171085 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.171123 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.171154 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.172949 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.175258 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.180916 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.181178 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.183754 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.187417 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.187472 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.187508 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.187545 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.187661 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.188220 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.188297 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.188651 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.189414 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.191854 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.192471 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.192548 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.192583 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.192644 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.192772 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.193088 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.193130 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.195105 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.195197 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.197629 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.197714 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.198140 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.200373 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.202284 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.202378 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.202659 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.202739 140665016369152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:59:26.202847 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.202884 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.202915 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.204777 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.207093 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.212660 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.212920 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.215856 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.219508 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.219563 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.219599 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.219630 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.219697 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.220249 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.220325 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.220673 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.221427 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.223831 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.224495 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.224571 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.224605 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.224664 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.224807 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.225130 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.225172 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.227074 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.227167 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.229585 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.229669 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.230094 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.232393 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.234288 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.234384 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.234668 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.234748 140665016369152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:59:26.234855 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.234893 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.234924 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.236889 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.239205 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.245010 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.245268 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.247795 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.251418 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.251473 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.251509 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.251540 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.251658 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.252219 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.252295 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.252644 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.253397 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.255791 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.256411 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.256489 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.256524 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.256584 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.256713 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.257031 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.257074 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.259041 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.259135 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.261552 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.261631 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.262065 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.264346 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.266262 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.266356 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.266641 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.266723 140665016369152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:59:26.266830 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.266870 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.266901 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.268764 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.271063 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.276653 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.276913 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.279499 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.283148 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.283202 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.283237 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.283269 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.283333 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.283894 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.283970 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.284322 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.285066 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.287508 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.288175 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.288254 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.288290 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.288349 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.288480 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.288796 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.288839 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.290744 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.290837 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.293272 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.293350 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.293780 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.296099 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.298023 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.298119 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.298401 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.298481 140665016369152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:59:26.298588 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.298626 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.298657 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.300451 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.302744 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.308369 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.308623 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.311169 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.314801 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.314855 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.314889 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.314920 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.314982 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.315591 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.315672 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.316029 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.316789 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.319228 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.319839 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.319916 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.319951 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.320010 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.320137 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.320450 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.320492 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.322405 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.322498 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.325344 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.325423 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.325856 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.328125 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.330048 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.330143 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.330429 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.330510 140665016369152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:59:26.330618 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.330656 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.330685 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.332473 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.334886 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.340460 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.340717 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.343261 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.346924 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.346979 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.347015 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.347046 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.347161 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.347721 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.347805 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.348159 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.348917 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.351342 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.351975 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.352055 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.352091 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.352152 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.352284 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.352606 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.352650 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.354619 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.354711 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.357107 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.357186 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.357601 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.359849 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.361744 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.361838 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.362122 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.362201 140665016369152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:59:26.362308 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.362347 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.362379 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.364232 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.366543 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.372176 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.372430 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.374946 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.378655 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.378709 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.378745 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.378775 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.378838 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.379392 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.379467 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.379820 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.380570 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.382992 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.383602 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.383678 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.383713 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.383771 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.383897 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.384213 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.384256 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.386239 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.386334 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.388774 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.388853 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.389278 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.391513 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.393408 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.393502 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.393795 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.393876 140665016369152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:59:26.393983 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.394021 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.394051 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.395926 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.398233 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.403847 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.404104 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.406666 140665016369152 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:26.410394 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.410449 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.410485 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.410516 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.410578 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.411132 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.411206 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.411557 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.412319 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.414755 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.415371 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.415448 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.415483 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.415543 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.415673 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.415992 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.416034 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.418017 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.418110 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.420546 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.420624 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.421045 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.423299 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.425196 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.425291 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.425578 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.425837 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.425907 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.425965 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426019 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426073 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426127 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426180 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426233 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426286 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426339 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426391 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426443 140665016369152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:59:26.426478 140665016369152 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:59:26.429355 140665016369152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:26.474249 140665016369152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.474334 140665016369152 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:59:26.474394 140665016369152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:59:26.474500 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.474537 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.474567 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.474631 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.476970 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.482364 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.482627 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.485172 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:26.498104 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.498158 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.498193 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.498224 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.498286 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.498850 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.498926 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.499284 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.499962 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.502498 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.503121 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.503198 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.503233 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.503294 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.503425 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.503534 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.503573 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.505417 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.505511 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.507879 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.507960 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.508069 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.510628 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.512477 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.512572 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.512857 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.512938 140665016369152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:59:26.513053 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.513093 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.513124 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.513188 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.515375 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.520759 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.521020 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.523644 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:26.536105 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.536159 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.536194 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.536225 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.536285 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.536835 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.536911 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.537262 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.538004 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.540426 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.541034 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.541110 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.541145 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.541205 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.541336 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.541443 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.541480 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.543343 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.543438 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.545835 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.545917 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.546024 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.548355 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.550220 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.550318 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.550616 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.550700 140665016369152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:59:26.550811 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.550858 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.550892 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.550957 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.553192 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.558629 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.558893 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.561538 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:26.574188 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.574243 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.574277 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.574308 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.574369 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.574919 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.574995 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.575346 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.576074 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.578503 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.579126 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.579205 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.579240 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.579300 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.579430 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.579538 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.579576 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.581429 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.581524 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.583914 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.583994 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.584103 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.586351 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.588181 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.588277 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.588560 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.588641 140665016369152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:59:26.588748 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.588787 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.588827 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.588893 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.591112 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.596501 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.596762 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.599405 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:26.612219 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.612274 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.612309 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.612340 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.612401 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.612955 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.613031 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.613385 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.614131 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.616549 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.617166 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.617244 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.617279 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.617340 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.617471 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.617580 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.617619 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.619482 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.619575 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.621964 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.622044 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.622154 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.624806 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.626662 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.626757 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.627041 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.627121 140665016369152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:59:26.627228 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.627267 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.627298 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.627369 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.629581 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.634951 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.635215 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.637855 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:26.650399 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.650454 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.650490 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.650521 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.650583 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.651137 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.651214 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.651569 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.652301 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.654731 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.655343 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.655420 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.655455 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.655514 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.655643 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.655751 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.655789 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.657632 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.657733 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.660109 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.660189 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.660297 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.662547 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.664388 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.664483 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.664768 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.664850 140665016369152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:59:26.664957 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.664995 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.665026 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.665096 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.667319 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.672728 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.672990 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.675637 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:26.688156 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.688211 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.688246 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.688278 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.688339 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.688893 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.688969 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.689320 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.690050 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.692466 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.693082 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.693159 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.693194 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.693255 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.693386 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.693494 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.693532 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.695377 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.695472 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.697848 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.697927 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.698034 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.700277 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.702131 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.702225 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.702515 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.702595 140665016369152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:59:26.702705 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.702744 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.702774 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.702839 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.705047 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.710669 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.710929 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.713740 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:26.726267 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.726322 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.726358 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.726389 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.726452 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.727012 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.727087 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.727440 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.728172 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.730610 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.731226 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.731302 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.731337 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.731396 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.731524 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.731631 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.731669 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.733518 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.733610 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.735980 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.736060 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.736170 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.738845 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.740834 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.740929 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.741218 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.741298 140665016369152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:59:26.741404 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.741442 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.741631 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.741703 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.743937 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.749379 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.749653 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.913657 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:26.927203 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.927292 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.927331 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.927364 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.927448 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.928083 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.928160 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.928522 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.929220 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.931803 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.932446 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.932524 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.932559 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.932620 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.932751 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.932865 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.932903 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.935022 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.935124 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.937571 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.937656 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.937769 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.940075 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.942022 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.942126 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.942428 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.942517 140665016369152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:59:26.942628 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.942847 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.942879 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.942948 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.945657 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.951449 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.951714 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.954432 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:26.967312 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:26.967370 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:26.967406 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:26.967438 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.967501 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.968067 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.968144 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.968499 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.969188 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.971704 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.972327 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.972404 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:26.972440 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:26.972500 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.972630 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:26.972739 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:26.972777 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.974711 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.974805 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.977185 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.977264 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:26.977373 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:26.979594 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:26.981503 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.981597 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:26.981887 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.981970 140665016369152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:59:26.982077 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:26.982115 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:26.982146 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:26.982212 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.984448 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:26.989860 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:26.990138 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:26.992778 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.005807 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.005861 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.005897 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.005928 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.005990 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.006551 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.006628 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.006984 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.007668 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.010121 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.010733 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.010809 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.010844 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.010904 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.011033 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.011140 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.011178 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.013101 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.013195 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.015602 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.015682 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.015790 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.018012 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.019942 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.020037 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.020324 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.020406 140665016369152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:59:27.020513 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.020551 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.020582 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.020646 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.022901 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.028323 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.028599 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.031240 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.043972 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.044027 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.044063 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.044093 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.044155 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.044713 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.044790 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.045144 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.045825 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.048427 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.049172 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.049249 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.049284 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.049343 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.049471 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.049579 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.049618 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.051538 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.051631 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.054030 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.054110 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.054219 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.056405 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.058321 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.058416 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.058702 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.058784 140665016369152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:59:27.058892 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.058931 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.058962 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.059026 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.061238 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.066619 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.066882 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.069522 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.082071 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.082127 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.082162 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.082193 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.082254 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.082808 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.082883 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.083232 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.083908 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.086341 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.086960 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.087038 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.087073 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.087133 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.087264 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.087373 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.087411 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.089501 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.089594 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.092011 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.092090 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.092199 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.094396 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.096637 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.096733 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.097026 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.097116 140665016369152 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:59:27.099954 140665016369152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:27.150446 140665016369152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.150530 140665016369152 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:59:27.150584 140665016369152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:59:27.150686 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.150723 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.150753 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.150825 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.153148 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.158550 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.158812 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.161677 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.174238 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.174293 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.174328 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.174360 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.174421 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.174985 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.175061 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.175412 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.176083 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.178483 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.179156 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.179233 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.179267 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.179327 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.179455 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.179560 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.179598 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.181413 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.181506 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.183889 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.183970 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.184078 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.186271 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.188390 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.188645 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.188930 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.189010 140665016369152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:59:27.189116 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.189156 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.189186 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.189251 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.191623 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.197025 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.197291 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.199925 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.212459 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.212514 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.212551 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.212582 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.212643 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.213203 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.213279 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.213629 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.214318 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.216746 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.217363 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.217439 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.217474 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.217534 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.217667 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.217779 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.217818 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.219734 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.219827 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.222490 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.222570 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.222680 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.224905 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.226779 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.226875 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.227166 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.227251 140665016369152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:59:27.227360 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.227398 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.227429 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.227494 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.229805 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.235231 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.235495 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.238166 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.250786 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.250841 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.250877 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.250909 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.250971 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.251532 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.251608 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.251959 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.252636 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.255549 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.256177 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.256255 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.256290 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.256351 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.256480 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.256589 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.256628 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.258496 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.258590 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.260974 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.261053 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.261162 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.263453 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.265313 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.265408 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.265701 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.265784 140665016369152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:59:27.265890 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.265928 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.265958 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.266021 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.268238 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.273679 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.273948 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.276671 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.289288 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.289342 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.289377 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.289407 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.289467 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.290275 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.290353 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.290706 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.291566 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.294116 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.294908 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.294986 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.295021 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.295081 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.295211 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.295319 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.295357 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.297211 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.297304 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.299742 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.299822 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.299930 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.302149 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.303994 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.304089 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.304378 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.304460 140665016369152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:59:27.304567 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.304606 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.304638 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.304703 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.306926 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.312398 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.312671 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.315260 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.328587 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.328642 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.328677 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.328709 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.328770 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.329321 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.329397 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.329754 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.330441 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.332875 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.333548 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.333625 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.333669 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.333732 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.333868 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.333977 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.334015 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.335882 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.335975 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.338433 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.338688 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.338798 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.340990 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.342927 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.343023 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.343310 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.343393 140665016369152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:59:27.343501 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.343539 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.343571 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.343636 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.345852 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.351248 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.351519 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.354155 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.367223 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.367280 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.367316 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.367347 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.367412 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.367972 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.368049 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.368403 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.369083 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.371584 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.372211 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.372287 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.372322 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.372382 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.372511 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.372621 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.372660 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.374532 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.374624 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.377005 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.377085 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.377193 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.379471 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.381337 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.381433 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.381731 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.381814 140665016369152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:59:27.381923 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.381963 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.381994 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.382057 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.384286 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.389704 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.389967 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.392621 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.405236 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.405290 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.405326 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.405357 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.405419 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.406036 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.406113 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.406461 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.407129 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.409567 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.410201 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.410279 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.410314 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.410373 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.410500 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.410609 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.410647 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.412507 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.412600 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.415045 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.415126 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.415236 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.417443 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.419306 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.419403 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.419690 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.419771 140665016369152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:59:27.419878 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.419917 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.419947 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.420011 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.422220 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.427695 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.427958 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.430524 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.443541 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.443596 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.443632 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.443664 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.443725 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.444281 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.444357 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.444710 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.445391 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.447833 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.448507 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.448584 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.448619 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.448679 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.448807 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.448915 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.448953 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.450818 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.450911 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.453286 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.453364 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.453474 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.455682 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.457609 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.457716 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.458007 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.458086 140665016369152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:59:27.458195 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.458233 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.458264 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.458328 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.460548 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.465987 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.466247 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.468806 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.481726 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.481786 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.481822 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.481853 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.481912 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.482466 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.482541 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.482887 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.483565 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.486035 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.486647 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.486721 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.486756 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.486815 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.486940 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.487047 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.487086 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.488940 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.489031 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.491411 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.491489 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.491596 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.493863 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.495694 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.495788 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.496075 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.496154 140665016369152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:59:27.496261 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.496299 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.496330 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.496394 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.498618 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.504003 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.504264 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.506908 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.519487 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.519548 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.519585 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.519617 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.519677 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.520232 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.520307 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.520666 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.521349 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.523842 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.524451 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.524527 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.524561 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.524619 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.524746 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.524855 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.524894 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.526776 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.526868 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.529238 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.529316 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.529427 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.531701 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.533544 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.533638 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.533940 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.534019 140665016369152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:59:27.534127 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.534166 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.534198 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.534262 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.536486 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.542006 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.542269 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.544912 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.557492 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.557547 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.557589 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.557623 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.557695 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.558258 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.558333 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.558686 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.559366 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.561859 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.562472 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.562548 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.562584 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.562644 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.562775 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.562883 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.562921 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.564777 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.564870 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.567255 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.567335 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.567446 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.569698 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.571547 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.571639 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.571922 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.572000 140665016369152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:59:27.572108 140665016369152 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:27.572146 140665016369152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:27.572177 140665016369152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:27.572240 140665016369152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.574459 140665016369152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:27.579892 140665016369152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.580154 140665016369152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:27.583156 140665016369152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:27.595780 140665016369152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:27.595835 140665016369152 attention.py:418] Single window, no scan.
I0123 11:59:27.595870 140665016369152 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:27.595907 140665016369152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.595971 140665016369152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.596535 140665016369152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.596610 140665016369152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.596963 140665016369152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.597647 140665016369152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.600135 140665016369152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.600748 140665016369152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.600822 140665016369152 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:27.600857 140665016369152 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:27.600917 140665016369152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.601043 140665016369152 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:27.601155 140665016369152 nn_components.py:325] mlp: activation = None
I0123 11:59:27.601193 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.603061 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.603153 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.605515 140665016369152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.605592 140665016369152 transformer_base.py:443] tbase: final FFN
I0123 11:59:27.605707 140665016369152 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:27.607948 140665016369152 nn_components.py:329] mlp: final activation = None
I0123 11:59:27.609801 140665016369152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.609894 140665016369152 nn_components.py:261] mlp: residual
I0123 11:59:27.610179 140665016369152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:27.610264 140665016369152 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:59:27.613083 140665016369152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
