I0123 21:34:57.013778 139673487245312 inference_utils.py:69] Parsing gin configuration.
I0123 21:34:57.013870 139673487245312 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 21:34:57.014056 139673487245312 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 21:34:57.014090 139673487245312 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 21:34:57.014120 139673487245312 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 21:34:57.014148 139673487245312 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 21:34:57.014174 139673487245312 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 21:34:57.014201 139673487245312 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 21:34:57.014228 139673487245312 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 21:34:57.014254 139673487245312 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 21:34:57.014279 139673487245312 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 21:34:57.014304 139673487245312 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 21:34:57.014348 139673487245312 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 21:34:57.014467 139673487245312 resource_reader.py:55] Path not found: base_htrans.gin
I0123 21:34:57.014651 139673487245312 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 21:34:57.014751 139673487245312 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 21:34:57.020992 139673487245312 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 21:34:57.021114 139673487245312 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 21:34:57.021438 139673487245312 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 21:34:57.021543 139673487245312 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 21:34:57.021837 139673487245312 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 21:34:57.021939 139673487245312 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 21:34:57.022346 139673487245312 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 21:34:57.022444 139673487245312 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 21:34:57.026044 139673487245312 training_loop.py:334] ==== Training loop: initializing model ====
I0123 21:34:57.131822 139673487245312 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 21:34:57.132531 139673487245312 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 21:34:57.139123 139673487245312 training_loop.py:335] Process 0 of 1
I0123 21:34:57.139178 139673487245312 training_loop.py:336] Local device count = 1
I0123 21:34:57.139217 139673487245312 training_loop.py:337] Number of replicas = 1
I0123 21:34:57.139248 139673487245312 training_loop.py:339] Using random number seed 42
I0123 21:34:57.614489 139673487245312 training_loop.py:359] Initializing the model.
I0123 21:34:58.034220 139673487245312 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.034461 139673487245312 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 21:34:58.034565 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.034646 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.034726 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.034808 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.034883 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.034954 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.035024 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.035094 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.035165 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.035237 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.035310 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.035383 139673487245312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:34:58.035424 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.035470 139673487245312 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:34:58.035586 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.035626 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.035658 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.037649 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.042986 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.053653 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.053932 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.058275 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.068900 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.068958 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.068998 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.069031 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.069093 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.070278 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.070357 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.071074 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.073516 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.079678 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.080982 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.081063 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.081099 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.081163 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.081292 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.081630 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.081685 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.083603 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.083702 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.086575 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.086656 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.087153 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.097265 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.106125 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.106222 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.106522 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.106604 139673487245312 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:34:58.106715 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.106754 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.106786 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.108624 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.111101 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.116827 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.117092 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.119894 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.123703 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.123759 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.123795 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.123826 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.123888 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.124452 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.124528 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.124888 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.125675 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.128173 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.128801 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.128877 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.128912 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.128973 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.129100 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.129422 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.129465 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.131431 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.131524 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.134031 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.134113 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.134538 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.136852 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.138762 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.138856 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.139154 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.139234 139673487245312 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:34:58.139344 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.139382 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.139414 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.141634 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.144037 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.150242 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.150567 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.153243 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.157133 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.157189 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.157226 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.157258 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.157320 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.157897 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.157975 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.158339 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.159115 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.161596 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.162268 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.162344 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.162380 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.162441 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.162570 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.162895 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.162938 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.164850 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.164942 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.167473 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.167559 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.168047 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.170326 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.172249 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.172345 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.172638 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.172720 139673487245312 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:34:58.172831 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.172870 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.172901 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.174807 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.177188 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.182770 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.183029 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.185648 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.189424 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.189478 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.189513 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.189544 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.189606 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.190178 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.190254 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.190616 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.191388 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.193981 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.194597 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.194676 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.194712 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.194777 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.194903 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.195225 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.195269 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.197139 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.197231 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.199814 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.199899 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.200334 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.202610 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.204520 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.204613 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.204909 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.204989 139673487245312 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:34:58.205100 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.205139 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.205170 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.207069 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.209471 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.215122 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.215384 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.218543 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.222450 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.222504 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.222544 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.222577 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.222639 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.223203 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.223278 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.223636 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.224407 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.226987 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.227605 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.227683 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.227718 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.227783 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.227918 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.228245 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.228288 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.230207 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.230300 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.232856 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.232935 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.233363 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.235635 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.237595 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.237697 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.237994 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.238073 139673487245312 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:34:58.238182 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.238221 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.238253 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.240104 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.242505 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.248112 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.248374 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.251142 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.254865 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.254920 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.254957 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.254989 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.255049 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.255649 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.255725 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.256084 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.256873 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.259374 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.259994 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.260070 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.260105 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.260165 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.260294 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.260609 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.260651 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.262568 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.262664 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.265213 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.265293 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.265732 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.268064 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.269972 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.270071 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.270369 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.270451 139673487245312 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:34:58.270563 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.270602 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.270634 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.272501 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.274955 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.280543 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.280799 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.283434 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.287238 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.287294 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.287330 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.287363 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.287428 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.287990 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.288066 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.288428 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.289195 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.291663 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.292278 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.292353 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.292388 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.292446 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.292569 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.292886 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.292929 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.295209 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.295305 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.297785 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.297863 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.298291 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.438142 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.440357 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.440506 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.440826 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.440917 139673487245312 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:34:58.441030 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.441070 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.441102 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.443145 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.445683 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.451400 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.451672 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.454349 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.458265 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.458320 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.458357 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.458391 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.458454 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.459068 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.459146 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.459516 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.460299 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.462899 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.463527 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.463605 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.463641 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.463703 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.463834 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.464163 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.464206 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.466116 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.466212 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.468739 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.468817 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.469302 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.471612 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.473535 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.473646 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.473945 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.474026 139673487245312 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:34:58.474137 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.474175 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.474206 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.476107 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.478497 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.484144 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.484412 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.487103 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.490867 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.490922 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.490962 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.490993 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.491054 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.491621 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.491697 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.492053 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.492828 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.495356 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.495968 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.496045 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.496081 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.496140 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.496273 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.496593 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.496636 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.498547 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.498641 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.501188 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.501281 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.501727 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.504000 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.505915 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.506010 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.506302 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.506388 139673487245312 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:34:58.506502 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.506541 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.506573 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.508455 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.510829 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.516791 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.517052 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.519751 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.523655 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.523713 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.523752 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.523786 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.523849 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.524406 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.524481 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.524837 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.525658 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.528148 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.528769 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.528846 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.528882 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.528942 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.529075 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.529393 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.529435 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.531356 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.531449 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.534015 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.534095 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.534527 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.536799 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.538776 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.538872 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.539166 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.539254 139673487245312 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:34:58.539368 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.539406 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.539438 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.541271 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.543735 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.549433 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.549705 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.552423 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.556158 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.556212 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.556249 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.556280 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.556383 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.556937 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.557013 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.557374 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.558163 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.560633 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.561251 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.561326 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.561362 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.561421 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.561552 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.561877 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.561920 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.563868 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.563961 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.566717 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.566797 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.567227 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.569535 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.571442 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.571540 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.571831 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.571913 139673487245312 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:34:58.572029 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.572069 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.572101 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.573949 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.576408 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.582012 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.582275 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.584892 139673487245312 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:34:58.588947 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.589003 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.589043 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.589076 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.589138 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.589886 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.589963 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.590320 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.591097 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.593560 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.594223 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.594302 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.594337 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.594399 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.594532 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.594857 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.594900 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.596837 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.596928 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.599417 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.599500 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.599935 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.602225 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.604143 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.604237 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.604531 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.604808 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.604878 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.604943 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605001 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605057 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605112 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605166 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605220 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605272 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605324 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605377 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605430 139673487245312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:34:58.605467 139673487245312 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:34:58.608968 139673487245312 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:34:58.655887 139673487245312 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.655972 139673487245312 decoder_stack.py:333] dstack: autoregressive generator.
I0123 21:34:58.656026 139673487245312 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:34:58.656129 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.656167 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.656196 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.656260 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.658671 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.664072 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.664327 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.666913 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:58.683058 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.683112 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.683147 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.683177 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.683238 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.684344 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.684421 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.685123 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.687091 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.691898 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.693196 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.693282 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.693318 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.693378 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.693509 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.693617 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.693661 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.695519 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.695612 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.698012 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.698092 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.698199 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.700427 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.702388 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.702484 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.702772 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.702852 139673487245312 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:34:58.702960 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.702999 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.703030 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.703095 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.705353 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.710834 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.711095 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.713808 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:58.726832 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.726888 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.726923 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.726955 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.727017 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.727573 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.727649 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.728010 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.728714 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.731230 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.731842 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.731924 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.731965 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.732028 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.732158 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.732268 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.732306 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.734246 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.734339 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.736759 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.736837 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.736946 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.739177 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.741117 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.741213 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.741503 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.741585 139673487245312 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:34:58.741701 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.741742 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.741774 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.741839 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.744089 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.749533 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.749801 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.752498 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:58.765191 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.765247 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.765283 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.765314 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.765376 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.765943 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.766020 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.766392 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.767089 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.769571 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.770207 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.770283 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.770317 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.770382 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.770507 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.770614 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.770652 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.772583 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.772675 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.775133 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.775212 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.775320 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.777529 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.779450 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.779546 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.779835 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.779916 139673487245312 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:34:58.780025 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.780063 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.780095 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.780159 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.782398 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.788007 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.788266 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.790954 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:58.807591 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.807670 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.807708 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.807741 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.807820 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.808428 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.808509 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.808886 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.809598 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.812157 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.812777 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.812855 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.812890 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.812956 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.813095 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.813210 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.813249 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.815599 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.815695 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.818165 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.818243 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.818352 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.820584 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.822467 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.822563 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.822848 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.822929 139673487245312 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:34:58.823040 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.823082 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.823114 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.823183 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.825509 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.830995 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.831260 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.833908 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:58.846618 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.846672 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.846708 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.846738 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.846800 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.847352 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.847426 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.847796 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.848486 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.851021 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.851640 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.851718 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.851753 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.851813 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.851952 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.852063 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.852101 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.854000 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.854092 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.856486 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.856563 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.856670 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.858953 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.860825 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.860921 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.861207 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.861288 139673487245312 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:34:58.861395 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.861434 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.861464 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.861529 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.863794 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.869266 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.869528 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.872241 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:58.884935 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.884991 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.885027 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.885057 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.885119 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.885806 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.885883 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.886244 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.886934 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.889399 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.890031 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.890108 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.890145 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.890208 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.890335 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.890448 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.890486 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.892417 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.892510 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.894925 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.895004 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.895117 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.897327 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.899189 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.899284 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.899570 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.899652 139673487245312 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:34:58.899760 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.899799 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.899831 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.899895 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.902148 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.907670 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.907927 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.910510 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:58.923506 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.923562 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.923598 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.923629 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.923690 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.924246 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.924322 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.924679 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.925369 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.927875 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.928535 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.928611 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.928646 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.928706 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.928835 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.928944 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.928988 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.930898 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.930991 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.933395 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.933473 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.933581 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.935800 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.937738 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.937835 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.938124 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.938205 139673487245312 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:34:58.938313 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.938352 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.938384 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.938448 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.940686 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.946135 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.946406 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.949083 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:58.961670 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:58.961726 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:58.961763 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:58.961794 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.961858 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.962448 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.962522 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.962879 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.963582 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.966066 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.966682 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.966757 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:58.966792 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:58.966855 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.966981 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:58.967089 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:58.967133 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.969016 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.969110 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.971588 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.971668 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:58.971776 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:58.973996 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:58.975855 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.975950 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:58.976236 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.976318 139673487245312 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:34:58.976427 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:58.976465 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:58.976497 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:58.976561 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.978819 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:58.984362 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:58.984619 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:58.987261 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:58.999955 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.000011 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.000047 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.000078 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.000140 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.000696 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.000771 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.001129 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.001826 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.004312 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.004990 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.005068 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.005103 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.005164 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.005292 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.005403 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.005442 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.007336 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.007430 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.009849 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.009929 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.010036 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.012268 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.014211 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.014307 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.014600 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.014681 139673487245312 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:34:59.014791 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.014830 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.014861 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.014925 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.017166 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.022604 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.022865 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.025831 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.038376 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.038431 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.038467 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.038499 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.038560 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.039164 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.039410 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.039769 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.040461 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.043105 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.043716 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.043792 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.043827 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.043885 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.044013 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.044123 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.044161 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.046042 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.046141 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.048619 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.048697 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.048805 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.051030 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.052884 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.052979 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.053264 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.053345 139673487245312 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:34:59.053452 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.053491 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.053522 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.053585 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.055813 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.061314 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.061568 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.064192 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.076788 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.076843 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.076879 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.076910 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.076971 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.077523 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.077601 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.077969 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.078654 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.081152 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.081808 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.081885 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.081919 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.081979 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.082110 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.082220 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.082259 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.084149 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.084248 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.086696 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.086775 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.086883 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.089088 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.091028 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.091124 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.091410 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.091491 139673487245312 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:34:59.091599 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.091638 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.091669 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.091732 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.093977 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.099388 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.099644 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.102268 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.114861 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.114917 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.114953 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.114986 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.115047 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.115607 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.115682 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.116039 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.116733 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.119289 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.119899 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.119975 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.120010 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.120069 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.120202 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.120315 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.120355 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.122244 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.122337 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.124751 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.124830 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.124937 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.127547 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.129423 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.129518 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.129820 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.129911 139673487245312 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:34:59.132806 139673487245312 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:34:59.187869 139673487245312 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.187955 139673487245312 decoder_stack.py:333] dstack: autoregressive generator.
I0123 21:34:59.188009 139673487245312 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:34:59.188112 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.188151 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.188182 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.188246 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.190589 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.195968 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.196226 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.198814 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.211130 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.211184 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.211220 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.211251 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.211314 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.211866 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.211942 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.212300 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.212980 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.215457 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.216071 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.216149 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.216184 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.216245 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.216372 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.216486 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.216526 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.218378 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.218472 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.220848 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.220927 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.221036 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.223283 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.225149 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.225245 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.225531 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.225612 139673487245312 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:34:59.225727 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.225766 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.225797 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.225862 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.228105 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.233442 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.233711 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.236311 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.248540 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.248595 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.248632 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.248664 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.248726 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.249275 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.249352 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.249715 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.250389 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.252889 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.253506 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.253584 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.253619 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.253685 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.253815 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.253922 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.253965 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.255808 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.255900 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.258273 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.258352 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.258460 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.260692 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.262529 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.262624 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.262911 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.262992 139673487245312 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:34:59.263099 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.263137 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.263168 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.263231 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.265433 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.270799 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.271058 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.273694 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.285853 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.285909 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.285945 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.285977 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.286038 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.286587 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.286664 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.287023 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.287699 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.290635 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.291247 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.291325 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.291360 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.291420 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.291549 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.291657 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.291696 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.293542 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.293634 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.295994 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.296072 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.296180 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.298423 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.300268 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.300363 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.300651 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.300731 139673487245312 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:34:59.300840 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.300879 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.300911 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.300975 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.303186 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.308534 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.308793 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.311454 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.323776 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.323831 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.323874 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.323915 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.323981 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.324536 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.324610 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.324967 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.325666 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.328155 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.328781 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.328857 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.328890 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.328950 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.329075 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.329181 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.329220 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.331101 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.331193 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.333565 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.333645 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.333754 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.336026 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.337880 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.337973 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.338258 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.338336 139673487245312 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:34:59.338443 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.338480 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.338510 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.338572 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.340782 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.346178 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.346436 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.349098 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.361608 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.361666 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.361701 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.361731 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.361791 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.362338 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.362413 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.362770 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.363448 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.365952 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.366565 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.366640 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.366675 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.366734 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.366859 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.366964 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.367000 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.368872 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.368968 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.371360 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.371437 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.371542 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.373806 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.375638 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.375731 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.376013 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.376093 139673487245312 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:34:59.376201 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.376239 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.376270 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.376332 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.378552 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.383902 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.384159 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.386817 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.399186 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.399239 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.399273 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.399304 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.399365 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.399917 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.399994 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.400348 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.401029 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.403940 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.404550 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.404624 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.404659 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.404716 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.404839 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.404948 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.404986 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.406858 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.406955 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.409314 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.409390 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.409500 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.411769 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.413607 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.413707 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.413993 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.414072 139673487245312 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:34:59.414179 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.414216 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.414247 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.414309 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.416516 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.421886 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.422138 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.424819 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.437137 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.437191 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.437225 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.437254 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.437313 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.437863 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.437938 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.438292 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.438966 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.441469 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.442090 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.442166 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.442200 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.442260 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.442384 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.442492 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.442528 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.444396 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.444486 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.446865 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.446942 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.447048 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.449298 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.451152 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.451246 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.451531 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.451610 139673487245312 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:34:59.451717 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.451754 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.451784 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.451847 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.454053 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.459413 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.459674 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.462348 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.474694 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.474747 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.474781 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.474812 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.474872 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.475428 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.475502 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.475855 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.476530 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.479037 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.479653 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.479729 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.479763 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.479823 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.479952 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.480060 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.480097 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.481976 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.482068 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.484432 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.484514 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.484622 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.486886 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.488726 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.488818 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.489103 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.489183 139673487245312 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:34:59.489289 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.489325 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.489355 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.489416 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.491613 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.496966 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.497219 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.499853 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.512181 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.512237 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.512272 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.512302 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.512362 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.512910 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.512984 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.513341 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.514034 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.516900 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.517509 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.517585 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.517620 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.517683 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.517809 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.517915 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.517951 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.519823 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.519914 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.522301 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.522386 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.522493 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.524750 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.526596 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.526689 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.526971 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.527050 139673487245312 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:34:59.527157 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.527194 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.527224 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.527286 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.529498 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.534879 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.535136 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.537783 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.550147 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.550201 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.550235 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.550265 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.550324 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.550880 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.550955 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.551305 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.551986 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.554491 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.555097 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.555172 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.555206 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.555264 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.555393 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.555500 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.555537 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.557875 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.557970 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.560330 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.560405 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.560516 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.562747 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.564560 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.564652 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.564935 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.565014 139673487245312 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:34:59.565119 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.565155 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.565185 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.565247 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.567451 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.572837 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.573095 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.575746 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.588089 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.588143 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.588178 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.588208 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.588269 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.588825 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.588900 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.589258 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.589941 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.592441 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.593050 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.593129 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.593164 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.593223 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.593348 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.593457 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.593495 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.595369 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.595459 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.597813 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.597890 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.597995 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.600249 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.602101 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.602195 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.602480 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.602560 139673487245312 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:34:59.602666 139673487245312 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:34:59.602704 139673487245312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:34:59.602734 139673487245312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:34:59.602802 139673487245312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.605034 139673487245312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:34:59.610421 139673487245312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.610677 139673487245312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:34:59.613319 139673487245312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:34:59.625663 139673487245312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:34:59.625718 139673487245312 attention.py:418] Single window, no scan.
I0123 21:34:59.625753 139673487245312 transformer_layer.py:389] tlayer: self-attention.
I0123 21:34:59.625782 139673487245312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.625842 139673487245312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.626393 139673487245312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.626467 139673487245312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.626822 139673487245312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.627505 139673487245312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.630370 139673487245312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.630988 139673487245312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.631064 139673487245312 transformer_layer.py:468] tlayer: End windows.
I0123 21:34:59.631097 139673487245312 transformer_layer.py:472] tlayer: final FFN.
I0123 21:34:59.631155 139673487245312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.631281 139673487245312 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:34:59.631389 139673487245312 nn_components.py:325] mlp: activation = None
I0123 21:34:59.631427 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.633286 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.633376 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.635754 139673487245312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.635832 139673487245312 transformer_base.py:443] tbase: final FFN
I0123 21:34:59.635938 139673487245312 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:34:59.638217 139673487245312 nn_components.py:329] mlp: final activation = None
I0123 21:34:59.640076 139673487245312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.640169 139673487245312 nn_components.py:261] mlp: residual
I0123 21:34:59.640456 139673487245312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:34:59.640539 139673487245312 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:34:59.643357 139673487245312 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:35:04.063457 139673487245312 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 21:35:04.595283 139673487245312 training_loop.py:409] No working directory specified.
I0123 21:35:04.595402 139673487245312 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 21:35:04.596183 139673487245312 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 21:35:07.841035 139673487245312 training_loop.py:447] Only restoring trainable parameters.
I0123 21:35:07.841672 139673487245312 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 21:35:07.841759 139673487245312 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.841811 139673487245312 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.841856 139673487245312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.841899 139673487245312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.841940 139673487245312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.841981 139673487245312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842021 139673487245312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842060 139673487245312 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.842098 139673487245312 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.842137 139673487245312 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842175 139673487245312 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.842213 139673487245312 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.842250 139673487245312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.842288 139673487245312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842325 139673487245312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.842363 139673487245312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842401 139673487245312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842438 139673487245312 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.842476 139673487245312 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.842525 139673487245312 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842563 139673487245312 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.842601 139673487245312 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.842638 139673487245312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.842675 139673487245312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842713 139673487245312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.842751 139673487245312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842788 139673487245312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842825 139673487245312 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.842862 139673487245312 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.842898 139673487245312 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.842935 139673487245312 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.842972 139673487245312 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.843008 139673487245312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.843045 139673487245312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843082 139673487245312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.843118 139673487245312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843154 139673487245312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843191 139673487245312 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.843227 139673487245312 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.843263 139673487245312 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843299 139673487245312 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.843337 139673487245312 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.843373 139673487245312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.843409 139673487245312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843445 139673487245312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.843487 139673487245312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843525 139673487245312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843562 139673487245312 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.843599 139673487245312 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.843634 139673487245312 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843670 139673487245312 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.843706 139673487245312 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.843741 139673487245312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.843778 139673487245312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843813 139673487245312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.843849 139673487245312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843886 139673487245312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.843921 139673487245312 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.843957 139673487245312 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.843993 139673487245312 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844029 139673487245312 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.844065 139673487245312 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.844101 139673487245312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.844139 139673487245312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844175 139673487245312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.844212 139673487245312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844249 139673487245312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844285 139673487245312 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.844320 139673487245312 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.844356 139673487245312 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844391 139673487245312 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.844427 139673487245312 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.844469 139673487245312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.844506 139673487245312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844542 139673487245312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.844578 139673487245312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844615 139673487245312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844651 139673487245312 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.844687 139673487245312 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.844723 139673487245312 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844759 139673487245312 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.844795 139673487245312 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.844832 139673487245312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.844868 139673487245312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844903 139673487245312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.844939 139673487245312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.844976 139673487245312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845014 139673487245312 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.845051 139673487245312 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.845086 139673487245312 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845122 139673487245312 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.845158 139673487245312 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.845193 139673487245312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.845229 139673487245312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845264 139673487245312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.845300 139673487245312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845337 139673487245312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845372 139673487245312 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.845408 139673487245312 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.845449 139673487245312 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845486 139673487245312 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.845524 139673487245312 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.845561 139673487245312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.845597 139673487245312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845633 139673487245312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.845686 139673487245312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845724 139673487245312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845760 139673487245312 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.845797 139673487245312 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.845833 139673487245312 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.845868 139673487245312 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.845904 139673487245312 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:35:07.845940 139673487245312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:35:07.845977 139673487245312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.846014 139673487245312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.846050 139673487245312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.846086 139673487245312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.846122 139673487245312 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:35:07.846158 139673487245312 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:35:07.846194 139673487245312 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:35:07.846231 139673487245312 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:35:07.846260 139673487245312 training_loop.py:725] Total parameters: 152072288
I0123 21:35:07.846482 139673487245312 training_loop.py:739] Total state size: 0
I0123 21:35:07.870658 139673487245312 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 21:35:07.870904 139673487245312 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 21:35:07.871323 139673487245312 training_loop.py:652] Compiling mode beam_search with jit.
I0123 21:35:07.871659 139673487245312 training_loop.py:89] registering functions: dict_keys([])
I0123 21:35:07.888165 139673487245312 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = on_line f c b, on_line f e a; g = on_line g c e, on_line g b a; h = circle h c g a; i = circle i b g e; j = on_line j h i, on_line j d g; k = on_circle k h g, on_line k f g; l = on_circle l i g, on_line l f g; m = midpoint m k l ? cong d j m j
I0123 21:35:10.012039 139673487245312 ddar.py:60] Depth 1/1000 time = 2.062898635864258
I0123 21:35:13.289201 139673487245312 ddar.py:60] Depth 2/1000 time = 3.2769718170166016
I0123 21:35:16.595086 139673487245312 ddar.py:60] Depth 3/1000 time = 3.3056938648223877
I0123 21:35:19.751680 139673487245312 ddar.py:60] Depth 4/1000 time = 3.156355619430542
I0123 21:35:23.230980 139673487245312 ddar.py:60] Depth 5/1000 time = 3.4787509441375732
I0123 21:35:26.884615 139673487245312 ddar.py:60] Depth 6/1000 time = 3.6501991748809814
I0123 21:35:31.279609 139673487245312 ddar.py:60] Depth 7/1000 time = 4.39476466178894
I0123 21:35:36.003861 139673487245312 ddar.py:60] Depth 8/1000 time = 4.72396445274353
I0123 21:35:41.600533 139673487245312 ddar.py:60] Depth 9/1000 time = 5.596485614776611
I0123 21:35:47.174052 139673487245312 ddar.py:60] Depth 10/1000 time = 5.573266267776489
I0123 21:35:53.380960 139673487245312 ddar.py:60] Depth 11/1000 time = 6.206618785858154
I0123 21:36:00.001076 139673487245312 ddar.py:60] Depth 12/1000 time = 6.619855880737305
I0123 21:36:00.033594 139673487245312 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L M : Points
DA = DB [00]
DB = DC [01]
DE = DB [02]
E,G,C are collinear [03]
A,B,G are collinear [04]
HC = HG [05]
HG = HA [06]
IB = IG [07]
IG = IE [08]
D,J,G are collinear [09]
I,J,H are collinear [10]
F,K,G are collinear [11]
HK = HG [12]
F,G,L are collinear [13]
IL = IG [14]
M,K,L are collinear [15]
MK = ML [16]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. DA = DB [00] & DE = DB [02] & DB = DC [01]   E,A,B,C are concyclic [17]
002. E,A,B,C are concyclic [17]   EBA = ECA [18]
003. HC = HG [05]   HCG = CGH [19]
004. HCG = CGH [19] & E,G,C are collinear [03]   HCE = (CE-GH) [20]
005. HG = HA [06] & HC = HG [05]   HC = HA [21]
006. HC = HA [21]   HCA = CAH [22]
007. HG = HA [06]   HGA = GAH [23]
008. HGA = GAH [23] & A,B,G are collinear [04]   (GH-AB) = BAH [24]
009. EBA = ECA [18] & HCE = (CE-GH) [20] & HCA = CAH [22] & (GH-AB) = BAH [24] (Angle chase)  BE  GH [25]
010. IB = IG [07] & IG = IE [08]   IE = IB [26]
011. DE = DB [02] & IE = IB [26]   BE  DI [27]
012. BE  GH [25] & BE  DI [27]   GH  DI [28]
013. IB = IG [07] & IL = IG [14] & IG = IE [08]   E,B,G,L are concyclic [29]
014. E,B,G,L are concyclic [29]   EGB = ELB [30]
015. E,B,G,L are concyclic [29]   EBG = ELG [31]
016. EGB = ELB [30] & E,G,C are collinear [03] & A,B,G are collinear [04]   (CE-AB) = ELB [32]
017. IL = IG [14] & IG = IE [08]   IE = IL [33]
018. IE = IL [33]   IEL = ELI [34]
019. IE = IB [26]   IEB = EBI [35]
020. IG = IE [08]   IEG = EGI [36]
021. IEG = EGI [36] & E,G,C are collinear [03]   IEC = (CE-GI) [37]
022. IB = IG [07] & IL = IG [14]   IL = IB [38]
023. IL = IB [38]   ILB = LBI [39]
024. EBA = ECA [18] & (CE-AB) = ELB [32] & IEL = ELI [34] & IEB = EBI [35] & IEC = (CE-GI) [37] & ILB = LBI [39] (Angle chase)  AC  GI [40]
025. DA = DB [00] & DB = DC [01]   DC = DA [41]
026. DC = DA [41] & HC = HA [21]   AC  DH [42]
027. DI  GH [28] & AC  GI [40] & AC  DH [42]   GHD = DIG [43]
028. AC  GI [40] & AC  DH [42]   GDH = DGI [44]
029. GHD = DIG [43] & GDH = DGI [44] (Similar Triangles)  GH = DI [45]
030. GHD = DIG [43] & GDH = DGI [44] (Similar Triangles)  DH = GI [46]
031. GH = DI [45] & HK = HG [12]   KH = DI [47]
032. IL = IG [14] & DH = GI [46]   HD = IL [48]
033. HK = HG [12] & HC = HG [05]   HC = HK [49]
034. HC = HK [49]   HCK = CKH [50]
035. DE = DB [02] & DB = DC [01]   DC = DE [51]
036. GH = DI [45] & HC = HG [05]   CH = DI [52]
037. IG = IE [08] & DH = GI [46]   HD = IE [53]
038. DC = DE [51] & CH = DI [52] & HD = IE [53] (SSS)  HDI = (CH-EI) [54]
039. AC  DH [42] & BE  DI [27]   (DI-EB) = (DH-AC) [55]
040. HC = HG [05] & HK = HG [12] & HG = HA [06]   A,K,G,C are concyclic [56]
041. A,K,G,C are concyclic [56]   AGK = ACK [57]
042. AGK = ACK [57] & A,B,G are collinear [04] & F,K,G are collinear [11] & EBG = ELG [31] & F,G,L are collinear [13]   BEL = ACK [58]
043. (DI-EB) = (DH-AC) [55] & BEL = ACK [58]   HDI = (CK-EL) [59]
044. (DI-EB) = (DH-AC) [55] & BEL = ACK [58]   (DI-EL) = (DH-CK) [60]
045. HDI = (CH-EI) [54] & HDI = (CK-EL) [59]   (KC-EL) = (HC-IE) [61]
046. HCK = CKH [50] & (KC-EL) = (HC-IE) [61]   CKH = IEL [62]
047. CKH = IEL [62] & IEL = ELI [34]   ELI = CKH [63]
048. (DI-EL) = (DH-CK) [60] & AC  GI [40] & AC  DH [42]   (DI-EL) = (IG-KC) [64]
049. ELI = CKH [63] & (DI-EL) = (IG-KC) [64]   (HK-GI) = LID [65]
050. (HK-GI) = LID [65] & GH  DI [28] & AC  GI [40] & AC  DH [42]   KHD = LID [66]
051. KH = DI [47] & HD = IL [48] & KHD = LID [66] (SAS)  DK = LD [67]
052. MK = ML [16] & DK = LD [67]   KL  MD [68]
053. M,K,L are collinear [15] & F,G,L are collinear [13] & F,K,G are collinear [11] & KL  MD [68]   GM  MD [69]
054. GH  DI [28] & D,J,G are collinear [09] & I,J,H are collinear [10]   DI:GH = JD:JG [70]
055. D,J,G are collinear [09] & DI:GH = JD:JG [70] & GH = DI [45]   J is midpoint of DG [71]
056. GM  MD [69] & J is midpoint of DG [71]   DJ = MJ
==========================

