I0123 17:29:27.079528 140487190380544 inference_utils.py:69] Parsing gin configuration.
I0123 17:29:27.079644 140487190380544 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 17:29:27.079847 140487190380544 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 17:29:27.079880 140487190380544 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 17:29:27.079909 140487190380544 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 17:29:27.079937 140487190380544 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 17:29:27.079963 140487190380544 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 17:29:27.079990 140487190380544 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 17:29:27.080018 140487190380544 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 17:29:27.080045 140487190380544 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 17:29:27.080071 140487190380544 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 17:29:27.080097 140487190380544 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 17:29:27.080144 140487190380544 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 17:29:27.080286 140487190380544 resource_reader.py:55] Path not found: base_htrans.gin
I0123 17:29:27.080515 140487190380544 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 17:29:27.080623 140487190380544 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 17:29:27.087038 140487190380544 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 17:29:27.087167 140487190380544 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 17:29:27.087488 140487190380544 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 17:29:27.087590 140487190380544 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 17:29:27.087864 140487190380544 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 17:29:27.087963 140487190380544 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 17:29:27.088366 140487190380544 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 17:29:27.088467 140487190380544 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 17:29:27.092132 140487190380544 training_loop.py:334] ==== Training loop: initializing model ====
I0123 17:29:27.199199 140487190380544 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 17:29:27.199970 140487190380544 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 17:29:27.206397 140487190380544 training_loop.py:335] Process 0 of 1
I0123 17:29:27.206452 140487190380544 training_loop.py:336] Local device count = 1
I0123 17:29:27.206491 140487190380544 training_loop.py:337] Number of replicas = 1
I0123 17:29:27.206523 140487190380544 training_loop.py:339] Using random number seed 42
I0123 17:29:27.711205 140487190380544 training_loop.py:359] Initializing the model.
I0123 17:29:28.154303 140487190380544 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.154631 140487190380544 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 17:29:28.154741 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.154821 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.154898 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.154981 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.155054 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.155123 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.155192 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.155262 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.155332 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.155402 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.155473 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.155541 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:29:28.155582 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.155628 140487190380544 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:29:28.155743 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.155782 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.155814 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.157866 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.163305 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.174058 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.174346 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.178698 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.189332 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.189389 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.189426 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.189458 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.189523 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.190725 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.190804 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.191520 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.193970 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.200140 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.201441 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.201523 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.201559 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.201620 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.201761 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.202095 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.202142 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.204050 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.204149 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.207052 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.207135 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.207625 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.218023 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.226890 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.226989 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.227287 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.227369 140487190380544 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:29:28.227478 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.227516 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.227545 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.229381 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.231862 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.237439 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.237710 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.240343 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.244134 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.244189 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.244225 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.244256 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.244318 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.244900 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.244976 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.245332 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.246105 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.248601 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.249226 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.249301 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.249335 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.249393 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.249520 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.249845 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.249889 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.251802 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.251894 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.254729 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.254808 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.255225 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.257524 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.259417 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.259512 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.259808 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.259888 140487190380544 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:29:28.259995 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.260034 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.260064 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.262292 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.264670 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.270230 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.270494 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.273149 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.276935 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.276990 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.277025 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.277056 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.277118 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.277684 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.277761 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.278118 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.278873 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.281365 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.282034 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.282111 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.282146 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.282205 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.282330 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.282650 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.282693 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.284591 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.284683 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.287198 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.287282 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.287758 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.290015 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.291914 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.292009 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.292301 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.292380 140487190380544 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:29:28.292486 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.292525 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.292555 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.294442 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.296821 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.302434 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.302697 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.305338 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.309083 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.309138 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.309173 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.309203 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.309264 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.309834 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.309910 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.310268 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.311037 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.313577 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.314195 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.314272 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.314308 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.314370 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.314496 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.314820 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.314862 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.316750 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.316842 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.319401 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.319490 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.319920 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.322176 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.324070 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.324164 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.324454 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.324534 140487190380544 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:29:28.324642 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.324681 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.324711 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.326625 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.328990 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.334600 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.334862 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.337862 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.341572 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.341625 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.341668 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.341699 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.341761 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.342318 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.342394 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.342749 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.343517 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.346076 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.346689 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.346765 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.346800 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.346861 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.346992 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.347311 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.347353 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.349261 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.349353 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.351920 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.352002 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.352424 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.354704 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.356635 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.356729 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.357019 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.357098 140487190380544 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:29:28.357206 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.357244 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.357274 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.359112 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.361476 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.367072 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.367333 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.370021 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.373710 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.373764 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.373799 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.373830 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.373892 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.374490 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.374567 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.374926 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.375692 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.378169 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.378776 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.378851 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.378885 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.378942 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.379066 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.379382 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.379429 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.381302 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.381393 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.383909 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.383987 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.384409 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.386706 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.388606 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.388699 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.388989 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.389069 140487190380544 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:29:28.389177 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.389215 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.389246 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.391072 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.393475 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.399009 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.399269 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.401891 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.405647 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.405701 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.405736 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.405767 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.405829 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.406378 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.406453 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.406816 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.407587 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.410062 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.410672 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.410748 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.410782 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.410840 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.410965 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.411287 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.411329 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.413569 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.413668 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.416133 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.416211 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.416639 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.556200 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.558471 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.558637 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.558957 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.559049 140487190380544 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:29:28.559164 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.559205 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.559237 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.561280 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.563798 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.569481 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.569763 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.572434 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.576340 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.576397 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.576434 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.576466 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.576530 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.577147 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.577224 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.577586 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.578379 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.580955 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.581598 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.581683 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.581718 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.581779 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.581905 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.582229 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.582272 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.584163 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.584255 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.586766 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.586845 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.587327 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.589578 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.591502 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.591604 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.591900 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.591979 140487190380544 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:29:28.592088 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.592126 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.592157 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.594053 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.596429 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.602076 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.602341 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.605029 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.608767 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.608822 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.608858 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.608890 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.608952 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.609520 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.609596 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.609963 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.610728 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.613296 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.613921 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.613999 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.614034 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.614093 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.614218 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.614542 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.614585 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.616465 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.616558 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.619094 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.619174 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.619606 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.622079 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.623985 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.624080 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.624369 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.624456 140487190380544 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:29:28.624567 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.624606 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.624636 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.626528 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.628903 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.634836 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.635098 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.637780 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.641473 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.641527 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.641563 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.641595 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.641664 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.642219 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.642293 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.642648 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.643460 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.645958 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.646572 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.646648 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.646683 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.646741 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.646870 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.647192 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.647234 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.649117 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.649209 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.651752 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.651837 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.652266 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.654533 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.656493 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.656587 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.656875 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.656964 140487190380544 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:29:28.657075 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.657114 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.657145 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.658975 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.661395 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.666920 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.667180 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.669828 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.673501 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.673555 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.673591 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.673622 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.673731 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.674289 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.674367 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.674723 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.675495 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.677965 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.678578 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.678653 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.678687 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.678746 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.678874 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.679192 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.679234 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.681160 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.681252 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.684017 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.684096 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.684521 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.686817 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.688703 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.688795 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.689083 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.689163 140487190380544 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:29:28.689280 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.689319 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.689350 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.691155 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.693577 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.699183 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.699444 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.702092 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:29:28.706179 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.706234 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.706270 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.706301 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.706362 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.706922 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.706997 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.707359 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.708121 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.710610 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.711235 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.711312 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.711347 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.711407 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.711537 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.711855 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.711897 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.718208 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.718352 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.721110 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.721190 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.721643 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.724011 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.725953 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.726051 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.726344 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.726650 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.726721 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.726788 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.726847 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.726902 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.726957 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.727009 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.727061 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.727113 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.727165 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.727217 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.727268 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:29:28.727305 140487190380544 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:29:28.730882 140487190380544 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:29:28.778798 140487190380544 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.778886 140487190380544 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:29:28.778941 140487190380544 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:29:28.779048 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.779088 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.779118 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.779185 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.781618 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.787139 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.787400 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.790088 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:28.806825 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.806883 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.806920 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.806951 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.807014 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.808159 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.808238 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.808949 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.810972 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.815754 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.817045 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.817129 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.817164 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.817223 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.817349 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.817458 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.817497 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.819415 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.819510 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.821954 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.822033 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.822147 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.824394 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.826349 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.826446 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.826737 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.826819 140487190380544 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:29:28.826928 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.826965 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.826996 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.827059 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.829320 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.834804 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.835065 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.837755 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:28.850742 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.850797 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.850831 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.850861 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.850922 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.851475 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.851551 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.851902 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.852594 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.855083 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.855695 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.855772 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.855811 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.855870 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.855998 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.856107 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.856146 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.858075 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.858171 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.860579 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.860657 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.860768 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.862998 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.864927 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.865022 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.865309 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.865390 140487190380544 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:29:28.865498 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.865537 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.865567 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.865629 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.867873 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.873323 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.873579 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.876285 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:28.889000 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.889056 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.889091 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.889121 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.889185 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.889747 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.889825 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.890174 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.890865 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.893373 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.893997 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.894074 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.894109 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.894172 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.894298 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.894406 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.894444 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.896366 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.896460 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.898906 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.898986 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.899093 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.901323 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.903259 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.903355 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.903645 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.903727 140487190380544 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:29:28.903837 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.903877 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.903907 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.903970 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.906236 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.911711 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.911971 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.914665 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:28.927351 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.927406 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.927443 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.927474 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.927534 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.928082 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.928158 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.928513 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.929213 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.931727 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.932341 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.932417 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.932451 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.932508 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.932646 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.932756 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.932795 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.935022 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.935116 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.937564 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.937649 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.937760 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.939969 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.941847 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.941943 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.942231 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.942313 140487190380544 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:29:28.942421 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.942460 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.942490 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.942552 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.944874 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.950361 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.950626 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.953270 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:28.966016 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:28.966072 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:28.966107 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:28.966138 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.966202 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.966753 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.966829 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.967189 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.967883 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.970441 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.971059 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.971138 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:28.971172 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:28.971230 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.971362 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:28.971472 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:28.971510 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.973407 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.973500 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.975918 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.975998 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:28.976104 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:28.978392 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:28.980257 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.980351 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:28.980638 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.980718 140487190380544 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:29:28.980826 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:28.980865 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:28.980895 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:28.980957 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.983221 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:28.988642 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:28.988899 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:28.991611 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.004250 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.004306 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.004340 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.004371 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.004432 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.004983 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.005060 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.005419 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.006125 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.008624 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.009231 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.009311 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.009345 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.009404 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.009533 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.009656 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.009697 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.011629 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.011723 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.014140 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.014223 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.014331 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.016542 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.018405 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.018500 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.018784 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.018865 140487190380544 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:29:29.018972 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.019011 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.019042 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.019103 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.021335 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.026900 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.027157 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.029778 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.042788 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.042843 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.042878 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.042908 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.042976 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.043526 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.043602 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.043958 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.044642 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.047123 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.047780 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.047856 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.047890 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.047948 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.048077 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.048186 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.048230 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.050094 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.050187 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.052579 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.052657 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.052764 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.054964 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.056881 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.056976 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.057263 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.057344 140487190380544 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:29:29.057454 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.057493 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.057523 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.057585 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.059829 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.065264 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.065533 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.068192 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.080697 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.080753 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.080787 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.080818 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.080879 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.081481 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.081557 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.081920 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.082610 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.085087 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.085708 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.085785 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.085820 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.085880 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.086011 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.086124 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.086169 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.088061 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.088154 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.090652 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.090732 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.090841 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.093041 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.094928 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.095024 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.095310 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.095391 140487190380544 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:29:29.095500 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.095538 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.095568 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.095630 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.097860 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.103359 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.103619 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.106263 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.118902 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.118958 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.118993 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.119024 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.119084 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.119638 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.119714 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.120071 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.120755 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.123282 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.123949 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.124026 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.124060 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.124118 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.124249 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.124363 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.124402 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.126281 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.126374 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.128778 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.128856 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.128962 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.131174 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.133096 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.133191 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.133476 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.133557 140487190380544 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:29:29.133672 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.133713 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.133743 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.133806 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.136041 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.141451 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.141715 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.144700 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.157228 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.157284 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.157319 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.157350 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.157412 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.158025 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.158101 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.158452 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.159139 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.161629 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.162257 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.162334 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.162368 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.162425 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.162554 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.162661 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.162699 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.164581 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.164679 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.167158 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.167237 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.167344 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.169560 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.171420 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.171515 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.171801 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.171881 140487190380544 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:29:29.171990 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.172028 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.172058 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.172121 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.174347 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.179838 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.180102 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.182741 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.195329 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.195384 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.195421 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.195451 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.195512 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.196068 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.196146 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.196503 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.197185 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.199677 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.200334 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.200410 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.200444 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.200501 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.200630 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.200739 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.200778 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.202653 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.202753 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.205169 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.205251 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.205359 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.207554 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.209468 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.209564 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.209856 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.209937 140487190380544 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:29:29.210045 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.210083 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.210114 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.210176 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.212408 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.217843 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.218101 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.220742 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.233276 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.233332 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.233367 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.233397 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.233459 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.234030 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.234107 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.234465 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.235161 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.237749 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.238359 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.238435 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.238470 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.238528 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.238654 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.238762 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.238800 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.240692 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.240786 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.243216 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.243300 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.243409 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.246020 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.247883 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.247977 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.248265 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.248352 140487190380544 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:29:29.251247 140487190380544 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:29:29.305938 140487190380544 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.306025 140487190380544 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:29:29.306079 140487190380544 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:29:29.306187 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.306226 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.306256 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.306320 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.308639 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.314025 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.314285 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.316860 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.329176 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.329232 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.329268 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.329299 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.329360 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.329916 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.329992 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.330350 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.331015 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.333506 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.334120 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.334197 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.334232 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.334290 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.334415 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.334530 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.334569 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.336384 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.336482 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.338868 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.338946 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.339053 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.341274 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.343107 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.343203 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.343487 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.343567 140487190380544 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:29:29.343674 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.343711 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.343740 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.343802 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.346033 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.351366 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.351621 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.354258 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.366357 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.366414 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.366449 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.366480 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.366543 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.367089 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.367165 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.367519 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.368195 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.370697 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.371307 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.371384 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.371419 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.371477 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.371603 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.371711 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.371756 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.373579 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.373678 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.376051 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.376130 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.376239 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.378469 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.380291 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.380384 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.380669 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.380750 140487190380544 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:29:29.380856 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.380893 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.380923 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.380986 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.383198 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.388515 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.388770 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.391399 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.403652 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.403708 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.403743 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.403774 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.403836 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.404384 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.404459 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.404810 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.405475 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.408392 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.409001 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.409078 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.409118 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.409177 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.409304 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.409413 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.409451 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.411271 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.411366 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.413748 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.413827 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.413933 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.416134 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.417967 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.418062 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.418347 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.418428 140487190380544 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:29:29.418535 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.418572 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.418603 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.418666 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.420881 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.426199 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.426457 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.429085 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.441328 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.441383 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.441420 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.441461 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.441524 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.442083 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.442159 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.442514 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.443190 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.445713 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.446330 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.446405 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.446439 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.446496 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.446620 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.446725 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.446767 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.448627 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.448719 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.451088 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.451166 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.451272 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.453516 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.455366 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.455459 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.455740 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.455820 140487190380544 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:29:29.455925 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.455962 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.455991 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.456051 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.458286 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.463633 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.463890 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.466571 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.478969 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.479022 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.479056 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.479085 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.479146 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.479704 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.479779 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.480132 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.480814 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.483360 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.483976 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.484051 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.484084 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.484141 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.484270 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.484378 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.484416 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.486293 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.486391 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.488773 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.488849 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.488956 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.491214 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.493057 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.493151 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.493435 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.493515 140487190380544 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:29:29.493620 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.493664 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.493694 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.493755 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.495983 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.501469 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.501732 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.504398 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.516798 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.516852 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.516886 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.516915 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.516976 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.517526 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.517601 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.517966 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.518648 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.521600 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.522223 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.522299 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.522332 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.522390 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.522515 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.522621 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.522658 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.524498 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.524594 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.526959 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.527036 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.527141 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.529403 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.531252 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.531345 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.531626 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.531705 140487190380544 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:29:29.531811 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.531848 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.531877 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.531937 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.534149 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.539500 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.539754 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.542409 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.554729 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.554782 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.554816 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.554844 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.554905 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.555461 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.555536 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.555893 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.556565 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.559087 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.559701 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.559775 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.559808 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.559864 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.559989 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.560096 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.560133 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.561982 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.562074 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.564452 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.564529 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.564634 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.566884 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.568717 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.568810 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.569089 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.569167 140487190380544 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:29:29.569273 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.569309 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.569338 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.569399 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.571605 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.576917 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.577174 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.579843 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.592225 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.592279 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.592313 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.592342 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.592407 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.592959 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.593033 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.593381 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.594077 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.596589 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.597208 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.597283 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.597317 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.597375 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.597500 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.597606 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.597651 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.599497 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.599588 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.601978 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.602061 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.602169 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.604429 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.606269 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.606363 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.606644 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.606723 140487190380544 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:29:29.606827 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.606863 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.606892 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.606953 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.609160 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.614497 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.614751 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.617408 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.629782 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.629837 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.629870 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.629899 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.629961 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.630512 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.630587 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.630947 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.631637 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.634574 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.635196 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.635272 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.635306 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.635363 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.635489 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.635597 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.635634 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.637489 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.637580 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.639971 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.640057 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.640165 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.642422 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.644253 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.644348 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.644633 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.644713 140487190380544 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:29:29.644820 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.644857 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.644887 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.644950 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.647182 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.652547 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.652803 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.655440 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.667805 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.667858 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.667891 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.667921 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.667982 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.668532 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.668606 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.668964 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.669656 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.672183 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.672801 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.672877 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.672910 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.672967 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.673093 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.673199 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.673236 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.675513 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.675607 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.677971 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.678050 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.678161 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.680386 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.682223 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.682318 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.682601 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.682682 140487190380544 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:29:29.682788 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.682826 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.682855 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.682916 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.685115 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.690497 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.690754 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.693399 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.705720 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.705773 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.705808 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.705837 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.705899 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.706451 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.706526 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.706882 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.707560 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.710100 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.710719 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.710795 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.710829 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.710886 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.711014 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.711119 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.711156 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.712994 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.713084 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.715469 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.715546 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.715651 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.717908 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.719753 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.719847 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.720129 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.720209 140487190380544 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:29:29.720315 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:29:29.720353 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:29:29.720382 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:29:29.720443 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.722673 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:29:29.728048 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.728304 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:29:29.730964 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:29:29.743295 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:29:29.743349 140487190380544 attention.py:418] Single window, no scan.
I0123 17:29:29.743383 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:29:29.743412 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.743474 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.744028 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.744102 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.744452 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.745137 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.748027 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.748645 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.748724 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:29:29.748757 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:29:29.748813 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.748938 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:29:29.749047 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:29:29.749085 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.750929 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.751022 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.753396 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.753476 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:29:29.753582 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:29:29.755827 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:29:29.757670 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.757764 140487190380544 nn_components.py:261] mlp: residual
I0123 17:29:29.758052 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:29.758136 140487190380544 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:29:29.760926 140487190380544 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:29:34.157973 140487190380544 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 17:29:34.690055 140487190380544 training_loop.py:409] No working directory specified.
I0123 17:29:34.690198 140487190380544 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 17:29:34.691018 140487190380544 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 17:29:38.139105 140487190380544 training_loop.py:447] Only restoring trainable parameters.
I0123 17:29:38.139808 140487190380544 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 17:29:38.139887 140487190380544 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.139938 140487190380544 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.139982 140487190380544 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.140025 140487190380544 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140064 140487190380544 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.140104 140487190380544 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140143 140487190380544 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140181 140487190380544 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.140219 140487190380544 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.140257 140487190380544 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140295 140487190380544 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.140331 140487190380544 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.140368 140487190380544 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.140404 140487190380544 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140441 140487190380544 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.140478 140487190380544 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140514 140487190380544 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140549 140487190380544 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.140585 140487190380544 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.140638 140487190380544 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140675 140487190380544 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.140711 140487190380544 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.140747 140487190380544 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.140782 140487190380544 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140817 140487190380544 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.140853 140487190380544 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140889 140487190380544 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.140925 140487190380544 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.140961 140487190380544 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.140996 140487190380544 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141031 140487190380544 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.141067 140487190380544 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.141104 140487190380544 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.141139 140487190380544 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141175 140487190380544 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.141210 140487190380544 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141245 140487190380544 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141281 140487190380544 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.141315 140487190380544 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.141352 140487190380544 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141387 140487190380544 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.141422 140487190380544 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.141458 140487190380544 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.141492 140487190380544 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141528 140487190380544 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.141568 140487190380544 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141605 140487190380544 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141647 140487190380544 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.141686 140487190380544 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.141723 140487190380544 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141758 140487190380544 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.141793 140487190380544 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.141828 140487190380544 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.141864 140487190380544 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141900 140487190380544 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.141935 140487190380544 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.141970 140487190380544 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142006 140487190380544 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.142041 140487190380544 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.142076 140487190380544 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142111 140487190380544 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.142146 140487190380544 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.142181 140487190380544 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.142217 140487190380544 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142253 140487190380544 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.142289 140487190380544 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142324 140487190380544 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142359 140487190380544 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.142395 140487190380544 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.142431 140487190380544 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142467 140487190380544 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.142503 140487190380544 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.142543 140487190380544 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.142580 140487190380544 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142616 140487190380544 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.142652 140487190380544 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142688 140487190380544 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142723 140487190380544 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.142758 140487190380544 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.142793 140487190380544 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142828 140487190380544 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.142863 140487190380544 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.142898 140487190380544 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.142934 140487190380544 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.142969 140487190380544 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.143004 140487190380544 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143039 140487190380544 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143074 140487190380544 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.143109 140487190380544 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.143144 140487190380544 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143178 140487190380544 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.143213 140487190380544 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.143249 140487190380544 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.143284 140487190380544 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143319 140487190380544 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.143354 140487190380544 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143388 140487190380544 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143424 140487190380544 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.143458 140487190380544 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.143498 140487190380544 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143535 140487190380544 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.143571 140487190380544 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.143606 140487190380544 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.143642 140487190380544 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143677 140487190380544 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.143712 140487190380544 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143748 140487190380544 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143784 140487190380544 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.143819 140487190380544 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.143855 140487190380544 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.143890 140487190380544 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.143924 140487190380544 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:29:38.143962 140487190380544 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:29:38.143998 140487190380544 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.144033 140487190380544 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.144070 140487190380544 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.144105 140487190380544 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.144139 140487190380544 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:29:38.144174 140487190380544 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:29:38.144208 140487190380544 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:29:38.144243 140487190380544 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:29:38.144270 140487190380544 training_loop.py:725] Total parameters: 152072288
I0123 17:29:38.144475 140487190380544 training_loop.py:739] Total state size: 0
I0123 17:29:38.166607 140487190380544 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 17:29:38.166870 140487190380544 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 17:29:38.167402 140487190380544 training_loop.py:652] Compiling mode beam_search with jit.
I0123 17:29:38.167741 140487190380544 training_loop.py:89] registering functions: dict_keys([])
I0123 17:29:38.187944 140487190380544 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m ? cong r b r m
I0123 17:29:43.754970 140487190380544 ddar.py:60] Depth 1/1000 time = 5.484962463378906
I0123 17:29:56.634105 140487190380544 ddar.py:60] Depth 2/1000 time = 12.878828287124634
I0123 17:30:14.198434 140487190380544 ddar.py:60] Depth 3/1000 time = 17.563894748687744
I0123 17:30:34.948845 140487190380544 ddar.py:60] Depth 4/1000 time = 20.749918699264526
I0123 17:30:56.572771 140487190380544 ddar.py:60] Depth 5/1000 time = 21.623461484909058
I0123 17:31:19.146710 140487190380544 ddar.py:60] Depth 6/1000 time = 22.573513507843018
I0123 17:31:41.014937 140487190380544 ddar.py:60] Depth 7/1000 time = 21.867811918258667
I0123 17:32:05.937380 140487190380544 ddar.py:60] Depth 8/1000 time = 24.9219651222229
I0123 17:32:32.835405 140487190380544 ddar.py:60] Depth 9/1000 time = 26.89747977256775
I0123 17:32:59.558158 140487190380544 ddar.py:60] Depth 10/1000 time = 26.722253561019897
I0123 17:33:28.952707 140487190380544 ddar.py:60] Depth 11/1000 time = 29.384562730789185
I0123 17:34:02.719426 140487190380544 ddar.py:60] Depth 12/1000 time = 33.766149044036865
I0123 17:34:37.132460 140487190380544 ddar.py:60] Depth 13/1000 time = 34.41254806518555
I0123 17:35:12.178565 140487190380544 ddar.py:60] Depth 14/1000 time = 35.045530796051025
I0123 17:35:49.447261 140487190380544 ddar.py:60] Depth 15/1000 time = 37.178938150405884
I0123 17:36:26.312335 140487190380544 ddar.py:60] Depth 16/1000 time = 36.864487648010254
I0123 17:37:02.883330 140487190380544 ddar.py:60] Depth 17/1000 time = 36.57044577598572
I0123 17:37:39.827616 140487190380544 ddar.py:60] Depth 18/1000 time = 36.943642377853394
I0123 17:37:40.536311 140487190380544 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:37:40.536455 140487190380544 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 17:37:40.536496 140487190380544 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C c e f 03 ; g : C a b g 04 T a b c g 05 ; h : C b f h 06 T b f c h 07 ; i : C a f i 08 T a f c i 09 ; j : C a c j 10 T a c f j 11 ; k : C a b k 12 T a b f k 13 ; l : C b c l 14 T b c f l 15 ; m : C c h m 16 C f l m 17 ; n : D g n i n 18 D h n i n 19 ; o : D j o k o 20 D k o l o 21 ; p : D g n n p 22 D j o o p 23 ; q : D g n n q 24 D j o o q 25 ; r : C b m r 26 C p q r 27 ? D r b r m {F1} x00
I0123 17:37:40.536531 140487190380544 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C c e f 03 ; g : C a b g 04 T a b c g 05 ; h : C b f h 06 T b f c h 07 ; i : C a f i 08 T a f c i 09 ; j : C a c j 10 T a c f j 11 ; k : C a b k 12 T a b f k 13 ; l : C b c l 14 T b c f l 15 ; m : C c h m 16 C f l m 17 ; n : D g n i n 18 D h n i n 19 ; o : D j o k o 20 D k o l o 21 ; p : D g n n p 22 D j o o p 23 ; q : D g n n q 24 D j o o q 25 ; r : C b m r 26 C p q r 27 ? D r b r m {F1} x00
I0123 17:37:40.697834 140487190380544 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.698069 140487190380544 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 17:37:40.698177 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698254 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698325 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698396 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698481 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698550 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698618 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698684 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698758 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698836 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698914 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.698979 140487190380544 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:37:40.699022 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.699068 140487190380544 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:37:40.699177 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.699218 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.699248 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.701232 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.703823 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.709721 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.709993 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.712686 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:40.716659 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:40.716716 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.716757 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:40.716790 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.716850 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.717549 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.717624 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.717995 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.718776 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.721354 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.721979 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.722056 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:40.722089 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:40.722149 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.722274 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:40.722599 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:40.722643 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.724646 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.724746 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.727218 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.727297 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:40.727716 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:40.730107 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.732058 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.732152 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.732439 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.732519 140487190380544 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:37:40.732625 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.732661 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.732691 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.734547 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.736874 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.742460 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.742721 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.745340 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:40.748959 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:40.749012 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.749046 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:40.749076 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.749136 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.749694 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.749768 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.750120 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.750873 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.753335 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.753994 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.754070 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:40.754103 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:40.754161 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.754287 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:40.754601 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:40.754642 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.756547 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.756638 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.759268 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.759347 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:40.759766 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:40.762263 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.764166 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.764259 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.764546 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.764625 140487190380544 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:37:40.764731 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.764768 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.764797 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.766582 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.768877 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.774894 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.775148 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.777700 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:40.781293 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:40.781348 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.781382 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:40.781412 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.781472 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.782075 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.782152 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.782502 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.783260 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.785698 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.786310 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.786385 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:40.786418 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:40.786474 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.786602 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:40.786915 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:40.786955 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.788914 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.789005 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.791429 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.791513 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:40.791927 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:40.794162 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.796053 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.796145 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.796432 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.796511 140487190380544 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:37:40.796617 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.796654 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.796683 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.798532 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.800826 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.806349 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.806602 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.809128 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:40.812764 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:40.812817 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.812851 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:40.812880 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.812940 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.813487 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.813562 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.813918 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.814674 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.817113 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.817731 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.817809 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:40.817842 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:40.817899 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.818025 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:40.818384 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:40.818426 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.820309 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.820400 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.822819 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.822898 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:40.823316 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:40.825521 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.827501 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.827596 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.827883 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.827963 140487190380544 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:37:40.828070 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.828106 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.828135 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.829939 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.832271 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.837858 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.838111 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.840652 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:40.844240 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:40.844294 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.844327 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:40.844357 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.844416 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.845006 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.845081 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.845430 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.846188 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.848604 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.849210 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.849284 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:40.849317 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:40.849373 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.849498 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:40.849878 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:40.849920 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.851850 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.851940 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.854361 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.854440 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:40.854854 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:40.857089 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.858971 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.859064 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.859346 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.859429 140487190380544 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:37:40.859535 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.859571 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.859599 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.861438 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.863731 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.869198 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.869452 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.871992 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:40.875625 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:40.875679 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.875712 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:40.875742 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.875803 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.876345 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.876419 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.876768 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.877519 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.879971 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.880578 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.880656 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:40.880688 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:40.880743 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.880896 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:40.881598 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:40.881647 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.883548 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.883639 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.886083 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.886162 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:40.886575 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:40.888797 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.890774 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.890867 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.891155 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.891234 140487190380544 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:37:40.891338 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.891374 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.891403 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.893163 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.895445 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.901033 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.901286 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.903841 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:40.907444 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:40.907498 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.907532 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:40.907561 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.907621 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.908213 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.908288 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.908640 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.909401 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.911888 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.912495 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.912571 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:40.912604 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:40.912660 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.912784 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:40.913097 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:40.913137 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.915091 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.915182 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.917593 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.917678 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:40.918100 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:40.920353 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.922262 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.922363 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.922648 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.922729 140487190380544 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:37:40.922834 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.922871 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.922900 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.924731 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.927040 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.932485 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.932737 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.935268 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:40.938916 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:40.938969 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.939003 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:40.939032 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.939092 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.939641 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.939715 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.940067 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.940819 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.943290 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.943903 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.943978 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:40.944010 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:40.944066 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.944190 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:40.944553 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:40.944594 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.946482 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.946573 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.948975 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.949052 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:40.949462 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:40.951696 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.953660 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.953755 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.954050 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.954130 140487190380544 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:37:40.954236 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.954273 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.954302 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.956062 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.958350 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.963923 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.964174 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.966713 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:40.970304 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:40.970358 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:40.970392 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:40.970422 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.970482 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.971082 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.971157 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.971508 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.972253 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.974706 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.975312 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.975389 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:40.975422 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:40.975480 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.975604 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:40.975915 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:40.975956 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.977897 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.977988 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.980525 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.980602 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:40.981013 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:40.983230 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:40.985107 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.985199 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:40.985482 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.985571 140487190380544 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:37:40.985684 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:40.985723 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:40.985752 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:40.987504 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.990183 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:40.995683 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:40.995935 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:40.998471 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:41.002063 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.002117 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.002151 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.002180 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.002240 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.212159 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.212417 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.212891 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.213793 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.216457 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.217148 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.217229 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.217265 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.217336 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.217470 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.217867 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.217911 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.220282 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.220379 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.223001 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.223083 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.223526 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.225924 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.227972 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.228069 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.228367 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.228483 140487190380544 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:37:41.228596 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.228636 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.228668 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.230554 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.232966 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.238659 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.238924 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.241525 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:41.245345 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.245401 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.245436 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.245465 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.245527 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.246103 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.246179 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.246536 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.247287 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.249765 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.250391 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.250467 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.250501 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.250560 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.250689 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.251011 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.251052 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.253035 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.253126 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.255604 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.255684 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.256110 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.258379 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.260285 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.260378 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.260669 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.260749 140487190380544 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:37:41.260867 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.260905 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.260934 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.262797 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.265128 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.270748 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.271003 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.273668 140487190380544 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:37:41.277313 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.277366 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.277400 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.277430 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.277491 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.278054 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.278129 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.278482 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.279238 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.281745 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.282356 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.282432 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.282465 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.282522 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.282647 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.282962 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.283002 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.284893 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.284984 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.287464 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.287543 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.287963 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.290276 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.292171 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.292263 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.292548 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.292795 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.292861 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.292924 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.292979 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.293030 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.293081 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.293132 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.293183 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.293234 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.293284 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.293334 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.293382 140487190380544 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:37:41.293417 140487190380544 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:37:41.296296 140487190380544 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:37:41.340833 140487190380544 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.340917 140487190380544 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:37:41.340969 140487190380544 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:37:41.341070 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.341107 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.341136 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.341197 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.343548 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.348902 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.349159 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.351752 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.364310 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.364365 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.364399 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.364428 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.364489 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.365100 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.365175 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.365532 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.366229 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.368697 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.369305 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.369388 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.369422 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.369480 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.369606 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.369724 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.369764 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.371600 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.371693 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.374451 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.374530 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.374637 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.376808 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.378647 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.378741 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.379030 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.379112 140487190380544 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:37:41.379220 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.379257 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.379285 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.379345 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.381548 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.386981 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.387238 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.389777 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.402084 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.402138 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.402172 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.402202 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.402262 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.402802 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.402876 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.403224 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.403888 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.406374 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.406973 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.407048 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.407085 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.407143 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.407269 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.407375 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.407412 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.409226 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.409317 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.411689 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.411766 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.411871 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.414093 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.415915 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.416008 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.416294 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.416375 140487190380544 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:37:41.416480 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.416517 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.416546 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.416606 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.418817 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.424184 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.424441 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.427000 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.439255 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.439310 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.439344 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.439373 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.439434 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.439979 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.440054 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.440406 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.441087 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.443580 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.444187 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.444262 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.444300 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.444358 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.444489 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.444596 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.444632 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.446471 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.446563 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.448936 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.449011 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.449116 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.451338 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.453167 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.453261 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.453545 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.453625 140487190380544 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:37:41.453739 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.453776 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.453806 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.453866 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.456064 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.461468 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.461738 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.464315 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.476880 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.476934 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.476969 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.476998 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.477057 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.477596 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.477677 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.478029 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.478698 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.481194 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.481806 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.481882 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.481915 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.481979 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.482105 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.482211 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.482247 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.484066 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.484156 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.486532 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.486610 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.486714 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.488940 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.490778 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.490871 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.491159 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.491239 140487190380544 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:37:41.491344 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.491380 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.491408 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.491467 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.493705 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.499094 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.499354 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.501927 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.514109 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.514163 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.514196 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.514225 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.514286 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.514833 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.514908 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.515265 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.515944 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.518445 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.519053 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.519128 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.519161 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.519218 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.519353 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.519462 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.519499 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.521329 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.521420 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.523808 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.523885 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.523991 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.526226 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.528059 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.528151 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.528441 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.528522 140487190380544 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:37:41.528629 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.528666 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.528695 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.528756 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.530979 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.536430 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.536686 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.539264 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.551557 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.551612 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.551647 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.551676 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.551739 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.552286 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.552361 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.552713 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.553387 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.555840 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.556819 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.556896 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.556929 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.556986 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.557121 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.557229 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.557266 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.559123 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.559215 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.561601 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.561685 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.561792 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.563960 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.565863 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.565956 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.566245 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.566324 140487190380544 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:37:41.566430 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.566466 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.566495 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.566556 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.568755 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.574172 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.574427 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.577062 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.589246 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.589306 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.589339 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.589368 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.589429 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.590041 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.590118 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.590473 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.591152 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.593573 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.594180 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.594254 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.594287 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.594342 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.594468 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.594585 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.594622 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.596443 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.596533 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.598966 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.599044 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.599150 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.601303 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.603165 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.603259 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.603549 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.603629 140487190380544 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:37:41.603737 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.603774 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.603803 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.603864 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.606098 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.611529 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.611799 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.614390 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.626710 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.626765 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.626799 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.626829 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.626889 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.627434 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.627509 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.627864 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.628552 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.631034 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.631690 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.631767 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.631800 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.631858 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.631985 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.632092 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.632147 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.633983 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.634074 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.636437 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.636513 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.636617 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.638780 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.640663 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.640755 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.641043 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.641122 140487190380544 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:37:41.641229 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.641266 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.641295 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.641357 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.643571 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.648946 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.649201 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.651842 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.664444 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.664497 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.664531 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.664561 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.664621 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.665216 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.665292 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.665652 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.666332 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.668776 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.669382 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.669456 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.669489 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.669546 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.669685 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.669796 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.669834 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.671673 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.671763 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.674202 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.674282 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.674388 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.676547 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.678401 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.678494 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.678778 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.678858 140487190380544 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:37:41.678964 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.679002 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.679031 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.679092 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.681302 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.686728 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.686985 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.689559 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.701846 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.701899 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.701933 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.701963 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.702024 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.702573 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.702648 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.703002 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.703676 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.706130 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.706783 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.706859 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.706892 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.706950 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.707077 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.707185 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.707222 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.709043 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.709136 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.711542 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.711620 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.711727 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.713887 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.715776 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.715868 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.716156 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.716237 140487190380544 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:37:41.716344 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.716382 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.716412 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.716473 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.718688 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.724052 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.724309 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.726933 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.739245 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.739299 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.739333 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.739362 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.739424 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.740030 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.740105 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.740462 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.741153 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.743651 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.744270 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.744346 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.744380 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.744438 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.744566 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.744673 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.744709 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.746547 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.746650 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.749130 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.749208 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.749316 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.751491 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.753319 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.753412 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.753708 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.753790 140487190380544 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:37:41.753896 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.753932 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.753961 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.754022 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.756238 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.762021 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.762278 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.764848 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.777236 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.777289 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.777322 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.777351 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.777411 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.777962 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.778037 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.778389 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.779081 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.781532 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.782200 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.782277 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.782310 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.782366 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.782495 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.782600 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.782636 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.784460 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.784557 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.786951 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.787029 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.787134 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.789280 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.791174 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.791268 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.791557 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.791642 140487190380544 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:37:41.794466 140487190380544 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:37:41.844334 140487190380544 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.844422 140487190380544 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:37:41.844473 140487190380544 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:37:41.844574 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.844609 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.844638 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.844698 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.847046 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.852570 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.852827 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.855401 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.867733 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.867788 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.867822 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.867852 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.867913 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.868461 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.868535 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.868889 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.869561 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.872054 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.872663 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.872740 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.872773 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.872831 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.872967 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.873076 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.873113 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.874939 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.875030 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.877410 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.877486 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.877592 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.879806 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.881629 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.881733 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.882020 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.882100 140487190380544 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:37:41.882206 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.882243 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.882272 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.882333 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.884525 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.889877 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.890136 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.892781 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.904857 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.904911 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.904944 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.904973 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.905035 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.905576 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.905658 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.906020 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.906702 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.909231 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.909853 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.909930 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.909964 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.910022 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.910150 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.910265 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.910303 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.912131 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.912222 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.914613 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.914691 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.914797 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.917029 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.918876 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.918969 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.919259 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.919340 140487190380544 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:37:41.919445 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.919482 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.919511 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.919571 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.921802 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.927151 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.927407 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.930536 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.942735 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.942790 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.942824 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.942854 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.942915 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.943459 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.943535 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.943886 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.944563 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.947073 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.947677 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.947752 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.947785 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.947843 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.947968 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.948074 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.948116 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.949943 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.950036 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.952406 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.952485 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.952592 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.954843 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.956675 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.956768 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.957055 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.957135 140487190380544 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:37:41.957240 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.957276 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.957305 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.957365 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.959570 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:41.964912 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.965168 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:41.967805 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:41.979920 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:41.979974 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:41.980009 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:41.980038 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.980099 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.980644 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.980718 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.981070 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.981752 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.984267 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.984870 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.984945 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:41.984979 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:41.985036 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.985162 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:41.985269 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:41.985306 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.987149 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.987241 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.989612 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.989697 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:41.989804 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:41.992040 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:41.993863 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.993955 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:41.994241 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.994321 140487190380544 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:37:41.994426 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:41.994463 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:41.994493 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:41.994554 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:41.996750 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:42.002042 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.002297 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:42.004900 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:42.016923 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:42.016977 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:42.017011 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:42.017040 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.017101 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.017652 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.017728 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.018083 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.018759 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.021238 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.021846 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.021922 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:42.021956 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:42.022012 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.022137 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:42.022244 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:42.022280 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.024075 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.024174 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.026528 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.026606 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:42.026712 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:42.028934 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.030760 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.030854 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.031140 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.031219 140487190380544 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:37:42.031324 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:42.031362 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:42.031391 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:42.031452 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.033661 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:42.038954 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.039211 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:42.042287 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:42.054370 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:42.054429 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:42.054463 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:42.054493 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.054554 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.055100 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.055174 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.055525 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.056202 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.058746 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.059350 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.059427 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:42.059460 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:42.059518 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.059645 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:42.059753 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:42.059789 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.061613 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.061717 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.064112 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.064189 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:42.064296 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:42.066518 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.068328 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.068420 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.068706 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.068786 140487190380544 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:37:42.068890 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:42.068927 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:42.068956 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:42.069017 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.071225 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:42.076509 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.076763 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:42.079387 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:42.091492 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:42.091546 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:42.091580 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:42.091610 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.091671 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.092213 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.092287 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.092641 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.093316 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.095814 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.096418 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.096493 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:42.096527 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:42.096584 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.096711 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:42.096817 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:42.096854 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.098680 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.098773 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.101142 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.101219 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:42.101323 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:42.103560 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.105396 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.105489 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.105785 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.105865 140487190380544 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:37:42.105971 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:42.106008 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:42.106037 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:42.106097 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.108314 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:42.113649 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.113904 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:42.116524 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:42.128646 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:42.128701 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:42.128735 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:42.128764 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.128824 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.129365 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.129439 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.129798 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.130468 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.132948 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.133545 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.133619 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:42.133659 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:42.133718 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.133842 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:42.133949 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:42.133985 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.135802 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.135890 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.138254 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.138335 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:42.138442 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:42.140652 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.142477 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.142570 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.142854 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.142932 140487190380544 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:37:42.143036 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:42.143073 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:42.143101 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:42.143161 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.145346 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:42.150640 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.150894 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:42.153908 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:42.166018 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:42.166073 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:42.166107 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:42.166137 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.166198 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.166739 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.166812 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.167158 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.167819 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.170324 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.170920 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.170995 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:42.171028 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:42.171089 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.171212 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:42.171319 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:42.171355 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.173158 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.173247 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.175620 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.175702 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:42.175810 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:42.178020 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.179831 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.179923 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.180206 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.180285 140487190380544 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:37:42.180389 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:42.180426 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:42.180455 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:42.180517 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.182723 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:42.188016 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.188268 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:42.190910 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:42.203085 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:42.203139 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:42.203173 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:42.203203 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.203261 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.203804 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.203878 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.204231 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.204909 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.207415 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.208020 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.208095 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:42.208128 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:42.208185 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.208310 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:42.208416 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:42.208453 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.210286 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.210378 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.212756 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.212837 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:42.212944 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:42.215191 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.217013 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.217105 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.217391 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.217469 140487190380544 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:37:42.217575 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:42.217612 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:42.217647 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:42.217712 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.219920 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:42.225248 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.225502 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:42.228218 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:42.240372 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:42.240425 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:42.240459 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:42.240488 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.240548 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.241086 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.241159 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.241509 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.242188 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.244678 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.245280 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.245353 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:42.245387 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:42.245444 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.245567 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:42.245681 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:42.245720 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.247530 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.247620 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.249987 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.250065 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:42.250182 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:42.252408 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.254225 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.254317 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.254604 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.254682 140487190380544 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:37:42.254786 140487190380544 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:37:42.254823 140487190380544 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:37:42.254851 140487190380544 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:37:42.254911 140487190380544 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.257096 140487190380544 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:37:42.262383 140487190380544 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.262639 140487190380544 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:37:42.265681 140487190380544 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:37:42.277798 140487190380544 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:37:42.277852 140487190380544 attention.py:418] Single window, no scan.
I0123 17:37:42.277885 140487190380544 transformer_layer.py:389] tlayer: self-attention.
I0123 17:37:42.277916 140487190380544 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.277976 140487190380544 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.278517 140487190380544 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.278590 140487190380544 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.278939 140487190380544 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.279604 140487190380544 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.282119 140487190380544 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.282728 140487190380544 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.282803 140487190380544 transformer_layer.py:468] tlayer: End windows.
I0123 17:37:42.282836 140487190380544 transformer_layer.py:472] tlayer: final FFN.
I0123 17:37:42.282894 140487190380544 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.283020 140487190380544 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:37:42.283128 140487190380544 nn_components.py:325] mlp: activation = None
I0123 17:37:42.283164 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.284983 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.285073 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.287446 140487190380544 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.287523 140487190380544 transformer_base.py:443] tbase: final FFN
I0123 17:37:42.287629 140487190380544 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:37:42.289878 140487190380544 nn_components.py:329] mlp: final activation = None
I0123 17:37:42.291712 140487190380544 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.291805 140487190380544 nn_components.py:261] mlp: residual
I0123 17:37:42.292098 140487190380544 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:42.292183 140487190380544 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:37:42.294991 140487190380544 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:37:59.792432 140487190380544 alphageometry.py:566] LM output (score=-2.227750): "s : P a d e s 28 T a e d s 29 ;"
I0123 17:37:59.792575 140487190380544 alphageometry.py:567] Translation: "s = on_pline s e a d, on_tline s d a e"

I0123 17:37:59.792618 140487190380544 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_pline s e a d, on_tline s d a e ? cong r b r m"
I0123 17:37:59.792868 140487190380544 graph.py:498] 
I0123 17:37:59.792926 140487190380544 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_pline s e a d, on_tline s d a e ? cong r b r m
I0123 17:38:06.072843 140487190380544 ddar.py:60] Depth 1/1000 time = 6.185739994049072
I0123 17:38:20.854999 140487190380544 ddar.py:60] Depth 2/1000 time = 14.781904220581055
I0123 17:38:40.975580 140487190380544 ddar.py:60] Depth 3/1000 time = 20.12029218673706
I0123 17:39:04.095680 140487190380544 ddar.py:60] Depth 4/1000 time = 23.119739770889282
I0123 17:39:28.765832 140487190380544 ddar.py:60] Depth 5/1000 time = 24.669825315475464
I0123 17:39:53.936033 140487190380544 ddar.py:60] Depth 6/1000 time = 25.169834852218628
I0123 17:40:18.627285 140487190380544 ddar.py:60] Depth 7/1000 time = 24.69076633453369
I0123 17:40:46.595051 140487190380544 ddar.py:60] Depth 8/1000 time = 27.967158555984497
I0123 17:41:15.910649 140487190380544 ddar.py:60] Depth 9/1000 time = 29.315041303634644
I0123 17:41:45.620169 140487190380544 ddar.py:60] Depth 10/1000 time = 29.708922863006592
I0123 17:42:18.102922 140487190380544 ddar.py:60] Depth 11/1000 time = 32.47246336936951
I0123 17:42:55.065921 140487190380544 ddar.py:60] Depth 12/1000 time = 36.962374210357666
I0123 17:43:33.300388 140487190380544 ddar.py:60] Depth 13/1000 time = 38.23388433456421
I0123 17:44:11.923961 140487190380544 ddar.py:60] Depth 14/1000 time = 38.623112201690674
I0123 17:44:53.710130 140487190380544 ddar.py:60] Depth 15/1000 time = 41.680946826934814
I0123 17:45:39.060111 140487190380544 ddar.py:60] Depth 16/1000 time = 45.349289417266846
I0123 17:46:22.751390 140487190380544 ddar.py:60] Depth 17/1000 time = 43.69061207771301
I0123 17:47:06.897382 140487190380544 ddar.py:60] Depth 18/1000 time = 44.14539933204651
I0123 17:47:07.676161 140487190380544 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:47:07.676357 140487190380544 alphageometry.py:566] LM output (score=-2.240124): "s : C c f s 28 D c s f s 29 ;"
I0123 17:47:07.676398 140487190380544 alphageometry.py:567] Translation: "s = on_line s c f, on_bline s f c"

I0123 17:47:07.676445 140487190380544 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_line s c f, on_bline s f c ? cong r b r m"
I0123 17:47:07.676676 140487190380544 graph.py:498] 
I0123 17:47:07.676739 140487190380544 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_line s c f, on_bline s f c ? cong r b r m
I0123 17:47:14.281286 140487190380544 ddar.py:60] Depth 1/1000 time = 6.507588624954224
I0123 17:47:32.041065 140487190380544 ddar.py:60] Depth 2/1000 time = 17.759454250335693
I0123 17:48:04.173168 140487190380544 ddar.py:60] Depth 3/1000 time = 32.13165020942688
I0123 17:48:41.878449 140487190380544 ddar.py:60] Depth 4/1000 time = 37.7047975063324
I0123 17:49:21.342013 140487190380544 ddar.py:60] Depth 5/1000 time = 39.46300292015076
I0123 17:50:01.409549 140487190380544 ddar.py:60] Depth 6/1000 time = 40.067062854766846
I0123 17:50:40.987938 140487190380544 ddar.py:60] Depth 7/1000 time = 39.577733516693115
I0123 17:51:24.316250 140487190380544 ddar.py:60] Depth 8/1000 time = 43.32766556739807
I0123 17:52:14.754867 140487190380544 ddar.py:60] Depth 9/1000 time = 50.43801808357239
I0123 17:53:01.895076 140487190380544 ddar.py:60] Depth 10/1000 time = 47.139612674713135
I0123 17:53:48.901027 140487190380544 ddar.py:60] Depth 11/1000 time = 47.00530171394348
I0123 17:54:35.973913 140487190380544 ddar.py:60] Depth 12/1000 time = 47.07112789154053
I0123 17:55:25.339255 140487190380544 ddar.py:60] Depth 13/1000 time = 49.3489716053009
I0123 17:56:22.168737 140487190380544 ddar.py:60] Depth 14/1000 time = 56.82900810241699
I0123 17:57:21.950207 140487190380544 ddar.py:60] Depth 15/1000 time = 59.780813694000244
I0123 17:58:25.077182 140487190380544 ddar.py:60] Depth 16/1000 time = 63.12632703781128
I0123 17:59:28.408106 140487190380544 ddar.py:60] Depth 17/1000 time = 63.33025360107422
I0123 18:00:35.382922 140487190380544 ddar.py:60] Depth 18/1000 time = 66.86623477935791
I0123 18:01:42.394239 140487190380544 ddar.py:60] Depth 19/1000 time = 67.01075506210327
I0123 18:02:49.855829 140487190380544 ddar.py:60] Depth 20/1000 time = 67.46106600761414
I0123 18:03:59.257226 140487190380544 ddar.py:60] Depth 21/1000 time = 68.13862299919128
I0123 18:03:59.262059 140487190380544 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:03:59.262282 140487190380544 alphageometry.py:566] LM output (score=-2.374992): "s : T h p l s 28 ;"
I0123 18:03:59.262329 140487190380544 alphageometry.py:567] Translation: "s = on_tline s l h p"

I0123 18:03:59.262393 140487190380544 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_tline s l h p ? cong r b r m"
I0123 18:03:59.262640 140487190380544 graph.py:498] 
I0123 18:03:59.262707 140487190380544 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_tline s l h p ? cong r b r m
I0123 18:04:04.674573 140487190380544 ddar.py:60] Depth 1/1000 time = 5.317926645278931
I0123 18:04:18.760487 140487190380544 ddar.py:60] Depth 2/1000 time = 14.085593223571777
I0123 18:04:39.079989 140487190380544 ddar.py:60] Depth 3/1000 time = 20.31914520263672
I0123 18:05:02.205058 140487190380544 ddar.py:60] Depth 4/1000 time = 23.12455177307129
I0123 18:05:27.204293 140487190380544 ddar.py:60] Depth 5/1000 time = 24.998709201812744
I0123 18:05:52.500978 140487190380544 ddar.py:60] Depth 6/1000 time = 25.2962749004364
I0123 18:06:17.310878 140487190380544 ddar.py:60] Depth 7/1000 time = 24.80941653251648
I0123 18:06:45.672661 140487190380544 ddar.py:60] Depth 8/1000 time = 28.361183881759644
I0123 18:07:14.967980 140487190380544 ddar.py:60] Depth 9/1000 time = 29.294737100601196
I0123 18:07:44.680163 140487190380544 ddar.py:60] Depth 10/1000 time = 29.711573123931885
I0123 18:08:17.716700 140487190380544 ddar.py:60] Depth 11/1000 time = 33.026132583618164
I0123 18:08:53.932419 140487190380544 ddar.py:60] Depth 12/1000 time = 36.21525049209595
I0123 18:09:31.864638 140487190380544 ddar.py:60] Depth 13/1000 time = 37.9316132068634
I0123 18:10:10.375544 140487190380544 ddar.py:60] Depth 14/1000 time = 38.51019811630249
I0123 18:10:50.748194 140487190380544 ddar.py:60] Depth 15/1000 time = 40.260077714920044
I0123 18:11:31.163992 140487190380544 ddar.py:60] Depth 16/1000 time = 40.41517233848572
I0123 18:12:11.924189 140487190380544 ddar.py:60] Depth 17/1000 time = 40.75968837738037
I0123 18:12:53.341081 140487190380544 ddar.py:60] Depth 18/1000 time = 41.41627216339111
I0123 18:12:54.112714 140487190380544 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:12:54.112877 140487190380544 alphageometry.py:566] LM output (score=-2.461801): "s : T b d b s 28 ;"
I0123 18:12:54.112917 140487190380544 alphageometry.py:567] Translation: "s = on_tline s b b d"

I0123 18:12:54.112964 140487190380544 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_tline s b b d ? cong r b r m"
I0123 18:12:54.113178 140487190380544 graph.py:498] 
I0123 18:12:54.113241 140487190380544 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_tline s b b d ? cong r b r m
I0123 18:13:00.158512 140487190380544 ddar.py:60] Depth 1/1000 time = 5.956245183944702
I0123 18:13:12.767348 140487190380544 ddar.py:60] Depth 2/1000 time = 12.608494997024536
I0123 18:13:35.284558 140487190380544 ddar.py:60] Depth 3/1000 time = 22.516722202301025
I0123 18:14:00.422805 140487190380544 ddar.py:60] Depth 4/1000 time = 25.137761116027832
I0123 18:14:27.494872 140487190380544 ddar.py:60] Depth 5/1000 time = 27.07166814804077
I0123 18:14:55.339522 140487190380544 ddar.py:60] Depth 6/1000 time = 27.844157934188843
I0123 18:15:21.824265 140487190380544 ddar.py:60] Depth 7/1000 time = 26.484098196029663
I0123 18:15:52.156817 140487190380544 ddar.py:60] Depth 8/1000 time = 30.331905841827393
I0123 18:16:24.490731 140487190380544 ddar.py:60] Depth 9/1000 time = 32.333303451538086
I0123 18:16:56.575470 140487190380544 ddar.py:60] Depth 10/1000 time = 32.08408975601196
I0123 18:17:32.749320 140487190380544 ddar.py:60] Depth 11/1000 time = 36.16293168067932
I0123 18:18:12.267128 140487190380544 ddar.py:60] Depth 12/1000 time = 39.51735544204712
I0123 18:18:52.480896 140487190380544 ddar.py:60] Depth 13/1000 time = 40.213284492492676
I0123 18:19:34.181764 140487190380544 ddar.py:60] Depth 14/1000 time = 41.7001736164093
I0123 18:20:15.439712 140487190380544 ddar.py:60] Depth 15/1000 time = 41.25733757019043
I0123 18:20:16.290284 140487190380544 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:20:16.290500 140487190380544 alphageometry.py:566] LM output (score=-2.557757): "s : C a e s 28 D a s e s 29 ;"
I0123 18:20:16.290544 140487190380544 alphageometry.py:567] Translation: "s = on_line s a e, on_bline s e a"

I0123 18:20:16.290593 140487190380544 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_line s a e, on_bline s e a ? cong r b r m"
I0123 18:20:16.290806 140487190380544 graph.py:498] 
I0123 18:20:16.290876 140487190380544 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_line s a e, on_bline s e a ? cong r b r m
I0123 18:20:22.719357 140487190380544 ddar.py:60] Depth 1/1000 time = 6.330997943878174
I0123 18:20:35.964772 140487190380544 ddar.py:60] Depth 2/1000 time = 13.245057582855225
I0123 18:20:58.668613 140487190380544 ddar.py:60] Depth 3/1000 time = 22.70331072807312
I0123 18:21:25.062417 140487190380544 ddar.py:60] Depth 4/1000 time = 26.3932466506958
I0123 18:21:53.152760 140487190380544 ddar.py:60] Depth 5/1000 time = 28.089810609817505
I0123 18:22:21.816297 140487190380544 ddar.py:60] Depth 6/1000 time = 28.66303586959839
I0123 18:22:49.297868 140487190380544 ddar.py:60] Depth 7/1000 time = 27.481017351150513
I0123 18:23:21.522615 140487190380544 ddar.py:60] Depth 8/1000 time = 32.22409224510193
I0123 18:23:54.173382 140487190380544 ddar.py:60] Depth 9/1000 time = 32.65015935897827
I0123 18:24:28.012220 140487190380544 ddar.py:60] Depth 10/1000 time = 33.83815050125122
I0123 18:25:01.820027 140487190380544 ddar.py:60] Depth 11/1000 time = 33.80625557899475
I0123 18:25:38.385443 140487190380544 ddar.py:60] Depth 12/1000 time = 36.55388426780701
I0123 18:26:19.376669 140487190380544 ddar.py:60] Depth 13/1000 time = 40.99060845375061
I0123 18:27:01.687741 140487190380544 ddar.py:60] Depth 14/1000 time = 42.31058859825134
I0123 18:27:44.366011 140487190380544 ddar.py:60] Depth 15/1000 time = 42.677565813064575
I0123 18:28:27.094300 140487190380544 ddar.py:60] Depth 16/1000 time = 42.72758507728577
I0123 18:28:27.975589 140487190380544 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:28:27.975804 140487190380544 alphageometry.py:566] LM output (score=-2.565035): "s : P d s e f 28 ;"
I0123 18:28:27.975845 140487190380544 alphageometry.py:567] Translation: "s = on_pline s d e f"

I0123 18:28:27.975906 140487190380544 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_pline s d e f ? cong r b r m"
I0123 18:28:27.976136 140487190380544 graph.py:498] 
I0123 18:28:27.976196 140487190380544 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_pline s d e f ? cong r b r m
I0123 18:28:33.935925 140487190380544 ddar.py:60] Depth 1/1000 time = 5.868123292922974
I0123 18:28:46.338805 140487190380544 ddar.py:60] Depth 2/1000 time = 12.40258264541626
I0123 18:29:05.010739 140487190380544 ddar.py:60] Depth 3/1000 time = 18.67156195640564
I0123 18:29:26.417362 140487190380544 ddar.py:60] Depth 4/1000 time = 21.406110048294067
I0123 18:29:49.230837 140487190380544 ddar.py:60] Depth 5/1000 time = 22.81289029121399
I0123 18:30:12.775705 140487190380544 ddar.py:60] Depth 6/1000 time = 23.5443172454834
I0123 18:30:35.490104 140487190380544 ddar.py:60] Depth 7/1000 time = 22.71397638320923
I0123 18:31:01.835564 140487190380544 ddar.py:60] Depth 8/1000 time = 26.345052480697632
I0123 18:31:30.325143 140487190380544 ddar.py:60] Depth 9/1000 time = 28.48915386199951
I0123 18:31:58.579991 140487190380544 ddar.py:60] Depth 10/1000 time = 28.25440239906311
I0123 18:32:29.882889 140487190380544 ddar.py:60] Depth 11/1000 time = 31.292861938476562
I0123 18:33:04.181957 140487190380544 ddar.py:60] Depth 12/1000 time = 34.29858756065369
I0123 18:33:40.045329 140487190380544 ddar.py:60] Depth 13/1000 time = 35.86286902427673
I0123 18:34:16.723022 140487190380544 ddar.py:60] Depth 14/1000 time = 36.67701053619385
I0123 18:34:55.152146 140487190380544 ddar.py:60] Depth 15/1000 time = 38.338130712509155
I0123 18:35:34.178758 140487190380544 ddar.py:60] Depth 16/1000 time = 39.02601194381714
I0123 18:36:13.087157 140487190380544 ddar.py:60] Depth 17/1000 time = 38.907854318618774
I0123 18:36:52.621428 140487190380544 ddar.py:60] Depth 18/1000 time = 39.53366470336914
I0123 18:36:53.315764 140487190380544 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:36:53.315975 140487190380544 alphageometry.py:566] LM output (score=-2.623874): "s : D c s h s 28 D f s h s 29 ;"
I0123 18:36:53.316020 140487190380544 alphageometry.py:567] Translation: "s = on_bline s h c, on_bline s h f"

I0123 18:36:53.316068 140487190380544 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_bline s h c, on_bline s h f ? cong r b r m"
I0123 18:36:53.316287 140487190380544 graph.py:498] 
I0123 18:36:53.316352 140487190380544 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_bline s h c, on_bline s h f ? cong r b r m
I0123 18:37:00.385419 140487190380544 ddar.py:60] Depth 1/1000 time = 6.947988510131836
I0123 18:37:12.972331 140487190380544 ddar.py:60] Depth 2/1000 time = 12.586599826812744
I0123 18:37:33.430477 140487190380544 ddar.py:60] Depth 3/1000 time = 20.457839727401733
I0123 18:37:56.839486 140487190380544 ddar.py:60] Depth 4/1000 time = 23.40864872932434
I0123 18:38:21.754582 140487190380544 ddar.py:60] Depth 5/1000 time = 24.91469097137451
I0123 18:38:46.999565 140487190380544 ddar.py:60] Depth 6/1000 time = 25.244494676589966
I0123 18:39:11.880093 140487190380544 ddar.py:60] Depth 7/1000 time = 24.879936695098877
I0123 18:39:40.136725 140487190380544 ddar.py:60] Depth 8/1000 time = 28.256142616271973
I0123 18:40:09.938869 140487190380544 ddar.py:60] Depth 9/1000 time = 29.801493406295776
I0123 18:40:39.978541 140487190380544 ddar.py:60] Depth 10/1000 time = 30.03900170326233
I0123 18:41:10.266538 140487190380544 ddar.py:60] Depth 11/1000 time = 30.286883115768433
I0123 18:41:41.491630 140487190380544 ddar.py:60] Depth 12/1000 time = 31.224462032318115
I0123 18:42:17.548185 140487190380544 ddar.py:60] Depth 13/1000 time = 36.05591559410095
I0123 18:43:05.297663 140487190380544 ddar.py:60] Depth 14/1000 time = 47.74884629249573
I0123 18:43:56.240884 140487190380544 ddar.py:60] Depth 15/1000 time = 50.9427375793457
I0123 18:44:46.755247 140487190380544 ddar.py:60] Depth 16/1000 time = 50.51371765136719
I0123 18:45:41.375622 140487190380544 ddar.py:60] Depth 17/1000 time = 54.619706869125366
I0123 18:46:33.708648 140487190380544 ddar.py:60] Depth 18/1000 time = 52.3323392868042
I0123 18:47:28.301809 140487190380544 ddar.py:60] Depth 19/1000 time = 54.57857704162598
I0123 18:48:32.001386 140487190380544 ddar.py:60] Depth 20/1000 time = 63.69891953468323
I0123 18:49:37.331393 140487190380544 ddar.py:60] Depth 21/1000 time = 65.32933783531189
I0123 18:50:47.298937 140487190380544 ddar.py:60] Depth 22/1000 time = 69.96687269210815
I0123 18:51:56.229543 140487190380544 ddar.py:60] Depth 23/1000 time = 68.92990326881409
I0123 18:53:06.150592 140487190380544 ddar.py:60] Depth 24/1000 time = 69.81457114219666
I0123 18:54:16.497839 140487190380544 ddar.py:60] Depth 25/1000 time = 70.346604347229
I0123 18:55:27.525390 140487190380544 ddar.py:60] Depth 26/1000 time = 71.02699899673462
I0123 18:56:37.642694 140487190380544 ddar.py:60] Depth 27/1000 time = 70.11660027503967
I0123 18:57:50.373799 140487190380544 ddar.py:60] Depth 28/1000 time = 71.40419793128967
I0123 18:57:50.378561 140487190380544 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:57:50.378749 140487190380544 alphageometry.py:566] LM output (score=-2.695183): "s : T a d a s 28 ;"
I0123 18:57:50.378788 140487190380544 alphageometry.py:567] Translation: "s = on_tline s a a d"

I0123 18:57:50.378849 140487190380544 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_tline s a a d ? cong r b r m"
I0123 18:57:50.379088 140487190380544 graph.py:498] 
I0123 18:57:50.379153 140487190380544 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c e; g = foot g c b a; h = foot h c b f; i = foot i c a f; j = foot j f c a; k = foot k f b a; l = foot l f c b; m = on_line m c h, on_line m f l; n = circle n g i h; o = circle o j k l; p = on_circle p n g, on_circle p o j; q = on_circle q n g, on_circle q o j; r = on_line r p q, on_line r b m; s = on_tline s a a d ? cong r b r m
I0123 18:57:56.343745 140487190380544 ddar.py:60] Depth 1/1000 time = 5.875296115875244
I0123 18:58:10.385391 140487190380544 ddar.py:60] Depth 2/1000 time = 14.041254997253418
I0123 18:58:32.357392 140487190380544 ddar.py:60] Depth 3/1000 time = 21.971691131591797
I0123 18:58:57.761284 140487190380544 ddar.py:60] Depth 4/1000 time = 25.403472185134888
I0123 18:59:25.140813 140487190380544 ddar.py:60] Depth 5/1000 time = 27.378917455673218
I0123 18:59:53.050253 140487190380544 ddar.py:60] Depth 6/1000 time = 27.90881657600403
I0123 19:00:20.139655 140487190380544 ddar.py:60] Depth 7/1000 time = 27.088812589645386
I0123 19:00:51.011803 140487190380544 ddar.py:60] Depth 8/1000 time = 30.87166476249695
I0123 19:01:23.321599 140487190380544 ddar.py:60] Depth 9/1000 time = 32.309194564819336
I0123 19:01:55.844351 140487190380544 ddar.py:60] Depth 10/1000 time = 32.522074937820435
I0123 19:01:55.855963 140487190380544 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:01:55.856020 140487190380544 alphageometry.py:585] Timeout.
