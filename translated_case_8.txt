I0123 10:57:44.374428 139792109113344 inference_utils.py:69] Parsing gin configuration.
I0123 10:57:44.374562 139792109113344 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 10:57:44.374799 139792109113344 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 10:57:44.374836 139792109113344 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 10:57:44.374868 139792109113344 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 10:57:44.374897 139792109113344 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 10:57:44.374926 139792109113344 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 10:57:44.374954 139792109113344 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 10:57:44.374982 139792109113344 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 10:57:44.375010 139792109113344 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 10:57:44.375038 139792109113344 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 10:57:44.375065 139792109113344 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 10:57:44.375124 139792109113344 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 10:57:44.375271 139792109113344 resource_reader.py:55] Path not found: base_htrans.gin
I0123 10:57:44.375526 139792109113344 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 10:57:44.375633 139792109113344 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 10:57:44.382029 139792109113344 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 10:57:44.382157 139792109113344 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 10:57:44.382497 139792109113344 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 10:57:44.382604 139792109113344 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 10:57:44.382897 139792109113344 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 10:57:44.382998 139792109113344 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 10:57:44.383409 139792109113344 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 10:57:44.383511 139792109113344 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 10:57:44.387286 139792109113344 training_loop.py:334] ==== Training loop: initializing model ====
I0123 10:57:44.486813 139792109113344 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 10:57:44.487547 139792109113344 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 10:57:44.494484 139792109113344 training_loop.py:335] Process 0 of 1
I0123 10:57:44.494540 139792109113344 training_loop.py:336] Local device count = 1
I0123 10:57:44.494580 139792109113344 training_loop.py:337] Number of replicas = 1
I0123 10:57:44.494612 139792109113344 training_loop.py:339] Using random number seed 42
I0123 10:57:44.991755 139792109113344 training_loop.py:359] Initializing the model.
I0123 10:57:45.403649 139792109113344 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.403937 139792109113344 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 10:57:45.404078 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.404183 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.404282 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.404389 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.404485 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.404579 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.404672 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.404765 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.404858 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.404951 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.405043 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.405133 139792109113344 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:57:45.405185 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.405245 139792109113344 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 10:57:45.405391 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.405445 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.405494 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.407534 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.412875 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.423567 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.423895 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.428318 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.438938 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.439004 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.439057 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.439100 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.439183 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.440475 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.440561 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.441354 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.443898 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.450102 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.451460 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.451547 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.451595 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.451676 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.451847 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.452242 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.452295 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.454338 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.454454 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.457402 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.457491 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.458072 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.468331 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.477114 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.477223 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.477591 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.477694 139792109113344 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 10:57:45.477842 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.477892 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.477935 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.479834 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.482387 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.488002 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.488312 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.491025 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.494903 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.494968 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.495015 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.495057 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.495138 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.495767 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.495851 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.496291 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.497122 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.499675 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.500355 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.500445 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.500493 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.500572 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.500741 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.501124 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.501176 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.503234 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.503339 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.505929 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.506022 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.506531 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.508913 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.511518 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.511693 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.512052 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.512147 139792109113344 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 10:57:45.512296 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.512345 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.512387 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.514732 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.517201 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.522904 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.523243 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.526056 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.530072 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.530140 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.530189 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.530232 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.530314 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.530977 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.531062 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.531507 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.532357 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.534981 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.535708 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.535795 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.535841 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.535921 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.536087 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.536471 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.536524 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.538565 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.538676 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.541316 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.541411 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.541998 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.544396 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.546407 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.546515 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.546879 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.546973 139792109113344 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 10:57:45.547120 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.547171 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.547213 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.549195 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.551698 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.557513 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.557835 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.560559 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.564429 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.564492 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.564541 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.564583 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.564663 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.565287 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.565372 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.565815 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.566655 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.569267 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.569957 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.570044 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.570091 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.570170 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.570340 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.570718 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.570771 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.572765 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.572871 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.575496 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.575590 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.576098 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.578439 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.580404 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.580510 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.580876 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.580967 139792109113344 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 10:57:45.581113 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.581162 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.581206 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.583187 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.585663 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.591361 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.591705 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.594822 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.598630 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.598693 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.598741 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.598783 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.598870 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.599519 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.599605 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.600041 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.600889 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.603532 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.604203 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.604292 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.604340 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.604416 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.604595 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.604969 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.605025 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.607016 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.607122 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.609768 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.609863 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.610367 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.612696 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.614733 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.614843 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.615207 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.615301 139792109113344 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 10:57:45.615447 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.615497 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.615539 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.617458 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.619957 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.625705 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.626013 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.628756 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.632569 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.632631 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.632683 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.632725 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.632807 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.633493 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.633579 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.634030 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.634881 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.637440 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.638114 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.638201 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.638248 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.638325 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.638496 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.638873 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.638926 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.640913 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.641019 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.643700 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.643790 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.644294 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.646688 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.648678 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.648788 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.649148 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.649242 139792109113344 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 10:57:45.649388 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.649437 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.649480 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.651391 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.653924 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.659613 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.659917 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.662609 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.666448 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.666513 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.666562 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.666603 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.666685 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.667325 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.667409 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.667840 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.668678 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.671231 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.671895 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.671982 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.672028 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.672103 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.672272 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.672648 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.672700 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.675057 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.675163 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.677748 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.677834 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.678333 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.818245 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.820503 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.820664 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.821045 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.821148 139792109113344 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 10:57:45.821302 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.821354 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.821398 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.823497 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.826098 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.831935 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.832268 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.835082 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.839129 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.839194 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.839245 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.839289 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.839372 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.840068 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.840157 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.840609 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.841475 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.844188 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.844886 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.844974 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.845023 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.845104 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.845273 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.845667 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.845721 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.847773 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.847879 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.850628 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.850722 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.851298 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.853724 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.855741 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.855856 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.856225 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.856323 139792109113344 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 10:57:45.856475 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.856527 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.856571 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.858596 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.861116 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.867030 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.867352 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.870195 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.874155 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.874223 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.874273 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.874316 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.874398 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.875061 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.875146 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.875588 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.876456 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.879143 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.879827 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.879913 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.879962 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.880040 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.880208 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.880594 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.880648 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.882696 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.882803 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.885715 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.885804 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.886316 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.888717 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.890748 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.890856 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.891221 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.891322 139792109113344 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 10:57:45.891472 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.891523 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.891566 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.893568 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.896076 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.902243 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.902560 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.905394 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.909296 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.909361 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.909411 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.909454 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.909534 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.910177 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.910269 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.910716 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.911614 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.914235 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.914925 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.915013 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.915062 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.915142 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.915310 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.915699 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.915753 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.917792 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.917901 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.920588 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.920677 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.921187 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.923616 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.925715 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.925827 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.926189 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.926290 139792109113344 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 10:57:45.926441 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.926491 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.926535 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.928482 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.931083 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.936879 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.937201 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.940033 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.943920 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.943984 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.944033 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.944076 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.944212 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.944893 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.944978 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.945429 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.946304 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.948940 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.949624 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.949724 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.949772 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.949852 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.950023 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.950404 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.950458 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.952546 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.952654 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.955603 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.955697 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.956205 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.958640 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.960642 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.960749 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.961106 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.961200 139792109113344 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 10:57:45.961358 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:45.961410 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:45.961453 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:45.963427 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.966019 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:45.971872 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.972179 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:45.974965 139792109113344 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:57:45.979211 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:45.979275 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:45.979325 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:45.979368 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.979450 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.980103 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.980189 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.980626 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.981472 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.984085 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.984767 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.984855 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:45.984902 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:45.984981 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.985152 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:45.985522 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:45.985576 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.987658 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.987770 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.990410 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.990500 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:45.991007 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:45.993416 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:45.995445 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.995555 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:45.995911 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:45.996248 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.996337 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.996424 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.996501 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.996577 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.996649 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.996722 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.996794 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.996864 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.996935 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.997004 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.997074 139792109113344 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:57:45.997125 139792109113344 decoder_stack.py:344] dstack: Final layernorm.
I0123 10:57:46.000793 139792109113344 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:57:46.048978 139792109113344 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.049073 139792109113344 decoder_stack.py:333] dstack: autoregressive generator.
I0123 10:57:46.049145 139792109113344 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 10:57:46.049283 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.049340 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.049390 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.049483 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.052036 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.057665 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.057976 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.060689 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.077406 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.077473 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.077523 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.077566 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.077659 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.078894 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.078983 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.079782 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.081874 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.086731 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.088093 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.088189 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.088239 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.088319 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.088488 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.088631 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.088687 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.090719 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.090826 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.093367 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.093456 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.093604 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.095941 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.097981 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.098091 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.098449 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.098545 139792109113344 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 10:57:46.098689 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.098739 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.098783 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.098868 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.101244 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.106857 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.107164 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.109943 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.123067 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.123131 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.123178 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.123219 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.123299 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.123921 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.124006 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.124448 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.125200 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.127748 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.128447 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.128537 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.128591 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.128669 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.128839 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.128984 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.129038 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.131055 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.131162 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.133658 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.133748 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.133892 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.136184 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.138144 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.138250 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.138605 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.138699 139792109113344 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 10:57:46.138844 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.138894 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.138936 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.139021 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.141359 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.146826 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.147135 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.149885 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.162559 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.162623 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.162670 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.162710 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.162790 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.163407 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.163492 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.163926 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.164688 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.167246 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.167925 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.168011 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.168058 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.168143 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.168308 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.168450 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.168500 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.174980 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.175128 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.177770 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.177857 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.178007 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.180382 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.182402 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.182509 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.182867 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.182969 139792109113344 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 10:57:46.183119 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.183171 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.183215 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.183303 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.185677 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.191231 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.191547 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.194336 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.207166 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.207230 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.207280 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.207322 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.207402 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.208051 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.208136 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.208574 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.209334 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.211933 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.212614 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.212700 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.212747 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.212823 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.213006 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.213149 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.213203 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.215504 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.215610 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.218105 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.218193 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.218340 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.220622 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.222547 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.222653 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.223002 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.223094 139792109113344 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 10:57:46.223237 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.223286 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.223328 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.223410 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.225819 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.231314 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.231624 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.234305 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.246886 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.246950 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.246997 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.247038 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.247119 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.247740 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.247826 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.248264 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.249020 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.251650 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.252328 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.252414 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.252462 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.252539 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.252715 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.252857 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.252912 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.254896 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.255002 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.257504 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.257594 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.257744 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.260082 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.261992 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.262099 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.262452 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.262545 139792109113344 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 10:57:46.262688 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.262737 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.262779 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.262862 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.265221 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.270705 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.271013 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.273750 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.286365 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.286428 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.286476 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.286518 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.286598 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.287220 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.287307 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.287741 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.288515 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.291067 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.291737 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.291823 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.291869 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.291950 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.292116 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.292265 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.292321 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.294345 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.294451 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.296928 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.297015 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.297158 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.299458 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.301376 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.301482 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.301844 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.301940 139792109113344 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 10:57:46.302084 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.302134 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.302176 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.302259 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.304612 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.310161 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.310466 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.313088 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.325978 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.326041 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.326088 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.326130 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.326211 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.326822 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.326910 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.327348 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.328102 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.330641 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.331366 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.331452 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.331499 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.331578 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.331751 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.331896 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.331958 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.333935 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.334041 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.336550 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.336639 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.336783 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.339067 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.341046 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.341153 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.341506 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.341600 139792109113344 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 10:57:46.341752 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.341804 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.341846 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.341930 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.344259 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.349749 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.350070 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.352794 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.365429 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.365494 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.365542 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.365583 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.365677 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.366364 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.366451 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.366891 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.367661 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.370242 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.370924 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.371011 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.371058 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.371135 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.371309 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.371452 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.371513 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.373475 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.373581 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.376132 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.376222 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.376367 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.378655 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.380578 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.380684 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.381038 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.381131 139792109113344 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 10:57:46.381277 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.381327 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.381369 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.381452 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.383801 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.389327 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.389636 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.392320 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.405006 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.405069 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.405117 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.405159 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.405238 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.405860 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.405946 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.406384 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.407139 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.409700 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.410423 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.410510 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.410557 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.410634 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.410808 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.410950 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.411004 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.412968 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.413074 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.415569 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.415659 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.415802 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.418105 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.420100 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.420206 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.420559 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.420653 139792109113344 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 10:57:46.420798 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.420848 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.420890 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.420974 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.423344 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.428853 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.429160 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.432232 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.444881 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.444945 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.444993 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.445035 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.445115 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.445777 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.445864 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.446298 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.447055 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.449614 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.450308 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.450395 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.450441 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.450519 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.450687 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.450829 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.450878 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.452830 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.452943 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.455519 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.455608 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.455752 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.458045 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.459949 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.460056 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.460415 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.460508 139792109113344 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 10:57:46.460650 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.460700 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.460743 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.460825 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.463171 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.468730 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.469039 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.471769 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.484337 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.484401 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.484449 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.484491 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.484572 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.485182 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.485266 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.485709 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.486463 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.489023 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.489752 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.489842 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.489888 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.489966 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.490142 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.490286 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.490340 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.492310 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.492421 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.494929 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.495017 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.495160 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.497441 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.499423 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.499530 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.499886 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.499981 139792109113344 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 10:57:46.500125 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.500174 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.500216 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.500299 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.502661 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.508157 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.508466 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.511170 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.523838 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.523904 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.523952 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.523994 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.524075 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.524720 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.524806 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.525244 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.526014 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.528640 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.529313 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.529399 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.529446 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.529525 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.529705 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.529851 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.529905 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.531864 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.531970 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.534475 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.534562 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.534707 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.537368 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.539299 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.539406 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.539760 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.539862 139792109113344 decoder_stack.py:344] dstack: Final layernorm.
I0123 10:57:46.542823 139792109113344 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:57:46.598125 139792109113344 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.598221 139792109113344 decoder_stack.py:333] dstack: autoregressive generator.
I0123 10:57:46.598291 139792109113344 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 10:57:46.598426 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.598479 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.598529 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.598615 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.601075 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.606556 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.606868 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.609515 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.622010 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.622074 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.622123 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.622164 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.622244 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.622863 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.622949 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.623381 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.624123 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.626692 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.627359 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.627445 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.627492 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.627570 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.627737 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.627886 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.627940 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.629905 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.630011 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.632476 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.632563 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.632708 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.635028 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.636958 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.637064 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.637418 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.637512 139792109113344 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 10:57:46.637665 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.637718 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.637761 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.637845 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.640199 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.645731 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.646042 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.648782 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.661229 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.661293 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.661342 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.661384 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.661465 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.662116 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.662204 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.662646 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.663400 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.666023 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.666696 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.666784 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.666832 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.666911 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.667079 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.667221 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.667283 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.669260 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.669367 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.671872 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.671961 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.672108 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.674477 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.676405 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.676512 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.676869 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.676965 139792109113344 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 10:57:46.677110 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.677160 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.677203 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.677287 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.679651 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.685153 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.685462 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.688226 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.700634 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.700698 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.700746 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.700788 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.700869 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.701483 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.701569 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.702011 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.702761 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.705811 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.706484 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.706571 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.706618 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.706698 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.706866 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.707010 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.707066 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.709039 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.709145 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.711685 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.711774 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.711920 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.714299 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.716239 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.716345 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.716703 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.716797 139792109113344 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 10:57:46.716941 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.716991 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.717034 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.717119 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.719470 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.724953 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.725267 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.728012 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.740524 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.740587 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.740638 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.740697 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.740779 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.741418 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.741502 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.741948 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.742700 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.745314 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.746002 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.746088 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.746135 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.746214 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.746381 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.746522 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.746575 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.748543 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.748647 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.751156 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.751243 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.751385 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.753762 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.755693 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.755798 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.756151 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.756243 139792109113344 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 10:57:46.756386 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.756434 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.756475 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.756559 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.758935 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.764456 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.764765 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.767553 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.780160 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.780225 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.780272 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.780313 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.780392 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.781009 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.781092 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.781527 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.782291 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.784898 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.785571 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.785668 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.785720 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.785799 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.785966 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.786108 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.786164 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.788130 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.788242 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.790756 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.790843 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.790986 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.793348 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.795284 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.795391 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.795745 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.795839 139792109113344 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 10:57:46.795982 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.796030 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.796072 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.796156 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.798522 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.804037 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.804346 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.807133 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.819692 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.819756 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.819803 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.819845 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.819923 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.820537 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.820620 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.821055 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.821820 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.824825 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.825502 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.825588 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.825634 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.825720 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.825887 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.826025 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.826074 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.828040 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.828152 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.830633 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.830719 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.830864 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.833220 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.835160 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.835268 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.835620 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.835712 139792109113344 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 10:57:46.835855 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.835903 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.835944 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.836027 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.838372 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.843888 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.844197 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.846961 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.859346 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.859408 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.859454 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.859495 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.859573 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.860188 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.860274 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.860707 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.861456 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.864063 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.864746 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.864831 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.864876 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.864953 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.865119 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.865257 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.865305 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.867250 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.867354 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.869836 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.869924 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.870068 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.872406 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.874300 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.874406 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.874757 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.874849 139792109113344 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 10:57:46.874989 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.875038 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.875080 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.875163 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.877494 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.882951 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.883264 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.886017 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.898488 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.898552 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.898598 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.898639 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.898718 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.899334 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.899422 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.899856 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.900605 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.903236 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.903908 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.903992 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.904038 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.904124 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.904291 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.904431 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.904481 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.906434 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.906538 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.909002 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.909093 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.909237 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.911576 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.913489 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.913593 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.913952 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.914047 139792109113344 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 10:57:46.914188 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.914236 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.914276 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.914357 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.916664 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.922067 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.922373 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.925115 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.937544 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.937608 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.937663 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.937707 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.937786 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.938405 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.938489 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.938917 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.939668 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.942651 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.943315 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.943399 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.943444 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.943520 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.943684 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.943822 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.943869 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.945814 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.945917 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.948374 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.948471 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.948615 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.950951 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.952843 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.952949 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.953298 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.953389 139792109113344 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 10:57:46.953530 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.953579 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.953621 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.953711 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.956042 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:46.961509 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.961827 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:46.964573 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:46.977083 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:46.977149 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:46.977195 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:46.977235 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.977313 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.977944 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.978030 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.978463 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.979216 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.981806 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.982480 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.982565 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:46.982609 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:46.982683 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.982845 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:46.982982 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:46.983029 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.985476 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.985581 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.988022 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.988109 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:46.988260 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:46.990586 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:46.992467 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.992570 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:46.992919 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.993011 139792109113344 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 10:57:46.993151 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:46.993198 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:46.993238 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:46.993320 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:46.995634 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:47.001066 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.001370 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:47.004119 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:47.016495 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:47.016559 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:47.016605 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:47.016645 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.016723 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.017338 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.017420 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.017856 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.018609 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.021206 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.021885 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.021971 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:47.022017 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:47.022093 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.022261 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:47.022400 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:47.022448 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:47.024391 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.024495 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:47.026973 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.027061 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:47.027203 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:47.029552 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:47.031452 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.031559 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:47.031909 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.032001 139792109113344 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 10:57:47.032142 139792109113344 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:57:47.032190 139792109113344 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:57:47.032231 139792109113344 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:57:47.032312 139792109113344 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.034642 139792109113344 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:57:47.040049 139792109113344 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.040355 139792109113344 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:57:47.043095 139792109113344 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:57:47.055490 139792109113344 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:57:47.055552 139792109113344 attention.py:418] Single window, no scan.
I0123 10:57:47.055597 139792109113344 transformer_layer.py:389] tlayer: self-attention.
I0123 10:57:47.055636 139792109113344 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.055715 139792109113344 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.056326 139792109113344 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.056409 139792109113344 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.056835 139792109113344 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.057593 139792109113344 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.060552 139792109113344 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.061221 139792109113344 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.061305 139792109113344 transformer_layer.py:468] tlayer: End windows.
I0123 10:57:47.061352 139792109113344 transformer_layer.py:472] tlayer: final FFN.
I0123 10:57:47.061426 139792109113344 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.061590 139792109113344 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:57:47.061742 139792109113344 nn_components.py:325] mlp: activation = None
I0123 10:57:47.061793 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:47.063733 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.063835 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:47.066291 139792109113344 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.066377 139792109113344 transformer_base.py:443] tbase: final FFN
I0123 10:57:47.066518 139792109113344 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:57:47.068843 139792109113344 nn_components.py:329] mlp: final activation = None
I0123 10:57:47.070759 139792109113344 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.070864 139792109113344 nn_components.py:261] mlp: residual
I0123 10:57:47.071216 139792109113344 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:47.071313 139792109113344 decoder_stack.py:344] dstack: Final layernorm.
I0123 10:57:47.074208 139792109113344 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:57:51.478424 139792109113344 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 10:57:51.991184 139792109113344 training_loop.py:409] No working directory specified.
I0123 10:57:51.991316 139792109113344 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 10:57:51.992122 139792109113344 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 10:57:55.348416 139792109113344 training_loop.py:447] Only restoring trainable parameters.
I0123 10:57:55.349084 139792109113344 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 10:57:55.349191 139792109113344 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.349245 139792109113344 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.349292 139792109113344 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.349337 139792109113344 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.349381 139792109113344 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.349426 139792109113344 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.349467 139792109113344 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.349507 139792109113344 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.349547 139792109113344 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.349585 139792109113344 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.349624 139792109113344 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.349680 139792109113344 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.349721 139792109113344 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.349760 139792109113344 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.349798 139792109113344 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.349836 139792109113344 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.349875 139792109113344 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.349913 139792109113344 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.349951 139792109113344 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.350007 139792109113344 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350048 139792109113344 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.350087 139792109113344 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.350125 139792109113344 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.350163 139792109113344 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350200 139792109113344 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.350238 139792109113344 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350275 139792109113344 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350312 139792109113344 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.350350 139792109113344 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.350387 139792109113344 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350424 139792109113344 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.350460 139792109113344 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.350497 139792109113344 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.350534 139792109113344 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350572 139792109113344 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.350609 139792109113344 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350645 139792109113344 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350681 139792109113344 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.350717 139792109113344 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.350754 139792109113344 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350790 139792109113344 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.350827 139792109113344 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.350864 139792109113344 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.350901 139792109113344 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.350937 139792109113344 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.350983 139792109113344 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351022 139792109113344 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351059 139792109113344 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.351095 139792109113344 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.351134 139792109113344 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351171 139792109113344 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.351208 139792109113344 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.351246 139792109113344 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.351283 139792109113344 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351319 139792109113344 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.351355 139792109113344 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351391 139792109113344 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351427 139792109113344 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.351465 139792109113344 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.351502 139792109113344 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351539 139792109113344 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.351576 139792109113344 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.351613 139792109113344 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.351649 139792109113344 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351686 139792109113344 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.351722 139792109113344 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351757 139792109113344 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351793 139792109113344 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.351829 139792109113344 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.351866 139792109113344 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.351902 139792109113344 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.351939 139792109113344 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.351982 139792109113344 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.352020 139792109113344 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352057 139792109113344 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.352094 139792109113344 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352131 139792109113344 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352168 139792109113344 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.352205 139792109113344 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.352241 139792109113344 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352278 139792109113344 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.352314 139792109113344 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.352350 139792109113344 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.352386 139792109113344 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352422 139792109113344 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.352458 139792109113344 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352496 139792109113344 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352531 139792109113344 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.352567 139792109113344 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.352604 139792109113344 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352640 139792109113344 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.352676 139792109113344 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.352712 139792109113344 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.352749 139792109113344 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352785 139792109113344 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.352821 139792109113344 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352857 139792109113344 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.352893 139792109113344 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.352930 139792109113344 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.352972 139792109113344 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.353010 139792109113344 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.353048 139792109113344 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.353085 139792109113344 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.353121 139792109113344 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.353158 139792109113344 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.353194 139792109113344 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.353230 139792109113344 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.353266 139792109113344 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.353303 139792109113344 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.353340 139792109113344 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.353377 139792109113344 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.353413 139792109113344 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:57:55.353450 139792109113344 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:57:55.353487 139792109113344 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.353524 139792109113344 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.353559 139792109113344 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.353596 139792109113344 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.353632 139792109113344 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:57:55.353679 139792109113344 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:57:55.353716 139792109113344 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:57:55.353752 139792109113344 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:57:55.353780 139792109113344 training_loop.py:725] Total parameters: 152072288
I0123 10:57:55.354015 139792109113344 training_loop.py:739] Total state size: 0
I0123 10:57:55.379096 139792109113344 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 10:57:55.379347 139792109113344 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 10:57:55.379781 139792109113344 training_loop.py:652] Compiling mode beam_search with jit.
I0123 10:57:55.380129 139792109113344 training_loop.py:89] registering functions: dict_keys([])
I0123 10:57:55.396449 139792109113344 graph.py:499] a b c = triangle a b c; d = midpoint d b a; e = foot e a c d; f = circle f c b e; g = on_circle g f b, on_line g a b; h = midpoint h c g ? cong b h a h
I0123 10:57:55.618803 139792109113344 ddar.py:60] Depth 1/1000 time = 0.19823050498962402
I0123 10:57:56.177858 139792109113344 ddar.py:60] Depth 2/1000 time = 0.5589480400085449
I0123 10:57:56.599977 139792109113344 ddar.py:60] Depth 3/1000 time = 0.4219624996185303
I0123 10:57:57.164750 139792109113344 ddar.py:60] Depth 4/1000 time = 0.5646457672119141
I0123 10:57:57.730901 139792109113344 ddar.py:60] Depth 5/1000 time = 0.5658552646636963
I0123 10:57:58.169246 139792109113344 ddar.py:60] Depth 6/1000 time = 0.4340062141418457
I0123 10:57:58.836074 139792109113344 ddar.py:60] Depth 7/1000 time = 0.6666810512542725
I0123 10:57:58.839308 139792109113344 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H : Points
D,A,B are collinear [00]
DB = DA [01]
E,C,D are collinear [02]
AE ⟂ CD [03]
FC = FB [04]
FB = FE [05]
FG = FB [06]
A,G,B are collinear [07]
H,C,G are collinear [08]
HC = HG [09]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. D,A,B are collinear [00] & DB = DA [01] ⇒  D is midpoint of AB [10]
002. H,C,G are collinear [08] & HC = HG [09] ⇒  H is midpoint of GC [11]
003. D is midpoint of AB [10] & H is midpoint of GC [11] ⇒  DA:AB = HC:CG [12]
004. DA:AB = HC:CG [12] & HC = HG [09] ⇒  AD:AB = HG:CG [13]
005. FC = FB [04] & FG = FB [06] & FB = FE [05] ⇒  E,C,G,B are concyclic [14]
006. E,C,G,B are concyclic [14] ⇒  ∠ECG = ∠EBG [15]
007. D,A,B are collinear [00] & ∠ECG = ∠EBG [15] & E,C,D are collinear [02] & A,G,B are collinear [07] ⇒  ∠EBD = ∠DCG [16]
008. E,C,D are collinear [02] & B,A,D are collinear [00] & A,G,B are collinear [07] ⇒  ∠EDB = ∠CDG [17]
009. ∠EBD = ∠DCG [16] & ∠EDB = ∠CDG [17] (Similar Triangles)⇒  BE:BD = CG:CD [18]
010. BE:BD = CG:CD [18] & DB = DA [01] ⇒  EB:AD = CG:CD [19]
011. DB = DA [01] & AD:AB = HG:CG [13] & EB:AD = CG:CD [19] (Ratio chase)⇒  HC:CD = EB:AB [20]
012. H,C,G are collinear [08] & ∠ECG = ∠EBG [15] & E,C,D are collinear [02] & A,G,B are collinear [07] ⇒  ∠HCD = ∠ABE [21]
013. CH:CD = BE:BA [20] & ∠HCD = ∠ABE [21] (Similar Triangles)⇒  ∠(AB-DH) = ∠(AE-CD) [22]
014. ∠(AB-DH) = ∠(AE-CD) [22] & AE ⟂ CD [03] ⇒  HD ⟂ AB [23]
015. D is midpoint of AB [10] & HD ⟂ AB [23] ⇒  HA = HB
==========================

