I0123 11:59:45.507966 139785946451968 inference_utils.py:69] Parsing gin configuration.
I0123 11:59:45.508062 139785946451968 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:59:45.508257 139785946451968 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:59:45.508291 139785946451968 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:59:45.508320 139785946451968 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:59:45.508347 139785946451968 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:59:45.508374 139785946451968 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:59:45.508400 139785946451968 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:59:45.508425 139785946451968 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:59:45.508451 139785946451968 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:59:45.508477 139785946451968 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:59:45.508502 139785946451968 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:59:45.508546 139785946451968 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:59:45.508676 139785946451968 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:59:45.508877 139785946451968 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:59:45.508980 139785946451968 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:59:45.515323 139785946451968 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:59:45.515444 139785946451968 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:59:45.515766 139785946451968 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:59:45.515870 139785946451968 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:59:45.516148 139785946451968 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:59:45.516248 139785946451968 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:59:45.516654 139785946451968 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:59:45.516754 139785946451968 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:59:45.520381 139785946451968 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:59:45.624761 139785946451968 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:59:45.625476 139785946451968 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:59:45.632180 139785946451968 training_loop.py:335] Process 0 of 1
I0123 11:59:45.632233 139785946451968 training_loop.py:336] Local device count = 1
I0123 11:59:45.632273 139785946451968 training_loop.py:337] Number of replicas = 1
I0123 11:59:45.632304 139785946451968 training_loop.py:339] Using random number seed 42
I0123 11:59:46.090015 139785946451968 training_loop.py:359] Initializing the model.
I0123 11:59:46.515636 139785946451968 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.515920 139785946451968 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:59:46.516024 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516101 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516180 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516260 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516331 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516398 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516464 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516530 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516596 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516661 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516726 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516791 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:59:46.516828 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.516871 139785946451968 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:59:46.516983 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.517023 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.517052 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.519072 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.524318 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.534841 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.535112 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.539411 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.550020 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.550079 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.550117 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.550150 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.550212 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.551408 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.551483 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.552179 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.554617 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.560248 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.561953 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.562032 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:46.562067 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:46.562127 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.562254 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:46.562585 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:46.562631 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.564518 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.564615 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.567443 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.567521 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:46.568007 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:46.577891 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.586486 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.586583 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.586873 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.586952 139785946451968 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:59:46.587060 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.587099 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.587129 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.588944 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.591381 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.596842 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.597094 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.599687 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.603420 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.603474 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.603509 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.603538 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.603599 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.604162 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.604236 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.604584 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.605328 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.607757 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.608360 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.608435 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:46.608468 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:46.608524 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.608649 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:46.608968 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:46.609009 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.610980 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.611079 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.613529 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.613608 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:46.614034 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:46.616328 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.618211 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.618304 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.618586 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.618664 139785946451968 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:59:46.618771 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.618809 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.618839 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.620733 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.623771 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.629785 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.630073 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.632724 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.636522 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.636578 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.636613 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.636643 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.636703 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.637264 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.637340 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.637693 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.638441 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.640883 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.641539 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.641614 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:46.641653 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:46.641712 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.641839 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:46.642159 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:46.642201 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.644086 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.644175 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.646650 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.646732 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:46.647210 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:46.649466 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.651382 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.651473 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.651759 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.651840 139785946451968 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:59:46.651949 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.651986 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.652016 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.653928 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.656279 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.661861 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.662124 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.664724 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.668477 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.668531 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.668565 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.668596 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.668656 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.669212 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.669288 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.669637 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.670428 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.672927 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.673535 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.673610 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:46.673651 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:46.673714 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.673843 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:46.674156 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:46.674199 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.676077 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.676167 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.678680 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.678764 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:46.679183 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:46.681398 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.683288 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.683383 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.683667 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.683745 139785946451968 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:59:46.683851 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.683889 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.683919 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.685797 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.688142 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.693712 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.693976 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.696620 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.700292 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.700346 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.700381 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.700410 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.700470 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.701059 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.701133 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.701483 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.702264 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.705102 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.705713 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.705790 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:46.705824 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:46.705884 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.706014 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:46.706331 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:46.706374 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.708244 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.708335 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.710845 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.710923 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:46.711349 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:46.713588 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.715543 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.715636 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.715920 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.715999 139785946451968 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:59:46.716107 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.716144 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.716174 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.718014 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.720355 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.725930 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.726181 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.728810 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.732540 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.732595 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.732629 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.732659 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.732720 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.733325 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.733401 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.733759 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.734529 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.736969 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.737577 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.737656 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:46.737691 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:46.737749 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.737873 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:46.738190 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:46.738234 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.740112 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.740203 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.742744 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.742822 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:46.743247 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:46.745545 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.747439 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.747539 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.747828 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.747908 139785946451968 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:59:46.748016 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.748055 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.748085 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.749925 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.752351 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.757925 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.758182 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.760793 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.764544 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.764598 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.764633 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.764662 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.764727 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.765282 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.765359 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.765716 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.766476 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.768899 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.769509 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.769583 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:46.769617 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:46.769678 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.769804 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:46.770144 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:46.770187 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.772126 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.772218 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.774753 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.774831 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:46.775260 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:46.777874 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.779764 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.779862 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.780151 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.780234 139785946451968 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:59:46.780342 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.780380 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.780409 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.919135 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.922302 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.928234 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.928538 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.931225 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.935104 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.935161 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.935199 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.935230 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.935296 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.935906 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.935984 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.936342 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.937106 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.939681 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.940321 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.940400 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:46.940435 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:46.940494 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.940621 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:46.940957 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:46.941001 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.942888 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.942982 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.945497 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.945575 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:46.946006 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:46.948281 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.950192 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.950295 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.950582 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.950665 139785946451968 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:59:46.950774 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.950813 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.950843 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.952772 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.955109 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.960686 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.960947 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.963617 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.967344 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.967400 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.967436 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.967466 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.967526 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.968089 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.968164 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.968516 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.969275 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.971817 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.972437 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.972514 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:46.972549 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:46.972607 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.972735 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:46.973060 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:46.973104 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.974981 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.975072 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.977590 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.977673 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:46.978097 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:46.980350 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:46.982306 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.982400 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:46.982682 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.982767 139785946451968 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:59:46.982877 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:46.982916 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:46.982948 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:46.984785 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.987193 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:46.992692 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.992958 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:46.995954 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:46.999647 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:46.999701 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:46.999737 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:46.999768 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:46.999828 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.000424 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.000502 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.000858 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.001611 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.004076 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.004698 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.004774 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.004808 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.004866 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.004992 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.005314 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.005357 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.007235 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.007326 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.009830 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.009911 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.010325 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.012596 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.014496 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.014590 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.014877 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.014962 139785946451968 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:59:47.015071 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.015110 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.015139 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.016961 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.019535 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.025204 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.025467 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.028079 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:47.031796 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.031850 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.031886 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.031916 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.031976 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.032531 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.032605 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.032954 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.033759 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.036207 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.036830 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.036905 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.036940 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.036996 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.037124 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.037444 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.037486 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.039420 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.039517 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.042214 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.042294 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.042712 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.045001 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.046883 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.046976 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.047260 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.047340 139785946451968 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:59:47.047454 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.047493 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.047522 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.049415 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.051764 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.057276 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.057550 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.060166 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:59:47.063897 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.063952 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.063988 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.064018 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.064079 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.064635 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.064710 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.065060 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.065816 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.068243 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.069208 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.069286 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.069321 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.069378 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.069506 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.069835 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.069880 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.071744 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.071835 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.074298 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.074380 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.074853 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.077044 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.078900 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.078994 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.079276 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.079549 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.079621 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.079688 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.079745 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.079798 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.079850 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.079902 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.079952 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.080003 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.080055 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.080106 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.080158 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:59:47.080195 139785946451968 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:59:47.083696 139785946451968 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:59:47.130890 139785946451968 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.130975 139785946451968 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:59:47.131028 139785946451968 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:59:47.131131 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.131168 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.131197 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.131259 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.133661 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.139045 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.139303 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.141906 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.158300 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.158356 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.158390 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.158421 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.158482 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.159590 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.159667 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.160367 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.162354 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.167004 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.168301 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.168384 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.168419 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.168476 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.168607 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.168714 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.168752 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.170641 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.170734 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.173116 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.173197 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.173304 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.175516 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.177431 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.177526 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.177819 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.177902 139785946451968 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:59:47.178011 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.178050 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.178080 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.178143 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.180346 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.185737 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.185993 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.188638 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.201663 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.201719 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.201754 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.201785 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.201846 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.202401 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.202476 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.202831 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.203523 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.205986 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.206597 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.206672 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.206711 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.206768 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.206895 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.207007 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.207046 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.208956 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.209047 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.211407 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.211485 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.211591 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.213780 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.215679 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.215772 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.216056 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.216136 139785946451968 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:59:47.216245 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.216284 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.216314 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.216376 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.218599 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.223979 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.224232 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.226902 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.239513 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.239568 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.239603 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.239632 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.239692 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.240240 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.240315 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.240674 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.241350 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.243792 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.244409 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.244485 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.244518 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.244579 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.244705 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.244812 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.244849 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.246761 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.246854 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.249236 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.249314 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.249421 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.251625 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.253526 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.253620 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.253913 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.253993 139785946451968 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:59:47.254102 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.254142 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.254173 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.254235 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.256442 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.261778 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.262033 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.264633 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.277202 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.277257 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.277293 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.277323 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.277384 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.282341 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.282455 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.282869 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.283602 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.286144 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.286804 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.286889 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.286923 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.286998 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.287134 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.287248 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.287287 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.289320 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.289415 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.291864 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.291944 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.292053 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.294305 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.296183 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.296278 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.296559 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.296642 139785946451968 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:59:47.296752 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.296792 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.296822 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.296885 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.299469 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.304894 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.305158 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.307793 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.320676 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.320730 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.320765 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.320796 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.320859 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.321413 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.321489 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.321848 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.322533 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.325040 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.325672 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.325748 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.325782 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.325839 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.325974 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.326082 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.326121 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.327997 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.328089 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.330491 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.330569 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.330674 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.332912 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.334778 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.334872 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.335150 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.335229 139785946451968 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:59:47.335337 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.335376 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.335405 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.335465 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.337679 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.343067 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.343323 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.346014 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.358756 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.358811 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.358846 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.358877 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.358937 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.359493 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.359568 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.359922 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.360613 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.363082 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.363695 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.363772 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.363806 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.363862 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.363991 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.364106 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.364144 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.366078 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.366171 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.368546 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.368622 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.368728 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.370985 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.372849 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.372942 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.373224 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.373304 139785946451968 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:59:47.373411 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.373450 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.373479 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.373541 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.375752 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.381249 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.381502 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.384077 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.396811 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.396866 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.396900 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.396929 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.396990 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.397540 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.397614 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.397970 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.398648 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.401089 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.402066 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.402144 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.402177 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.402234 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.402366 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.402474 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.402516 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.404391 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.404482 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.406855 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.406932 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.407036 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.409221 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.411137 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.411233 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.411513 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.411592 139785946451968 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:59:47.411698 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.411737 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.411766 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.411826 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.414023 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.419392 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.419659 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.422299 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.434893 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.434949 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.434984 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.435014 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.435075 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.435676 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.435750 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.436101 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.436776 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.439208 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.439830 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.439905 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.439939 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.439996 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.440124 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.440233 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.440277 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.442133 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.442226 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.444635 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.444711 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.444817 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.446998 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.448824 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.448916 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.449192 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.449271 139785946451968 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:59:47.449378 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.449417 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.449448 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.449509 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.451711 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.457155 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.457408 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.460031 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.472673 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.472729 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.472765 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.472796 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.472858 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.473421 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.473497 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.473862 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.474553 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.477017 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.477687 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.477765 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.477798 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.477856 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.477983 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.478090 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.478128 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.479993 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.480085 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.482432 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.482511 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.482616 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.484805 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.486732 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.486827 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.487109 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.487190 139785946451968 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:59:47.487297 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.487335 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.487365 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.487426 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.489604 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.494954 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.495208 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.497832 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.510671 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.510727 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.510762 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.510792 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.510852 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.511455 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.511530 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.511882 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.512561 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.515016 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.515635 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.515711 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.515745 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.515802 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.515932 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.516039 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.516076 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.517933 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.518032 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.520444 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.520522 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.520627 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.522824 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.524651 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.524743 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.525022 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.525102 139785946451968 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:59:47.525210 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.525249 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.525279 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.525338 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.527556 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.532982 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.533237 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.535812 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.548415 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.548471 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.548506 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.548536 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.548597 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.549151 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.549226 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.549570 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.550250 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.552689 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.553346 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.553423 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.553457 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.553513 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.553644 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.553753 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.553791 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.555652 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.555749 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.558150 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.558229 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.558335 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.560497 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.562414 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.562509 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.562792 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.562872 139785946451968 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:59:47.562978 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.563016 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.563047 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.563108 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.565310 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.570691 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.570947 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.573589 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.586194 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.586251 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.586286 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.586316 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.586376 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.586931 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.587005 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.587356 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.588078 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.590538 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.591153 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.591228 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.591263 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.591320 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.591449 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.591556 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.591594 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.593445 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.593536 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.595918 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.595996 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.596101 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.598334 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.600220 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.600320 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.600610 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.600697 139785946451968 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:59:47.603533 139785946451968 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:59:47.659572 139785946451968 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.659658 139785946451968 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:59:47.659713 139785946451968 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:59:47.659817 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.659856 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.659886 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.659947 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.662592 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.667890 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.668145 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.670689 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.683047 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.683102 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.683138 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.683167 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.683229 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.683788 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.683863 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.684217 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.684886 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.687367 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.687980 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.688056 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.688090 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.688147 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.688274 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.688388 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.688426 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.690247 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.690340 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.692698 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.692776 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.692882 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.695095 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.696911 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.697008 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.697294 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.697374 139785946451968 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:59:47.697479 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.697517 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.697546 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.697606 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.699801 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.705090 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.705349 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.707964 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.720210 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.720266 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.720301 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.720332 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.720393 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.720941 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.721014 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.721359 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.722025 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.724483 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.725089 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.725165 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.725199 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.725258 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.725383 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.725492 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.725538 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.727367 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.727461 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.729825 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.729905 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.730014 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.732221 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.734042 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.734137 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.734420 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.734501 139785946451968 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:59:47.734608 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.734647 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.734678 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.734739 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.736925 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.742194 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.742450 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.745047 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.757246 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.757301 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.757337 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.757368 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.757429 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.757982 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.758056 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.758406 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.759082 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.761548 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.762159 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.762236 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.762270 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.762328 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.762454 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.762560 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.762599 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.764405 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.764496 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.766846 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.766925 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.767030 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.769687 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.771531 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.771625 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.771906 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.771987 139785946451968 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:59:47.772092 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.772130 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.772160 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.772222 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.774418 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.779681 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.779939 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.782541 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.794858 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.794914 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.794951 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.794996 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.795059 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.795613 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.795686 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.796038 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.796712 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.799233 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.799834 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.799907 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.799939 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.799996 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.800118 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.800222 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.800265 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.802102 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.802193 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.804551 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.804627 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.804732 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.806992 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.808831 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.808923 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.809201 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.809278 139785946451968 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:59:47.809383 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.809420 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.809448 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.809507 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.811703 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.816997 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.817249 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.819871 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.832323 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.832376 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.832410 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.832439 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.832499 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.833049 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.833121 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.833464 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.834139 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.836642 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.837251 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.837325 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.837357 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.837413 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.837536 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.837649 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.837689 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.839539 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.839634 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.842008 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.842085 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.842188 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.844441 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.846280 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.846373 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.846652 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.846730 139785946451968 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:59:47.846834 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.846871 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.846899 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.846959 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.849145 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.854496 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.854752 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.857408 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.869968 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.870022 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.870056 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.870085 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.870144 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.870693 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.870766 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.871122 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.871794 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.874322 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.874935 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.875010 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.875044 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.875099 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.875224 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.875329 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.875365 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.877199 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.877297 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.879669 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.879746 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.879850 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.882506 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.884420 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.884514 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.884793 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.884873 139785946451968 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:59:47.884977 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.885013 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.885041 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.885099 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.887302 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.892634 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.892888 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.895507 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.907944 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.907997 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.908030 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.908059 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.908118 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.908674 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.908746 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.909094 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.909780 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.912279 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.912905 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.912980 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.913013 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.913069 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.913192 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.913297 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.913334 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.915178 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.915268 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.917622 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.917710 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.917815 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.920065 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.921901 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.921994 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.922276 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.922356 139785946451968 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:59:47.922461 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.922498 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.922527 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.922586 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.924787 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.930110 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.930367 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.932991 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.945495 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.945548 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.945582 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.945611 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.945677 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.946239 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.946311 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.946662 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.947349 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.949842 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.950462 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.950536 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.950568 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.950623 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.950746 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.950851 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.950888 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.952718 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.952808 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.955172 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.955256 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.955362 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.957609 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.959455 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.959548 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.959828 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.959907 139785946451968 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:59:47.960011 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.960047 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.960074 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.960133 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.962342 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:47.967674 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.967927 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:47.970547 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:47.982999 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:47.983052 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:47.983086 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:47.983115 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.983175 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.983733 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.983806 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.984154 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.984824 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.987347 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.987954 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.988028 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:47.988060 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:47.988115 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.988239 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:47.988349 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:47.988387 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.990231 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.990322 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.992681 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.992763 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:47.992873 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:47.995556 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:47.997397 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.997490 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:47.997781 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:47.997861 139785946451968 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:59:47.997966 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:47.998003 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:47.998031 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:47.998090 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.000271 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:48.005621 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.005877 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:48.008483 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:48.020975 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:48.021028 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:48.021062 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:48.021090 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.021149 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.021708 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.021782 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.022135 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.022815 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.025312 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.025934 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.026009 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:48.026041 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:48.026094 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.026218 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:48.026327 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:48.026364 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:48.028648 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.028740 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:48.031083 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.031159 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:48.031272 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:48.033485 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:48.035295 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.035387 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:48.035664 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.035744 139785946451968 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:59:48.035849 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:48.035885 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:48.035913 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:48.035971 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.038160 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:48.043433 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.043684 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:48.046330 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:48.058759 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:48.058813 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:48.058851 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:48.058881 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.058942 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.059491 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.059565 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.059918 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.060595 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.063083 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.063699 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.063773 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:48.063805 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:48.063858 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.063986 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:48.064091 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:48.064128 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:48.065968 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.066059 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:48.068400 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.068476 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:48.068583 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:48.070823 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:48.072643 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.072736 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:48.073013 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.073091 139785946451968 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:59:48.073195 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:59:48.073231 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:59:48.073259 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:59:48.073318 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.075514 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:59:48.080841 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.081099 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:59:48.083763 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:59:48.096256 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:59:48.096309 139785946451968 attention.py:418] Single window, no scan.
I0123 11:59:48.096343 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 11:59:48.096371 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.096431 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.096981 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.097054 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.097404 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.098093 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.100595 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.101227 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.101303 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 11:59:48.101336 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 11:59:48.101391 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.101519 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:59:48.101625 139785946451968 nn_components.py:325] mlp: activation = None
I0123 11:59:48.101667 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:48.103514 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.103604 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:48.105945 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.106021 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 11:59:48.106124 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:59:48.108761 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 11:59:48.110612 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.110705 139785946451968 nn_components.py:261] mlp: residual
I0123 11:59:48.110984 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:48.111068 139785946451968 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:59:48.113869 139785946451968 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:59:52.510135 139785946451968 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:59:53.010113 139785946451968 training_loop.py:409] No working directory specified.
I0123 11:59:53.010229 139785946451968 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:59:53.010998 139785946451968 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:59:55.932624 139785946451968 training_loop.py:447] Only restoring trainable parameters.
I0123 11:59:55.933315 139785946451968 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:59:55.933373 139785946451968 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.933418 139785946451968 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.933459 139785946451968 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.933498 139785946451968 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.933538 139785946451968 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.933577 139785946451968 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.933615 139785946451968 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.933670 139785946451968 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.933711 139785946451968 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.933748 139785946451968 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.933786 139785946451968 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.933821 139785946451968 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.933858 139785946451968 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.933894 139785946451968 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.933929 139785946451968 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.933966 139785946451968 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934002 139785946451968 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934038 139785946451968 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.934074 139785946451968 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.934124 139785946451968 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934162 139785946451968 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.934199 139785946451968 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.934235 139785946451968 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.934271 139785946451968 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934307 139785946451968 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.934342 139785946451968 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934379 139785946451968 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934416 139785946451968 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.934453 139785946451968 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.934490 139785946451968 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934526 139785946451968 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.934562 139785946451968 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.934597 139785946451968 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.934633 139785946451968 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934669 139785946451968 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.934705 139785946451968 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934741 139785946451968 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934776 139785946451968 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.934811 139785946451968 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.934846 139785946451968 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.934881 139785946451968 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.934916 139785946451968 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.934951 139785946451968 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.934986 139785946451968 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935021 139785946451968 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.935061 139785946451968 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935098 139785946451968 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935136 139785946451968 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.935171 139785946451968 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.935206 139785946451968 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935241 139785946451968 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.935277 139785946451968 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.935312 139785946451968 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.935347 139785946451968 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935382 139785946451968 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.935417 139785946451968 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935451 139785946451968 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935485 139785946451968 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.935520 139785946451968 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.935554 139785946451968 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935588 139785946451968 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.935623 139785946451968 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.935658 139785946451968 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.935692 139785946451968 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935726 139785946451968 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.935761 139785946451968 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935795 139785946451968 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935829 139785946451968 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.935864 139785946451968 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.935898 139785946451968 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.935933 139785946451968 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.935967 139785946451968 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.936007 139785946451968 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.936044 139785946451968 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936079 139785946451968 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.936114 139785946451968 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936149 139785946451968 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936183 139785946451968 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.936218 139785946451968 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.936252 139785946451968 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936287 139785946451968 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.936321 139785946451968 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.936357 139785946451968 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.936396 139785946451968 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936432 139785946451968 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.936467 139785946451968 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936502 139785946451968 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936537 139785946451968 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.936571 139785946451968 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.936606 139785946451968 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936640 139785946451968 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.936674 139785946451968 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.936709 139785946451968 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.936743 139785946451968 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936779 139785946451968 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.936814 139785946451968 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936848 139785946451968 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936883 139785946451968 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.936918 139785946451968 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.936957 139785946451968 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.936992 139785946451968 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.937027 139785946451968 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.937062 139785946451968 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.937097 139785946451968 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.937131 139785946451968 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.937165 139785946451968 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.937200 139785946451968 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.937236 139785946451968 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.937271 139785946451968 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.937307 139785946451968 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.937342 139785946451968 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.937376 139785946451968 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:59:55.937411 139785946451968 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:59:55.937446 139785946451968 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.937480 139785946451968 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.937515 139785946451968 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.937550 139785946451968 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.937584 139785946451968 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:59:55.937618 139785946451968 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:59:55.937660 139785946451968 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:59:55.937698 139785946451968 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:59:55.937726 139785946451968 training_loop.py:725] Total parameters: 152072288
I0123 11:59:55.937934 139785946451968 training_loop.py:739] Total state size: 0
I0123 11:59:55.958374 139785946451968 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:59:55.958632 139785946451968 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:59:55.958987 139785946451968 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:59:55.959306 139785946451968 training_loop.py:89] registering functions: dict_keys([])
I0123 11:59:55.975638 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g ? cong b k k m
I0123 11:59:58.709368 139785946451968 ddar.py:60] Depth 1/1000 time = 2.6809732913970947
I0123 12:00:03.667010 139785946451968 ddar.py:60] Depth 2/1000 time = 4.957481384277344
I0123 12:00:09.460936 139785946451968 ddar.py:60] Depth 3/1000 time = 5.793737411499023
I0123 12:00:14.988223 139785946451968 ddar.py:60] Depth 4/1000 time = 5.527064561843872
I0123 12:00:20.654561 139785946451968 ddar.py:60] Depth 5/1000 time = 5.665919542312622
I0123 12:00:27.581928 139785946451968 ddar.py:60] Depth 6/1000 time = 6.859691619873047
I0123 12:00:34.085590 139785946451968 ddar.py:60] Depth 7/1000 time = 6.503441333770752
I0123 12:00:34.092216 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:00:34.092334 139785946451968 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 12:00:34.092373 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00
I0123 12:00:34.092406 139785946451968 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00
I0123 12:00:34.238080 139785946451968 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.238273 139785946451968 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:00:34.238372 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.238445 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.238512 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.238577 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.238641 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.238705 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.238770 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.238833 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.238895 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.238957 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.239018 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.239080 139785946451968 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:00:34.239120 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.239163 139785946451968 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:00:34.239270 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.239307 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.239335 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.241596 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.244103 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.249724 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.249993 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.252624 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.256499 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.256554 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.256590 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.256621 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.256681 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.257306 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.257379 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.257742 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.258507 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.261021 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.261638 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.261726 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.261759 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.261815 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.261939 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.262253 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.262293 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.264147 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.264237 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.266615 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.266690 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.267109 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.269464 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.271351 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.271444 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.271725 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.271802 139785946451968 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:00:34.271907 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.271944 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.271972 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.273743 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.276013 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.281507 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.281766 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.284287 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.288196 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.288249 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.288283 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.288311 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.288369 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.288970 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.289045 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.289394 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.290146 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.292547 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.293152 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.293226 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.293258 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.293313 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.293436 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.293756 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.293797 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.295735 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.295826 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.298213 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.298290 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.298697 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.300885 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.302744 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.302836 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.303114 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.303192 139785946451968 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:00:34.303298 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.303335 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.303363 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.305172 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.307434 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.312868 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.313116 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.315613 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.319223 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.319276 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.319309 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.319338 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.319398 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.319940 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.320012 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.320356 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.321085 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.323465 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.324067 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.324143 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.324175 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.324230 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.324354 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.324708 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.324749 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.326594 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.326688 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.329045 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.329120 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.329525 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.331725 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.333647 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.333743 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.334025 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.334103 139785946451968 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:00:34.334205 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.334242 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.334270 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.336002 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.338231 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.343681 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.343928 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.346406 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.349950 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.350002 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.350035 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.350064 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.350123 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.351054 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.351128 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.351471 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.352204 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.354572 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.355173 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.355247 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.355280 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.355334 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.355455 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.355760 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.355800 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.357706 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.357797 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.360162 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.360237 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.360645 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.362827 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.364666 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.364756 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.365033 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.365109 139785946451968 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:00:34.365211 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.365247 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.365274 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.367084 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.369309 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.374730 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.374986 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.377543 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.381076 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.381128 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.381160 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.381187 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.381246 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.381792 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.381865 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.382205 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.382936 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.385336 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.385996 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.386071 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.386104 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.386157 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.386280 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.386584 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.386624 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.388616 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.388704 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.391194 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.391271 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.391683 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.394386 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.396247 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.396339 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.396620 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.396697 139785946451968 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:00:34.396801 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.396837 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.396865 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.398594 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.400985 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.406544 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.406793 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.409302 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.412865 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.412918 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.412951 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.412980 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.413086 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.413631 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.413713 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.414058 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.414796 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.417201 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.417812 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.417887 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.417919 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.417973 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.418125 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.418434 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.418474 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.420393 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.420482 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.422872 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.422948 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.423357 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.425539 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.427396 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.427489 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.427770 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.427847 139785946451968 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:00:34.427951 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.427988 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.428016 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.429828 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.432099 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.437531 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.437790 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.440357 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.443971 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.444024 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.444057 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.444086 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.444145 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.444689 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.444762 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.445107 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.445860 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.448261 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.448913 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.448988 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.449020 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.449074 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.449200 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.449504 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.449544 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.451412 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.451502 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.453885 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.453961 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.454371 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.456949 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.458830 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.458921 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.459202 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.459279 139785946451968 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:00:34.459383 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.459419 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.459446 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.461178 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.463432 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.468924 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.469174 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.471692 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.475272 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.475331 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.475364 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.475393 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.475502 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.476057 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.476131 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.476480 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.477226 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.479619 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.480224 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.480298 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.480330 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.480384 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.480507 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.480809 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.480850 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.482761 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.482851 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.485220 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.485295 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.485715 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.487917 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.489791 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.489884 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.490163 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.490240 139785946451968 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:00:34.490344 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.490381 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.490410 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.492219 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.494446 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.500164 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.500413 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.502980 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.506530 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.506582 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.506621 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.506651 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.506712 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.507266 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.507341 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.507688 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.508421 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.510809 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.511418 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.511493 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.511524 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.511579 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.511705 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.512066 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.512107 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.513973 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.514063 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.516441 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.516517 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.516935 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.519123 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.521058 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.521149 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.521429 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.521509 139785946451968 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:00:34.521614 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.521656 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.521687 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.523429 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.525687 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.531208 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.531456 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.533967 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.537562 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.537615 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.537654 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.537693 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.537755 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.538367 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.538443 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.538790 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.539540 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.541950 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.542562 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.542636 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.542669 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.542723 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.542851 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.543162 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.543206 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.545081 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.545171 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.547736 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.547814 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.548227 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.550473 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.552402 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.552494 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.552772 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.552851 139785946451968 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:00:34.552954 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.552990 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.553018 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.554771 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.557116 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.562612 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.562862 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.565366 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.568956 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.569009 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.569041 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.569070 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.569535 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.570105 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.570179 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.570531 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.571275 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.573684 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.574297 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.574371 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.574404 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.574459 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.574583 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.574893 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.574934 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.576882 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.576972 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.579373 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.579450 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.579864 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.582104 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.583957 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.584048 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.584328 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.584407 139785946451968 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:00:34.584511 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.584547 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.584575 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.586401 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.588630 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.594132 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.594380 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.596887 139785946451968 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:00:34.600703 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.600755 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.600788 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.600816 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.601036 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.601579 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.601658 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.602016 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.602765 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.605195 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.605823 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.605897 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.605930 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.605984 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.606108 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.606420 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.606461 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.608417 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.608506 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.610988 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.611069 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.611500 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.613721 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.615628 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.615718 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.615996 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.616234 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616297 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616349 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616399 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616449 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616498 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616546 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616595 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616642 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616689 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616736 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616783 139785946451968 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:00:34.616816 139785946451968 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:00:34.619660 139785946451968 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:00:34.664017 139785946451968 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.664099 139785946451968 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:00:34.664150 139785946451968 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:00:34.664250 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.664287 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.664314 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.664372 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.666715 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.672024 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.672279 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.674830 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:34.687648 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.687701 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.687735 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.687763 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.687826 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.688390 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.688464 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.688821 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.689510 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.692013 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.692622 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.692695 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.692727 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.692782 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.692907 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.693011 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.693046 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.694854 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.694946 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.697262 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.697337 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.697442 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.699676 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.701471 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.701562 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.701853 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.701933 139785946451968 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:00:34.702035 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.702072 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.702100 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.702159 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.704314 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.709589 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.709850 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.712500 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:34.724877 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.724930 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.724963 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.724992 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.725052 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.725597 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.725676 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.726025 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.726764 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.729191 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.729815 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.729892 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.729924 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.729980 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.730107 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.730216 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.730252 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.732063 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.732151 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.734493 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.734570 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.734676 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.736899 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.738737 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.738834 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.739116 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.739195 139785946451968 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:00:34.739300 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.739337 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.739365 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.739425 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.741616 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.746976 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.747228 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.750245 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:34.762644 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.762696 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.762731 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.762759 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.762821 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.763371 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.763445 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.763791 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.764517 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.766955 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.767560 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.767636 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.767668 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.767722 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.767847 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.767950 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.767987 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.769849 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.769938 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.772338 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.772414 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.772519 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.774780 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.776626 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.776724 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.777007 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.777086 139785946451968 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:00:34.777190 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.777227 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.777256 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.777314 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.779511 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.784846 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.785097 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.787719 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:34.800081 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.800133 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.800166 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.800194 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.800255 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.800798 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.800871 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.801214 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.801941 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.804355 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.804964 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.805040 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.805072 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.805128 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.805252 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.805358 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.805394 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.807251 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.807341 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.809691 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.809768 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.809873 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.812116 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.813966 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.814059 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.814350 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.814429 139785946451968 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:00:34.814534 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.814570 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.814598 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.814657 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.816837 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.822225 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.822478 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.825147 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:34.837692 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.837745 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.837778 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.837807 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.837867 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.838407 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.838480 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.838831 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.839560 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.842001 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.842613 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.842688 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.842720 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.842774 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.842898 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.843002 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.843038 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.844865 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.844953 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.847299 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.847376 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.847482 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.849727 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.851553 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.851645 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.851927 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.852012 139785946451968 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:00:34.852119 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.852157 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.852186 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.852245 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.854438 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.860094 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.860361 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.863535 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:34.876791 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.876846 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.876880 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.876909 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.876971 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.877524 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.877598 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.877969 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.878728 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.881203 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.881823 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.881899 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.881934 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.881992 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.882122 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.882230 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.882268 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.884157 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.884249 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.886670 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.886750 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.886859 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.889127 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.891022 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.891120 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.891418 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.891505 139785946451968 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:00:34.891613 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.891649 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.891678 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.891737 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.893954 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.899419 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.899673 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.902312 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:34.914845 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.914901 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.914936 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.914967 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.915029 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.915593 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.915666 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.916011 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.916738 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.919235 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.919848 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.919923 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.919956 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.920011 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.920135 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.920240 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.920276 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.922118 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.922212 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.924589 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.924664 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.924769 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.927039 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.928856 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.928947 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.929229 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.929306 139785946451968 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:00:34.929416 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.929453 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.929481 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.929540 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.931779 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.937111 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.937374 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.940005 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:34.952307 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.952360 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.952394 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.952423 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.952483 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.953025 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.953098 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.953446 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.954174 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.956588 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.957194 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.957268 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.957300 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.957355 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.957481 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.957587 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.957624 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.959456 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.959546 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.961899 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.961976 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.962080 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:34.964298 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.966127 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.966220 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.966504 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.966583 139785946451968 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:00:34.966687 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:34.966728 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:34.966758 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:34.966817 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.969004 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:34.974326 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.974584 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:34.977566 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:34.989947 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:34.990001 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:34.990035 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:34.990063 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.990123 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.990669 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.990744 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.991092 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.991760 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.994268 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.994881 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.994956 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:34.994989 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:34.995045 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.995172 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:34.995280 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:34.995317 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:34.997151 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.997241 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:34.999588 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:34.999666 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:34.999771 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.001999 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.003815 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.003906 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.004185 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.004263 139785946451968 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:00:35.004370 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.004407 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.004442 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.004502 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.006696 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.011969 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.012220 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.014855 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.027237 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.027290 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.027324 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.027352 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.027411 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.027952 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.028023 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.028368 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.029031 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.031522 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.032124 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.032198 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.032230 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.032286 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.032413 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.032518 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.032556 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.034386 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.034477 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.036840 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.036916 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.037020 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.039248 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.041079 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.041171 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.041454 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.041533 139785946451968 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:00:35.041638 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.041683 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.041718 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.041779 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.043973 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.049267 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.049524 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.052143 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.064474 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.064528 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.064561 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.064589 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.064649 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.065194 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.065266 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.065613 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.066293 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.068768 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.069373 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.069448 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.069480 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.069534 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.069666 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.069774 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.069811 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.071612 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.071702 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.074043 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.074120 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.074224 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.076458 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.078281 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.078373 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.078654 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.078732 139785946451968 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:00:35.078838 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.078874 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.078902 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.078966 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.081155 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.086491 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.086743 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.089696 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.101995 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.102048 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.102082 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.102110 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.102169 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.102716 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.102789 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.103137 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.103812 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.106294 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.106894 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.106968 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.107001 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.107055 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.107183 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.107289 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.107325 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.109143 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.109232 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.111593 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.111670 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.111776 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.113988 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.115818 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.115909 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.116193 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.116276 139785946451968 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:00:35.119082 139785946451968 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:00:35.168187 139785946451968 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.168269 139785946451968 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:00:35.168327 139785946451968 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:00:35.168429 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.168465 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.168492 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.168551 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.170789 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.176098 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.176350 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.178871 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.191130 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.191184 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.191217 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.191246 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.191306 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.191851 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.191925 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.192268 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.192933 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.195317 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.195914 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.195987 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.196019 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.196073 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.196198 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.196301 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.196337 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.198213 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.198304 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.200654 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.200730 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.200833 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.202971 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.204763 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.204854 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.205136 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.205220 139785946451968 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:00:35.205324 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.205360 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.205388 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.205446 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.207625 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.212956 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.213209 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.215748 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.228012 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.228065 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.228098 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.228127 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.228186 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.228832 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.228905 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.229255 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.229940 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.232310 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.232913 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.232988 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.233020 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.233078 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.233201 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.233307 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.233343 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.235206 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.235298 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.237622 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.237704 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.237811 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.239950 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.241765 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.241862 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.242145 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.242224 139785946451968 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:00:35.242336 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.242374 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.242402 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.242462 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.244632 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.249935 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.250188 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.252679 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.265300 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.265353 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.265387 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.265417 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.265477 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.266025 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.266099 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.266443 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.267100 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.269447 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.270057 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.270132 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.270164 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.270220 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.270344 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.270449 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.270485 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.272334 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.272423 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.274758 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.274835 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.274941 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.277077 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.278897 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.278988 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.279272 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.279350 139785946451968 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:00:35.279453 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.279495 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.279524 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.279583 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.281770 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.287108 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.287360 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.289875 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.302222 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.302276 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.302309 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.302338 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.302398 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.302943 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.303015 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.303365 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.304032 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.306459 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.307074 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.307150 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.307183 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.307238 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.307362 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.307467 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.307504 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.309389 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.309480 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.311838 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.311916 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.312022 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.314229 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.316052 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.316146 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.316430 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.316510 139785946451968 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:00:35.316615 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.316651 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.316685 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.316746 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.318939 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.324293 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.324547 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.327113 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.339416 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.339469 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.339502 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.339531 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.339591 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.340132 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.340205 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.340553 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.341227 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.343643 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.344249 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.344323 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.344356 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.344410 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.344532 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.344637 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.344673 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.346556 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.346647 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.348996 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.349072 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.349177 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.351340 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.353149 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.353240 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.353521 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.353599 139785946451968 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:00:35.353709 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.353746 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.353774 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.353839 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.356007 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.361348 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.361600 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.364108 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.376770 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.376823 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.376856 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.376884 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.376944 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.377487 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.377561 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.377915 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.378580 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.381000 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.381604 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.381685 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.381719 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.381775 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.381899 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.382004 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.382040 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.383917 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.384008 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.386329 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.386405 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.386511 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.388652 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.390449 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.390541 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.390820 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.390897 139785946451968 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:00:35.391002 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.391038 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.391067 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.391134 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.393314 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.398639 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.398893 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.401436 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.413955 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.414009 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.414042 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.414071 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.414131 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.414679 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.414752 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.415099 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.415769 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.418205 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.418807 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.418882 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.418915 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.418970 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.419093 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.419198 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.419234 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.421121 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.421211 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.423559 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.423636 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.423740 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.425903 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.427729 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.427821 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.428103 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.428182 139785946451968 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:00:35.428285 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.428321 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.428349 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.428408 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.430605 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.435963 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.436220 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.438775 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.451151 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.451204 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.451237 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.451267 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.451328 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.451874 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.451947 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.452298 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.452965 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.455470 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.456088 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.456161 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.456194 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.456249 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.456372 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.456477 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.456514 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.458397 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.458487 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.460815 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.460889 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.460994 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.463141 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.464953 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.465045 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.465330 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.465409 139785946451968 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:00:35.465513 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.465550 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.465578 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.465637 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.467833 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.473165 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.473416 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.476020 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.488749 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.488802 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.488835 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.488863 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.488920 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.489462 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.489533 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.489888 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.490566 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.492993 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.493602 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.493680 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.493713 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.493767 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.493889 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.493993 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.494029 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.495903 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.495992 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.498348 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.498424 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.498529 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.500687 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.502501 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.502593 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.502879 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.502956 139785946451968 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:00:35.503059 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.503096 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.503124 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.503182 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.505343 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.510696 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.510947 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.513488 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.525784 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.525838 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.525870 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.525898 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.525960 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.526502 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.526575 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.526919 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.527582 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.529969 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.530570 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.530643 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.530676 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.530730 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.530854 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.530960 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.530997 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.532866 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.532955 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.535316 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.535391 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.535495 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.537637 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.539446 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.539538 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.539822 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.539898 139785946451968 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:00:35.540003 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.540039 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.540067 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.540125 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.542308 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.547626 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.547887 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.550460 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.562761 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.562814 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.562848 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.562877 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.562936 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.563478 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.563549 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.563889 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.564554 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.566984 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.567587 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.567660 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.567692 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.567747 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.567869 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.567972 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.568008 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.569909 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.570000 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.572344 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.572418 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.572522 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.574683 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.576487 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.576578 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.576861 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.576939 139785946451968 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:00:35.577044 139785946451968 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:00:35.577081 139785946451968 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:00:35.577110 139785946451968 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:00:35.577169 139785946451968 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.579337 139785946451968 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:00:35.584691 139785946451968 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.584954 139785946451968 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:00:35.587507 139785946451968 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:00:35.600182 139785946451968 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:00:35.600236 139785946451968 attention.py:418] Single window, no scan.
I0123 12:00:35.600270 139785946451968 transformer_layer.py:389] tlayer: self-attention.
I0123 12:00:35.600299 139785946451968 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.600356 139785946451968 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.600900 139785946451968 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.600971 139785946451968 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.601314 139785946451968 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.601987 139785946451968 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.604396 139785946451968 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.605004 139785946451968 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.605077 139785946451968 transformer_layer.py:468] tlayer: End windows.
I0123 12:00:35.605109 139785946451968 transformer_layer.py:472] tlayer: final FFN.
I0123 12:00:35.605163 139785946451968 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.605287 139785946451968 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:00:35.605390 139785946451968 nn_components.py:325] mlp: activation = None
I0123 12:00:35.605427 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.607309 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.607397 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.609745 139785946451968 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.609821 139785946451968 transformer_base.py:443] tbase: final FFN
I0123 12:00:35.609927 139785946451968 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:00:35.612089 139785946451968 nn_components.py:329] mlp: final activation = None
I0123 12:00:35.613920 139785946451968 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.614011 139785946451968 nn_components.py:261] mlp: residual
I0123 12:00:35.614294 139785946451968 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:35.614375 139785946451968 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:00:35.617149 139785946451968 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:00:51.511921 139785946451968 alphageometry.py:566] LM output (score=-0.081046): "n : C a c n 20 T a c h n 21 ;"
I0123 12:00:51.512073 139785946451968 alphageometry.py:567] Translation: "n = on_line n a c, on_tline n h a c"

I0123 12:00:51.512116 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c ? cong b k k m"
I0123 12:00:51.512290 139785946451968 graph.py:498] 
I0123 12:00:51.512346 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c ? cong b k k m
I0123 12:00:55.891839 139785946451968 ddar.py:60] Depth 1/1000 time = 4.330111503601074
I0123 12:01:03.273886 139785946451968 ddar.py:60] Depth 2/1000 time = 7.381857872009277
I0123 12:01:11.617621 139785946451968 ddar.py:60] Depth 3/1000 time = 8.343557834625244
I0123 12:01:20.756082 139785946451968 ddar.py:60] Depth 4/1000 time = 9.138267278671265
I0123 12:01:29.496330 139785946451968 ddar.py:60] Depth 5/1000 time = 8.739722728729248
I0123 12:01:39.714623 139785946451968 ddar.py:60] Depth 6/1000 time = 10.136497974395752
I0123 12:01:50.020287 139785946451968 ddar.py:60] Depth 7/1000 time = 10.30545687675476
I0123 12:01:59.727239 139785946451968 ddar.py:60] Depth 8/1000 time = 9.693724393844604
I0123 12:01:59.731129 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:01:59.731277 139785946451968 alphageometry.py:566] LM output (score=-1.531432): "n : C a g n 20 T a g h n 21 ;"
I0123 12:01:59.731317 139785946451968 alphageometry.py:567] Translation: "n = on_line n a g, on_tline n h a g"

I0123 12:01:59.731366 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a g, on_tline n h a g ? cong b k k m"
I0123 12:01:59.731541 139785946451968 graph.py:498] 
I0123 12:01:59.731598 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a g, on_tline n h a g ? cong b k k m
I0123 12:02:03.367709 139785946451968 ddar.py:60] Depth 1/1000 time = 3.5726819038391113
I0123 12:02:09.044431 139785946451968 ddar.py:60] Depth 2/1000 time = 5.676554918289185
I0123 12:02:16.716892 139785946451968 ddar.py:60] Depth 3/1000 time = 7.672284841537476
I0123 12:02:24.709513 139785946451968 ddar.py:60] Depth 4/1000 time = 7.9924139976501465
I0123 12:02:32.269688 139785946451968 ddar.py:60] Depth 5/1000 time = 7.559765338897705
I0123 12:02:41.239146 139785946451968 ddar.py:60] Depth 6/1000 time = 8.886256694793701
I0123 12:02:50.096192 139785946451968 ddar.py:60] Depth 7/1000 time = 8.856848001480103
I0123 12:02:59.142759 139785946451968 ddar.py:60] Depth 8/1000 time = 9.03632378578186
I0123 12:02:59.146369 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:02:59.146472 139785946451968 alphageometry.py:566] LM output (score=-1.952273): "n : D h i h n 20 T a c h n 21 ;"
I0123 12:02:59.146509 139785946451968 alphageometry.py:567] Translation: "n = on_circle n h i, on_tline n h a c"

I0123 12:02:59.146550 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h a c ? cong b k k m"
I0123 12:02:59.146714 139785946451968 graph.py:498] 
I0123 12:02:59.146771 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h a c ? cong b k k m
I0123 12:03:02.107660 139785946451968 ddar.py:60] Depth 1/1000 time = 2.900466203689575
I0123 12:03:07.803904 139785946451968 ddar.py:60] Depth 2/1000 time = 5.696049928665161
I0123 12:03:14.285810 139785946451968 ddar.py:60] Depth 3/1000 time = 6.481709241867065
I0123 12:03:20.565665 139785946451968 ddar.py:60] Depth 4/1000 time = 6.279670238494873
I0123 12:03:27.476302 139785946451968 ddar.py:60] Depth 5/1000 time = 6.9102373123168945
I0123 12:03:34.284450 139785946451968 ddar.py:60] Depth 6/1000 time = 6.807759761810303
I0123 12:03:42.229413 139785946451968 ddar.py:60] Depth 7/1000 time = 7.869034051895142
I0123 12:03:49.829560 139785946451968 ddar.py:60] Depth 8/1000 time = 7.599940061569214
I0123 12:03:49.836402 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:03:49.836506 139785946451968 alphageometry.py:566] LM output (score=-2.426180): "n : T a c h n 20 ;"
I0123 12:03:49.836543 139785946451968 alphageometry.py:567] Translation: "n = on_tline n h a c"

I0123 12:03:49.836585 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h a c ? cong b k k m"
I0123 12:03:49.836744 139785946451968 graph.py:498] 
I0123 12:03:49.836801 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h a c ? cong b k k m
I0123 12:03:52.957499 139785946451968 ddar.py:60] Depth 1/1000 time = 3.0502586364746094
I0123 12:03:58.635739 139785946451968 ddar.py:60] Depth 2/1000 time = 5.67808198928833
I0123 12:04:05.250113 139785946451968 ddar.py:60] Depth 3/1000 time = 6.6141862869262695
I0123 12:04:11.461431 139785946451968 ddar.py:60] Depth 4/1000 time = 6.21110200881958
I0123 12:04:18.116551 139785946451968 ddar.py:60] Depth 5/1000 time = 6.654580593109131
I0123 12:04:24.617186 139785946451968 ddar.py:60] Depth 6/1000 time = 6.497680902481079
I0123 12:04:32.144910 139785946451968 ddar.py:60] Depth 7/1000 time = 7.455934762954712
I0123 12:04:39.470097 139785946451968 ddar.py:60] Depth 8/1000 time = 7.324979782104492
I0123 12:04:39.476695 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:04:39.476802 139785946451968 alphageometry.py:566] LM output (score=-2.829051): "n : D h n h i 20 T a c h n 21 ;"
I0123 12:04:39.476842 139785946451968 alphageometry.py:567] Translation: "n = on_circle n h i, on_tline n h a c"

I0123 12:04:39.476880 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h a c ? cong b k k m"
I0123 12:04:39.477041 139785946451968 graph.py:498] 
I0123 12:04:39.477102 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h a c ? cong b k k m
I0123 12:04:42.708968 139785946451968 ddar.py:60] Depth 1/1000 time = 3.1649653911590576
I0123 12:04:48.224606 139785946451968 ddar.py:60] Depth 2/1000 time = 5.5154688358306885
I0123 12:04:55.011663 139785946451968 ddar.py:60] Depth 3/1000 time = 6.786874294281006
I0123 12:05:01.673938 139785946451968 ddar.py:60] Depth 4/1000 time = 6.6620848178863525
I0123 12:05:08.334964 139785946451968 ddar.py:60] Depth 5/1000 time = 6.660656929016113
I0123 12:05:15.098825 139785946451968 ddar.py:60] Depth 6/1000 time = 6.763473987579346
I0123 12:05:22.987422 139785946451968 ddar.py:60] Depth 7/1000 time = 7.811463117599487
I0123 12:05:30.787676 139785946451968 ddar.py:60] Depth 8/1000 time = 7.800084114074707
I0123 12:05:30.794789 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:05:30.794889 139785946451968 alphageometry.py:566] LM output (score=-3.061094): "n : T a g h n 20 ;"
I0123 12:05:30.794926 139785946451968 alphageometry.py:567] Translation: "n = on_tline n h a g"

I0123 12:05:30.794965 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h a g ? cong b k k m"
I0123 12:05:30.795130 139785946451968 graph.py:498] 
I0123 12:05:30.795189 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h a g ? cong b k k m
I0123 12:05:34.028141 139785946451968 ddar.py:60] Depth 1/1000 time = 3.1858465671539307
I0123 12:05:39.102770 139785946451968 ddar.py:60] Depth 2/1000 time = 5.074455261230469
I0123 12:05:45.731516 139785946451968 ddar.py:60] Depth 3/1000 time = 6.628563404083252
I0123 12:05:52.312392 139785946451968 ddar.py:60] Depth 4/1000 time = 6.5806872844696045
I0123 12:05:58.937399 139785946451968 ddar.py:60] Depth 5/1000 time = 6.624589204788208
I0123 12:06:06.569247 139785946451968 ddar.py:60] Depth 6/1000 time = 7.551952123641968
I0123 12:06:14.455674 139785946451968 ddar.py:60] Depth 7/1000 time = 7.886261463165283
I0123 12:06:14.462556 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:06:14.462667 139785946451968 alphageometry.py:566] LM output (score=-3.563754): "n : C a b n 20 D a n b n 21 ;"
I0123 12:06:14.462710 139785946451968 alphageometry.py:567] Translation: "n = on_line n a b, on_bline n b a"

I0123 12:06:14.462754 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a b, on_bline n b a ? cong b k k m"
I0123 12:06:14.462923 139785946451968 graph.py:498] 
I0123 12:06:14.462986 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a b, on_bline n b a ? cong b k k m
I0123 12:06:17.676379 139785946451968 ddar.py:60] Depth 1/1000 time = 3.1529054641723633
I0123 12:06:24.943052 139785946451968 ddar.py:60] Depth 2/1000 time = 7.266474008560181
I0123 12:06:33.825937 139785946451968 ddar.py:60] Depth 3/1000 time = 8.882718324661255
I0123 12:06:43.198342 139785946451968 ddar.py:60] Depth 4/1000 time = 9.372210264205933
I0123 12:06:52.778396 139785946451968 ddar.py:60] Depth 5/1000 time = 9.579851150512695
I0123 12:07:02.424843 139785946451968 ddar.py:60] Depth 6/1000 time = 9.645978689193726
I0123 12:07:13.552509 139785946451968 ddar.py:60] Depth 7/1000 time = 11.042326211929321
I0123 12:07:24.189114 139785946451968 ddar.py:60] Depth 8/1000 time = 10.636415958404541
I0123 12:07:34.528949 139785946451968 ddar.py:60] Depth 9/1000 time = 10.316125392913818
I0123 12:07:34.531198 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:07:34.531303 139785946451968 alphageometry.py:566] LM output (score=-3.620122): "n : C b g n 20 T b g h n 21 ;"
I0123 12:07:34.531340 139785946451968 alphageometry.py:567] Translation: "n = on_line n b g, on_tline n h b g"

I0123 12:07:34.531383 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n b g, on_tline n h b g ? cong b k k m"
I0123 12:07:34.531548 139785946451968 graph.py:498] 
I0123 12:07:34.531606 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n b g, on_tline n h b g ? cong b k k m
I0123 12:07:38.222880 139785946451968 ddar.py:60] Depth 1/1000 time = 3.6432101726531982
I0123 12:07:44.780879 139785946451968 ddar.py:60] Depth 2/1000 time = 6.557819604873657
I0123 12:07:52.354075 139785946451968 ddar.py:60] Depth 3/1000 time = 7.572977781295776
I0123 12:08:00.363993 139785946451968 ddar.py:60] Depth 4/1000 time = 8.009734630584717
I0123 12:08:08.507067 139785946451968 ddar.py:60] Depth 5/1000 time = 8.142695665359497
I0123 12:08:17.333185 139785946451968 ddar.py:60] Depth 6/1000 time = 8.747499942779541
I0123 12:08:26.684369 139785946451968 ddar.py:60] Depth 7/1000 time = 9.350945472717285
I0123 12:08:35.595480 139785946451968 ddar.py:60] Depth 8/1000 time = 8.896100282669067
I0123 12:08:35.597377 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:08:35.597492 139785946451968 alphageometry.py:566] LM output (score=-3.643826): "n : C g h n 20 D g n h n 21 ;"
I0123 12:08:35.597532 139785946451968 alphageometry.py:567] Translation: "n = on_line n g h, on_bline n h g"

I0123 12:08:35.597570 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n g h, on_bline n h g ? cong b k k m"
I0123 12:08:35.597738 139785946451968 graph.py:498] 
I0123 12:08:35.597796 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n g h, on_bline n h g ? cong b k k m
I0123 12:08:39.149244 139785946451968 ddar.py:60] Depth 1/1000 time = 3.491431713104248
I0123 12:08:44.708204 139785946451968 ddar.py:60] Depth 2/1000 time = 5.558791160583496
I0123 12:08:52.000981 139785946451968 ddar.py:60] Depth 3/1000 time = 7.292579650878906
I0123 12:08:59.430609 139785946451968 ddar.py:60] Depth 4/1000 time = 7.429433345794678
I0123 12:09:06.265095 139785946451968 ddar.py:60] Depth 5/1000 time = 6.834073781967163
I0123 12:09:15.009394 139785946451968 ddar.py:60] Depth 6/1000 time = 8.6714448928833
I0123 12:09:23.070808 139785946451968 ddar.py:60] Depth 7/1000 time = 8.061232566833496
I0123 12:09:31.259378 139785946451968 ddar.py:60] Depth 8/1000 time = 8.16740107536316
I0123 12:09:31.261644 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:09:31.261776 139785946451968 alphageometry.py:566] LM output (score=-3.722795): "n : D h i h n 20 ;"
I0123 12:09:31.261814 139785946451968 alphageometry.py:567] Translation: "n = on_circle n h i"

I0123 12:09:31.261865 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i ? cong b k k m"
I0123 12:09:31.262036 139785946451968 graph.py:498] 
I0123 12:09:31.262092 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i ? cong b k k m
I0123 12:09:34.663455 139785946451968 ddar.py:60] Depth 1/1000 time = 3.3429296016693115
I0123 12:09:40.133731 139785946451968 ddar.py:60] Depth 2/1000 time = 5.470117568969727
I0123 12:09:46.609432 139785946451968 ddar.py:60] Depth 3/1000 time = 6.475530624389648
I0123 12:09:52.932785 139785946451968 ddar.py:60] Depth 4/1000 time = 6.323148965835571
I0123 12:09:59.556528 139785946451968 ddar.py:60] Depth 5/1000 time = 6.623297214508057
I0123 12:10:07.120449 139785946451968 ddar.py:60] Depth 6/1000 time = 7.495108604431152
I0123 12:10:14.342983 139785946451968 ddar.py:60] Depth 7/1000 time = 7.222323894500732
I0123 12:10:14.349833 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:10:14.349934 139785946451968 alphageometry.py:566] LM output (score=-3.741008): "n : T a h h n 20 ;"
I0123 12:10:14.349971 139785946451968 alphageometry.py:567] Translation: "n = on_tline n h a h"

I0123 12:10:14.350012 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h a h ? cong b k k m"
I0123 12:10:14.350171 139785946451968 graph.py:498] 
I0123 12:10:14.350228 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h a h ? cong b k k m
I0123 12:10:17.755754 139785946451968 ddar.py:60] Depth 1/1000 time = 3.350828170776367
I0123 12:10:22.940213 139785946451968 ddar.py:60] Depth 2/1000 time = 5.184287071228027
I0123 12:10:29.480854 139785946451968 ddar.py:60] Depth 3/1000 time = 6.540469408035278
I0123 12:10:35.924201 139785946451968 ddar.py:60] Depth 4/1000 time = 6.443148374557495
I0123 12:10:42.638852 139785946451968 ddar.py:60] Depth 5/1000 time = 6.714141130447388
I0123 12:10:50.176281 139785946451968 ddar.py:60] Depth 6/1000 time = 7.453051567077637
I0123 12:10:57.955666 139785946451968 ddar.py:60] Depth 7/1000 time = 7.779180526733398
I0123 12:10:57.962632 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:10:57.962732 139785946451968 alphageometry.py:566] LM output (score=-3.809910): "n : D c h c n 20 T c h c n 21 ;"
I0123 12:10:57.962769 139785946451968 alphageometry.py:567] Translation: "n = on_circle n c h, on_tline n c c h"

I0123 12:10:57.962810 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n c h, on_tline n c c h ? cong b k k m"
I0123 12:10:57.962982 139785946451968 graph.py:498] 
I0123 12:10:57.963042 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n c h, on_tline n c c h ? cong b k k m
I0123 12:11:01.175217 139785946451968 ddar.py:60] Depth 1/1000 time = 3.1455330848693848
I0123 12:11:06.889491 139785946451968 ddar.py:60] Depth 2/1000 time = 5.714108943939209
I0123 12:11:13.836065 139785946451968 ddar.py:60] Depth 3/1000 time = 6.946412801742554
I0123 12:11:20.569519 139785946451968 ddar.py:60] Depth 4/1000 time = 6.733252763748169
I0123 12:11:27.222867 139785946451968 ddar.py:60] Depth 5/1000 time = 6.652928113937378
I0123 12:11:34.216259 139785946451968 ddar.py:60] Depth 6/1000 time = 6.992761611938477
I0123 12:11:42.024752 139785946451968 ddar.py:60] Depth 7/1000 time = 7.721995830535889
I0123 12:11:49.995813 139785946451968 ddar.py:60] Depth 8/1000 time = 7.9708616733551025
I0123 12:11:50.002673 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:11:50.002760 139785946451968 alphageometry.py:566] LM output (score=-3.924994): "n : T a g g n 20 ;"
I0123 12:11:50.002797 139785946451968 alphageometry.py:567] Translation: "n = on_tline n g a g"

I0123 12:11:50.002834 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n g a g ? cong b k k m"
I0123 12:11:50.003000 139785946451968 graph.py:498] 
I0123 12:11:50.003055 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n g a g ? cong b k k m
I0123 12:11:52.850629 139785946451968 ddar.py:60] Depth 1/1000 time = 2.7999136447906494
I0123 12:11:58.250544 139785946451968 ddar.py:60] Depth 2/1000 time = 5.399704694747925
I0123 12:12:05.628850 139785946451968 ddar.py:60] Depth 3/1000 time = 7.378024101257324
I0123 12:12:12.531371 139785946451968 ddar.py:60] Depth 4/1000 time = 6.902323246002197
I0123 12:12:19.458005 139785946451968 ddar.py:60] Depth 5/1000 time = 6.926193714141846
I0123 12:12:27.135157 139785946451968 ddar.py:60] Depth 6/1000 time = 7.5978312492370605
I0123 12:12:35.490952 139785946451968 ddar.py:60] Depth 7/1000 time = 8.355623483657837
I0123 12:12:35.497807 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:12:35.497905 139785946451968 alphageometry.py:566] LM output (score=-4.026709): "n : D h i h n 20 T a g h n 21 ;"
I0123 12:12:35.497941 139785946451968 alphageometry.py:567] Translation: "n = on_circle n h i, on_tline n h a g"

I0123 12:12:35.497977 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h a g ? cong b k k m"
I0123 12:12:35.498136 139785946451968 graph.py:498] 
I0123 12:12:35.498192 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h a g ? cong b k k m
I0123 12:12:38.768585 139785946451968 ddar.py:60] Depth 1/1000 time = 3.2052650451660156
I0123 12:12:44.420259 139785946451968 ddar.py:60] Depth 2/1000 time = 5.651484489440918
I0123 12:12:51.328421 139785946451968 ddar.py:60] Depth 3/1000 time = 6.907958984375
I0123 12:12:57.861474 139785946451968 ddar.py:60] Depth 4/1000 time = 6.532870292663574
I0123 12:13:05.114748 139785946451968 ddar.py:60] Depth 5/1000 time = 7.252771615982056
I0123 12:13:13.201725 139785946451968 ddar.py:60] Depth 6/1000 time = 7.995557546615601
I0123 12:13:21.156935 139785946451968 ddar.py:60] Depth 7/1000 time = 7.955034017562866
I0123 12:13:21.163616 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:13:21.163704 139785946451968 alphageometry.py:566] LM output (score=-4.028741): "n : D a b a n 20 T a b a n 21 ;"
I0123 12:13:21.163741 139785946451968 alphageometry.py:567] Translation: "n = on_circle n a b, on_tline n a a b"

I0123 12:13:21.163779 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n a b, on_tline n a a b ? cong b k k m"
I0123 12:13:21.163935 139785946451968 graph.py:498] 
I0123 12:13:21.163993 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n a b, on_tline n a a b ? cong b k k m
I0123 12:13:24.552424 139785946451968 ddar.py:60] Depth 1/1000 time = 3.3285939693450928
I0123 12:13:30.876197 139785946451968 ddar.py:60] Depth 2/1000 time = 6.323487281799316
I0123 12:13:37.751167 139785946451968 ddar.py:60] Depth 3/1000 time = 6.874804258346558
I0123 12:13:45.883855 139785946451968 ddar.py:60] Depth 4/1000 time = 8.132461547851562
I0123 12:13:53.520530 139785946451968 ddar.py:60] Depth 5/1000 time = 7.636380910873413
I0123 12:14:00.958428 139785946451968 ddar.py:60] Depth 6/1000 time = 7.437488317489624
I0123 12:14:08.890346 139785946451968 ddar.py:60] Depth 7/1000 time = 7.930867910385132
I0123 12:14:17.889099 139785946451968 ddar.py:60] Depth 8/1000 time = 8.913367509841919
I0123 12:14:26.417777 139785946451968 ddar.py:60] Depth 9/1000 time = 8.528504848480225
I0123 12:14:35.390842 139785946451968 ddar.py:60] Depth 10/1000 time = 8.949516296386719
I0123 12:14:35.392850 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:14:35.392939 139785946451968 alphageometry.py:566] LM output (score=-4.280652): "n : P a b c n 20 ;"
I0123 12:14:35.392975 139785946451968 alphageometry.py:567] Translation: "n = on_pline n c a b"

I0123 12:14:35.393012 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_pline n c a b ? cong b k k m"
I0123 12:14:35.393171 139785946451968 graph.py:498] 
I0123 12:14:35.393225 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_pline n c a b ? cong b k k m
I0123 12:14:38.746932 139785946451968 ddar.py:60] Depth 1/1000 time = 3.297560453414917
I0123 12:14:44.130336 139785946451968 ddar.py:60] Depth 2/1000 time = 5.383236885070801
I0123 12:14:54.055702 139785946451968 ddar.py:60] Depth 3/1000 time = 9.925159215927124
I0123 12:15:04.032102 139785946451968 ddar.py:60] Depth 4/1000 time = 9.97617793083191
I0123 12:15:14.033195 139785946451968 ddar.py:60] Depth 5/1000 time = 10.000648498535156
I0123 12:15:24.798260 139785946451968 ddar.py:60] Depth 6/1000 time = 10.694793939590454
I0123 12:15:35.434938 139785946451968 ddar.py:60] Depth 7/1000 time = 10.636442184448242
I0123 12:15:35.441591 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:15:35.441715 139785946451968 alphageometry.py:566] LM output (score=-4.284878): "n : D c e e n 20 T c e e n 21 ;"
I0123 12:15:35.441753 139785946451968 alphageometry.py:567] Translation: "n = on_circle n e c, on_tline n e c e"

I0123 12:15:35.441803 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n e c, on_tline n e c e ? cong b k k m"
I0123 12:15:35.441979 139785946451968 graph.py:498] 
I0123 12:15:35.442035 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n e c, on_tline n e c e ? cong b k k m
I0123 12:15:38.922760 139785946451968 ddar.py:60] Depth 1/1000 time = 3.424539089202881
I0123 12:15:46.045334 139785946451968 ddar.py:60] Depth 2/1000 time = 7.1224141120910645
I0123 12:15:54.169373 139785946451968 ddar.py:60] Depth 3/1000 time = 8.123812437057495
I0123 12:16:02.013609 139785946451968 ddar.py:60] Depth 4/1000 time = 7.843944787979126
I0123 12:16:10.079965 139785946451968 ddar.py:60] Depth 5/1000 time = 8.065944194793701
I0123 12:16:18.621935 139785946451968 ddar.py:60] Depth 6/1000 time = 8.540736198425293
I0123 12:16:26.828210 139785946451968 ddar.py:60] Depth 7/1000 time = 8.202905178070068
I0123 12:16:36.203346 139785946451968 ddar.py:60] Depth 8/1000 time = 9.292119264602661
I0123 12:16:45.686386 139785946451968 ddar.py:60] Depth 9/1000 time = 9.48277473449707
I0123 12:16:45.693772 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:16:45.693866 139785946451968 alphageometry.py:566] LM output (score=-4.359851): "n : T b c h n 20 ;"
I0123 12:16:45.693903 139785946451968 alphageometry.py:567] Translation: "n = on_tline n h b c"

I0123 12:16:45.693940 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h b c ? cong b k k m"
I0123 12:16:45.694106 139785946451968 graph.py:498] 
I0123 12:16:45.694163 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h b c ? cong b k k m
I0123 12:16:48.655684 139785946451968 ddar.py:60] Depth 1/1000 time = 2.9123947620391846
I0123 12:16:54.364071 139785946451968 ddar.py:60] Depth 2/1000 time = 5.708211660385132
I0123 12:17:01.367062 139785946451968 ddar.py:60] Depth 3/1000 time = 7.002794027328491
I0123 12:17:07.991657 139785946451968 ddar.py:60] Depth 4/1000 time = 6.624370098114014
I0123 12:17:15.026467 139785946451968 ddar.py:60] Depth 5/1000 time = 7.034357309341431
I0123 12:17:22.801993 139785946451968 ddar.py:60] Depth 6/1000 time = 7.692511320114136
I0123 12:17:30.715455 139785946451968 ddar.py:60] Depth 7/1000 time = 7.9132513999938965
I0123 12:17:30.722074 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:17:30.722175 139785946451968 alphageometry.py:566] LM output (score=-4.384147): "n : T h i h n 20 ;"
I0123 12:17:30.722211 139785946451968 alphageometry.py:567] Translation: "n = on_tline n h h i"

I0123 12:17:30.722248 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h h i ? cong b k k m"
I0123 12:17:30.722402 139785946451968 graph.py:498] 
I0123 12:17:30.722458 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h h i ? cong b k k m
I0123 12:17:34.180187 139785946451968 ddar.py:60] Depth 1/1000 time = 3.4069130420684814
I0123 12:17:40.318120 139785946451968 ddar.py:60] Depth 2/1000 time = 6.137775897979736
I0123 12:17:47.028429 139785946451968 ddar.py:60] Depth 3/1000 time = 6.7101218700408936
I0123 12:17:53.544547 139785946451968 ddar.py:60] Depth 4/1000 time = 6.515920162200928
I0123 12:18:00.477178 139785946451968 ddar.py:60] Depth 5/1000 time = 6.93210506439209
I0123 12:18:08.344199 139785946451968 ddar.py:60] Depth 6/1000 time = 7.796104431152344
I0123 12:18:16.080513 139785946451968 ddar.py:60] Depth 7/1000 time = 7.73613715171814
I0123 12:18:16.087415 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:18:16.087517 139785946451968 alphageometry.py:566] LM output (score=-4.506062): "n : D c d d n 20 ;"
I0123 12:18:16.087553 139785946451968 alphageometry.py:567] Translation: "n = on_circle n d c"

I0123 12:18:16.087589 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n d c ? cong b k k m"
I0123 12:18:16.087745 139785946451968 graph.py:498] 
I0123 12:18:16.087800 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n d c ? cong b k k m
I0123 12:18:19.717225 139785946451968 ddar.py:60] Depth 1/1000 time = 3.575669288635254
I0123 12:18:26.990218 139785946451968 ddar.py:60] Depth 2/1000 time = 7.272806167602539
I0123 12:18:35.252017 139785946451968 ddar.py:60] Depth 3/1000 time = 8.261594533920288
I0123 12:18:43.204616 139785946451968 ddar.py:60] Depth 4/1000 time = 7.952410936355591
I0123 12:18:51.248161 139785946451968 ddar.py:60] Depth 5/1000 time = 8.04311490058899
I0123 12:19:00.732545 139785946451968 ddar.py:60] Depth 6/1000 time = 9.385499238967896
I0123 12:19:10.160389 139785946451968 ddar.py:60] Depth 7/1000 time = 9.427598476409912
I0123 12:19:10.167715 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:19:10.167839 139785946451968 alphageometry.py:566] LM output (score=-4.507252): "n : T b c b n 20 ;"
I0123 12:19:10.167875 139785946451968 alphageometry.py:567] Translation: "n = on_tline n b b c"

I0123 12:19:10.167927 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n b b c ? cong b k k m"
I0123 12:19:10.168123 139785946451968 graph.py:498] 
I0123 12:19:10.168180 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n b b c ? cong b k k m
I0123 12:19:13.623637 139785946451968 ddar.py:60] Depth 1/1000 time = 3.4075264930725098
I0123 12:19:19.061893 139785946451968 ddar.py:60] Depth 2/1000 time = 5.438080787658691
I0123 12:19:26.185360 139785946451968 ddar.py:60] Depth 3/1000 time = 7.123298406600952
I0123 12:19:33.379894 139785946451968 ddar.py:60] Depth 4/1000 time = 7.1943359375
I0123 12:19:40.199414 139785946451968 ddar.py:60] Depth 5/1000 time = 6.819056034088135
I0123 12:19:48.336515 139785946451968 ddar.py:60] Depth 6/1000 time = 8.054041862487793
I0123 12:19:56.387702 139785946451968 ddar.py:60] Depth 7/1000 time = 8.050987720489502
I0123 12:19:56.394466 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:19:56.394571 139785946451968 alphageometry.py:566] LM output (score=-4.520089): "n : T a b b n 20 ;"
I0123 12:19:56.394608 139785946451968 alphageometry.py:567] Translation: "n = on_tline n b a b"

I0123 12:19:56.394647 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n b a b ? cong b k k m"
I0123 12:19:56.394808 139785946451968 graph.py:498] 
I0123 12:19:56.394864 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n b a b ? cong b k k m
I0123 12:19:59.900128 139785946451968 ddar.py:60] Depth 1/1000 time = 3.456742763519287
I0123 12:20:05.766092 139785946451968 ddar.py:60] Depth 2/1000 time = 5.865794658660889
I0123 12:20:12.804121 139785946451968 ddar.py:60] Depth 3/1000 time = 7.037775278091431
I0123 12:20:19.967280 139785946451968 ddar.py:60] Depth 4/1000 time = 7.162832975387573
I0123 12:20:27.503413 139785946451968 ddar.py:60] Depth 5/1000 time = 7.5357654094696045
I0123 12:20:35.206097 139785946451968 ddar.py:60] Depth 6/1000 time = 7.6235880851745605
I0123 12:20:43.735262 139785946451968 ddar.py:60] Depth 7/1000 time = 8.528860807418823
I0123 12:20:43.741696 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:20:43.741799 139785946451968 alphageometry.py:566] LM output (score=-4.612572): "n : D h n h i 20 T a g h n 21 ;"
I0123 12:20:43.741836 139785946451968 alphageometry.py:567] Translation: "n = on_circle n h i, on_tline n h a g"

I0123 12:20:43.741876 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h a g ? cong b k k m"
I0123 12:20:43.742039 139785946451968 graph.py:498] 
I0123 12:20:43.742096 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h a g ? cong b k k m
I0123 12:20:46.919397 139785946451968 ddar.py:60] Depth 1/1000 time = 3.116412401199341
I0123 12:20:53.014059 139785946451968 ddar.py:60] Depth 2/1000 time = 6.094505310058594
I0123 12:21:00.118359 139785946451968 ddar.py:60] Depth 3/1000 time = 7.104104280471802
I0123 12:21:07.135572 139785946451968 ddar.py:60] Depth 4/1000 time = 7.016999244689941
I0123 12:21:14.166584 139785946451968 ddar.py:60] Depth 5/1000 time = 7.0304741859436035
I0123 12:21:22.852298 139785946451968 ddar.py:60] Depth 6/1000 time = 8.595565557479858
I0123 12:21:31.272799 139785946451968 ddar.py:60] Depth 7/1000 time = 8.420308828353882
I0123 12:21:31.279945 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:21:31.280067 139785946451968 alphageometry.py:566] LM output (score=-4.687425): "n : T a b a n 20 ;"
I0123 12:21:31.280106 139785946451968 alphageometry.py:567] Translation: "n = on_tline n a a b"

I0123 12:21:31.280144 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n a a b ? cong b k k m"
I0123 12:21:31.280305 139785946451968 graph.py:498] 
I0123 12:21:31.280363 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n a a b ? cong b k k m
I0123 12:21:34.429470 139785946451968 ddar.py:60] Depth 1/1000 time = 3.092803955078125
I0123 12:21:40.838891 139785946451968 ddar.py:60] Depth 2/1000 time = 6.409218072891235
I0123 12:21:48.159013 139785946451968 ddar.py:60] Depth 3/1000 time = 7.319934844970703
I0123 12:21:55.728855 139785946451968 ddar.py:60] Depth 4/1000 time = 7.56967568397522
I0123 12:22:03.515581 139785946451968 ddar.py:60] Depth 5/1000 time = 7.786482095718384
I0123 12:22:10.809374 139785946451968 ddar.py:60] Depth 6/1000 time = 7.293366432189941
I0123 12:22:19.521788 139785946451968 ddar.py:60] Depth 7/1000 time = 8.629640817642212
I0123 12:22:28.185108 139785946451968 ddar.py:60] Depth 8/1000 time = 8.663024187088013
I0123 12:22:36.900675 139785946451968 ddar.py:60] Depth 9/1000 time = 8.69261384010315
I0123 12:22:36.902670 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:22:36.902767 139785946451968 alphageometry.py:566] LM output (score=-4.695155): "n : D h i h n 20 T h i h n 21 ;"
I0123 12:22:36.902805 139785946451968 alphageometry.py:567] Translation: "n = on_circle n h i, on_tline n h h i"

I0123 12:22:36.902848 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h h i ? cong b k k m"
I0123 12:22:36.903013 139785946451968 graph.py:498] 
I0123 12:22:36.903071 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n h i, on_tline n h h i ? cong b k k m
I0123 12:22:40.726025 139785946451968 ddar.py:60] Depth 1/1000 time = 3.7572827339172363
I0123 12:22:46.758791 139785946451968 ddar.py:60] Depth 2/1000 time = 6.032492160797119
I0123 12:22:53.223066 139785946451968 ddar.py:60] Depth 3/1000 time = 6.464078664779663
I0123 12:23:00.694363 139785946451968 ddar.py:60] Depth 4/1000 time = 7.471115589141846
I0123 12:23:07.339707 139785946451968 ddar.py:60] Depth 5/1000 time = 6.644851446151733
I0123 12:23:14.741997 139785946451968 ddar.py:60] Depth 6/1000 time = 7.401324033737183
I0123 12:23:22.747810 139785946451968 ddar.py:60] Depth 7/1000 time = 7.930805206298828
I0123 12:23:30.667617 139785946451968 ddar.py:60] Depth 8/1000 time = 7.9196038246154785
I0123 12:23:30.674516 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:23:30.674614 139785946451968 alphageometry.py:566] LM output (score=-4.699433): "n : P a b c n 20 T a b a n 21 ;"
I0123 12:23:30.674650 139785946451968 alphageometry.py:567] Translation: "n = on_pline n c a b, on_tline n a a b"

I0123 12:23:30.674686 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_pline n c a b, on_tline n a a b ? cong b k k m"
I0123 12:23:30.674846 139785946451968 graph.py:498] 
I0123 12:23:30.674904 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_pline n c a b, on_tline n a a b ? cong b k k m
I0123 12:23:34.370334 139785946451968 ddar.py:60] Depth 1/1000 time = 3.640587329864502
I0123 12:23:41.238278 139785946451968 ddar.py:60] Depth 2/1000 time = 6.86777138710022
I0123 12:23:54.315096 139785946451968 ddar.py:60] Depth 3/1000 time = 13.076616287231445
I0123 12:24:07.379862 139785946451968 ddar.py:60] Depth 4/1000 time = 13.064515113830566
I0123 12:24:20.385560 139785946451968 ddar.py:60] Depth 5/1000 time = 13.004995584487915
I0123 12:24:35.135120 139785946451968 ddar.py:60] Depth 6/1000 time = 14.663622856140137
I0123 12:24:49.901327 139785946451968 ddar.py:60] Depth 7/1000 time = 14.765955686569214
I0123 12:25:03.780297 139785946451968 ddar.py:60] Depth 8/1000 time = 13.878641366958618
I0123 12:25:03.800625 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:25:03.800734 139785946451968 alphageometry.py:566] LM output (score=-4.707469): "n : T a g f n 20 ;"
I0123 12:25:03.800771 139785946451968 alphageometry.py:567] Translation: "n = on_tline n f a g"

I0123 12:25:03.800808 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n f a g ? cong b k k m"
I0123 12:25:03.800973 139785946451968 graph.py:498] 
I0123 12:25:03.801029 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n f a g ? cong b k k m
I0123 12:25:06.573877 139785946451968 ddar.py:60] Depth 1/1000 time = 2.724412441253662
I0123 12:25:12.451139 139785946451968 ddar.py:60] Depth 2/1000 time = 5.877063989639282
I0123 12:25:19.311687 139785946451968 ddar.py:60] Depth 3/1000 time = 6.860336065292358
I0123 12:25:26.166987 139785946451968 ddar.py:60] Depth 4/1000 time = 6.85512900352478
I0123 12:25:33.065081 139785946451968 ddar.py:60] Depth 5/1000 time = 6.897670269012451
I0123 12:25:40.819978 139785946451968 ddar.py:60] Depth 6/1000 time = 7.675745487213135
I0123 12:25:48.445802 139785946451968 ddar.py:60] Depth 7/1000 time = 7.6256513595581055
I0123 12:25:48.452218 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:25:48.452325 139785946451968 alphageometry.py:566] LM output (score=-4.709209): "n : T a h a n 20 ;"
I0123 12:25:48.452362 139785946451968 alphageometry.py:567] Translation: "n = on_tline n a a h"

I0123 12:25:48.452400 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n a a h ? cong b k k m"
I0123 12:25:48.452556 139785946451968 graph.py:498] 
I0123 12:25:48.452612 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n a a h ? cong b k k m
I0123 12:25:52.165144 139785946451968 ddar.py:60] Depth 1/1000 time = 3.6655709743499756
I0123 12:25:57.385103 139785946451968 ddar.py:60] Depth 2/1000 time = 5.219754219055176
I0123 12:26:03.902923 139785946451968 ddar.py:60] Depth 3/1000 time = 6.517646789550781
I0123 12:26:10.599283 139785946451968 ddar.py:60] Depth 4/1000 time = 6.696190595626831
I0123 12:26:17.278260 139785946451968 ddar.py:60] Depth 5/1000 time = 6.6783857345581055
I0123 12:26:25.299444 139785946451968 ddar.py:60] Depth 6/1000 time = 7.937156677246094
I0123 12:26:33.272943 139785946451968 ddar.py:60] Depth 7/1000 time = 7.9732606410980225
I0123 12:26:33.279704 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:26:33.279830 139785946451968 alphageometry.py:566] LM output (score=-4.716706): "n : T c h h n 20 ;"
I0123 12:26:33.279866 139785946451968 alphageometry.py:567] Translation: "n = on_tline n h c h"

I0123 12:26:33.279916 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h c h ? cong b k k m"
I0123 12:26:33.280094 139785946451968 graph.py:498] 
I0123 12:26:33.280148 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n h c h ? cong b k k m
I0123 12:26:36.550357 139785946451968 ddar.py:60] Depth 1/1000 time = 3.2231056690216064
I0123 12:26:42.286716 139785946451968 ddar.py:60] Depth 2/1000 time = 5.736187219619751
I0123 12:26:48.493946 139785946451968 ddar.py:60] Depth 3/1000 time = 6.207059621810913
I0123 12:26:55.756716 139785946451968 ddar.py:60] Depth 4/1000 time = 7.262574672698975
I0123 12:27:02.105660 139785946451968 ddar.py:60] Depth 5/1000 time = 6.348487854003906
I0123 12:27:10.392130 139785946451968 ddar.py:60] Depth 6/1000 time = 8.206521987915039
I0123 12:27:18.332540 139785946451968 ddar.py:60] Depth 7/1000 time = 7.940207242965698
I0123 12:27:18.339256 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:27:18.339353 139785946451968 alphageometry.py:566] LM output (score=-4.722641): "n : T a g a n 20 ;"
I0123 12:27:18.339389 139785946451968 alphageometry.py:567] Translation: "n = on_tline n a a g"

I0123 12:27:18.339427 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n a a g ? cong b k k m"
I0123 12:27:18.339595 139785946451968 graph.py:498] 
I0123 12:27:18.339650 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n a a g ? cong b k k m
I0123 12:27:21.198019 139785946451968 ddar.py:60] Depth 1/1000 time = 2.8116819858551025
I0123 12:27:27.248362 139785946451968 ddar.py:60] Depth 2/1000 time = 6.050168037414551
I0123 12:27:33.943762 139785946451968 ddar.py:60] Depth 3/1000 time = 6.695232391357422
I0123 12:27:40.951236 139785946451968 ddar.py:60] Depth 4/1000 time = 7.007256269454956
I0123 12:27:47.434973 139785946451968 ddar.py:60] Depth 5/1000 time = 6.483339548110962
I0123 12:27:55.340042 139785946451968 ddar.py:60] Depth 6/1000 time = 7.8238255977630615
I0123 12:28:03.194964 139785946451968 ddar.py:60] Depth 7/1000 time = 7.85462212562561
I0123 12:28:03.201185 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:28:03.201284 139785946451968 alphageometry.py:566] LM output (score=-4.739164): "n : T a b f n 20 ;"
I0123 12:28:03.201320 139785946451968 alphageometry.py:567] Translation: "n = on_tline n f a b"

I0123 12:28:03.201358 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n f a b ? cong b k k m"
I0123 12:28:03.201517 139785946451968 graph.py:498] 
I0123 12:28:03.201575 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_tline n f a b ? cong b k k m
I0123 12:28:06.521432 139785946451968 ddar.py:60] Depth 1/1000 time = 3.2722277641296387
I0123 12:28:12.560979 139785946451968 ddar.py:60] Depth 2/1000 time = 6.039383172988892
I0123 12:28:19.655344 139785946451968 ddar.py:60] Depth 3/1000 time = 7.09415340423584
I0123 12:28:27.376250 139785946451968 ddar.py:60] Depth 4/1000 time = 7.720679998397827
I0123 12:28:34.125175 139785946451968 ddar.py:60] Depth 5/1000 time = 6.748476266860962
I0123 12:28:42.874821 139785946451968 ddar.py:60] Depth 6/1000 time = 8.670733451843262
I0123 12:28:51.476476 139785946451968 ddar.py:60] Depth 7/1000 time = 8.601471662521362
I0123 12:28:51.483174 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:28:51.483290 139785946451968 alphageometry.py:566] LM output (score=-4.761248): "n : D a b a n 20 D a b b n 21 ;"
I0123 12:28:51.483334 139785946451968 alphageometry.py:567] Translation: "n = on_circle n a b, on_circle n b a"

I0123 12:28:51.483373 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n a b, on_circle n b a ? cong b k k m"
I0123 12:28:51.483537 139785946451968 graph.py:498] 
I0123 12:28:51.483597 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_circle n a b, on_circle n b a ? cong b k k m
I0123 12:28:55.088759 139785946451968 ddar.py:60] Depth 1/1000 time = 3.537879228591919
I0123 12:29:02.030199 139785946451968 ddar.py:60] Depth 2/1000 time = 6.941232919692993
I0123 12:29:09.807254 139785946451968 ddar.py:60] Depth 3/1000 time = 7.776869773864746
I0123 12:29:17.213933 139785946451968 ddar.py:60] Depth 4/1000 time = 7.406505584716797
I0123 12:29:25.918084 139785946451968 ddar.py:60] Depth 5/1000 time = 8.703954935073853
I0123 12:29:34.301604 139785946451968 ddar.py:60] Depth 6/1000 time = 8.383332014083862
I0123 12:29:42.711986 139785946451968 ddar.py:60] Depth 7/1000 time = 8.409971475601196
I0123 12:29:51.569208 139785946451968 ddar.py:60] Depth 8/1000 time = 8.855256080627441
I0123 12:30:01.485804 139785946451968 ddar.py:60] Depth 9/1000 time = 9.813178300857544
I0123 12:30:10.821874 139785946451968 ddar.py:60] Depth 10/1000 time = 9.335876226425171
I0123 12:30:20.751386 139785946451968 ddar.py:60] Depth 11/1000 time = 9.90442419052124
I0123 12:30:20.753612 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:30:20.753800 139785946451968 alphageometry.py:540] Depth 1. There are 32 nodes to expand:
I0123 12:30:20.753847 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : C a c n 20 T a c h n 21 ; x00
I0123 12:30:20.753883 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : C a g n 20 T a g h n 21 ; x00
I0123 12:30:20.753914 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D h i h n 20 T a c h n 21 ; x00
I0123 12:30:20.753944 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a c h n 20 ; x00
I0123 12:30:20.753975 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D h n h i 20 T a c h n 21 ; x00
I0123 12:30:20.754004 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a g h n 20 ; x00
I0123 12:30:20.754033 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : C a b n 20 D a n b n 21 ; x00
I0123 12:30:20.754073 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : C b g n 20 T b g h n 21 ; x00
I0123 12:30:20.754102 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : C g h n 20 D g n h n 21 ; x00
I0123 12:30:20.754130 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D h i h n 20 ; x00
I0123 12:30:20.754157 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a h h n 20 ; x00
I0123 12:30:20.754183 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D c h c n 20 T c h c n 21 ; x00
I0123 12:30:20.754211 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a g g n 20 ; x00
I0123 12:30:20.754240 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D h i h n 20 T a g h n 21 ; x00
I0123 12:30:20.754267 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D a b a n 20 T a b a n 21 ; x00
I0123 12:30:20.754299 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : P a b c n 20 ; x00
I0123 12:30:20.754327 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D c e e n 20 T c e e n 21 ; x00
I0123 12:30:20.754354 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T b c h n 20 ; x00
I0123 12:30:20.754381 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T h i h n 20 ; x00
I0123 12:30:20.754407 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D c d d n 20 ; x00
I0123 12:30:20.754434 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T b c b n 20 ; x00
I0123 12:30:20.754460 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a b b n 20 ; x00
I0123 12:30:20.754487 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D h n h i 20 T a g h n 21 ; x00
I0123 12:30:20.754513 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a b a n 20 ; x00
I0123 12:30:20.754542 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D h i h n 20 T h i h n 21 ; x00
I0123 12:30:20.754570 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : P a b c n 20 T a b a n 21 ; x00
I0123 12:30:20.754595 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a g f n 20 ; x00
I0123 12:30:20.754621 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a h a n 20 ; x00
I0123 12:30:20.754646 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T c h h n 20 ; x00
I0123 12:30:20.754670 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a g a n 20 ; x00
I0123 12:30:20.754693 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : T a b f n 20 ; x00
I0123 12:30:20.754715 139785946451968 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : D a b a n 20 D a b b n 21 ; x00
I0123 12:30:20.754743 139785946451968 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : C a c n 20 T a c h n 21 ; x00
I0123 12:30:30.650323 139785946451968 alphageometry.py:566] LM output (score=-0.094689): "o : C a g o 22 T a g h o 23 ;"
I0123 12:30:30.650473 139785946451968 alphageometry.py:567] Translation: "o = on_line o a g, on_tline o h a g"

I0123 12:30:30.650517 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_line o a g, on_tline o h a g ? cong b k k m"
I0123 12:30:30.650691 139785946451968 graph.py:498] 
I0123 12:30:30.650749 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_line o a g, on_tline o h a g ? cong b k k m
I0123 12:30:36.477421 139785946451968 ddar.py:60] Depth 1/1000 time = 5.768580436706543
I0123 12:30:46.910503 139785946451968 ddar.py:60] Depth 2/1000 time = 10.432917594909668
I0123 12:31:00.177654 139785946451968 ddar.py:60] Depth 3/1000 time = 13.266976594924927
I0123 12:31:14.257725 139785946451968 ddar.py:60] Depth 4/1000 time = 14.079883098602295
I0123 12:31:28.446230 139785946451968 ddar.py:60] Depth 5/1000 time = 14.187990427017212
I0123 12:31:45.147077 139785946451968 ddar.py:60] Depth 6/1000 time = 16.590388774871826
I0123 12:32:01.856267 139785946451968 ddar.py:60] Depth 7/1000 time = 16.708815097808838
I0123 12:32:18.971798 139785946451968 ddar.py:60] Depth 8/1000 time = 17.05199122428894
I0123 12:32:18.981280 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:32:18.981350 139785946451968 alphageometry.py:566] LM output (score=-1.309115): "o : D h o h n 22 T a g h o 23 ;"
I0123 12:32:18.981386 139785946451968 alphageometry.py:567] Translation: "o = on_circle o h n, on_tline o h a g"

I0123 12:32:18.981431 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o h n, on_tline o h a g ? cong b k k m"
I0123 12:32:18.981614 139785946451968 graph.py:498] 
I0123 12:32:18.981684 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o h n, on_tline o h a g ? cong b k k m
I0123 12:32:23.248783 139785946451968 ddar.py:60] Depth 1/1000 time = 4.2017340660095215
I0123 12:32:32.186568 139785946451968 ddar.py:60] Depth 2/1000 time = 8.93763279914856
I0123 12:32:43.691357 139785946451968 ddar.py:60] Depth 3/1000 time = 11.504605531692505
I0123 12:32:55.259935 139785946451968 ddar.py:60] Depth 4/1000 time = 11.568310737609863
I0123 12:33:06.052201 139785946451968 ddar.py:60] Depth 5/1000 time = 10.791651487350464
I0123 12:33:17.429561 139785946451968 ddar.py:60] Depth 6/1000 time = 11.375533103942871
I0123 12:33:28.998795 139785946451968 ddar.py:60] Depth 7/1000 time = 11.569036960601807
I0123 12:33:42.287211 139785946451968 ddar.py:60] Depth 8/1000 time = 13.28821349143982
I0123 12:33:54.963982 139785946451968 ddar.py:60] Depth 9/1000 time = 12.676566123962402
I0123 12:34:07.982833 139785946451968 ddar.py:60] Depth 10/1000 time = 13.018582344055176
I0123 12:34:21.094516 139785946451968 ddar.py:60] Depth 11/1000 time = 13.11135721206665
I0123 12:34:35.787118 139785946451968 ddar.py:60] Depth 12/1000 time = 14.692403078079224
I0123 12:34:50.260760 139785946451968 ddar.py:60] Depth 13/1000 time = 14.4734365940094
I0123 12:35:07.737630 139785946451968 ddar.py:60] Depth 14/1000 time = 17.348989725112915
I0123 12:35:24.941371 139785946451968 ddar.py:60] Depth 15/1000 time = 17.203355312347412
I0123 12:35:41.538899 139785946451968 ddar.py:60] Depth 16/1000 time = 16.56556749343872
I0123 12:35:58.720004 139785946451968 ddar.py:60] Depth 17/1000 time = 17.180700063705444
I0123 12:35:58.756059 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:35:58.756151 139785946451968 alphageometry.py:566] LM output (score=-1.543754): "o : D h n h o 22 T a g h o 23 ;"
I0123 12:35:58.756187 139785946451968 alphageometry.py:567] Translation: "o = on_circle o h n, on_tline o h a g"

I0123 12:35:58.756235 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o h n, on_tline o h a g ? cong b k k m"
I0123 12:35:58.756435 139785946451968 graph.py:498] 
I0123 12:35:58.756493 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o h n, on_tline o h a g ? cong b k k m
I0123 12:36:03.642317 139785946451968 ddar.py:60] Depth 1/1000 time = 4.813615798950195
I0123 12:36:12.937099 139785946451968 ddar.py:60] Depth 2/1000 time = 9.294615745544434
I0123 12:36:24.304311 139785946451968 ddar.py:60] Depth 3/1000 time = 11.36702275276184
I0123 12:36:35.050444 139785946451968 ddar.py:60] Depth 4/1000 time = 10.745901823043823
I0123 12:36:46.234252 139785946451968 ddar.py:60] Depth 5/1000 time = 11.1832275390625
I0123 12:36:58.182990 139785946451968 ddar.py:60] Depth 6/1000 time = 11.946868658065796
I0123 12:37:09.347493 139785946451968 ddar.py:60] Depth 7/1000 time = 11.164280414581299
I0123 12:37:22.762051 139785946451968 ddar.py:60] Depth 8/1000 time = 13.414353132247925
I0123 12:37:35.582630 139785946451968 ddar.py:60] Depth 9/1000 time = 12.82035493850708
I0123 12:37:48.692369 139785946451968 ddar.py:60] Depth 10/1000 time = 13.109472751617432
I0123 12:38:02.040279 139785946451968 ddar.py:60] Depth 11/1000 time = 13.347611904144287
I0123 12:38:16.827516 139785946451968 ddar.py:60] Depth 12/1000 time = 14.787044763565063
I0123 12:38:31.077524 139785946451968 ddar.py:60] Depth 13/1000 time = 14.249774694442749
I0123 12:38:48.213956 139785946451968 ddar.py:60] Depth 14/1000 time = 17.00745987892151
I0123 12:39:05.135255 139785946451968 ddar.py:60] Depth 15/1000 time = 16.920862913131714
I0123 12:39:21.390516 139785946451968 ddar.py:60] Depth 16/1000 time = 16.225006580352783
I0123 12:39:38.577362 139785946451968 ddar.py:60] Depth 17/1000 time = 17.18649959564209
I0123 12:39:38.613527 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:39:38.613592 139785946451968 alphageometry.py:566] LM output (score=-1.869593): "o : D h o h i 22 T a g h o 23 ;"
I0123 12:39:38.613625 139785946451968 alphageometry.py:567] Translation: "o = on_circle o h i, on_tline o h a g"

I0123 12:39:38.613692 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o h i, on_tline o h a g ? cong b k k m"
I0123 12:39:38.613881 139785946451968 graph.py:498] 
I0123 12:39:38.613943 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o h i, on_tline o h a g ? cong b k k m
I0123 12:39:43.443031 139785946451968 ddar.py:60] Depth 1/1000 time = 4.759545564651489
I0123 12:39:52.728801 139785946451968 ddar.py:60] Depth 2/1000 time = 9.285587549209595
I0123 12:40:03.926631 139785946451968 ddar.py:60] Depth 3/1000 time = 11.197643041610718
I0123 12:40:15.190739 139785946451968 ddar.py:60] Depth 4/1000 time = 11.263896703720093
I0123 12:40:26.569656 139785946451968 ddar.py:60] Depth 5/1000 time = 11.37847089767456
I0123 12:40:37.520603 139785946451968 ddar.py:60] Depth 6/1000 time = 10.949021100997925
I0123 12:40:49.200368 139785946451968 ddar.py:60] Depth 7/1000 time = 11.679472208023071
I0123 12:41:01.437198 139785946451968 ddar.py:60] Depth 8/1000 time = 12.23663854598999
I0123 12:41:14.480252 139785946451968 ddar.py:60] Depth 9/1000 time = 13.04278302192688
I0123 12:41:27.514357 139785946451968 ddar.py:60] Depth 10/1000 time = 13.03380298614502
I0123 12:41:41.329514 139785946451968 ddar.py:60] Depth 11/1000 time = 13.814980745315552
I0123 12:41:54.724045 139785946451968 ddar.py:60] Depth 12/1000 time = 13.394285202026367
I0123 12:42:09.093177 139785946451968 ddar.py:60] Depth 13/1000 time = 14.368778228759766
I0123 12:42:26.893401 139785946451968 ddar.py:60] Depth 14/1000 time = 17.682741165161133
I0123 12:42:43.355534 139785946451968 ddar.py:60] Depth 15/1000 time = 16.461802005767822
I0123 12:42:43.429292 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:43.429354 139785946451968 alphageometry.py:566] LM output (score=-2.298958): "o : D h i h o 22 T a g h o 23 ;"
I0123 12:42:43.429390 139785946451968 alphageometry.py:567] Translation: "o = on_circle o h i, on_tline o h a g"

I0123 12:42:43.429429 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o h i, on_tline o h a g ? cong b k k m"
I0123 12:42:43.429806 139785946451968 graph.py:498] 
I0123 12:42:43.429868 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o h i, on_tline o h a g ? cong b k k m
I0123 12:42:48.882674 139785946451968 ddar.py:60] Depth 1/1000 time = 5.387606620788574
I0123 12:42:57.674685 139785946451968 ddar.py:60] Depth 2/1000 time = 8.791805982589722
I0123 12:43:09.374962 139785946451968 ddar.py:60] Depth 3/1000 time = 11.700052738189697
I0123 12:43:21.025607 139785946451968 ddar.py:60] Depth 4/1000 time = 11.65033483505249
I0123 12:43:32.816108 139785946451968 ddar.py:60] Depth 5/1000 time = 11.790011405944824
I0123 12:43:44.009661 139785946451968 ddar.py:60] Depth 6/1000 time = 11.193103551864624
I0123 12:43:55.706828 139785946451968 ddar.py:60] Depth 7/1000 time = 11.696859359741211
I0123 12:44:09.963379 139785946451968 ddar.py:60] Depth 8/1000 time = 14.148761749267578
I0123 12:44:22.803035 139785946451968 ddar.py:60] Depth 9/1000 time = 12.839377403259277
I0123 12:44:22.828320 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:44:22.828424 139785946451968 alphageometry.py:566] LM output (score=-3.184688): "o : T a g h o 22 ;"
I0123 12:44:22.828459 139785946451968 alphageometry.py:567] Translation: "o = on_tline o h a g"

I0123 12:44:22.828511 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_tline o h a g ? cong b k k m"
I0123 12:44:22.828702 139785946451968 graph.py:498] 
I0123 12:44:22.828760 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_tline o h a g ? cong b k k m
I0123 12:44:28.192336 139785946451968 ddar.py:60] Depth 1/1000 time = 5.303975343704224
I0123 12:44:36.643096 139785946451968 ddar.py:60] Depth 2/1000 time = 8.450605154037476
I0123 12:44:47.152366 139785946451968 ddar.py:60] Depth 3/1000 time = 10.509084939956665
I0123 12:44:57.971989 139785946451968 ddar.py:60] Depth 4/1000 time = 10.8193519115448
I0123 12:45:08.910692 139785946451968 ddar.py:60] Depth 5/1000 time = 10.937865734100342
I0123 12:45:20.076413 139785946451968 ddar.py:60] Depth 6/1000 time = 11.160971641540527
I0123 12:45:31.848393 139785946451968 ddar.py:60] Depth 7/1000 time = 11.681364059448242
I0123 12:45:44.079598 139785946451968 ddar.py:60] Depth 8/1000 time = 12.231013298034668
I0123 12:45:55.731628 139785946451968 ddar.py:60] Depth 9/1000 time = 11.638249397277832
I0123 12:45:55.735771 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:45:55.735851 139785946451968 alphageometry.py:566] LM output (score=-3.389687): "o : D a n n o 22 D a m m o 23 ;"
I0123 12:45:55.735886 139785946451968 alphageometry.py:567] Translation: "o = on_circle o n a, on_circle o m a"

I0123 12:45:55.735935 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n a, on_circle o m a ? cong b k k m"
I0123 12:45:55.736121 139785946451968 graph.py:498] 
I0123 12:45:55.736179 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n a, on_circle o m a ? cong b k k m
I0123 12:46:00.859557 139785946451968 ddar.py:60] Depth 1/1000 time = 5.038659334182739
I0123 12:46:10.774985 139785946451968 ddar.py:60] Depth 2/1000 time = 9.915205240249634
I0123 12:46:21.856965 139785946451968 ddar.py:60] Depth 3/1000 time = 11.081689834594727
I0123 12:46:33.054934 139785946451968 ddar.py:60] Depth 4/1000 time = 11.197722673416138
I0123 12:46:44.162678 139785946451968 ddar.py:60] Depth 5/1000 time = 11.10721206665039
I0123 12:46:57.150001 139785946451968 ddar.py:60] Depth 6/1000 time = 12.890523433685303
I0123 12:47:10.005991 139785946451968 ddar.py:60] Depth 7/1000 time = 12.855724096298218
I0123 12:47:22.941595 139785946451968 ddar.py:60] Depth 8/1000 time = 12.919295072555542
I0123 12:47:22.945824 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:47:22.945895 139785946451968 alphageometry.py:566] LM output (score=-3.449615): "o : D a n n o 22 T a n n o 23 ;"
I0123 12:47:22.945939 139785946451968 alphageometry.py:567] Translation: "o = on_circle o n a, on_tline o n a n"

I0123 12:47:22.945979 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n a, on_tline o n a n ? cong b k k m"
I0123 12:47:22.946159 139785946451968 graph.py:498] 
I0123 12:47:22.946218 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n a, on_tline o n a n ? cong b k k m
I0123 12:47:28.764069 139785946451968 ddar.py:60] Depth 1/1000 time = 5.745688199996948
I0123 12:47:39.256211 139785946451968 ddar.py:60] Depth 2/1000 time = 10.491962671279907
I0123 12:47:50.743885 139785946451968 ddar.py:60] Depth 3/1000 time = 11.487462043762207
I0123 12:48:02.834820 139785946451968 ddar.py:60] Depth 4/1000 time = 12.090659856796265
I0123 12:48:14.921597 139785946451968 ddar.py:60] Depth 5/1000 time = 12.086134672164917
I0123 12:48:27.758795 139785946451968 ddar.py:60] Depth 6/1000 time = 12.836309671401978
I0123 12:48:40.627454 139785946451968 ddar.py:60] Depth 7/1000 time = 12.786593914031982
I0123 12:48:53.956705 139785946451968 ddar.py:60] Depth 8/1000 time = 13.328932285308838
I0123 12:49:07.359779 139785946451968 ddar.py:60] Depth 9/1000 time = 13.38942551612854
I0123 12:49:07.364310 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:49:07.364367 139785946451968 alphageometry.py:566] LM output (score=-3.521112): "o : D a b a o 22 T a b a o 23 ;"
I0123 12:49:07.364402 139785946451968 alphageometry.py:567] Translation: "o = on_circle o a b, on_tline o a a b"

I0123 12:49:07.364448 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o a b, on_tline o a a b ? cong b k k m"
I0123 12:49:07.364622 139785946451968 graph.py:498] 
I0123 12:49:07.364681 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o a b, on_tline o a a b ? cong b k k m
I0123 12:49:12.461029 139785946451968 ddar.py:60] Depth 1/1000 time = 5.017934083938599
I0123 12:49:22.038163 139785946451968 ddar.py:60] Depth 2/1000 time = 9.576944828033447
I0123 12:49:33.889402 139785946451968 ddar.py:60] Depth 3/1000 time = 11.85106897354126
I0123 12:49:46.043552 139785946451968 ddar.py:60] Depth 4/1000 time = 12.153968572616577
I0123 12:49:58.960950 139785946451968 ddar.py:60] Depth 5/1000 time = 12.917187452316284
I0123 12:50:10.688134 139785946451968 ddar.py:60] Depth 6/1000 time = 11.7267165184021
I0123 12:50:22.865316 139785946451968 ddar.py:60] Depth 7/1000 time = 12.175971984863281
I0123 12:50:36.559088 139785946451968 ddar.py:60] Depth 8/1000 time = 13.592140674591064
I0123 12:50:50.810220 139785946451968 ddar.py:60] Depth 9/1000 time = 14.250841856002808
I0123 12:51:04.731954 139785946451968 ddar.py:60] Depth 10/1000 time = 13.906213760375977
I0123 12:51:18.138280 139785946451968 ddar.py:60] Depth 11/1000 time = 13.38518500328064
I0123 12:51:18.142401 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:51:18.142458 139785946451968 alphageometry.py:566] LM output (score=-3.578530): "o : D f o i o 22 ^ i f i o f o f i 23 ;"
I0123 12:51:18.142492 139785946451968 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ i f i o f o f i"

I0123 12:51:18.142528 139785946451968 alphageometry.py:566] LM output (score=-3.730893): "o : D a b b o 22 T a b b o 23 ;"
I0123 12:51:18.142555 139785946451968 alphageometry.py:567] Translation: "o = on_circle o b a, on_tline o b a b"

I0123 12:51:18.142592 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o b a, on_tline o b a b ? cong b k k m"
I0123 12:51:18.142765 139785946451968 graph.py:498] 
I0123 12:51:18.142822 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o b a, on_tline o b a b ? cong b k k m
I0123 12:51:23.288922 139785946451968 ddar.py:60] Depth 1/1000 time = 5.082239389419556
I0123 12:51:32.655055 139785946451968 ddar.py:60] Depth 2/1000 time = 9.365954399108887
I0123 12:51:44.534317 139785946451968 ddar.py:60] Depth 3/1000 time = 11.879045963287354
I0123 12:51:55.794229 139785946451968 ddar.py:60] Depth 4/1000 time = 11.259687662124634
I0123 12:52:08.340319 139785946451968 ddar.py:60] Depth 5/1000 time = 12.545560598373413
I0123 12:52:19.778579 139785946451968 ddar.py:60] Depth 6/1000 time = 11.436992168426514
I0123 12:52:32.374213 139785946451968 ddar.py:60] Depth 7/1000 time = 12.496304750442505
I0123 12:52:45.575091 139785946451968 ddar.py:60] Depth 8/1000 time = 13.200588941574097
I0123 12:52:58.232375 139785946451968 ddar.py:60] Depth 9/1000 time = 12.642408609390259
I0123 12:52:58.236326 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:52:58.236388 139785946451968 alphageometry.py:566] LM output (score=-3.836123): "o : D e o h n 22 P e o h n 23 ;"
I0123 12:52:58.236423 139785946451968 alphageometry.py:567] Translation: "o = eqdistance o e h n, on_pline o e h n"

I0123 12:52:58.236464 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = eqdistance o e h n, on_pline o e h n ? cong b k k m"
I0123 12:52:58.236639 139785946451968 graph.py:498] 
I0123 12:52:58.236696 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = eqdistance o e h n, on_pline o e h n ? cong b k k m
I0123 12:53:03.517874 139785946451968 ddar.py:60] Depth 1/1000 time = 5.214487075805664
I0123 12:53:13.516586 139785946451968 ddar.py:60] Depth 2/1000 time = 9.998532772064209
I0123 12:53:26.169260 139785946451968 ddar.py:60] Depth 3/1000 time = 12.652469873428345
I0123 12:53:39.640844 139785946451968 ddar.py:60] Depth 4/1000 time = 13.471399068832397
I0123 12:53:53.118233 139785946451968 ddar.py:60] Depth 5/1000 time = 13.476820945739746
I0123 12:54:08.734974 139785946451968 ddar.py:60] Depth 6/1000 time = 15.524619579315186
I0123 12:54:23.168074 139785946451968 ddar.py:60] Depth 7/1000 time = 14.432907819747925
I0123 12:54:37.682514 139785946451968 ddar.py:60] Depth 8/1000 time = 14.499885320663452
I0123 12:54:37.687104 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:54:37.687165 139785946451968 alphageometry.py:566] LM output (score=-3.891841): "o : D n l n o 22 T n l n o 23 ;"
I0123 12:54:37.687198 139785946451968 alphageometry.py:567] Translation: "o = on_circle o n l, on_tline o n n l"

I0123 12:54:37.687243 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n l, on_tline o n n l ? cong b k k m"
I0123 12:54:37.687417 139785946451968 graph.py:498] 
I0123 12:54:37.687475 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n l, on_tline o n n l ? cong b k k m
I0123 12:54:42.315264 139785946451968 ddar.py:60] Depth 1/1000 time = 4.563531398773193
I0123 12:54:51.371798 139785946451968 ddar.py:60] Depth 2/1000 time = 9.056337118148804
I0123 12:55:02.109125 139785946451968 ddar.py:60] Depth 3/1000 time = 10.737058162689209
I0123 12:55:12.396898 139785946451968 ddar.py:60] Depth 4/1000 time = 10.287532091140747
I0123 12:55:23.179790 139785946451968 ddar.py:60] Depth 5/1000 time = 10.782322645187378
I0123 12:55:34.725267 139785946451968 ddar.py:60] Depth 6/1000 time = 11.544713020324707
I0123 12:55:47.035570 139785946451968 ddar.py:60] Depth 7/1000 time = 12.218300819396973
I0123 12:55:59.127875 139785946451968 ddar.py:60] Depth 8/1000 time = 12.09211254119873
I0123 12:56:11.360452 139785946451968 ddar.py:60] Depth 9/1000 time = 12.217031955718994
I0123 12:56:11.365380 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:56:11.365473 139785946451968 alphageometry.py:566] LM output (score=-3.917894): "o : D n k n o 22 T n k n o 23 ;"
I0123 12:56:11.365509 139785946451968 alphageometry.py:567] Translation: "o = on_circle o n k, on_tline o n n k"

I0123 12:56:11.365558 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n k, on_tline o n n k ? cong b k k m"
I0123 12:56:11.365764 139785946451968 graph.py:498] 
I0123 12:56:11.365823 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n k, on_tline o n n k ? cong b k k m
I0123 12:56:16.633153 139785946451968 ddar.py:60] Depth 1/1000 time = 5.199135780334473
I0123 12:56:25.320309 139785946451968 ddar.py:60] Depth 2/1000 time = 8.686996936798096
I0123 12:56:36.721311 139785946451968 ddar.py:60] Depth 3/1000 time = 11.400822401046753
I0123 12:56:46.974625 139785946451968 ddar.py:60] Depth 4/1000 time = 10.253111362457275
I0123 12:56:57.840656 139785946451968 ddar.py:60] Depth 5/1000 time = 10.865586996078491
I0123 12:57:09.499538 139785946451968 ddar.py:60] Depth 6/1000 time = 11.658114910125732
I0123 12:57:21.031959 139785946451968 ddar.py:60] Depth 7/1000 time = 11.440354108810425
I0123 12:57:33.828390 139785946451968 ddar.py:60] Depth 8/1000 time = 12.796185970306396
I0123 12:57:45.937086 139785946451968 ddar.py:60] Depth 9/1000 time = 12.093597650527954
I0123 12:57:45.941171 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:57:45.941231 139785946451968 alphageometry.py:566] LM output (score=-3.930501): "o : D c h c o 22 T c h c o 23 ;"
I0123 12:57:45.941266 139785946451968 alphageometry.py:567] Translation: "o = on_circle o c h, on_tline o c c h"

I0123 12:57:45.941307 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o c h, on_tline o c c h ? cong b k k m"
I0123 12:57:45.941485 139785946451968 graph.py:498] 
I0123 12:57:45.941542 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o c h, on_tline o c c h ? cong b k k m
I0123 12:57:51.248082 139785946451968 ddar.py:60] Depth 1/1000 time = 5.231235027313232
I0123 12:58:00.017925 139785946451968 ddar.py:60] Depth 2/1000 time = 8.76967453956604
I0123 12:58:10.493042 139785946451968 ddar.py:60] Depth 3/1000 time = 10.47494649887085
I0123 12:58:21.903202 139785946451968 ddar.py:60] Depth 4/1000 time = 11.409951210021973
I0123 12:58:32.009196 139785946451968 ddar.py:60] Depth 5/1000 time = 10.105554580688477
I0123 12:58:42.794188 139785946451968 ddar.py:60] Depth 6/1000 time = 10.784210443496704
I0123 12:58:54.922627 139785946451968 ddar.py:60] Depth 7/1000 time = 12.025457859039307
I0123 12:59:07.645609 139785946451968 ddar.py:60] Depth 8/1000 time = 12.722707986831665
I0123 12:59:07.661429 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:59:07.661490 139785946451968 alphageometry.py:566] LM output (score=-3.936252): "o : D a o b o 22 ;"
I0123 12:59:07.661525 139785946451968 alphageometry.py:567] Translation: "o = on_bline o b a"

I0123 12:59:07.661566 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_bline o b a ? cong b k k m"
I0123 12:59:07.661747 139785946451968 graph.py:498] 
I0123 12:59:07.661807 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_bline o b a ? cong b k k m
I0123 12:59:12.462316 139785946451968 ddar.py:60] Depth 1/1000 time = 4.738156080245972
I0123 12:59:22.062324 139785946451968 ddar.py:60] Depth 2/1000 time = 9.599835872650146
I0123 12:59:32.803409 139785946451968 ddar.py:60] Depth 3/1000 time = 10.74089765548706
I0123 12:59:45.279345 139785946451968 ddar.py:60] Depth 4/1000 time = 12.475759506225586
I0123 12:59:58.105535 139785946451968 ddar.py:60] Depth 5/1000 time = 12.825966119766235
I0123 13:00:10.313636 139785946451968 ddar.py:60] Depth 6/1000 time = 12.207584619522095
I0123 13:00:24.155519 139785946451968 ddar.py:60] Depth 7/1000 time = 13.722448110580444
I0123 13:00:38.449028 139785946451968 ddar.py:60] Depth 8/1000 time = 14.293238639831543
I0123 13:00:52.151475 139785946451968 ddar.py:60] Depth 9/1000 time = 13.686819314956665
I0123 13:01:06.572736 139785946451968 ddar.py:60] Depth 10/1000 time = 14.399479627609253
I0123 13:01:06.576687 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:01:06.576747 139785946451968 alphageometry.py:566] LM output (score=-3.941877): "o : C b g o 22 T b g h o 23 ;"
I0123 13:01:06.576781 139785946451968 alphageometry.py:567] Translation: "o = on_line o b g, on_tline o h b g"

I0123 13:01:06.576818 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_line o b g, on_tline o h b g ? cong b k k m"
I0123 13:01:06.576988 139785946451968 graph.py:498] 
I0123 13:01:06.577045 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_line o b g, on_tline o h b g ? cong b k k m
I0123 13:01:11.982562 139785946451968 ddar.py:60] Depth 1/1000 time = 5.353410959243774
I0123 13:01:23.439126 139785946451968 ddar.py:60] Depth 2/1000 time = 11.456395387649536
I0123 13:01:35.472053 139785946451968 ddar.py:60] Depth 3/1000 time = 12.0327308177948
I0123 13:01:49.018211 139785946451968 ddar.py:60] Depth 4/1000 time = 13.545934200286865
I0123 13:02:01.202780 139785946451968 ddar.py:60] Depth 5/1000 time = 12.184123992919922
I0123 13:02:16.080095 139785946451968 ddar.py:60] Depth 6/1000 time = 14.782057523727417
I0123 13:02:30.253767 139785946451968 ddar.py:60] Depth 7/1000 time = 14.173469543457031
I0123 13:02:44.441264 139785946451968 ddar.py:60] Depth 8/1000 time = 14.162996292114258
I0123 13:02:44.444837 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:02:44.444893 139785946451968 alphageometry.py:566] LM output (score=-4.002340): "o : D a b a o 22 D a b b o 23 ;"
I0123 13:02:44.444928 139785946451968 alphageometry.py:567] Translation: "o = on_circle o a b, on_circle o b a"

I0123 13:02:44.444969 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o a b, on_circle o b a ? cong b k k m"
I0123 13:02:44.445142 139785946451968 graph.py:498] 
I0123 13:02:44.445199 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o a b, on_circle o b a ? cong b k k m
I0123 13:02:49.975604 139785946451968 ddar.py:60] Depth 1/1000 time = 5.460586786270142
I0123 13:03:00.199995 139785946451968 ddar.py:60] Depth 2/1000 time = 10.224178075790405
I0123 13:03:11.821741 139785946451968 ddar.py:60] Depth 3/1000 time = 11.621492385864258
I0123 13:03:24.915474 139785946451968 ddar.py:60] Depth 4/1000 time = 13.093561172485352
I0123 13:03:37.633429 139785946451968 ddar.py:60] Depth 5/1000 time = 12.717730522155762
I0123 13:03:50.378411 139785946451968 ddar.py:60] Depth 6/1000 time = 12.744426965713501
I0123 13:04:03.090417 139785946451968 ddar.py:60] Depth 7/1000 time = 12.709896802902222
I0123 13:04:17.175276 139785946451968 ddar.py:60] Depth 8/1000 time = 13.967716932296753
I0123 13:04:31.021290 139785946451968 ddar.py:60] Depth 9/1000 time = 13.845796346664429
I0123 13:04:45.712314 139785946451968 ddar.py:60] Depth 10/1000 time = 14.67509412765503
I0123 13:05:00.490997 139785946451968 ddar.py:60] Depth 11/1000 time = 14.756827592849731
I0123 13:05:00.496371 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:05:00.496434 139785946451968 alphageometry.py:566] LM output (score=-4.026296): "o : D f o i o 22 D f o n o 23 ;"
I0123 13:05:00.496468 139785946451968 alphageometry.py:567] Translation: "o = on_bline o i f, on_bline o n f"

I0123 13:05:00.496510 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_bline o i f, on_bline o n f ? cong b k k m"
I0123 13:05:00.496681 139785946451968 graph.py:498] 
I0123 13:05:00.496739 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_bline o i f, on_bline o n f ? cong b k k m
I0123 13:05:05.472488 139785946451968 ddar.py:60] Depth 1/1000 time = 4.903775930404663
I0123 13:05:15.499936 139785946451968 ddar.py:60] Depth 2/1000 time = 10.027282238006592
I0123 13:05:27.512979 139785946451968 ddar.py:60] Depth 3/1000 time = 12.012850284576416
I0123 13:05:40.521715 139785946451968 ddar.py:60] Depth 4/1000 time = 13.008475303649902
I0123 13:05:53.060806 139785946451968 ddar.py:60] Depth 5/1000 time = 12.538781642913818
I0123 13:06:05.608066 139785946451968 ddar.py:60] Depth 6/1000 time = 12.547075986862183
I0123 13:06:18.065876 139785946451968 ddar.py:60] Depth 7/1000 time = 12.457248210906982
I0123 13:06:32.097831 139785946451968 ddar.py:60] Depth 8/1000 time = 13.921324729919434
I0123 13:06:46.672227 139785946451968 ddar.py:60] Depth 9/1000 time = 14.574185132980347
I0123 13:07:00.485969 139785946451968 ddar.py:60] Depth 10/1000 time = 13.782891511917114
I0123 13:07:00.493759 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:07:00.493819 139785946451968 alphageometry.py:566] LM output (score=-4.058034): "o : D c e e o 22 T c e e o 23 ;"
I0123 13:07:00.493854 139785946451968 alphageometry.py:567] Translation: "o = on_circle o e c, on_tline o e c e"

I0123 13:07:00.493896 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o e c, on_tline o e c e ? cong b k k m"
I0123 13:07:00.494073 139785946451968 graph.py:498] 
I0123 13:07:00.494133 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o e c, on_tline o e c e ? cong b k k m
I0123 13:07:06.035407 139785946451968 ddar.py:60] Depth 1/1000 time = 5.472858428955078
I0123 13:07:15.737652 139785946451968 ddar.py:60] Depth 2/1000 time = 9.702051877975464
I0123 13:07:28.628429 139785946451968 ddar.py:60] Depth 3/1000 time = 12.890575647354126
I0123 13:07:41.654537 139785946451968 ddar.py:60] Depth 4/1000 time = 13.025834083557129
I0123 13:07:54.601570 139785946451968 ddar.py:60] Depth 5/1000 time = 12.946407079696655
I0123 13:08:07.681651 139785946451968 ddar.py:60] Depth 6/1000 time = 13.078551530838013
I0123 13:08:22.004581 139785946451968 ddar.py:60] Depth 7/1000 time = 14.229000329971313
I0123 13:08:36.261781 139785946451968 ddar.py:60] Depth 8/1000 time = 14.256909847259521
I0123 13:08:50.493856 139785946451968 ddar.py:60] Depth 9/1000 time = 14.216257572174072
I0123 13:08:50.497839 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:08:50.497899 139785946451968 alphageometry.py:566] LM output (score=-4.068612): "o : C a h o 22 D a o h o 23 ;"
I0123 13:08:50.497934 139785946451968 alphageometry.py:567] Translation: "o = on_line o a h, on_bline o h a"

I0123 13:08:50.497972 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_line o a h, on_bline o h a ? cong b k k m"
I0123 13:08:50.498144 139785946451968 graph.py:498] 
I0123 13:08:50.498201 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_line o a h, on_bline o h a ? cong b k k m
I0123 13:08:56.077927 139785946451968 ddar.py:60] Depth 1/1000 time = 5.516923189163208
I0123 13:09:06.058695 139785946451968 ddar.py:60] Depth 2/1000 time = 9.98058295249939
I0123 13:09:17.502731 139785946451968 ddar.py:60] Depth 3/1000 time = 11.44384479522705
I0123 13:09:29.219916 139785946451968 ddar.py:60] Depth 4/1000 time = 11.71696949005127
I0123 13:09:40.903238 139785946451968 ddar.py:60] Depth 5/1000 time = 11.682854890823364
I0123 13:09:55.407805 139785946451968 ddar.py:60] Depth 6/1000 time = 14.417962789535522
I0123 13:10:09.127572 139785946451968 ddar.py:60] Depth 7/1000 time = 13.71955156326294
I0123 13:10:22.890424 139785946451968 ddar.py:60] Depth 8/1000 time = 13.738592863082886
I0123 13:10:22.895084 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:10:22.895173 139785946451968 alphageometry.py:566] LM output (score=-4.112582): "o : D e h e o 22 T e h e o 23 ;"
I0123 13:10:22.895209 139785946451968 alphageometry.py:567] Translation: "o = on_circle o e h, on_tline o e e h"

I0123 13:10:22.895258 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o e h, on_tline o e e h ? cong b k k m"
I0123 13:10:22.895458 139785946451968 graph.py:498] 
I0123 13:10:22.895517 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o e h, on_tline o e e h ? cong b k k m
I0123 13:10:28.505529 139785946451968 ddar.py:60] Depth 1/1000 time = 5.545352458953857
I0123 13:10:37.777725 139785946451968 ddar.py:60] Depth 2/1000 time = 9.272020101547241
I0123 13:10:48.282705 139785946451968 ddar.py:60] Depth 3/1000 time = 10.504798173904419
I0123 13:10:58.827530 139785946451968 ddar.py:60] Depth 4/1000 time = 10.544611692428589
I0123 13:11:10.237986 139785946451968 ddar.py:60] Depth 5/1000 time = 11.410022020339966
I0123 13:11:20.956731 139785946451968 ddar.py:60] Depth 6/1000 time = 10.717990159988403
I0123 13:11:32.847163 139785946451968 ddar.py:60] Depth 7/1000 time = 11.798559188842773
I0123 13:11:44.555188 139785946451968 ddar.py:60] Depth 8/1000 time = 11.707780838012695
I0123 13:11:56.944267 139785946451968 ddar.py:60] Depth 9/1000 time = 12.373290300369263
I0123 13:11:56.948030 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:11:56.948100 139785946451968 alphageometry.py:566] LM output (score=-4.149279): "o : D e k e o 22 T e k e o 23 ;"
I0123 13:11:56.948135 139785946451968 alphageometry.py:567] Translation: "o = on_circle o e k, on_tline o e e k"

I0123 13:11:56.948179 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o e k, on_tline o e e k ? cong b k k m"
I0123 13:11:56.948358 139785946451968 graph.py:498] 
I0123 13:11:56.948417 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o e k, on_tline o e e k ? cong b k k m
I0123 13:12:02.564361 139785946451968 ddar.py:60] Depth 1/1000 time = 5.5518176555633545
I0123 13:12:11.522557 139785946451968 ddar.py:60] Depth 2/1000 time = 8.957970380783081
I0123 13:12:22.850750 139785946451968 ddar.py:60] Depth 3/1000 time = 11.32796859741211
I0123 13:12:33.544318 139785946451968 ddar.py:60] Depth 4/1000 time = 10.693283796310425
I0123 13:12:45.109966 139785946451968 ddar.py:60] Depth 5/1000 time = 11.565231323242188
I0123 13:12:55.954776 139785946451968 ddar.py:60] Depth 6/1000 time = 10.843942165374756
I0123 13:13:08.199695 139785946451968 ddar.py:60] Depth 7/1000 time = 12.153502225875854
I0123 13:13:20.118686 139785946451968 ddar.py:60] Depth 8/1000 time = 11.918739795684814
I0123 13:13:32.844887 139785946451968 ddar.py:60] Depth 9/1000 time = 12.711097478866577
I0123 13:13:32.848381 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:13:32.848440 139785946451968 alphageometry.py:566] LM output (score=-4.163160): "o : C a b o 22 D a o b o 23 ;"
I0123 13:13:32.848475 139785946451968 alphageometry.py:567] Translation: "o = on_line o a b, on_bline o b a"

I0123 13:13:32.848512 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_line o a b, on_bline o b a ? cong b k k m"
I0123 13:13:32.848685 139785946451968 graph.py:498] 
I0123 13:13:32.848742 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_line o a b, on_bline o b a ? cong b k k m
I0123 13:13:38.642627 139785946451968 ddar.py:60] Depth 1/1000 time = 5.714743614196777
I0123 13:13:50.398790 139785946451968 ddar.py:60] Depth 2/1000 time = 11.755960941314697
I0123 13:14:05.109178 139785946451968 ddar.py:60] Depth 3/1000 time = 14.710147380828857
I0123 13:14:20.335456 139785946451968 ddar.py:60] Depth 4/1000 time = 15.225975751876831
I0123 13:14:34.795949 139785946451968 ddar.py:60] Depth 5/1000 time = 14.45990514755249
I0123 13:14:51.223981 139785946451968 ddar.py:60] Depth 6/1000 time = 16.32739520072937
I0123 13:15:08.371702 139785946451968 ddar.py:60] Depth 7/1000 time = 17.14737296104431
I0123 13:15:25.090051 139785946451968 ddar.py:60] Depth 8/1000 time = 16.701161861419678
I0123 13:15:40.971877 139785946451968 ddar.py:60] Depth 9/1000 time = 15.865162134170532
I0123 13:15:40.976047 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:15:40.976119 139785946451968 alphageometry.py:566] LM output (score=-4.166055): "o : D e g e o 22 T e g e o 23 ;"
I0123 13:15:40.976154 139785946451968 alphageometry.py:567] Translation: "o = on_circle o e g, on_tline o e e g"

I0123 13:15:40.976192 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o e g, on_tline o e e g ? cong b k k m"
I0123 13:15:40.976371 139785946451968 graph.py:498] 
I0123 13:15:40.976427 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o e g, on_tline o e e g ? cong b k k m
I0123 13:15:46.023076 139785946451968 ddar.py:60] Depth 1/1000 time = 4.983248949050903
I0123 13:15:55.311932 139785946451968 ddar.py:60] Depth 2/1000 time = 9.288644313812256
I0123 13:16:07.004912 139785946451968 ddar.py:60] Depth 3/1000 time = 11.692751407623291
I0123 13:16:17.825911 139785946451968 ddar.py:60] Depth 4/1000 time = 10.82071590423584
I0123 13:16:28.731743 139785946451968 ddar.py:60] Depth 5/1000 time = 10.905401468276978
I0123 13:16:39.519511 139785946451968 ddar.py:60] Depth 6/1000 time = 10.786991119384766
I0123 13:16:52.470143 139785946451968 ddar.py:60] Depth 7/1000 time = 12.854623079299927
I0123 13:17:04.572490 139785946451968 ddar.py:60] Depth 8/1000 time = 12.102088689804077
I0123 13:17:17.462283 139785946451968 ddar.py:60] Depth 9/1000 time = 12.874547958374023
I0123 13:17:17.466411 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:17:17.466476 139785946451968 alphageometry.py:566] LM output (score=-4.178098): "o : D i k o k 22 T a g h o 23 ;"
I0123 13:17:17.466510 139785946451968 alphageometry.py:567] Translation: "o = on_circle o k i, on_tline o h a g"

I0123 13:17:17.466551 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o k i, on_tline o h a g ? cong b k k m"
I0123 13:17:17.466729 139785946451968 graph.py:498] 
I0123 13:17:17.466787 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o k i, on_tline o h a g ? cong b k k m
I0123 13:17:22.530913 139785946451968 ddar.py:60] Depth 1/1000 time = 4.970522403717041
I0123 13:17:31.731770 139785946451968 ddar.py:60] Depth 2/1000 time = 9.200578451156616
I0123 13:17:43.100096 139785946451968 ddar.py:60] Depth 3/1000 time = 11.368098497390747
I0123 13:17:55.021573 139785946451968 ddar.py:60] Depth 4/1000 time = 11.921185493469238
I0123 13:18:06.380020 139785946451968 ddar.py:60] Depth 5/1000 time = 11.358001947402954
I0123 13:18:17.627820 139785946451968 ddar.py:60] Depth 6/1000 time = 11.242119789123535
I0123 13:18:31.115008 139785946451968 ddar.py:60] Depth 7/1000 time = 13.394994258880615
I0123 13:18:44.365506 139785946451968 ddar.py:60] Depth 8/1000 time = 13.25027847290039
I0123 13:18:56.936514 139785946451968 ddar.py:60] Depth 9/1000 time = 12.556782722473145
I0123 13:18:56.940153 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:18:56.940219 139785946451968 alphageometry.py:566] LM output (score=-4.219975): "o : D a n n o 22 D a n n o 23 ;"
I0123 13:18:56.940253 139785946451968 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2622, in add_clause
    nums = draw_fn()
  File "/home/chi/alphageometry-test/graph.py", line 2608, in draw_fn
    return nm.reduce(to_be_intersected, existing_points)
  File "/home/chi/alphageometry-test/numericals.py", line 1310, in reduce
    result = a.intersect(b)
  File "/home/chi/alphageometry-test/numericals.py", line 426, in intersect
    return circle_circle_intersection(self, obj)
  File "/home/chi/alphageometry-test/numericals.py", line 490, in circle_circle_intersection
    raise InvalidQuadSolveError()
numericals.InvalidQuadSolveError
"

I0123 13:18:56.940410 139785946451968 alphageometry.py:566] LM output (score=-4.296855): "o : D n j n o 22 T n j n o 23 ;"
I0123 13:18:56.940441 139785946451968 alphageometry.py:567] Translation: "o = on_circle o n j, on_tline o n n j"

I0123 13:18:56.940472 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n j, on_tline o n n j ? cong b k k m"
I0123 13:18:56.940653 139785946451968 graph.py:498] 
I0123 13:18:56.940712 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n j, on_tline o n n j ? cong b k k m
I0123 13:19:02.755412 139785946451968 ddar.py:60] Depth 1/1000 time = 5.7501115798950195
I0123 13:19:11.080473 139785946451968 ddar.py:60] Depth 2/1000 time = 8.32484769821167
I0123 13:19:22.481528 139785946451968 ddar.py:60] Depth 3/1000 time = 11.400815725326538
I0123 13:19:33.472481 139785946451968 ddar.py:60] Depth 4/1000 time = 10.990421056747437
I0123 13:19:44.572568 139785946451968 ddar.py:60] Depth 5/1000 time = 11.099631309509277
I0123 13:19:54.698472 139785946451968 ddar.py:60] Depth 6/1000 time = 10.125163793563843
I0123 13:20:07.662990 139785946451968 ddar.py:60] Depth 7/1000 time = 12.871395587921143
I0123 13:20:19.813840 139785946451968 ddar.py:60] Depth 8/1000 time = 12.150616884231567
I0123 13:20:31.968623 139785946451968 ddar.py:60] Depth 9/1000 time = 12.139615774154663
I0123 13:20:31.972367 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:20:31.972430 139785946451968 alphageometry.py:566] LM output (score=-4.320796): "o : D c n n o 22 T c n n o 23 ;"
I0123 13:20:31.972464 139785946451968 alphageometry.py:567] Translation: "o = on_circle o n c, on_tline o n c n"

I0123 13:20:31.972511 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n c, on_tline o n c n ? cong b k k m"
I0123 13:20:31.972698 139785946451968 graph.py:498] 
I0123 13:20:31.972758 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n c, on_tline o n c n ? cong b k k m
I0123 13:20:37.944802 139785946451968 ddar.py:60] Depth 1/1000 time = 5.894830942153931
I0123 13:20:49.524898 139785946451968 ddar.py:60] Depth 2/1000 time = 11.579887628555298
I0123 13:21:01.508013 139785946451968 ddar.py:60] Depth 3/1000 time = 11.982929468154907
I0123 13:21:13.677423 139785946451968 ddar.py:60] Depth 4/1000 time = 12.169220447540283
I0123 13:21:25.828873 139785946451968 ddar.py:60] Depth 5/1000 time = 12.15103030204773
I0123 13:21:38.839431 139785946451968 ddar.py:60] Depth 6/1000 time = 13.00965142250061
I0123 13:21:52.331481 139785946451968 ddar.py:60] Depth 7/1000 time = 13.410736322402954
I0123 13:22:05.663435 139785946451968 ddar.py:60] Depth 8/1000 time = 13.331755638122559
I0123 13:22:19.814412 139785946451968 ddar.py:60] Depth 9/1000 time = 14.137617588043213
I0123 13:22:19.818254 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:22:19.818335 139785946451968 alphageometry.py:566] LM output (score=-4.420913): "o : D a n e o 22 D a o e n 23 ;"
I0123 13:22:19.818369 139785946451968 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2622, in add_clause
    nums = draw_fn()
  File "/home/chi/alphageometry-test/graph.py", line 2608, in draw_fn
    return nm.reduce(to_be_intersected, existing_points)
  File "/home/chi/alphageometry-test/numericals.py", line 1310, in reduce
    result = a.intersect(b)
  File "/home/chi/alphageometry-test/numericals.py", line 426, in intersect
    return circle_circle_intersection(self, obj)
  File "/home/chi/alphageometry-test/numericals.py", line 495, in circle_circle_intersection
    raise InvalidQuadSolveError()
numericals.InvalidQuadSolveError
"

I0123 13:22:19.818485 139785946451968 alphageometry.py:566] LM output (score=-4.575597): "o : D n k n o 22 D n k k o 23 ;"
I0123 13:22:19.818515 139785946451968 alphageometry.py:567] Translation: "o = on_circle o n k, on_circle o k n"

I0123 13:22:19.818550 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n k, on_circle o k n ? cong b k k m"
I0123 13:22:19.818737 139785946451968 graph.py:498] 
I0123 13:22:19.818795 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_circle o n k, on_circle o k n ? cong b k k m
I0123 13:22:25.072742 139785946451968 ddar.py:60] Depth 1/1000 time = 5.184955835342407
I0123 13:22:33.497958 139785946451968 ddar.py:60] Depth 2/1000 time = 8.425055980682373
I0123 13:22:45.162969 139785946451968 ddar.py:60] Depth 3/1000 time = 11.66481328010559
I0123 13:22:56.158146 139785946451968 ddar.py:60] Depth 4/1000 time = 10.994941473007202
I0123 13:23:06.309659 139785946451968 ddar.py:60] Depth 5/1000 time = 10.150994777679443
I0123 13:23:18.080366 139785946451968 ddar.py:60] Depth 6/1000 time = 11.769819498062134
I0123 13:23:30.534808 139785946451968 ddar.py:60] Depth 7/1000 time = 12.36876916885376
I0123 13:23:41.947055 139785946451968 ddar.py:60] Depth 8/1000 time = 11.412036657333374
I0123 13:23:55.057764 139785946451968 ddar.py:60] Depth 9/1000 time = 13.096802234649658
I0123 13:23:55.061916 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:23:55.062001 139785946451968 alphageometry.py:566] LM output (score=-4.637261): "o : D f o i o 22 ;"
I0123 13:23:55.062039 139785946451968 alphageometry.py:567] Translation: "o = on_bline o i f"

I0123 13:23:55.062084 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_bline o i f ? cong b k k m"
I0123 13:23:55.062276 139785946451968 graph.py:498] 
I0123 13:23:55.062335 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a c, on_tline n h a c; o = on_bline o i f ? cong b k k m
I0123 13:24:00.413011 139785946451968 ddar.py:60] Depth 1/1000 time = 5.256245374679565
I0123 13:24:09.849772 139785946451968 ddar.py:60] Depth 2/1000 time = 9.436591625213623
I0123 13:24:19.702413 139785946451968 ddar.py:60] Depth 3/1000 time = 9.852447986602783
I0123 13:24:30.389708 139785946451968 ddar.py:60] Depth 4/1000 time = 10.68705439567566
I0123 13:24:41.121434 139785946451968 ddar.py:60] Depth 5/1000 time = 10.731273889541626
I0123 13:24:51.837394 139785946451968 ddar.py:60] Depth 6/1000 time = 10.711678266525269
I0123 13:25:03.887948 139785946451968 ddar.py:60] Depth 7/1000 time = 11.952588081359863
I0123 13:25:16.665213 139785946451968 ddar.py:60] Depth 8/1000 time = 12.77701449394226
I0123 13:25:29.106816 139785946451968 ddar.py:60] Depth 9/1000 time = 12.426888704299927
I0123 13:25:29.110843 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:25:29.110930 139785946451968 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : C a c e 02 D a e c e 03 ; f : C b e f 04 D b e e f 05 ; g : C c f g 06 D c d d g 07 ; h : ^ a c a h a h a g 08 ^ c a c h c h c g 09 ; i : C c g i 10 T c g h i 11 ; j : C c g j 12 ^ b c b j b j b g 13 ; k : C b j k 14 T i j i k 15 ; l : D b l g l 16 D g l k l 17 ; m : C c g m 18 D g l l m 19 ? D b k k m {F1} x00 n : C a g n 20 T a g h n 21 ; x00
I0123 13:25:38.983002 139785946451968 alphageometry.py:566] LM output (score=-0.041779): "o : C a c o 22 T a c h o 23 ;"
I0123 13:25:38.983481 139785946451968 alphageometry.py:567] Translation: "o = on_line o a c, on_tline o h a c"

I0123 13:25:38.983558 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a g, on_tline n h a g; o = on_line o a c, on_tline o h a c ? cong b k k m"
I0123 13:25:38.983804 139785946451968 graph.py:498] 
I0123 13:25:38.983877 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a g, on_tline n h a g; o = on_line o a c, on_tline o h a c ? cong b k k m
I0123 13:25:48.342059 139785946451968 ddar.py:60] Depth 1/1000 time = 9.29587173461914
I0123 13:26:03.739795 139785946451968 ddar.py:60] Depth 2/1000 time = 15.397271156311035
I0123 13:26:21.227292 139785946451968 ddar.py:60] Depth 3/1000 time = 17.487223148345947
I0123 13:26:40.552612 139785946451968 ddar.py:60] Depth 4/1000 time = 19.3251314163208
I0123 13:26:59.887153 139785946451968 ddar.py:60] Depth 5/1000 time = 19.334070682525635
I0123 13:27:17.173679 139785946451968 ddar.py:60] Depth 6/1000 time = 17.18011450767517
I0123 13:27:33.739006 139785946451968 ddar.py:60] Depth 7/1000 time = 16.56503415107727
I0123 13:27:51.149733 139785946451968 ddar.py:60] Depth 8/1000 time = 17.348572969436646
I0123 13:27:51.157662 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:27:51.157729 139785946451968 alphageometry.py:566] LM output (score=-1.600436): "o : D h n h o 22 T a c h o 23 ;"
I0123 13:27:51.157764 139785946451968 alphageometry.py:567] Translation: "o = on_circle o h n, on_tline o h a c"

I0123 13:27:51.157807 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a g, on_tline n h a g; o = on_circle o h n, on_tline o h a c ? cong b k k m"
I0123 13:27:51.157986 139785946451968 graph.py:498] 
I0123 13:27:51.158043 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a g, on_tline n h a g; o = on_circle o h n, on_tline o h a c ? cong b k k m
I0123 13:27:55.103597 139785946451968 ddar.py:60] Depth 1/1000 time = 3.8817970752716064
I0123 13:28:02.268227 139785946451968 ddar.py:60] Depth 2/1000 time = 7.164462566375732
I0123 13:28:12.930600 139785946451968 ddar.py:60] Depth 3/1000 time = 10.662204504013062
I0123 13:28:21.878178 139785946451968 ddar.py:60] Depth 4/1000 time = 8.947350263595581
I0123 13:28:31.504776 139785946451968 ddar.py:60] Depth 5/1000 time = 9.626042604446411
I0123 13:28:41.105397 139785946451968 ddar.py:60] Depth 6/1000 time = 9.600216150283813
I0123 13:28:51.403391 139785946451968 ddar.py:60] Depth 7/1000 time = 10.292726993560791
I0123 13:29:02.393524 139785946451968 ddar.py:60] Depth 8/1000 time = 10.88935899734497
I0123 13:29:14.140695 139785946451968 ddar.py:60] Depth 9/1000 time = 11.746978998184204
I0123 13:29:25.246876 139785946451968 ddar.py:60] Depth 10/1000 time = 11.095144510269165
I0123 13:29:25.251820 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:25.251893 139785946451968 alphageometry.py:566] LM output (score=-1.872805): "o : D h o h n 22 T a c h o 23 ;"
I0123 13:29:25.251926 139785946451968 alphageometry.py:567] Translation: "o = on_circle o h n, on_tline o h a c"

I0123 13:29:25.251973 139785946451968 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a g, on_tline n h a g; o = on_circle o h n, on_tline o h a c ? cong b k k m"
I0123 13:29:25.252146 139785946451968 graph.py:498] 
I0123 13:29:25.252200 139785946451968 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = midpoint e a c; f = mirror f b e; g = on_circle g d c, on_line g f c; h = incenter h a g c; i = foot i h g c; j = angle_bisector j c b g, on_line j c g; k = lc_tangent k i j, on_line k b j; l = circle l b g k; m = on_circle m l g, on_line m c g; n = on_line n a g, on_tline n h a g; o = on_circle o h n, on_tline o h a c ? cong b k k m
I0123 13:29:29.260249 139785946451968 ddar.py:60] Depth 1/1000 time = 3.943913698196411
I0123 13:29:36.107600 139785946451968 ddar.py:60] Depth 2/1000 time = 6.847118854522705
I0123 13:29:46.420772 139785946451968 ddar.py:60] Depth 3/1000 time = 10.312990665435791
I0123 13:29:55.151273 139785946451968 ddar.py:60] Depth 4/1000 time = 8.730333805084229
I0123 13:30:04.595668 139785946451968 ddar.py:60] Depth 5/1000 time = 9.444001197814941
I0123 13:30:04.597404 139785946451968 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:30:04.597453 139785946451968 alphageometry.py:585] Timeout.
