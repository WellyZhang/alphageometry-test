I0123 13:30:25.238259 140277666050048 inference_utils.py:69] Parsing gin configuration.
I0123 13:30:25.238362 140277666050048 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 13:30:25.238570 140277666050048 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 13:30:25.238605 140277666050048 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 13:30:25.238635 140277666050048 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 13:30:25.238663 140277666050048 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 13:30:25.238690 140277666050048 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 13:30:25.238716 140277666050048 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 13:30:25.238743 140277666050048 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 13:30:25.238769 140277666050048 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 13:30:25.238795 140277666050048 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 13:30:25.238821 140277666050048 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 13:30:25.238867 140277666050048 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 13:30:25.239003 140277666050048 resource_reader.py:55] Path not found: base_htrans.gin
I0123 13:30:25.239207 140277666050048 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 13:30:25.239315 140277666050048 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 13:30:25.245704 140277666050048 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 13:30:25.245832 140277666050048 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 13:30:25.246159 140277666050048 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 13:30:25.246265 140277666050048 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 13:30:25.246546 140277666050048 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 13:30:25.246647 140277666050048 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 13:30:25.247061 140277666050048 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 13:30:25.247162 140277666050048 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 13:30:25.250866 140277666050048 training_loop.py:334] ==== Training loop: initializing model ====
I0123 13:30:25.353480 140277666050048 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 13:30:25.354226 140277666050048 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 13:30:25.360962 140277666050048 training_loop.py:335] Process 0 of 1
I0123 13:30:25.361016 140277666050048 training_loop.py:336] Local device count = 1
I0123 13:30:25.361056 140277666050048 training_loop.py:337] Number of replicas = 1
I0123 13:30:25.361088 140277666050048 training_loop.py:339] Using random number seed 42
I0123 13:30:25.848203 140277666050048 training_loop.py:359] Initializing the model.
I0123 13:30:26.222950 140277666050048 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.223220 140277666050048 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 13:30:26.223326 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.223404 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.223481 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.223564 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.223637 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.223708 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.223777 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.223847 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.223915 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.223984 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.224052 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.224122 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:30:26.224161 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.224206 140277666050048 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:30:26.224318 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.224357 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.224388 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.226431 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.231990 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.242617 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.242902 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.247225 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.257848 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.257907 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.257946 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.257978 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.258042 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.259234 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.259312 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.260018 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.262471 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.268130 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.269870 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.269951 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.269987 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.270048 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.270177 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.270514 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.270561 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.272463 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.272562 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.275412 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.275492 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.275983 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.286037 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.294756 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.294854 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.295148 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.295229 140277666050048 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:30:26.295339 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.295377 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.295409 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.297249 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.299851 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.305382 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.305654 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.308263 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.312057 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.312114 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.312150 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.312181 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.312247 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.312819 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.312895 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.313251 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.314021 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.316471 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.317082 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.317158 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.317193 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.317249 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.317378 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.317708 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.317752 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.319665 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.319762 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.322225 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.322307 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.322730 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.325029 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.327066 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.327160 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.327443 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.327522 140277666050048 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:30:26.327630 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.327667 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.327698 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.329583 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.331925 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.337843 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.338106 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.340720 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.344550 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.344605 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.344640 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.344671 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.344733 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.345294 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.345370 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.345732 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.346491 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.348970 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.349636 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.349720 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.349756 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.349813 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.349941 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.350262 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.350306 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.352206 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.352300 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.354793 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.354876 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.355360 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.357618 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.359536 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.359631 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.359919 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.360000 140277666050048 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:30:26.360109 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.360146 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.360177 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.362061 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.364413 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.369982 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.370249 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.372840 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.376614 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.376670 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.376707 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.376738 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.376800 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.377368 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.377444 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.377799 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.378558 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.381063 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.381689 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.381767 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.381802 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.381860 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.381987 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.382307 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.382350 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.384236 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.384328 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.386862 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.386947 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.387389 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.389625 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.391526 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.391620 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.391909 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.391989 140277666050048 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:30:26.392098 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.392137 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.392168 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.394074 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.396447 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.402067 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.402333 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.404985 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.409533 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.409650 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.409688 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.409720 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.409790 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.410374 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.410454 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.410813 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.411593 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.414465 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.415087 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.415169 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.415204 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.415261 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.415396 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.415740 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.415782 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.417696 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.417789 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.420335 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.420413 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.420847 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.423128 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.425071 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.425165 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.425457 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.425543 140277666050048 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:30:26.425659 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.425699 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.425729 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.427687 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.430058 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.435622 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.435878 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.438548 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.442315 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.442370 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.442410 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.442441 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.442504 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.443112 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.443191 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.443547 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.444317 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.446797 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.447405 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.447481 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.447515 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.447577 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.447705 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.448020 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.448063 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.449975 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.450069 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.452595 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.452674 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.453101 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.455411 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.457309 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.457403 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.457699 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.457780 140277666050048 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:30:26.457888 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.457926 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.457955 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.459751 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.462188 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.467919 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.468183 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.470812 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.474586 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.474641 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.474677 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.474708 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.474769 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.475325 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.475400 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.475758 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.476526 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.478983 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.479596 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.479671 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.479706 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.479763 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.479892 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.480213 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.480255 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.482220 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.482313 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.484763 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.484845 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.485270 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.487891 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.489801 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.489901 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.490186 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.490266 140277666050048 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:30:26.490372 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.490410 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.490440 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.631631 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.634742 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.640623 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.640923 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.643629 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.647565 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.647624 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.647662 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.647694 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.647760 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.648377 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.648456 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.648824 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.649614 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.652175 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.652813 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.652892 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.652930 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.652991 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.653119 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.653460 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.653503 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.655434 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.655529 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.658118 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.658199 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.658639 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.660953 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.662867 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.662976 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.663274 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.663357 140277666050048 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:30:26.663466 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.663504 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.663535 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.665462 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.667836 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.673491 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.673760 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.676451 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.680279 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.680335 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.680376 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.680408 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.680469 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.681032 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.681109 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.681471 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.682254 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.684789 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.685407 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.685484 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.685519 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.685578 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.685714 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.686042 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.686086 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.687992 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.688085 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.690640 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.690720 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.691152 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.693430 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.695403 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.695501 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.695790 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.695877 140277666050048 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:30:26.695988 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.696028 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.696058 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.698105 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.700646 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.706178 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.706441 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.709477 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.713230 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.713286 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.713322 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.713352 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.713414 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.714033 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.714111 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.714470 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.715243 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.717718 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.718341 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.718418 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.718454 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.718511 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.718638 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.718959 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.719002 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.720898 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.720991 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.723541 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.723624 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.724051 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.726371 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.728272 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.728367 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.728659 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.728746 140277666050048 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:30:26.728856 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.728895 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.728925 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.730749 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.733198 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.738803 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.739063 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.741709 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.745509 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.745563 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.745599 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.745629 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.745700 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.746270 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.746348 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.746707 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.747481 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.749958 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.750579 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.750655 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.750689 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.750747 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.750877 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.751194 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.751237 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.753210 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.753304 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.756077 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.756158 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.756590 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.758920 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.760811 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.760906 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.761196 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.761278 140277666050048 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:30:26.761395 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.761433 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.761463 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.763358 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.765721 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.771289 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.771548 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.774195 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:30:26.778017 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.778072 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.778108 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.778139 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.778200 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.778765 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.778840 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.779193 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.779946 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.782411 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.783388 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.783465 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.783499 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.783557 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.783682 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.784001 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.784044 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.785939 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.786036 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.788500 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.788578 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.789055 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.791291 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.793184 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.793277 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.793564 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.793845 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.793915 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.793980 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794035 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794087 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794139 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794192 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794244 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794296 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794348 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794399 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794449 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:30:26.794486 140277666050048 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:30:26.797961 140277666050048 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:30:26.845861 140277666050048 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.845947 140277666050048 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:30:26.846002 140277666050048 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:30:26.846106 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.846144 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.846174 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.846239 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.848654 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.854115 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.854378 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.856976 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:26.874307 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.874364 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.874401 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.874432 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.874494 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.875630 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.875710 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.876416 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.878413 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.883127 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.884453 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.884539 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.884576 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.884635 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.884765 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.884873 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.884912 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.886820 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.886915 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.889315 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.889394 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.889502 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.891732 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.893683 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.893780 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.894069 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.894152 140277666050048 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:30:26.894263 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.894302 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.894333 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.894398 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.896640 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.902122 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.902382 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.905055 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:26.918461 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.918519 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.918556 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.918588 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.918653 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.919245 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.919323 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.919680 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.920373 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.922898 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.923524 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.923602 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.923643 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.923702 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.923838 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.923950 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.923989 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.925913 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.926011 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.928439 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.928521 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.928629 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.930890 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.932813 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.932907 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.933188 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.933268 140277666050048 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:30:26.933374 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.933413 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.933444 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.933506 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.935788 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.941277 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.941538 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.944276 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:26.957237 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.957292 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.957327 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.957356 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.957417 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.957985 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.958066 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.958433 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.959154 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.961609 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.962252 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.962332 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:26.962368 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:26.962432 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.962565 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:26.962813 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:26.962853 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.964964 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.965057 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.967542 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.967623 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:26.967731 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:26.969943 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:26.971867 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.971962 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:26.972245 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.972326 140277666050048 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:30:26.972435 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:26.972474 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:26.972504 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:26.972566 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.974794 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:26.980230 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.980488 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:26.983179 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:26.996009 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:26.996065 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:26.996101 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:26.996132 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.996194 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.996753 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.996829 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.997181 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:26.997874 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.000338 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.000955 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.001032 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.001066 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.001127 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.001264 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.001373 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.001412 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.003360 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.003455 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.005862 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.005942 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.006048 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.008257 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.010108 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.010205 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.010489 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.010570 140277666050048 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:30:27.010678 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.010717 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.010748 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.010811 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.013389 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.018902 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.019173 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.021801 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.034590 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.034647 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.034682 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.034713 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.034776 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.035336 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.035413 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.035780 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.036470 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.038993 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.039621 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.039700 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.039735 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.039793 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.039929 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.040039 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.040077 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.041966 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.042060 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.044440 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.044519 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.044625 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.046908 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.048945 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.049040 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.049322 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.049403 140277666050048 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:30:27.049511 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.049549 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.049580 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.049650 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.052059 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.057499 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.057769 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.060442 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.079156 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.079240 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.079280 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.079312 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.079390 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.080006 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.080084 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.080454 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.081171 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.083714 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.084342 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.084419 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.084454 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.084514 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.084644 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.084766 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.084805 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.086842 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.086937 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.089422 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.089501 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.089612 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.091876 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.093750 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.093845 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.094129 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.094211 140277666050048 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:30:27.094322 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.094364 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.094395 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.094462 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.096687 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.102239 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.102495 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.105114 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.117940 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.117995 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.118029 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.118058 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.118119 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.118678 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.118758 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.119111 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.119793 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.122274 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.123303 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.123383 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.123418 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.123476 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.123608 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.123722 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.123766 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.125668 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.125764 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.128166 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.128246 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.128352 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.130585 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.132514 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.132611 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.132895 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.132977 140277666050048 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:30:27.133085 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.133124 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.133155 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.133215 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.135440 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.140878 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.141147 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.143796 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.156585 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.156641 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.156677 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.156707 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.156769 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.157371 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.157449 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.157822 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.158515 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.160982 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.161607 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.161694 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.161729 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.161791 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.161920 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.162038 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.162083 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.163947 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.164041 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.166496 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.166577 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.166686 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.168905 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.170933 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.171029 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.171312 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.171394 140277666050048 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:30:27.171502 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.171541 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.171572 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.171634 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.173871 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.179385 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.179643 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.182265 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.195064 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.195122 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.195157 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.195187 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.195248 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.195812 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.195890 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.196244 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.196931 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.199418 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.200101 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.200180 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.200214 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.200272 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.200401 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.200514 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.200553 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.202448 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.202543 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.204916 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.204995 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.205102 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.207352 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.209298 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.209395 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.209686 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.209772 140277666050048 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:30:27.209882 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.209921 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.209950 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.210011 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.212246 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.217691 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.217950 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.220652 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.233822 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.233879 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.233914 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.233944 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.234007 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.234617 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.234696 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.235054 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.235754 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.238239 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.238863 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.238941 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.238975 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.239031 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.239161 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.239274 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.239313 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.241196 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.241297 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.243769 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.243850 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.243958 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.246191 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.248052 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.248147 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.248430 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.248511 140277666050048 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:30:27.248620 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.248658 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.248688 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.248750 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.250984 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.256514 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.256770 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.259412 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.272198 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.272254 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.272291 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.272321 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.272383 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.272937 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.273017 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.273374 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.274070 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.276552 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.277226 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.277304 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.277338 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.277394 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.277525 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.277635 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.277681 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.279578 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.279679 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.282106 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.282189 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.282298 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.284502 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.286435 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.286532 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.286814 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.286896 140277666050048 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:30:27.287005 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.287044 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.287074 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.287135 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.289347 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.294779 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.295038 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.297720 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.310442 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.310500 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.310535 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.310564 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.310626 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.311185 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.311260 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.311617 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.312354 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.314870 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.315498 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.315576 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.315611 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.315668 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.315799 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.315909 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.315951 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.317833 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.317928 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.320324 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.320409 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.320517 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.322816 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.324680 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.324776 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.325060 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.325149 140277666050048 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:30:27.328022 140277666050048 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:30:27.384101 140277666050048 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.384187 140277666050048 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:30:27.384241 140277666050048 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:30:27.384346 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.384385 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.384415 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.384477 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.387142 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.392471 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.392724 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.395267 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.407691 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.407748 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.407783 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.407813 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.407874 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.408428 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.408504 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.408853 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.409521 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.411989 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.412605 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.412684 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.412719 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.412778 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.412908 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.413025 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.413064 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.414898 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.414994 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.417362 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.417441 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.417548 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.419786 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.421633 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.421736 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.422019 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.422100 140277666050048 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:30:27.422207 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.422246 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.422276 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.422338 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.424540 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.429882 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.430139 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.432758 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.445228 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.445284 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.445320 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.445352 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.445415 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.445978 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.446056 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.446413 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.447094 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.449580 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.450197 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.450276 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.450311 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.450368 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.450495 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.450604 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.450648 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.452661 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.452754 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.455120 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.455199 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.455308 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.457535 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.459368 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.459463 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.459747 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.459828 140277666050048 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:30:27.459935 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.459973 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.460004 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.460066 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.462275 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.467576 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.467833 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.470463 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.482785 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.482841 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.482877 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.482908 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.482971 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.483525 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.483603 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.483956 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.484626 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.487145 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.487758 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.487836 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.487871 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.487929 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.488058 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.488167 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.488206 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.490041 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.490136 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.492480 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.492558 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.492666 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.495363 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.497544 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.497646 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.497933 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.498014 140277666050048 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:30:27.498122 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.498161 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.498191 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.498253 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.500454 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.505786 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.506047 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.508669 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.521126 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.521182 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.521225 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.521267 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.521332 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.521899 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.521976 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.522328 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.523008 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.525518 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.526149 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.526227 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.526260 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.526317 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.526447 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.526557 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.526596 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.528475 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.528566 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.530946 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.531024 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.531131 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.533401 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.535264 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.535358 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.535637 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.535716 140277666050048 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:30:27.535822 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.535859 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.535888 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.535948 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.538165 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.543533 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.543789 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.546452 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.558967 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.559022 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.559056 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.559086 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.559147 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.559698 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.559772 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.560118 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.560794 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.563306 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.563931 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.564007 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.564040 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.564096 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.564221 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.564328 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.564365 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.566205 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.566308 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.568679 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.568756 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.568862 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.571122 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.572974 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.573066 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.573345 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.573426 140277666050048 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:30:27.573531 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.573568 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.573598 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.573665 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.575866 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.581212 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.581468 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.584118 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.596703 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.596758 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.596793 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.596822 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.596882 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.597435 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.597511 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.598040 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.598721 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.601255 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.601879 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.601954 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.601988 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.602043 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.602169 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.602276 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.602312 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.604160 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.604258 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.606631 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.606709 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.606821 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.609500 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.611359 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.611454 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.611735 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.611815 140277666050048 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:30:27.611922 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.611960 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.611989 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.612050 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.614276 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.619640 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.619897 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.622579 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.635163 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.635217 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.635251 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.635281 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.635341 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.635900 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.635973 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.636324 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.637008 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.639504 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.640127 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.640202 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.640235 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.640291 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.640415 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.640522 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.640559 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.642426 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.642519 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.644886 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.644963 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.645068 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.647338 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.649180 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.649273 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.649551 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.649630 140277666050048 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:30:27.649744 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.649782 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.649811 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.649873 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.652074 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.657466 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.657737 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.660415 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.672982 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.673036 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.673071 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.673100 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.673161 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.673724 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.673799 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.674150 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.674834 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.677337 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.677968 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.678045 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.678079 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.678137 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.678263 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.678369 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.678406 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.680255 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.680347 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.682718 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.682802 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.682912 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.685169 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.687026 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.687121 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.687403 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.687484 140277666050048 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:30:27.687590 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.687628 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.687658 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.687718 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.689944 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.695313 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.695571 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.698422 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.711306 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.711359 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.711394 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.711423 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.711483 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.712229 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.712305 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.712655 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.713336 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.715879 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.716487 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.716562 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.716595 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.716651 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.716777 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.716884 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.716921 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.718782 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.718875 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.721244 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.721325 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.721433 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.724093 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.725962 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.726057 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.726340 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.726421 140277666050048 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:30:27.726527 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.726564 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.726594 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.726655 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.728877 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.734267 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.734525 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.737172 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.749740 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.749793 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.749828 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.749856 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.749915 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.750472 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.750547 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.750895 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.751574 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.754104 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.754719 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.754794 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.754827 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.754881 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.755005 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.755111 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.755149 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.757521 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.757614 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.759997 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.760077 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.760191 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.762425 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.764253 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.764345 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.764626 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.764706 140277666050048 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:30:27.764812 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.764849 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.764879 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.764939 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.767169 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.772564 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.772821 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.775479 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.788015 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.788069 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.788104 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.788133 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.788194 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.788750 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.788825 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.789175 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.789864 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.792387 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.793002 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.793078 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.793112 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.793167 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.793296 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.793403 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.793439 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.795306 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.795398 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.797767 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.797845 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.797951 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.800209 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.802052 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.802147 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.802427 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.802507 140277666050048 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:30:27.802613 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:30:27.802650 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:30:27.802679 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:30:27.802739 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.804956 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:30:27.810364 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.810623 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:30:27.813591 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:30:27.826504 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:30:27.826758 140277666050048 attention.py:418] Single window, no scan.
I0123 13:30:27.826792 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:30:27.826822 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.826884 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.827436 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.827515 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.828051 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.828739 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.831408 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.832200 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.832276 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:30:27.832310 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:30:27.832366 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.832496 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:30:27.832603 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:30:27.832639 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.834500 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.834592 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.836944 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.837021 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:30:27.837127 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:30:27.839768 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:30:27.841621 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.841723 140277666050048 nn_components.py:261] mlp: residual
I0123 13:30:27.842006 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:27.842090 140277666050048 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:30:27.844894 140277666050048 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:30:32.266800 140277666050048 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 13:30:32.855341 140277666050048 training_loop.py:409] No working directory specified.
I0123 13:30:32.855473 140277666050048 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 13:30:32.856267 140277666050048 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 13:30:36.048258 140277666050048 training_loop.py:447] Only restoring trainable parameters.
I0123 13:30:36.048876 140277666050048 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 13:30:36.048955 140277666050048 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.049006 140277666050048 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.049052 140277666050048 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.049095 140277666050048 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049137 140277666050048 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.049176 140277666050048 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049216 140277666050048 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049254 140277666050048 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.049292 140277666050048 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.049328 140277666050048 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049365 140277666050048 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.049404 140277666050048 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.049442 140277666050048 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.049480 140277666050048 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049517 140277666050048 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.049554 140277666050048 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049590 140277666050048 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049626 140277666050048 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.049673 140277666050048 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.049724 140277666050048 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049764 140277666050048 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.049802 140277666050048 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.049839 140277666050048 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.049876 140277666050048 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049912 140277666050048 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.049948 140277666050048 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.049983 140277666050048 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050019 140277666050048 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.050055 140277666050048 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.050091 140277666050048 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050127 140277666050048 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.050163 140277666050048 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.050200 140277666050048 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.050236 140277666050048 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050272 140277666050048 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.050309 140277666050048 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050345 140277666050048 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050381 140277666050048 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.050417 140277666050048 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.050453 140277666050048 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050489 140277666050048 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.050525 140277666050048 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.050560 140277666050048 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.050596 140277666050048 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050633 140277666050048 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.050675 140277666050048 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050713 140277666050048 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050750 140277666050048 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.050786 140277666050048 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.050823 140277666050048 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.050858 140277666050048 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.050895 140277666050048 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.050931 140277666050048 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.050966 140277666050048 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051002 140277666050048 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.051038 140277666050048 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051074 140277666050048 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051110 140277666050048 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.051147 140277666050048 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.051184 140277666050048 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051222 140277666050048 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.051258 140277666050048 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.051294 140277666050048 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.051330 140277666050048 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051366 140277666050048 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.051401 140277666050048 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051436 140277666050048 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051471 140277666050048 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.051506 140277666050048 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.051541 140277666050048 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051577 140277666050048 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.051613 140277666050048 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.051653 140277666050048 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.051691 140277666050048 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051727 140277666050048 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.051763 140277666050048 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051798 140277666050048 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051833 140277666050048 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.051868 140277666050048 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.051903 140277666050048 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.051938 140277666050048 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.051973 140277666050048 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.052009 140277666050048 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.052045 140277666050048 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052081 140277666050048 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.052116 140277666050048 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052150 140277666050048 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052185 140277666050048 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.052220 140277666050048 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.052254 140277666050048 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052290 140277666050048 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.052325 140277666050048 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.052359 140277666050048 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.052394 140277666050048 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052428 140277666050048 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.052464 140277666050048 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052500 140277666050048 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052535 140277666050048 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.052570 140277666050048 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.052611 140277666050048 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052648 140277666050048 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.052684 140277666050048 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.052720 140277666050048 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.052756 140277666050048 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052791 140277666050048 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.052827 140277666050048 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052862 140277666050048 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.052898 140277666050048 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.052933 140277666050048 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.052968 140277666050048 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.053003 140277666050048 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.053038 140277666050048 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:30:36.053073 140277666050048 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:30:36.053108 140277666050048 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.053143 140277666050048 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.053179 140277666050048 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.053214 140277666050048 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.053249 140277666050048 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:30:36.053283 140277666050048 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:30:36.053317 140277666050048 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:30:36.053352 140277666050048 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:30:36.053379 140277666050048 training_loop.py:725] Total parameters: 152072288
I0123 13:30:36.053587 140277666050048 training_loop.py:739] Total state size: 0
I0123 13:30:36.076886 140277666050048 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 13:30:36.077113 140277666050048 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 13:30:36.077491 140277666050048 training_loop.py:652] Compiling mode beam_search with jit.
I0123 13:30:36.077815 140277666050048 training_loop.py:89] registering functions: dict_keys([])
I0123 13:30:36.094316 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m ? coll a m n
I0123 13:30:37.453206 140277666050048 ddar.py:60] Depth 1/1000 time = 1.3227002620697021
I0123 13:30:40.156924 140277666050048 ddar.py:60] Depth 2/1000 time = 2.7035465240478516
I0123 13:30:45.939364 140277666050048 ddar.py:60] Depth 3/1000 time = 5.782247066497803
I0123 13:30:54.199332 140277666050048 ddar.py:60] Depth 4/1000 time = 8.259751796722412
I0123 13:31:03.428048 140277666050048 ddar.py:60] Depth 5/1000 time = 9.228474617004395
I0123 13:31:12.803954 140277666050048 ddar.py:60] Depth 6/1000 time = 9.37558913230896
I0123 13:31:22.149262 140277666050048 ddar.py:60] Depth 7/1000 time = 9.34485912322998
I0123 13:31:22.217027 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:31:22.217112 140277666050048 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 13:31:22.217148 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00
I0123 13:31:22.217180 140277666050048 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00
I0123 13:31:22.363826 140277666050048 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.364014 140277666050048 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 13:31:22.364118 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364193 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364265 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364334 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364402 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364471 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364539 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364607 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364674 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364743 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364810 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364877 140277666050048 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:31:22.364915 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.364958 140277666050048 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:31:22.365064 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.365103 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.365133 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.367024 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.369533 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.375674 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.375951 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.378600 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.382597 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.382654 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.382690 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.382725 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.382787 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.383399 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.383476 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.383837 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.384612 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.387140 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.387782 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.387858 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.387892 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.387952 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.388079 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.388403 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.388445 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.390443 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.390537 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.392983 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.393059 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.393483 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.395822 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.397757 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.397851 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.398140 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.398221 140277666050048 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:31:22.398329 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.398366 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.398396 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.400254 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.402588 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.408211 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.408471 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.411600 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.415353 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.415409 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.415445 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.415474 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.415536 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.416098 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.416174 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.416529 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.417298 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.419765 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.420444 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.420521 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.420556 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.420614 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.420741 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.421056 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.421096 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.423025 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.423118 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.425560 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.425638 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.426067 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.428425 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.431348 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.431444 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.431736 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.431820 140277666050048 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:31:22.431928 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.431966 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.431996 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.433835 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.436173 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.441869 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.442123 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.444641 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.448266 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.448320 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.448353 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.448383 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.448443 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.449053 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.449131 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.449481 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.450230 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.452637 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.453257 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.453334 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.453367 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.453424 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.453550 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.453868 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.453911 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.455856 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.455948 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.458362 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.458439 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.458851 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.461118 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.463038 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.463132 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.463417 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.463496 140277666050048 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:31:22.463603 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.463640 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.463670 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.465533 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.467842 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.473542 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.473810 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.476462 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.480143 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.480197 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.480231 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.480261 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.480322 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.480874 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.480949 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.481294 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.482051 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.484460 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.485083 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.485159 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.485193 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.485249 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.485373 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.485743 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.485786 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.487680 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.487770 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.490181 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.490258 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.490675 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.492897 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.494871 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.494966 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.495248 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.495328 140277666050048 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:31:22.495432 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.495470 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.495499 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.497262 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.499561 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.505168 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.505429 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.507971 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.511790 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.511845 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.511879 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.511908 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.511970 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.512578 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.512653 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.513005 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.513767 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.516157 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.516770 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.516846 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.516879 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.516935 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.517062 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.517375 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.517417 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.519725 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.519818 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.522270 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.522349 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.522762 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.525022 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.526932 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.527026 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.527312 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.527391 140277666050048 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:31:22.527499 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.527537 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.527566 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.529401 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.531715 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.537288 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.537541 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.540108 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.543793 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.543848 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.543881 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.543911 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.543971 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.544525 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.544600 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.544947 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.545701 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.548225 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.548849 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.548925 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.548959 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.549014 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.549168 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.549533 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.549576 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.551545 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.551637 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.554061 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.554138 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.554554 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.556767 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.558755 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.558849 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.559133 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.559213 140277666050048 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:31:22.559320 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.559357 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.559386 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.561149 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.563436 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.569051 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.569307 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.571826 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.575450 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.575504 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.575537 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.575566 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.575626 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.576228 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.576303 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.576652 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.577403 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.579818 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.580439 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.580517 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.580551 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.580609 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.580737 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.581050 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.581092 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.583046 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.583138 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.585548 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.585625 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.586052 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.588271 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.590151 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.590245 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.590528 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.590607 140277666050048 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:31:22.590712 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.590750 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.590780 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.592632 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.594912 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.600424 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.600674 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.603225 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.606887 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.606948 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.606983 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.607012 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.607074 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.607628 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.607702 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.608042 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.608779 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.611261 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.611923 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.612000 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.612034 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.612091 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.612218 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.612576 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.612617 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.614509 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.614600 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.617002 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.617080 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.617497 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.619737 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.621711 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.621803 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.622087 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.622167 140277666050048 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:31:22.622271 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.622308 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.622336 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.624093 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.626379 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.632369 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.632622 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.635176 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.638823 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.638877 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.638917 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.638949 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.639011 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.639627 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.639703 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.640056 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.640808 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.643239 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.643852 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.643929 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.643962 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.644018 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.644145 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.644457 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.644497 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.646406 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.646498 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.648976 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.649054 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.649474 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.651694 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.653583 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.653682 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.653965 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.654046 140277666050048 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:31:22.654152 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.654189 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.654218 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.655982 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.658355 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.663842 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.664097 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.666624 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.670250 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.670305 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.670338 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.670369 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.670491 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.671051 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.671126 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.671474 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.672227 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.674673 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.675294 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.675370 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.675404 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.675460 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.675588 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.675898 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.675938 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.677825 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.677918 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.680374 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.680454 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.680878 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.683126 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.685017 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.685110 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.685394 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.685472 140277666050048 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:31:22.685577 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.685614 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.685648 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.687420 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.689781 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.695302 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.695556 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.698092 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.701702 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.701756 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.701789 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.701819 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.701938 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.702497 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.702573 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.702924 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.703672 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.706092 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.706717 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.706792 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.706825 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.706880 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.707007 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.707315 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.707356 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.709237 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.709327 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.711810 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.711889 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.712309 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.714577 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.716460 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.716553 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.716840 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.716921 140277666050048 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:31:22.717028 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.717064 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.717093 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.718852 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.721232 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.726947 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.727203 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.730146 140277666050048 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:31:22.733889 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.733943 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.733977 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.734007 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.734122 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.734688 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.734761 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.735110 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.735859 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.738305 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.738925 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.739001 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.739034 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.739090 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.739217 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.739532 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.739574 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.741466 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.741557 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.744041 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.744120 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.744540 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.746784 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.748666 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.748760 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.749043 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.749294 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749360 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749416 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749470 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749522 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749574 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749625 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749684 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749737 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749788 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749839 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749890 140277666050048 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:31:22.749925 140277666050048 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:31:22.752789 140277666050048 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:31:22.797628 140277666050048 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.797718 140277666050048 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:31:22.797769 140277666050048 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:31:22.797872 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.797908 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.797937 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.797997 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.800353 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.805712 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.805968 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.808513 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:22.821740 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.821795 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.821830 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.821859 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.821920 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.822483 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.822559 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.822919 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.823611 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.826141 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.826764 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.826840 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.826873 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.826930 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.827057 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.827163 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.827201 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.829046 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.829138 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.831703 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.831781 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.831890 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.834153 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.835985 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.836086 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.836373 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.836453 140277666050048 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:31:22.836558 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.836596 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.836625 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.836686 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.838900 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.844269 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.844528 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.847172 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:22.859619 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.859674 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.859708 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.859737 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.859800 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.860533 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.860609 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.860967 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.861709 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.864143 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.864760 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.864837 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.864871 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.864927 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.865055 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.865163 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.865200 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.867042 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.867135 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.869509 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.869587 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.869704 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.871967 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.873832 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.873934 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.874224 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.874305 140277666050048 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:31:22.874412 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.874449 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.874479 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.874539 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.876756 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.882135 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.882398 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.885024 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:22.897455 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.897510 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.897544 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.897574 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.897636 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.898200 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.898276 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.898625 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.899349 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.901788 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.902398 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.902474 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.902508 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.902565 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.902693 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.902799 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.902836 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.904680 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.904772 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.907156 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.907234 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.907342 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.909605 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.911447 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.911542 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.911835 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.911916 140277666050048 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:31:22.912022 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.912059 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.912089 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.912149 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.914371 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.919733 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.919991 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.922654 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:22.935472 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.935529 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.935564 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.935593 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.935654 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.936198 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.936274 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.936621 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.937340 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.939757 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.940373 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.940450 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.940484 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.940541 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.940667 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.940774 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.940810 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.942656 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.942749 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.945120 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.945198 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.945304 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.947555 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.949400 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.949493 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.949786 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.949874 140277666050048 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:31:22.949983 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.950020 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.950050 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.950109 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.952332 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.957733 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.957991 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.960787 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:22.973209 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:22.973263 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:22.973296 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:22.973325 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.973387 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.973945 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.974023 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.974376 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.975101 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.977516 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.978147 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.978224 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:22.978258 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:22.978312 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.978439 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:22.978546 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:22.978583 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.980406 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.980498 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.982876 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.982954 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:22.983061 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:22.985318 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:22.987168 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.987262 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:22.987548 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.987635 140277666050048 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:31:22.987743 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:22.987782 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:22.987811 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:22.987872 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.990103 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:22.995991 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:22.996249 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:22.998905 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.011743 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.011797 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.011832 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.011862 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.011924 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.012475 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.012549 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.012896 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.013621 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.016074 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.016690 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.016767 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.016801 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.016857 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.016985 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.017092 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.017130 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.018968 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.019061 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.021452 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.021530 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.021636 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.023887 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.025747 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.025841 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.026126 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.026206 140277666050048 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:31:23.026318 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.026356 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.026384 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.026445 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.028656 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.034034 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.034291 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.036920 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.049776 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.049830 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.049863 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.049892 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.049955 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.050501 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.050575 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.050926 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.051649 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.054121 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.054736 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.054812 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.054846 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.054901 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.055027 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.055134 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.055170 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.057023 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.057115 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.059483 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.059562 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.059669 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.061941 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.063794 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.063887 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.064172 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.064253 140277666050048 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:31:23.064359 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.064403 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.064433 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.064494 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.066727 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.072096 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.072359 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.075003 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.087581 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.087636 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.087670 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.087698 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.087758 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.088311 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.088386 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.088739 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.089471 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.091920 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.092535 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.092612 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.092645 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.092699 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.092827 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.092937 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.092975 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.094842 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.094934 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.097318 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.097396 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.097502 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.099766 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.101631 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.101731 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.102015 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.102095 140277666050048 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:31:23.102201 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.102238 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.102272 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.102336 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.104561 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.110096 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.110354 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.113019 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.125674 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.125728 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.125763 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.125792 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.125854 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.126412 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.126488 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.126839 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.127525 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.130044 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.130665 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.130742 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.130775 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.130830 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.130962 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.131071 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.131108 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.132957 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.133049 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.135421 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.135499 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.135605 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.137872 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.139726 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.139819 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.140102 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.140182 140277666050048 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:31:23.140288 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.140324 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.140365 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.140429 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.142666 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.148070 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.148330 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.150998 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.163848 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.163903 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.163937 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.163966 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.164031 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.164575 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.164652 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.165001 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.165692 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.168174 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.168785 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.168861 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.168895 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.168950 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.169079 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.169186 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.169223 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.171084 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.171175 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.173521 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.173598 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.173711 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.175947 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.177803 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.177896 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.178183 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.178263 140277666050048 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:31:23.178370 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.178407 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.178437 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.178504 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.180722 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.186094 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.186352 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.189000 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.201488 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.201542 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.201577 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.201606 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.201674 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.202225 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.202301 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.202647 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.203322 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.205819 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.206432 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.206508 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.206542 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.206598 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.206728 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.206837 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.206874 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.208722 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.208814 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.211181 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.211259 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.211364 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.213597 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.215440 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.215533 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.215986 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.216066 140277666050048 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:31:23.216173 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.216210 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.216239 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.216300 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.218539 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.223927 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.224187 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.226845 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.239356 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.239411 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.239445 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.239475 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.239536 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.240091 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.240166 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.240515 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.241199 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.243793 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.244440 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.244519 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.244555 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.244613 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.244745 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.244858 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.244897 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.246852 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.246949 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.249438 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.249519 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.249631 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.251996 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.253958 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.254056 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.254357 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.254445 140277666050048 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:31:23.257533 140277666050048 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:31:23.309230 140277666050048 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.309313 140277666050048 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:31:23.309372 140277666050048 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:31:23.309475 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.309512 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.309541 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.309602 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.311955 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.317460 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.317732 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.320371 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.332983 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.333038 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.333072 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.333101 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.333163 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.333721 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.333798 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.334154 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.334847 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.337267 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.337880 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.337957 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.337990 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.338050 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.338184 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.338295 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.338334 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.340648 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.340739 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.343095 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.343173 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.343280 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.345445 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.347287 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.347381 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.347671 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.347752 140277666050048 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:31:23.347864 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.347902 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.347931 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.347991 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.350214 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.355810 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.356245 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.358801 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.371547 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.371601 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.371635 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.371664 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.371724 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.372273 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.372348 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.372697 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.373366 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.375787 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.376394 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.376469 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.376502 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.376558 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.376684 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.376791 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.376829 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.378745 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.378837 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.381198 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.381275 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.381384 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.383566 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.385391 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.385485 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.385780 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.385861 140277666050048 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:31:23.385968 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.386011 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.386042 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.386103 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.388313 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.393702 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.393963 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.396513 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.408917 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.408971 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.409005 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.409034 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.409096 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.409650 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.409726 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.410080 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.410752 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.413147 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.413758 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.413835 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.413868 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.413923 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.414050 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.414155 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.414192 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.416076 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.416167 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.418547 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.418625 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.418733 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.420921 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.422766 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.422860 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.423149 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.423229 140277666050048 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:31:23.423334 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.423371 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.423406 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.423469 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.425677 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.431112 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.431370 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.433915 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.446380 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.446434 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.446468 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.446498 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.446558 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.447108 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.447183 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.447531 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.448199 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.450636 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.451252 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.451328 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.451362 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.451418 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.451547 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.451654 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.451692 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.454075 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.454169 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.456547 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.456625 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.456734 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.458935 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.460774 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.460867 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.461153 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.461233 140277666050048 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:31:23.461340 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.461378 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.461412 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.461474 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.463697 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.469145 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.469408 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.472157 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.484648 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.484702 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.484735 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.484765 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.484825 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.485379 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.485454 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.485812 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.486483 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.488888 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.489500 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.489576 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.489609 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.489672 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.489802 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.489908 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.489945 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.491864 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.491954 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.494327 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.494405 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.494512 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.496701 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.498532 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.498625 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.498910 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.498990 140277666050048 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:31:23.499096 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.499133 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.499162 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.499229 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.501434 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.506872 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.507132 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.509692 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.522289 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.522345 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.522378 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.522407 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.522468 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.523023 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.523099 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.523449 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.524118 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.526677 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.527288 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.527364 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.527397 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.527453 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.527579 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.527687 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.527724 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.529630 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.529731 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.532094 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.532171 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.532276 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.534473 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.536297 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.536391 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.536675 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.536756 140277666050048 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:31:23.536862 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.536899 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.536929 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.536989 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.539193 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.544602 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.544861 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.547422 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.560352 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.560408 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.560444 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.560474 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.560539 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.561126 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.561204 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.561567 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.562283 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.564805 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.565436 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.565515 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.565550 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.565608 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.565754 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.565867 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.565906 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.568314 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.568411 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.570893 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.570975 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.571090 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.573379 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.575308 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.575407 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.575703 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.575788 140277666050048 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:31:23.575898 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.575936 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.575965 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.576027 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.578350 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.583903 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.584164 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.586796 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.599827 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.599886 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.599923 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.599956 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.600021 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.600607 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.600685 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.601046 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.601768 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.604296 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.604937 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.605014 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.605048 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.605106 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.605235 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.605348 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.605387 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.607384 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.607478 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.609957 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.610038 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.610151 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.612442 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.614370 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.614468 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.614764 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.614846 140277666050048 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:31:23.614957 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.614996 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.615027 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.615090 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.617388 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.623100 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.623368 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.626248 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.639301 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.639357 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.639393 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.639424 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.639485 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.640044 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.640119 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.640465 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.641136 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.643642 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.644256 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.644331 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.644366 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.644423 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.644548 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.644654 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.644690 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.646636 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.646730 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.649114 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.649190 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.649295 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.651560 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.653440 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.653533 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.653834 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.653914 140277666050048 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:31:23.654022 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.654061 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.654091 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.654154 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.656402 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.661943 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.662220 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.664869 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.677661 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.677719 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.677753 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.677782 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.677841 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.678411 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.678490 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.678858 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.679558 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.682004 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.682647 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.682726 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.682760 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.682820 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.682952 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.683063 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.683101 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.685421 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.685513 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.687968 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.688045 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.688151 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.690386 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.692287 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.692379 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.692669 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.692748 140277666050048 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:31:23.692852 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.692890 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.692919 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.692980 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.695272 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.700812 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.701076 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.703732 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.716559 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.716612 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.716646 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.716675 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.716734 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.717288 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.717362 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.717722 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.718422 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.720911 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.721517 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.721590 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.721624 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.721690 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.721817 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.721924 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.721964 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.723945 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.724036 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.726460 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.726541 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.726656 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.729064 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.730981 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.731078 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.731375 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.731458 140277666050048 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:31:23.731564 140277666050048 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:31:23.731601 140277666050048 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:31:23.731630 140277666050048 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:31:23.731690 140277666050048 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.733912 140277666050048 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:31:23.739749 140277666050048 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.740017 140277666050048 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:31:23.742641 140277666050048 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:31:23.755462 140277666050048 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:31:23.755517 140277666050048 attention.py:418] Single window, no scan.
I0123 13:31:23.755550 140277666050048 transformer_layer.py:389] tlayer: self-attention.
I0123 13:31:23.755580 140277666050048 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.755640 140277666050048 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.756197 140277666050048 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.756272 140277666050048 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.756624 140277666050048 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.757306 140277666050048 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.759815 140277666050048 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.760429 140277666050048 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.760504 140277666050048 transformer_layer.py:468] tlayer: End windows.
I0123 13:31:23.760537 140277666050048 transformer_layer.py:472] tlayer: final FFN.
I0123 13:31:23.760592 140277666050048 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.760717 140277666050048 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:31:23.760825 140277666050048 nn_components.py:325] mlp: activation = None
I0123 13:31:23.760862 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.762819 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.762913 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.765321 140277666050048 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.765398 140277666050048 transformer_base.py:443] tbase: final FFN
I0123 13:31:23.765506 140277666050048 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:31:23.767793 140277666050048 nn_components.py:329] mlp: final activation = None
I0123 13:31:23.769638 140277666050048 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.769738 140277666050048 nn_components.py:261] mlp: residual
I0123 13:31:23.770033 140277666050048 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:23.770123 140277666050048 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:31:23.772989 140277666050048 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:31:37.390937 140277666050048 alphageometry.py:566] LM output (score=-0.750041): "p : T b d b p 21 ;"
I0123 13:31:37.391176 140277666050048 alphageometry.py:567] Translation: "p = on_tline p b b d"

I0123 13:31:37.391230 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d ? coll a m n"
I0123 13:31:37.391389 140277666050048 graph.py:498] 
I0123 13:31:37.391462 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d ? coll a m n
I0123 13:31:39.142594 140277666050048 ddar.py:60] Depth 1/1000 time = 1.7129487991333008
I0123 13:31:42.105819 140277666050048 ddar.py:60] Depth 2/1000 time = 2.9630508422851562
I0123 13:31:49.566910 140277666050048 ddar.py:60] Depth 3/1000 time = 7.460904121398926
I0123 13:31:59.154832 140277666050048 ddar.py:60] Depth 4/1000 time = 9.587742567062378
I0123 13:32:09.094212 140277666050048 ddar.py:60] Depth 5/1000 time = 9.939195156097412
I0123 13:32:19.577330 140277666050048 ddar.py:60] Depth 6/1000 time = 10.482927322387695
I0123 13:32:29.701441 140277666050048 ddar.py:60] Depth 7/1000 time = 10.123687744140625
I0123 13:32:29.778513 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:32:29.778611 140277666050048 alphageometry.py:566] LM output (score=-1.708882): "p : T b c c p 21 ;"
I0123 13:32:29.778648 140277666050048 alphageometry.py:567] Translation: "p = on_tline p c b c"

I0123 13:32:29.778684 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c ? coll a m n"
I0123 13:32:29.778844 140277666050048 graph.py:498] 
I0123 13:32:29.778896 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c ? coll a m n
I0123 13:32:31.312951 140277666050048 ddar.py:60] Depth 1/1000 time = 1.495758056640625
I0123 13:32:34.344626 140277666050048 ddar.py:60] Depth 2/1000 time = 3.0314817428588867
I0123 13:32:40.229072 140277666050048 ddar.py:60] Depth 3/1000 time = 5.884239912033081
I0123 13:32:48.673000 140277666050048 ddar.py:60] Depth 4/1000 time = 8.443727970123291
I0123 13:32:58.158855 140277666050048 ddar.py:60] Depth 5/1000 time = 9.485626220703125
I0123 13:33:08.007721 140277666050048 ddar.py:60] Depth 6/1000 time = 9.848557710647583
I0123 13:33:17.829734 140277666050048 ddar.py:60] Depth 7/1000 time = 9.821523189544678
I0123 13:33:17.903509 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:33:17.903615 140277666050048 alphageometry.py:566] LM output (score=-1.784533): "p : T g i g p 21 ;"
I0123 13:33:17.903653 140277666050048 alphageometry.py:567] Translation: "p = on_tline p g g i"

I0123 13:33:17.903690 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g g i ? coll a m n"
I0123 13:33:17.903853 140277666050048 graph.py:498] 
I0123 13:33:17.903911 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g g i ? coll a m n
I0123 13:33:19.932689 140277666050048 ddar.py:60] Depth 1/1000 time = 1.996171474456787
I0123 13:33:23.582423 140277666050048 ddar.py:60] Depth 2/1000 time = 3.649440288543701
I0123 13:33:29.888103 140277666050048 ddar.py:60] Depth 3/1000 time = 6.305487871170044
I0123 13:33:39.650650 140277666050048 ddar.py:60] Depth 4/1000 time = 9.762371063232422
I0123 13:33:50.165941 140277666050048 ddar.py:60] Depth 5/1000 time = 10.515082836151123
I0123 13:34:00.812525 140277666050048 ddar.py:60] Depth 6/1000 time = 10.646355390548706
I0123 13:34:11.301782 140277666050048 ddar.py:60] Depth 7/1000 time = 10.488822937011719
I0123 13:34:11.369135 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:34:11.369260 140277666050048 alphageometry.py:566] LM output (score=-1.898836): "p : T d g g p 21 ;"
I0123 13:34:11.369299 140277666050048 alphageometry.py:567] Translation: "p = on_tline p g d g"

I0123 13:34:11.369335 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g d g ? coll a m n"
I0123 13:34:11.369501 140277666050048 graph.py:498] 
I0123 13:34:11.369558 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g d g ? coll a m n
I0123 13:34:13.417026 140277666050048 ddar.py:60] Depth 1/1000 time = 2.0119264125823975
I0123 13:34:17.056347 140277666050048 ddar.py:60] Depth 2/1000 time = 3.6391375064849854
I0123 13:34:23.726064 140277666050048 ddar.py:60] Depth 3/1000 time = 6.669493198394775
I0123 13:34:32.921234 140277666050048 ddar.py:60] Depth 4/1000 time = 9.1948721408844
I0123 13:34:43.333154 140277666050048 ddar.py:60] Depth 5/1000 time = 10.41172480583191
I0123 13:34:53.983746 140277666050048 ddar.py:60] Depth 6/1000 time = 10.65038251876831
I0123 13:35:04.818240 140277666050048 ddar.py:60] Depth 7/1000 time = 10.834078550338745
I0123 13:35:04.888890 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:35:04.889001 140277666050048 alphageometry.py:566] LM output (score=-2.182722): "p : T b g g p 21 ;"
I0123 13:35:04.889039 140277666050048 alphageometry.py:567] Translation: "p = on_tline p g b g"

I0123 13:35:04.889077 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g b g ? coll a m n"
I0123 13:35:04.889242 140277666050048 graph.py:498] 
I0123 13:35:04.889298 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g b g ? coll a m n
I0123 13:35:06.516685 140277666050048 ddar.py:60] Depth 1/1000 time = 1.586015224456787
I0123 13:35:09.566675 140277666050048 ddar.py:60] Depth 2/1000 time = 3.049651861190796
I0123 13:35:16.230242 140277666050048 ddar.py:60] Depth 3/1000 time = 6.663360595703125
I0123 13:35:25.176448 140277666050048 ddar.py:60] Depth 4/1000 time = 8.946017980575562
I0123 13:35:35.426487 140277666050048 ddar.py:60] Depth 5/1000 time = 10.24982237815857
I0123 13:35:45.777621 140277666050048 ddar.py:60] Depth 6/1000 time = 10.350908041000366
I0123 13:35:55.848741 140277666050048 ddar.py:60] Depth 7/1000 time = 10.070639371871948
I0123 13:35:55.927568 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:35:55.927697 140277666050048 alphageometry.py:566] LM output (score=-2.334275): "p : T d g d p 21 ;"
I0123 13:35:55.927736 140277666050048 alphageometry.py:567] Translation: "p = on_tline p d d g"

I0123 13:35:55.927786 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p d d g ? coll a m n"
I0123 13:35:55.927978 140277666050048 graph.py:498] 
I0123 13:35:55.928034 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p d d g ? coll a m n
I0123 13:35:57.336786 140277666050048 ddar.py:60] Depth 1/1000 time = 1.3677294254302979
I0123 13:36:00.480010 140277666050048 ddar.py:60] Depth 2/1000 time = 3.1430485248565674
I0123 13:36:06.572998 140277666050048 ddar.py:60] Depth 3/1000 time = 6.092811822891235
I0123 13:36:15.478849 140277666050048 ddar.py:60] Depth 4/1000 time = 8.905602216720581
I0123 13:36:25.436846 140277666050048 ddar.py:60] Depth 5/1000 time = 9.957669258117676
I0123 13:36:35.079347 140277666050048 ddar.py:60] Depth 6/1000 time = 9.642238855361938
I0123 13:36:44.977240 140277666050048 ddar.py:60] Depth 7/1000 time = 9.89727783203125
I0123 13:36:45.048062 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:36:45.048165 140277666050048 alphageometry.py:566] LM output (score=-2.366800): "p : T b i b p 21 ;"
I0123 13:36:45.048202 140277666050048 alphageometry.py:567] Translation: "p = on_tline p b b i"

I0123 13:36:45.048238 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b i ? coll a m n"
I0123 13:36:45.048403 140277666050048 graph.py:498] 
I0123 13:36:45.048462 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b i ? coll a m n
I0123 13:36:46.436493 140277666050048 ddar.py:60] Depth 1/1000 time = 1.3476271629333496
I0123 13:36:49.497432 140277666050048 ddar.py:60] Depth 2/1000 time = 3.0607738494873047
I0123 13:36:55.990799 140277666050048 ddar.py:60] Depth 3/1000 time = 6.493111848831177
I0123 13:37:05.209763 140277666050048 ddar.py:60] Depth 4/1000 time = 9.218641996383667
I0123 13:37:15.776811 140277666050048 ddar.py:60] Depth 5/1000 time = 10.566818237304688
I0123 13:37:26.519960 140277666050048 ddar.py:60] Depth 6/1000 time = 10.74283742904663
I0123 13:37:37.284143 140277666050048 ddar.py:60] Depth 7/1000 time = 10.763681411743164
I0123 13:37:37.367765 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:37:37.367900 140277666050048 alphageometry.py:566] LM output (score=-2.373420): "p : T b j b p 21 ;"
I0123 13:37:37.367938 140277666050048 alphageometry.py:567] Translation: "p = on_tline p b b j"

I0123 13:37:37.367987 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b j ? coll a m n"
I0123 13:37:37.368167 140277666050048 graph.py:498] 
I0123 13:37:37.368224 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b j ? coll a m n
I0123 13:37:38.781049 140277666050048 ddar.py:60] Depth 1/1000 time = 1.371471881866455
I0123 13:37:42.194473 140277666050048 ddar.py:60] Depth 2/1000 time = 3.4132282733917236
I0123 13:37:48.494066 140277666050048 ddar.py:60] Depth 3/1000 time = 6.2993879318237305
I0123 13:37:57.451388 140277666050048 ddar.py:60] Depth 4/1000 time = 8.957135915756226
I0123 13:38:07.903960 140277666050048 ddar.py:60] Depth 5/1000 time = 10.452348470687866
I0123 13:38:18.442329 140277666050048 ddar.py:60] Depth 6/1000 time = 10.538161754608154
I0123 13:38:28.667192 140277666050048 ddar.py:60] Depth 7/1000 time = 10.224410057067871
I0123 13:38:28.744884 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:38:28.744990 140277666050048 alphageometry.py:566] LM output (score=-2.467539): "p : T b e b p 21 ;"
I0123 13:38:28.745030 140277666050048 alphageometry.py:567] Translation: "p = on_tline p b b e"

I0123 13:38:28.745071 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b e ? coll a m n"
I0123 13:38:28.745234 140277666050048 graph.py:498] 
I0123 13:38:28.745291 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b e ? coll a m n
I0123 13:38:30.154024 140277666050048 ddar.py:60] Depth 1/1000 time = 1.368171215057373
I0123 13:38:33.095907 140277666050048 ddar.py:60] Depth 2/1000 time = 2.941701889038086
I0123 13:38:39.136105 140277666050048 ddar.py:60] Depth 3/1000 time = 6.040009260177612
I0123 13:38:47.738802 140277666050048 ddar.py:60] Depth 4/1000 time = 8.602439880371094
I0123 13:38:57.510101 140277666050048 ddar.py:60] Depth 5/1000 time = 9.771062850952148
I0123 13:39:07.153020 140277666050048 ddar.py:60] Depth 6/1000 time = 9.642645835876465
I0123 13:39:17.127781 140277666050048 ddar.py:60] Depth 7/1000 time = 9.974168062210083
I0123 13:39:17.203649 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:39:17.203760 140277666050048 alphageometry.py:566] LM output (score=-2.501112): "p : T b g b p 21 ;"
I0123 13:39:17.203799 140277666050048 alphageometry.py:567] Translation: "p = on_tline p b b g"

I0123 13:39:17.203838 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b g ? coll a m n"
I0123 13:39:17.204006 140277666050048 graph.py:498] 
I0123 13:39:17.204067 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b g ? coll a m n
I0123 13:39:18.611589 140277666050048 ddar.py:60] Depth 1/1000 time = 1.3667325973510742
I0123 13:39:22.028904 140277666050048 ddar.py:60] Depth 2/1000 time = 3.4171090126037598
I0123 13:39:28.639439 140277666050048 ddar.py:60] Depth 3/1000 time = 6.610312461853027
I0123 13:39:38.096776 140277666050048 ddar.py:60] Depth 4/1000 time = 9.457098007202148
I0123 13:39:48.778754 140277666050048 ddar.py:60] Depth 5/1000 time = 10.681709289550781
I0123 13:39:59.465910 140277666050048 ddar.py:60] Depth 6/1000 time = 10.686807632446289
I0123 13:40:09.890337 140277666050048 ddar.py:60] Depth 7/1000 time = 10.424001693725586
I0123 13:40:09.971893 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:40:09.971998 140277666050048 alphageometry.py:566] LM output (score=-2.568206): "p : T b h b p 21 ;"
I0123 13:40:09.972037 140277666050048 alphageometry.py:567] Translation: "p = on_tline p b b h"

I0123 13:40:09.972089 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b h ? coll a m n"
I0123 13:40:09.972256 140277666050048 graph.py:498] 
I0123 13:40:09.972315 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b h ? coll a m n
I0123 13:40:11.086854 140277666050048 ddar.py:60] Depth 1/1000 time = 1.0684490203857422
I0123 13:40:14.649518 140277666050048 ddar.py:60] Depth 2/1000 time = 3.562490940093994
I0123 13:40:21.005071 140277666050048 ddar.py:60] Depth 3/1000 time = 6.35535192489624
I0123 13:40:29.763469 140277666050048 ddar.py:60] Depth 4/1000 time = 8.758159637451172
I0123 13:40:40.251996 140277666050048 ddar.py:60] Depth 5/1000 time = 10.488188743591309
I0123 13:40:50.547612 140277666050048 ddar.py:60] Depth 6/1000 time = 10.2953519821167
I0123 13:41:00.831801 140277666050048 ddar.py:60] Depth 7/1000 time = 10.283645153045654
I0123 13:41:00.911936 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:41:00.912045 140277666050048 alphageometry.py:566] LM output (score=-2.569391): "p : C b f p 21 D b f b p 22 ;"
I0123 13:41:00.912084 140277666050048 alphageometry.py:567] Translation: "p = on_line p b f, on_circle p b f"

I0123 13:41:00.912123 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b f, on_circle p b f ? coll a m n"
I0123 13:41:00.912287 140277666050048 graph.py:498] 
I0123 13:41:00.912350 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b f, on_circle p b f ? coll a m n
I0123 13:41:02.945030 140277666050048 ddar.py:60] Depth 1/1000 time = 1.993919849395752
I0123 13:41:06.626458 140277666050048 ddar.py:60] Depth 2/1000 time = 3.681241512298584
I0123 13:41:13.760855 140277666050048 ddar.py:60] Depth 3/1000 time = 7.134176969528198
I0123 13:41:23.483763 140277666050048 ddar.py:60] Depth 4/1000 time = 9.722694873809814
I0123 13:41:34.868077 140277666050048 ddar.py:60] Depth 5/1000 time = 11.384034633636475
I0123 13:41:46.540891 140277666050048 ddar.py:60] Depth 6/1000 time = 11.672464847564697
I0123 13:41:58.437293 140277666050048 ddar.py:60] Depth 7/1000 time = 11.895879745483398
I0123 13:42:10.456856 140277666050048 ddar.py:60] Depth 8/1000 time = 12.018404006958008
I0123 13:42:10.532660 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:42:10.532783 140277666050048 alphageometry.py:566] LM output (score=-2.638973): "p : C b k p 21 D b p k p 22 ;"
I0123 13:42:10.532824 140277666050048 alphageometry.py:567] Translation: "p = on_line p b k, on_bline p k b"

I0123 13:42:10.532865 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b k, on_bline p k b ? coll a m n"
I0123 13:42:10.533038 140277666050048 graph.py:498] 
I0123 13:42:10.533107 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b k, on_bline p k b ? coll a m n
I0123 13:42:11.701919 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1294894218444824
I0123 13:42:15.128573 140277666050048 ddar.py:60] Depth 2/1000 time = 3.4264652729034424
I0123 13:42:21.357724 140277666050048 ddar.py:60] Depth 3/1000 time = 6.228943347930908
I0123 13:42:31.226385 140277666050048 ddar.py:60] Depth 4/1000 time = 9.868454933166504
I0123 13:42:42.326724 140277666050048 ddar.py:60] Depth 5/1000 time = 11.100069522857666
I0123 13:42:52.872043 140277666050048 ddar.py:60] Depth 6/1000 time = 10.544981002807617
I0123 13:43:03.807410 140277666050048 ddar.py:60] Depth 7/1000 time = 10.935096502304077
I0123 13:43:14.744223 140277666050048 ddar.py:60] Depth 8/1000 time = 10.93619680404663
I0123 13:43:14.836375 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:43:14.836485 140277666050048 alphageometry.py:566] LM output (score=-2.679066): "p : C b d p 21 D b d d p 22 ;"
I0123 13:43:14.836521 140277666050048 alphageometry.py:567] Translation: "p = on_line p b d, on_circle p d b"

I0123 13:43:14.836558 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b d, on_circle p d b ? coll a m n"
I0123 13:43:14.836719 140277666050048 graph.py:498] 
I0123 13:43:14.836777 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b d, on_circle p d b ? coll a m n
I0123 13:43:16.444749 140277666050048 ddar.py:60] Depth 1/1000 time = 1.5722556114196777
I0123 13:43:20.538674 140277666050048 ddar.py:60] Depth 2/1000 time = 4.093708276748657
I0123 13:43:30.397199 140277666050048 ddar.py:60] Depth 3/1000 time = 9.858224868774414
I0123 13:43:41.521004 140277666050048 ddar.py:60] Depth 4/1000 time = 11.123577117919922
I0123 13:43:52.950982 140277666050048 ddar.py:60] Depth 5/1000 time = 11.429673194885254
I0123 13:44:04.814257 140277666050048 ddar.py:60] Depth 6/1000 time = 11.863012552261353
I0123 13:44:16.715702 140277666050048 ddar.py:60] Depth 7/1000 time = 11.90085744857788
I0123 13:44:16.813826 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:44:16.813936 140277666050048 alphageometry.py:566] LM output (score=-2.767910): "p : C b h p 21 D b h h p 22 ;"
I0123 13:44:16.813974 140277666050048 alphageometry.py:567] Translation: "p = on_line p b h, on_circle p h b"

I0123 13:44:16.814012 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b h, on_circle p h b ? coll a m n"
I0123 13:44:16.814179 140277666050048 graph.py:498] 
I0123 13:44:16.814237 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b h, on_circle p h b ? coll a m n
I0123 13:44:18.350741 140277666050048 ddar.py:60] Depth 1/1000 time = 1.4975323677062988
I0123 13:44:21.462502 140277666050048 ddar.py:60] Depth 2/1000 time = 3.1115920543670654
I0123 13:44:27.538470 140277666050048 ddar.py:60] Depth 3/1000 time = 6.0757293701171875
I0123 13:44:37.049938 140277666050048 ddar.py:60] Depth 4/1000 time = 9.51114296913147
I0123 13:44:49.346751 140277666050048 ddar.py:60] Depth 5/1000 time = 12.29656457901001
I0123 13:45:01.978740 140277666050048 ddar.py:60] Depth 6/1000 time = 12.631672143936157
I0123 13:45:14.241147 140277666050048 ddar.py:60] Depth 7/1000 time = 12.262181520462036
I0123 13:45:26.340883 140277666050048 ddar.py:60] Depth 8/1000 time = 12.099236488342285
I0123 13:45:26.441586 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:45:26.441695 140277666050048 alphageometry.py:566] LM output (score=-2.774641): "p : C b h p 21 D b p h p 22 ;"
I0123 13:45:26.441734 140277666050048 alphageometry.py:567] Translation: "p = on_line p b h, on_bline p h b"

I0123 13:45:26.441772 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b h, on_bline p h b ? coll a m n"
I0123 13:45:26.441940 140277666050048 graph.py:498] 
I0123 13:45:26.441995 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b h, on_bline p h b ? coll a m n
I0123 13:45:28.037708 140277666050048 ddar.py:60] Depth 1/1000 time = 1.5542902946472168
I0123 13:45:31.836224 140277666050048 ddar.py:60] Depth 2/1000 time = 3.798220634460449
I0123 13:45:39.152200 140277666050048 ddar.py:60] Depth 3/1000 time = 7.315781593322754
I0123 13:45:48.996638 140277666050048 ddar.py:60] Depth 4/1000 time = 9.844266891479492
I0123 13:46:00.799281 140277666050048 ddar.py:60] Depth 5/1000 time = 11.802433490753174
I0123 13:46:12.323926 140277666050048 ddar.py:60] Depth 6/1000 time = 11.524375438690186
I0123 13:46:23.840555 140277666050048 ddar.py:60] Depth 7/1000 time = 11.516306400299072
I0123 13:46:35.722943 140277666050048 ddar.py:60] Depth 8/1000 time = 11.881823062896729
I0123 13:46:35.819155 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:46:35.819288 140277666050048 alphageometry.py:566] LM output (score=-2.786433): "p : T b c b p 21 ;"
I0123 13:46:35.819326 140277666050048 alphageometry.py:567] Translation: "p = on_tline p b b c"

I0123 13:46:35.819375 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b c ? coll a m n"
I0123 13:46:35.819555 140277666050048 graph.py:498] 
I0123 13:46:35.819610 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b c ? coll a m n
I0123 13:46:37.347667 140277666050048 ddar.py:60] Depth 1/1000 time = 1.4879295825958252
I0123 13:46:40.439626 140277666050048 ddar.py:60] Depth 2/1000 time = 3.0917842388153076
I0123 13:46:46.425788 140277666050048 ddar.py:60] Depth 3/1000 time = 5.985995769500732
I0123 13:46:55.370666 140277666050048 ddar.py:60] Depth 4/1000 time = 8.944657802581787
I0123 13:47:05.271215 140277666050048 ddar.py:60] Depth 5/1000 time = 9.90025806427002
I0123 13:47:15.277307 140277666050048 ddar.py:60] Depth 6/1000 time = 10.005843162536621
I0123 13:47:24.857225 140277666050048 ddar.py:60] Depth 7/1000 time = 9.579274415969849
I0123 13:47:24.933517 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:47:24.933630 140277666050048 alphageometry.py:566] LM output (score=-2.850863): "p : T c d c p 21 ;"
I0123 13:47:24.933674 140277666050048 alphageometry.py:567] Translation: "p = on_tline p c c d"

I0123 13:47:24.933712 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c c d ? coll a m n"
I0123 13:47:24.933881 140277666050048 graph.py:498] 
I0123 13:47:24.933939 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c c d ? coll a m n
I0123 13:47:26.777154 140277666050048 ddar.py:60] Depth 1/1000 time = 1.8000154495239258
I0123 13:47:30.130566 140277666050048 ddar.py:60] Depth 2/1000 time = 3.353243589401245
I0123 13:47:37.764405 140277666050048 ddar.py:60] Depth 3/1000 time = 7.633593559265137
I0123 13:47:47.267392 140277666050048 ddar.py:60] Depth 4/1000 time = 9.502662181854248
I0123 13:47:57.158369 140277666050048 ddar.py:60] Depth 5/1000 time = 9.890795469284058
I0123 13:48:08.032026 140277666050048 ddar.py:60] Depth 6/1000 time = 10.873430490493774
I0123 13:48:18.051469 140277666050048 ddar.py:60] Depth 7/1000 time = 10.019032716751099
I0123 13:48:18.133354 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:48:18.133483 140277666050048 alphageometry.py:566] LM output (score=-2.881071): "p : T b f b p 21 ;"
I0123 13:48:18.133521 140277666050048 alphageometry.py:567] Translation: "p = on_tline p b b f"

I0123 13:48:18.133561 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b f ? coll a m n"
I0123 13:48:18.133738 140277666050048 graph.py:498] 
I0123 13:48:18.133798 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b f ? coll a m n
I0123 13:48:19.653065 140277666050048 ddar.py:60] Depth 1/1000 time = 1.4784398078918457
I0123 13:48:22.822492 140277666050048 ddar.py:60] Depth 2/1000 time = 3.1691253185272217
I0123 13:48:29.246502 140277666050048 ddar.py:60] Depth 3/1000 time = 6.423820495605469
I0123 13:48:37.722488 140277666050048 ddar.py:60] Depth 4/1000 time = 8.475802659988403
I0123 13:48:47.882740 140277666050048 ddar.py:60] Depth 5/1000 time = 10.160046815872192
I0123 13:48:57.764116 140277666050048 ddar.py:60] Depth 6/1000 time = 9.88114070892334
I0123 13:49:07.218066 140277666050048 ddar.py:60] Depth 7/1000 time = 9.453447103500366
I0123 13:49:07.289171 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:49:07.289304 140277666050048 alphageometry.py:566] LM output (score=-2.890864): "p : P b e k p 21 ;"
I0123 13:49:07.289342 140277666050048 alphageometry.py:567] Translation: "p = on_pline p k b e"

I0123 13:49:07.289392 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_pline p k b e ? coll a m n"
I0123 13:49:07.289566 140277666050048 graph.py:498] 
I0123 13:49:07.289622 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_pline p k b e ? coll a m n
I0123 13:49:09.227474 140277666050048 ddar.py:60] Depth 1/1000 time = 1.8975677490234375
I0123 13:49:11.958255 140277666050048 ddar.py:60] Depth 2/1000 time = 2.7306222915649414
I0123 13:49:18.383066 140277666050048 ddar.py:60] Depth 3/1000 time = 6.4246416091918945
I0123 13:49:26.913848 140277666050048 ddar.py:60] Depth 4/1000 time = 8.530538558959961
I0123 13:49:37.345899 140277666050048 ddar.py:60] Depth 5/1000 time = 10.43173336982727
I0123 13:49:46.820643 140277666050048 ddar.py:60] Depth 6/1000 time = 9.47450065612793
I0123 13:49:56.671050 140277666050048 ddar.py:60] Depth 7/1000 time = 9.84982419013977
I0123 13:49:56.740215 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:49:56.740319 140277666050048 alphageometry.py:566] LM output (score=-2.991892): "p : P b e o p 21 ;"
I0123 13:49:56.740355 140277666050048 alphageometry.py:567] Translation: "p = on_pline p o b e"

I0123 13:49:56.740391 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_pline p o b e ? coll a m n"
I0123 13:49:56.740557 140277666050048 graph.py:498] 
I0123 13:49:56.740614 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_pline p o b e ? coll a m n
I0123 13:49:58.637414 140277666050048 ddar.py:60] Depth 1/1000 time = 1.8567988872528076
I0123 13:50:01.840916 140277666050048 ddar.py:60] Depth 2/1000 time = 3.203336000442505
I0123 13:50:07.933861 140277666050048 ddar.py:60] Depth 3/1000 time = 6.092722177505493
I0123 13:50:16.508305 140277666050048 ddar.py:60] Depth 4/1000 time = 8.574142932891846
I0123 13:50:26.413945 140277666050048 ddar.py:60] Depth 5/1000 time = 9.90545129776001
I0123 13:50:36.329874 140277666050048 ddar.py:60] Depth 6/1000 time = 9.915696382522583
I0123 13:50:46.659035 140277666050048 ddar.py:60] Depth 7/1000 time = 10.328688144683838
I0123 13:50:46.729499 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:50:46.729605 140277666050048 alphageometry.py:566] LM output (score=-3.055666): "p : D b o b p 21 D b o o p 22 ;"
I0123 13:50:46.729645 140277666050048 alphageometry.py:567] Translation: "p = on_circle p b o, on_circle p o b"

I0123 13:50:46.729684 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_circle p b o, on_circle p o b ? coll a m n"
I0123 13:50:46.729855 140277666050048 graph.py:498] 
I0123 13:50:46.729914 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_circle p b o, on_circle p o b ? coll a m n
I0123 13:50:47.962128 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1906676292419434
I0123 13:50:51.427519 140277666050048 ddar.py:60] Depth 2/1000 time = 3.465202569961548
I0123 13:50:57.977538 140277666050048 ddar.py:60] Depth 3/1000 time = 6.549823999404907
I0123 13:51:07.498219 140277666050048 ddar.py:60] Depth 4/1000 time = 9.520494937896729
I0123 13:51:18.861287 140277666050048 ddar.py:60] Depth 5/1000 time = 11.362854957580566
I0123 13:51:30.400154 140277666050048 ddar.py:60] Depth 6/1000 time = 11.538618803024292
I0123 13:51:42.060530 140277666050048 ddar.py:60] Depth 7/1000 time = 11.659910440444946
I0123 13:51:53.530991 140277666050048 ddar.py:60] Depth 8/1000 time = 11.469518661499023
I0123 13:52:05.106947 140277666050048 ddar.py:60] Depth 9/1000 time = 11.573642015457153
I0123 13:52:05.188382 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:52:05.188493 140277666050048 alphageometry.py:566] LM output (score=-3.104885): "p : T c g g p 21 ;"
I0123 13:52:05.188529 140277666050048 alphageometry.py:567] Translation: "p = on_tline p g c g"

I0123 13:52:05.188567 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g c g ? coll a m n"
I0123 13:52:05.188740 140277666050048 graph.py:498] 
I0123 13:52:05.188799 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g c g ? coll a m n
I0123 13:52:06.951039 140277666050048 ddar.py:60] Depth 1/1000 time = 1.729517936706543
I0123 13:52:10.862527 140277666050048 ddar.py:60] Depth 2/1000 time = 3.911320447921753
I0123 13:52:17.848280 140277666050048 ddar.py:60] Depth 3/1000 time = 6.985516309738159
I0123 13:52:28.476074 140277666050048 ddar.py:60] Depth 4/1000 time = 10.6274893283844
I0123 13:52:41.223242 140277666050048 ddar.py:60] Depth 5/1000 time = 12.746955633163452
I0123 13:52:53.073009 140277666050048 ddar.py:60] Depth 6/1000 time = 11.849552869796753
I0123 13:53:04.780274 140277666050048 ddar.py:60] Depth 7/1000 time = 11.706788063049316
I0123 13:53:04.854078 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:53:04.854207 140277666050048 alphageometry.py:566] LM output (score=-3.154477): "p : D b o b p 21 D c o c p 22 ;"
I0123 13:53:04.854243 140277666050048 alphageometry.py:567] Translation: "p = on_circle p b o, on_circle p c o"

I0123 13:53:04.854295 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_circle p b o, on_circle p c o ? coll a m n"
I0123 13:53:04.854479 140277666050048 graph.py:498] 
I0123 13:53:04.854534 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_circle p b o, on_circle p c o ? coll a m n
I0123 13:53:06.570544 140277666050048 ddar.py:60] Depth 1/1000 time = 1.6706297397613525
I0123 13:53:10.263602 140277666050048 ddar.py:60] Depth 2/1000 time = 3.6928868293762207
I0123 13:53:17.802742 140277666050048 ddar.py:60] Depth 3/1000 time = 7.538965225219727
I0123 13:53:28.506109 140277666050048 ddar.py:60] Depth 4/1000 time = 10.703155040740967
I0123 13:53:40.039142 140277666050048 ddar.py:60] Depth 5/1000 time = 11.53283977508545
I0123 13:53:52.225637 140277666050048 ddar.py:60] Depth 6/1000 time = 12.186276197433472
I0123 13:54:03.985764 140277666050048 ddar.py:60] Depth 7/1000 time = 11.759445905685425
I0123 13:54:15.798537 140277666050048 ddar.py:60] Depth 8/1000 time = 11.784407615661621
I0123 13:54:27.744604 140277666050048 ddar.py:60] Depth 9/1000 time = 11.909067869186401
I0123 13:54:39.377336 140277666050048 ddar.py:60] Depth 10/1000 time = 11.597229480743408
I0123 13:54:39.387003 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:54:39.387110 140277666050048 alphageometry.py:566] LM output (score=-3.154649): "p : T b n b p 21 ;"
I0123 13:54:39.387147 140277666050048 alphageometry.py:567] Translation: "p = on_tline p b b n"

I0123 13:54:39.387185 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b n ? coll a m n"
I0123 13:54:39.387353 140277666050048 graph.py:498] 
I0123 13:54:39.387410 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b n ? coll a m n
I0123 13:54:41.385926 140277666050048 ddar.py:60] Depth 1/1000 time = 1.9581630229949951
I0123 13:54:44.215194 140277666050048 ddar.py:60] Depth 2/1000 time = 2.8291032314300537
I0123 13:54:50.832324 140277666050048 ddar.py:60] Depth 3/1000 time = 6.616932153701782
I0123 13:54:59.034171 140277666050048 ddar.py:60] Depth 4/1000 time = 8.201651811599731
I0123 13:55:09.080129 140277666050048 ddar.py:60] Depth 5/1000 time = 10.045717477798462
I0123 13:55:19.166627 140277666050048 ddar.py:60] Depth 6/1000 time = 10.086182594299316
I0123 13:55:29.801159 140277666050048 ddar.py:60] Depth 7/1000 time = 10.63405466079712
I0123 13:55:29.867425 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:55:29.867522 140277666050048 alphageometry.py:566] LM output (score=-3.172437): "p : P b c h p 21 ;"
I0123 13:55:29.867560 140277666050048 alphageometry.py:567] Translation: "p = on_pline p h b c"

I0123 13:55:29.867596 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_pline p h b c ? coll a m n"
I0123 13:55:29.867755 140277666050048 graph.py:498] 
I0123 13:55:29.867808 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_pline p h b c ? coll a m n
I0123 13:55:30.973380 140277666050048 ddar.py:60] Depth 1/1000 time = 1.0622937679290771
I0123 13:55:34.263319 140277666050048 ddar.py:60] Depth 2/1000 time = 3.289750814437866
I0123 13:55:40.446782 140277666050048 ddar.py:60] Depth 3/1000 time = 6.1832873821258545
I0123 13:55:48.692263 140277666050048 ddar.py:60] Depth 4/1000 time = 8.245301246643066
I0123 13:55:58.731463 140277666050048 ddar.py:60] Depth 5/1000 time = 10.038969039916992
I0123 13:56:08.779805 140277666050048 ddar.py:60] Depth 6/1000 time = 10.048119306564331
I0123 13:56:19.015897 140277666050048 ddar.py:60] Depth 7/1000 time = 10.235517024993896
I0123 13:56:19.085175 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:56:19.085272 140277666050048 alphageometry.py:566] LM output (score=-3.192471): "p : T b f g p 21 ;"
I0123 13:56:19.085307 140277666050048 alphageometry.py:567] Translation: "p = on_tline p g b f"

I0123 13:56:19.085344 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g b f ? coll a m n"
I0123 13:56:19.085502 140277666050048 graph.py:498] 
I0123 13:56:19.085567 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p g b f ? coll a m n
I0123 13:56:20.184990 140277666050048 ddar.py:60] Depth 1/1000 time = 1.0592422485351562
I0123 13:56:23.494648 140277666050048 ddar.py:60] Depth 2/1000 time = 3.3094842433929443
I0123 13:56:29.734677 140277666050048 ddar.py:60] Depth 3/1000 time = 6.239858865737915
I0123 13:56:38.185616 140277666050048 ddar.py:60] Depth 4/1000 time = 8.450733661651611
I0123 13:56:48.356151 140277666050048 ddar.py:60] Depth 5/1000 time = 10.17029857635498
I0123 13:56:58.178761 140277666050048 ddar.py:60] Depth 6/1000 time = 9.82234263420105
I0123 13:57:08.928245 140277666050048 ddar.py:60] Depth 7/1000 time = 10.748833179473877
I0123 13:57:09.001867 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:57:09.001976 140277666050048 alphageometry.py:566] LM output (score=-3.256724): "p : P b c f p 21 ;"
I0123 13:57:09.002013 140277666050048 alphageometry.py:567] Translation: "p = on_pline p f b c"

I0123 13:57:09.002051 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_pline p f b c ? coll a m n"
I0123 13:57:09.002219 140277666050048 graph.py:498] 
I0123 13:57:09.002276 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_pline p f b c ? coll a m n
I0123 13:57:10.589043 140277666050048 ddar.py:60] Depth 1/1000 time = 1.5444624423980713
I0123 13:57:13.441838 140277666050048 ddar.py:60] Depth 2/1000 time = 2.852619171142578
I0123 13:57:19.710848 140277666050048 ddar.py:60] Depth 3/1000 time = 6.268791437149048
I0123 13:57:28.601949 140277666050048 ddar.py:60] Depth 4/1000 time = 8.890881299972534
I0123 13:57:39.001878 140277666050048 ddar.py:60] Depth 5/1000 time = 10.399684190750122
I0123 13:57:48.960056 140277666050048 ddar.py:60] Depth 6/1000 time = 9.957849979400635
I0123 13:58:00.042826 140277666050048 ddar.py:60] Depth 7/1000 time = 11.082251787185669
I0123 13:58:00.114591 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:58:00.114689 140277666050048 alphageometry.py:566] LM output (score=-3.310740): "p : D d g d p 21 D g f f p 22 ;"
I0123 13:58:00.114727 140277666050048 alphageometry.py:567] Translation: "p = on_circle p d g, on_circle p f g"

I0123 13:58:00.114766 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_circle p d g, on_circle p f g ? coll a m n"
I0123 13:58:00.114937 140277666050048 graph.py:498] 
I0123 13:58:00.114994 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_circle p d g, on_circle p f g ? coll a m n
I0123 13:58:01.337700 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1728856563568115
I0123 13:58:05.211036 140277666050048 ddar.py:60] Depth 2/1000 time = 3.8731558322906494
I0123 13:58:13.046711 140277666050048 ddar.py:60] Depth 3/1000 time = 7.835503578186035
I0123 13:58:28.046730 140277666050048 ddar.py:60] Depth 4/1000 time = 14.999817371368408
I0123 13:58:45.417617 140277666050048 ddar.py:60] Depth 5/1000 time = 17.37059450149536
I0123 13:59:03.474356 140277666050048 ddar.py:60] Depth 6/1000 time = 18.056352853775024
I0123 13:59:20.958494 140277666050048 ddar.py:60] Depth 7/1000 time = 17.483309745788574
I0123 13:59:38.527072 140277666050048 ddar.py:60] Depth 8/1000 time = 17.492942333221436
I0123 13:59:56.638581 140277666050048 ddar.py:60] Depth 9/1000 time = 18.032201290130615
I0123 13:59:56.650394 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:59:56.650505 140277666050048 alphageometry.py:566] LM output (score=-3.320358): "p : C b c p 21 T b c i p 22 ;"
I0123 13:59:56.650543 140277666050048 alphageometry.py:567] Translation: "p = on_line p b c, on_tline p i b c"

I0123 13:59:56.650581 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b c, on_tline p i b c ? coll a m n"
I0123 13:59:56.650760 140277666050048 graph.py:498] 
I0123 13:59:56.650817 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_line p b c, on_tline p i b c ? coll a m n
I0123 13:59:59.195525 140277666050048 ddar.py:60] Depth 1/1000 time = 2.5039589405059814
I0123 14:00:03.286832 140277666050048 ddar.py:60] Depth 2/1000 time = 4.091112375259399
I0123 14:00:11.785181 140277666050048 ddar.py:60] Depth 3/1000 time = 8.498146533966064
I0123 14:00:23.619460 140277666050048 ddar.py:60] Depth 4/1000 time = 11.834036588668823
I0123 14:00:36.958778 140277666050048 ddar.py:60] Depth 5/1000 time = 13.339005947113037
I0123 14:00:50.578801 140277666050048 ddar.py:60] Depth 6/1000 time = 13.619807720184326
I0123 14:01:03.613422 140277666050048 ddar.py:60] Depth 7/1000 time = 13.034053802490234
I0123 14:01:03.721833 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:01:03.721978 140277666050048 alphageometry.py:566] LM output (score=-3.356231): "p : D d g d p 21 ;"
I0123 14:01:03.722016 140277666050048 alphageometry.py:567] Translation: "p = on_circle p d g"

I0123 14:01:03.722066 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_circle p d g ? coll a m n"
I0123 14:01:03.722246 140277666050048 graph.py:498] 
I0123 14:01:03.722302 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_circle p d g ? coll a m n
I0123 14:01:04.835146 140277666050048 ddar.py:60] Depth 1/1000 time = 1.0730853080749512
I0123 14:01:08.331150 140277666050048 ddar.py:60] Depth 2/1000 time = 3.495809555053711
I0123 14:01:14.247089 140277666050048 ddar.py:60] Depth 3/1000 time = 5.915769338607788
I0123 14:01:23.746169 140277666050048 ddar.py:60] Depth 4/1000 time = 9.498836040496826
I0123 14:01:33.793951 140277666050048 ddar.py:60] Depth 5/1000 time = 10.047480583190918
I0123 14:01:43.685021 140277666050048 ddar.py:60] Depth 6/1000 time = 9.890871286392212
I0123 14:01:54.155771 140277666050048 ddar.py:60] Depth 7/1000 time = 10.470309019088745
I0123 14:01:54.226882 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:01:54.226983 140277666050048 alphageometry.py:566] LM output (score=-3.385612): "p : ^ d b d e p b p e 21 ;"
I0123 14:01:54.227031 140277666050048 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ d b d e p b p e"

I0123 14:01:54.227072 140277666050048 alphageometry.py:540] Depth 1. There are 31 nodes to expand:
I0123 14:01:54.227103 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b d b p 21 ; x00
I0123 14:01:54.227131 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b c c p 21 ; x00
I0123 14:01:54.227155 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T g i g p 21 ; x00
I0123 14:01:54.227179 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T d g g p 21 ; x00
I0123 14:01:54.227203 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b g g p 21 ; x00
I0123 14:01:54.227226 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T d g d p 21 ; x00
I0123 14:01:54.227249 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b i b p 21 ; x00
I0123 14:01:54.227272 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b j b p 21 ; x00
I0123 14:01:54.227294 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b e b p 21 ; x00
I0123 14:01:54.227321 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b g b p 21 ; x00
I0123 14:01:54.227345 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b h b p 21 ; x00
I0123 14:01:54.227367 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : C b f p 21 D b f b p 22 ; x00
I0123 14:01:54.227390 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : C b k p 21 D b p k p 22 ; x00
I0123 14:01:54.227413 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : C b d p 21 D b d d p 22 ; x00
I0123 14:01:54.227435 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : C b h p 21 D b h h p 22 ; x00
I0123 14:01:54.227458 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : C b h p 21 D b p h p 22 ; x00
I0123 14:01:54.227480 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b c b p 21 ; x00
I0123 14:01:54.227503 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T c d c p 21 ; x00
I0123 14:01:54.227529 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b f b p 21 ; x00
I0123 14:01:54.227554 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : P b e k p 21 ; x00
I0123 14:01:54.227578 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : P b e o p 21 ; x00
I0123 14:01:54.227600 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : D b o b p 21 D b o o p 22 ; x00
I0123 14:01:54.227623 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T c g g p 21 ; x00
I0123 14:01:54.227645 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : D b o b p 21 D c o c p 22 ; x00
I0123 14:01:54.227668 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b n b p 21 ; x00
I0123 14:01:54.227690 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : P b c h p 21 ; x00
I0123 14:01:54.227713 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b f g p 21 ; x00
I0123 14:01:54.227738 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : P b c f p 21 ; x00
I0123 14:01:54.227762 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : D d g d p 21 D g f f p 22 ; x00
I0123 14:01:54.227784 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : C b c p 21 T b c i p 22 ; x00
I0123 14:01:54.227806 140277666050048 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : D d g d p 21 ; x00
I0123 14:01:54.227831 140277666050048 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b d b p 21 ; x00
I0123 14:02:01.264439 140277666050048 alphageometry.py:566] LM output (score=-1.313188): "q : T b c c q 22 ;"
I0123 14:02:01.264590 140277666050048 alphageometry.py:567] Translation: "q = on_tline q c b c"

I0123 14:02:01.264640 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q c b c ? coll a m n"
I0123 14:02:01.264870 140277666050048 graph.py:498] 
I0123 14:02:01.264933 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q c b c ? coll a m n
I0123 14:02:02.458021 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1439616680145264
I0123 14:02:06.247354 140277666050048 ddar.py:60] Depth 2/1000 time = 3.789151430130005
I0123 14:02:14.282078 140277666050048 ddar.py:60] Depth 3/1000 time = 8.034542083740234
I0123 14:02:25.325378 140277666050048 ddar.py:60] Depth 4/1000 time = 11.043095588684082
I0123 14:02:36.618940 140277666050048 ddar.py:60] Depth 5/1000 time = 11.293356657028198
I0123 14:02:47.440977 140277666050048 ddar.py:60] Depth 6/1000 time = 10.821850299835205
I0123 14:02:58.145001 140277666050048 ddar.py:60] Depth 7/1000 time = 10.703583002090454
I0123 14:02:58.237005 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:02:58.237071 140277666050048 alphageometry.py:566] LM output (score=-1.439693): "q : P b p d q 22 ;"
I0123 14:02:58.237118 140277666050048 alphageometry.py:567] Translation: "q = on_pline q d b p"

I0123 14:02:58.237159 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_pline q d b p ? coll a m n"
I0123 14:02:58.237331 140277666050048 graph.py:498] 
I0123 14:02:58.237389 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_pline q d b p ? coll a m n
I0123 14:03:00.028520 140277666050048 ddar.py:60] Depth 1/1000 time = 1.7341234683990479
I0123 14:03:03.851219 140277666050048 ddar.py:60] Depth 2/1000 time = 3.822420358657837
I0123 14:03:12.497883 140277666050048 ddar.py:60] Depth 3/1000 time = 8.646491050720215
I0123 14:03:22.933605 140277666050048 ddar.py:60] Depth 4/1000 time = 10.435519218444824
I0123 14:03:33.629073 140277666050048 ddar.py:60] Depth 5/1000 time = 10.695245742797852
I0123 14:03:44.452234 140277666050048 ddar.py:60] Depth 6/1000 time = 10.82292628288269
I0123 14:03:55.832601 140277666050048 ddar.py:60] Depth 7/1000 time = 11.379764318466187
I0123 14:03:55.915841 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:03:55.915923 140277666050048 alphageometry.py:566] LM output (score=-1.580127): "q : T b p h q 22 ;"
I0123 14:03:55.915960 140277666050048 alphageometry.py:567] Translation: "q = on_tline q h b p"

I0123 14:03:55.916011 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b p ? coll a m n"
I0123 14:03:55.916203 140277666050048 graph.py:498] 
I0123 14:03:55.916263 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b p ? coll a m n
I0123 14:03:57.085741 140277666050048 ddar.py:60] Depth 1/1000 time = 1.118912696838379
I0123 14:04:00.963451 140277666050048 ddar.py:60] Depth 2/1000 time = 3.877507209777832
I0123 14:04:08.463009 140277666050048 ddar.py:60] Depth 3/1000 time = 7.4993813037872314
I0123 14:04:18.568118 140277666050048 ddar.py:60] Depth 4/1000 time = 10.104852199554443
I0123 14:04:29.645883 140277666050048 ddar.py:60] Depth 5/1000 time = 11.077446460723877
I0123 14:04:40.342121 140277666050048 ddar.py:60] Depth 6/1000 time = 10.69601058959961
I0123 14:04:50.937904 140277666050048 ddar.py:60] Depth 7/1000 time = 10.59521746635437
I0123 14:04:51.020313 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:04:51.020399 140277666050048 alphageometry.py:566] LM output (score=-1.582259): "q : T g i g q 22 ;"
I0123 14:04:51.020435 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g g i"

I0123 14:04:51.020484 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g g i ? coll a m n"
I0123 14:04:51.020669 140277666050048 graph.py:498] 
I0123 14:04:51.020727 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g g i ? coll a m n
I0123 14:04:53.651312 140277666050048 ddar.py:60] Depth 1/1000 time = 2.5893871784210205
I0123 14:04:57.856856 140277666050048 ddar.py:60] Depth 2/1000 time = 4.2053728103637695
I0123 14:05:06.533599 140277666050048 ddar.py:60] Depth 3/1000 time = 8.676564455032349
I0123 14:05:18.787039 140277666050048 ddar.py:60] Depth 4/1000 time = 12.253225326538086
I0123 14:05:30.977963 140277666050048 ddar.py:60] Depth 5/1000 time = 12.190638780593872
I0123 14:05:43.092478 140277666050048 ddar.py:60] Depth 6/1000 time = 12.114170551300049
I0123 14:05:55.177163 140277666050048 ddar.py:60] Depth 7/1000 time = 12.08424973487854
I0123 14:05:55.254450 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:05:55.254515 140277666050048 alphageometry.py:566] LM output (score=-1.651075): "q : T b c h q 22 ;"
I0123 14:05:55.254550 140277666050048 alphageometry.py:567] Translation: "q = on_tline q h b c"

I0123 14:05:55.254586 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b c ? coll a m n"
I0123 14:05:55.254755 140277666050048 graph.py:498] 
I0123 14:05:55.254811 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b c ? coll a m n
I0123 14:05:57.012572 140277666050048 ddar.py:60] Depth 1/1000 time = 1.7080364227294922
I0123 14:06:00.912603 140277666050048 ddar.py:60] Depth 2/1000 time = 3.899864673614502
I0123 14:06:08.574783 140277666050048 ddar.py:60] Depth 3/1000 time = 7.661930561065674
I0123 14:06:19.153527 140277666050048 ddar.py:60] Depth 4/1000 time = 10.578433513641357
I0123 14:06:29.912615 140277666050048 ddar.py:60] Depth 5/1000 time = 10.758817434310913
I0123 14:06:41.429284 140277666050048 ddar.py:60] Depth 6/1000 time = 11.516278743743896
I0123 14:06:52.807774 140277666050048 ddar.py:60] Depth 7/1000 time = 11.377876996994019
I0123 14:06:52.894398 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:06:52.894511 140277666050048 alphageometry.py:566] LM output (score=-1.988199): "q : T b h h q 22 ;"
I0123 14:06:52.894549 140277666050048 alphageometry.py:567] Translation: "q = on_tline q h b h"

I0123 14:06:52.894586 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b h ? coll a m n"
I0123 14:06:52.894771 140277666050048 graph.py:498] 
I0123 14:06:52.894835 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b h ? coll a m n
I0123 14:06:54.702867 140277666050048 ddar.py:60] Depth 1/1000 time = 1.7568457126617432
I0123 14:06:58.335203 140277666050048 ddar.py:60] Depth 2/1000 time = 3.632166862487793
I0123 14:07:07.631705 140277666050048 ddar.py:60] Depth 3/1000 time = 9.296308517456055
I0123 14:07:18.272830 140277666050048 ddar.py:60] Depth 4/1000 time = 10.640872955322266
I0123 14:07:29.684006 140277666050048 ddar.py:60] Depth 5/1000 time = 11.410860300064087
I0123 14:07:41.827334 140277666050048 ddar.py:60] Depth 6/1000 time = 12.143096923828125
I0123 14:07:53.396414 140277666050048 ddar.py:60] Depth 7/1000 time = 11.568631410598755
I0123 14:07:53.491532 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:07:53.491606 140277666050048 alphageometry.py:566] LM output (score=-2.030419): "q : T b c f q 22 ;"
I0123 14:07:53.491641 140277666050048 alphageometry.py:567] Translation: "q = on_tline q f b c"

I0123 14:07:53.491679 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q f b c ? coll a m n"
I0123 14:07:53.491855 140277666050048 graph.py:498] 
I0123 14:07:53.491914 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q f b c ? coll a m n
I0123 14:07:55.317790 140277666050048 ddar.py:60] Depth 1/1000 time = 1.7729074954986572
I0123 14:07:59.329175 140277666050048 ddar.py:60] Depth 2/1000 time = 4.011080503463745
I0123 14:08:07.255031 140277666050048 ddar.py:60] Depth 3/1000 time = 7.925684213638306
I0123 14:08:17.644958 140277666050048 ddar.py:60] Depth 4/1000 time = 10.389682292938232
I0123 14:08:28.352889 140277666050048 ddar.py:60] Depth 5/1000 time = 10.707592964172363
I0123 14:08:39.882345 140277666050048 ddar.py:60] Depth 6/1000 time = 11.52922773361206
I0123 14:08:50.684463 140277666050048 ddar.py:60] Depth 7/1000 time = 10.80157732963562
I0123 14:08:50.768492 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:08:50.768574 140277666050048 alphageometry.py:566] LM output (score=-2.047038): "q : T b p g q 22 ;"
I0123 14:08:50.768610 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g b p"

I0123 14:08:50.768646 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g b p ? coll a m n"
I0123 14:08:50.768825 140277666050048 graph.py:498] 
I0123 14:08:50.768884 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g b p ? coll a m n
I0123 14:08:51.935302 140277666050048 ddar.py:60] Depth 1/1000 time = 1.116835355758667
I0123 14:08:55.873744 140277666050048 ddar.py:60] Depth 2/1000 time = 3.9381299018859863
I0123 14:09:03.190456 140277666050048 ddar.py:60] Depth 3/1000 time = 7.316519498825073
I0123 14:09:14.071924 140277666050048 ddar.py:60] Depth 4/1000 time = 10.881211757659912
I0123 14:09:24.555655 140277666050048 ddar.py:60] Depth 5/1000 time = 10.483422040939331
I0123 14:09:35.977719 140277666050048 ddar.py:60] Depth 6/1000 time = 11.421837568283081
I0123 14:09:46.646115 140277666050048 ddar.py:60] Depth 7/1000 time = 10.667802333831787
I0123 14:09:46.722878 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:09:46.722971 140277666050048 alphageometry.py:566] LM output (score=-2.122879): "q : T d g g q 22 ;"
I0123 14:09:46.723007 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g d g"

I0123 14:09:46.723053 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g d g ? coll a m n"
I0123 14:09:46.723253 140277666050048 graph.py:498] 
I0123 14:09:46.723311 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g d g ? coll a m n
I0123 14:09:48.847032 140277666050048 ddar.py:60] Depth 1/1000 time = 2.082597255706787
I0123 14:09:53.144829 140277666050048 ddar.py:60] Depth 2/1000 time = 4.297608137130737
I0123 14:10:01.823228 140277666050048 ddar.py:60] Depth 3/1000 time = 8.678230285644531
I0123 14:10:13.434414 140277666050048 ddar.py:60] Depth 4/1000 time = 11.610962390899658
I0123 14:10:24.972521 140277666050048 ddar.py:60] Depth 5/1000 time = 11.537861585617065
I0123 14:10:37.038701 140277666050048 ddar.py:60] Depth 6/1000 time = 12.065861701965332
I0123 14:10:49.800906 140277666050048 ddar.py:60] Depth 7/1000 time = 12.761786937713623
I0123 14:10:49.878047 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:10:49.878110 140277666050048 alphageometry.py:566] LM output (score=-2.198221): "q : T b e b q 22 ;"
I0123 14:10:49.878144 140277666050048 alphageometry.py:567] Translation: "q = on_tline q b b e"

I0123 14:10:49.878184 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b e ? coll a m n"
I0123 14:10:49.878352 140277666050048 graph.py:498] 
I0123 14:10:49.878408 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b e ? coll a m n
I0123 14:10:51.090015 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1617670059204102
I0123 14:10:54.458217 140277666050048 ddar.py:60] Depth 2/1000 time = 3.3680272102355957
I0123 14:11:03.038746 140277666050048 ddar.py:60] Depth 3/1000 time = 8.58027458190918
I0123 14:11:14.097712 140277666050048 ddar.py:60] Depth 4/1000 time = 11.058626651763916
I0123 14:11:25.200258 140277666050048 ddar.py:60] Depth 5/1000 time = 11.102288246154785
I0123 14:11:36.151171 140277666050048 ddar.py:60] Depth 6/1000 time = 10.950586318969727
I0123 14:11:47.776820 140277666050048 ddar.py:60] Depth 7/1000 time = 11.625131130218506
I0123 14:11:47.857943 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:11:47.858027 140277666050048 alphageometry.py:566] LM output (score=-2.369900): "q : T c g g q 22 ;"
I0123 14:11:47.858062 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g c g"

I0123 14:11:47.858099 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g c g ? coll a m n"
I0123 14:11:47.858272 140277666050048 graph.py:498] 
I0123 14:11:47.858328 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g c g ? coll a m n
I0123 14:11:49.897118 140277666050048 ddar.py:60] Depth 1/1000 time = 1.9976887702941895
I0123 14:11:54.144548 140277666050048 ddar.py:60] Depth 2/1000 time = 4.247248888015747
I0123 14:12:03.493592 140277666050048 ddar.py:60] Depth 3/1000 time = 9.348838806152344
I0123 14:12:15.710962 140277666050048 ddar.py:60] Depth 4/1000 time = 12.217094659805298
I0123 14:12:28.868623 140277666050048 ddar.py:60] Depth 5/1000 time = 13.157341718673706
I0123 14:12:41.394600 140277666050048 ddar.py:60] Depth 6/1000 time = 12.525705575942993
I0123 14:12:55.270126 140277666050048 ddar.py:60] Depth 7/1000 time = 13.874947309494019
I0123 14:12:55.356127 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:12:55.356224 140277666050048 alphageometry.py:566] LM output (score=-2.501922): "q : T b h b q 22 ;"
I0123 14:12:55.356260 140277666050048 alphageometry.py:567] Translation: "q = on_tline q b b h"

I0123 14:12:55.356313 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b h ? coll a m n"
I0123 14:12:55.356502 140277666050048 graph.py:498] 
I0123 14:12:55.356559 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b h ? coll a m n
I0123 14:12:56.567558 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1579978466033936
I0123 14:13:00.318479 140277666050048 ddar.py:60] Depth 2/1000 time = 3.750744342803955
I0123 14:13:09.251178 140277666050048 ddar.py:60] Depth 3/1000 time = 8.932515621185303
I0123 14:13:20.136747 140277666050048 ddar.py:60] Depth 4/1000 time = 10.88534951210022
I0123 14:13:32.551935 140277666050048 ddar.py:60] Depth 5/1000 time = 12.414917469024658
I0123 14:13:44.197258 140277666050048 ddar.py:60] Depth 6/1000 time = 11.644993782043457
I0123 14:13:55.855359 140277666050048 ddar.py:60] Depth 7/1000 time = 11.657676696777344
I0123 14:13:55.951284 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:13:55.951358 140277666050048 alphageometry.py:566] LM output (score=-2.505171): "q : T b p b q 22 ;"
I0123 14:13:55.951395 140277666050048 alphageometry.py:567] Translation: "q = on_tline q b b p"

I0123 14:13:55.951439 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b p ? coll a m n"
I0123 14:13:55.951611 140277666050048 graph.py:498] 
I0123 14:13:55.951668 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b p ? coll a m n
I0123 14:13:57.172667 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1748969554901123
I0123 14:14:00.551476 140277666050048 ddar.py:60] Depth 2/1000 time = 3.3786368370056152
I0123 14:14:08.917988 140277666050048 ddar.py:60] Depth 3/1000 time = 8.3662691116333
I0123 14:14:20.158041 140277666050048 ddar.py:60] Depth 4/1000 time = 11.239742040634155
I0123 14:14:31.513875 140277666050048 ddar.py:60] Depth 5/1000 time = 11.355587720870972
I0123 14:14:42.730737 140277666050048 ddar.py:60] Depth 6/1000 time = 11.21653437614441
I0123 14:14:53.957177 140277666050048 ddar.py:60] Depth 7/1000 time = 11.225967168807983
I0123 14:14:54.033935 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:14:54.033994 140277666050048 alphageometry.py:566] LM output (score=-2.543806): "q : T b c b q 22 ;"
I0123 14:14:54.034030 140277666050048 alphageometry.py:567] Translation: "q = on_tline q b b c"

I0123 14:14:54.034065 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b c ? coll a m n"
I0123 14:14:54.034236 140277666050048 graph.py:498] 
I0123 14:14:54.034292 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b c ? coll a m n
I0123 14:14:55.861232 140277666050048 ddar.py:60] Depth 1/1000 time = 1.777651071548462
I0123 14:14:59.923277 140277666050048 ddar.py:60] Depth 2/1000 time = 4.061877012252808
I0123 14:15:08.301975 140277666050048 ddar.py:60] Depth 3/1000 time = 8.378477096557617
I0123 14:15:18.172150 140277666050048 ddar.py:60] Depth 4/1000 time = 9.869972229003906
I0123 14:15:29.041469 140277666050048 ddar.py:60] Depth 5/1000 time = 10.869118928909302
I0123 14:15:39.979160 140277666050048 ddar.py:60] Depth 6/1000 time = 10.93742561340332
I0123 14:15:50.938190 140277666050048 ddar.py:60] Depth 7/1000 time = 10.958444356918335
I0123 14:15:51.019417 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:15:51.019487 140277666050048 alphageometry.py:566] LM output (score=-2.562668): "q : T b g g q 22 ;"
I0123 14:15:51.019522 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g b g"

I0123 14:15:51.019558 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g b g ? coll a m n"
I0123 14:15:51.019733 140277666050048 graph.py:498] 
I0123 14:15:51.019792 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g b g ? coll a m n
I0123 14:15:52.885272 140277666050048 ddar.py:60] Depth 1/1000 time = 1.8148193359375
I0123 14:15:57.157150 140277666050048 ddar.py:60] Depth 2/1000 time = 4.271704196929932
I0123 14:16:05.874036 140277666050048 ddar.py:60] Depth 3/1000 time = 8.7166907787323
I0123 14:16:16.835784 140277666050048 ddar.py:60] Depth 4/1000 time = 10.961499452590942
I0123 14:16:29.374258 140277666050048 ddar.py:60] Depth 5/1000 time = 12.538148641586304
I0123 14:16:41.362197 140277666050048 ddar.py:60] Depth 6/1000 time = 11.987730026245117
I0123 14:16:53.289826 140277666050048 ddar.py:60] Depth 7/1000 time = 11.927078247070312
I0123 14:16:53.388898 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:16:53.388990 140277666050048 alphageometry.py:566] LM output (score=-2.622418): "q : T b e g q 22 ;"
I0123 14:16:53.389028 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g b e"

I0123 14:16:53.389079 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g b e ? coll a m n"
I0123 14:16:53.389292 140277666050048 graph.py:498] 
I0123 14:16:53.389353 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g b e ? coll a m n
I0123 14:16:54.586481 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1457951068878174
I0123 14:16:58.718034 140277666050048 ddar.py:60] Depth 2/1000 time = 4.131357192993164
I0123 14:17:06.516062 140277666050048 ddar.py:60] Depth 3/1000 time = 7.797841310501099
I0123 14:17:17.985590 140277666050048 ddar.py:60] Depth 4/1000 time = 11.469318151473999
I0123 14:17:29.328144 140277666050048 ddar.py:60] Depth 5/1000 time = 11.342324018478394
I0123 14:17:39.833495 140277666050048 ddar.py:60] Depth 6/1000 time = 10.505107402801514
I0123 14:17:51.727644 140277666050048 ddar.py:60] Depth 7/1000 time = 11.893566846847534
I0123 14:17:51.811754 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:17:51.811847 140277666050048 alphageometry.py:566] LM output (score=-2.778443): "q : T b p h b 22 ;"
I0123 14:17:51.811883 140277666050048 alphageometry.py:567] Translation: "ERROR: q not found in predicate args."

I0123 14:17:51.811932 140277666050048 alphageometry.py:566] LM output (score=-2.817857): "q : T b d h q 22 ;"
I0123 14:17:51.811960 140277666050048 alphageometry.py:567] Translation: "q = on_tline q h b d"

I0123 14:17:51.811993 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b d ? coll a m n"
I0123 14:17:51.812178 140277666050048 graph.py:498] 
I0123 14:17:51.812234 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b d ? coll a m n
I0123 14:17:52.966974 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1020166873931885
I0123 14:17:57.087286 140277666050048 ddar.py:60] Depth 2/1000 time = 4.120134592056274
I0123 14:18:04.730741 140277666050048 ddar.py:60] Depth 3/1000 time = 7.6432600021362305
I0123 14:18:15.174799 140277666050048 ddar.py:60] Depth 4/1000 time = 10.443838119506836
I0123 14:18:26.980682 140277666050048 ddar.py:60] Depth 5/1000 time = 11.8056001663208
I0123 14:18:38.109218 140277666050048 ddar.py:60] Depth 6/1000 time = 11.12818694114685
I0123 14:18:49.081832 140277666050048 ddar.py:60] Depth 7/1000 time = 10.972057580947876
I0123 14:18:49.161854 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:18:49.161966 140277666050048 alphageometry.py:566] LM output (score=-2.819043): "q : T b j h q 22 ;"
I0123 14:18:49.162001 140277666050048 alphageometry.py:567] Translation: "q = on_tline q h b j"

I0123 14:18:49.162037 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b j ? coll a m n"
I0123 14:18:49.162219 140277666050048 graph.py:498] 
I0123 14:18:49.162278 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b j ? coll a m n
I0123 14:18:50.377311 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1640722751617432
I0123 14:18:54.813731 140277666050048 ddar.py:60] Depth 2/1000 time = 4.436234712600708
I0123 14:19:03.548983 140277666050048 ddar.py:60] Depth 3/1000 time = 8.73501205444336
I0123 14:19:14.498589 140277666050048 ddar.py:60] Depth 4/1000 time = 10.949295997619629
I0123 14:19:26.248484 140277666050048 ddar.py:60] Depth 5/1000 time = 11.749638319015503
I0123 14:19:38.163695 140277666050048 ddar.py:60] Depth 6/1000 time = 11.914889812469482
I0123 14:19:50.032233 140277666050048 ddar.py:60] Depth 7/1000 time = 11.868088722229004
I0123 14:19:50.128978 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:19:50.129045 140277666050048 alphageometry.py:566] LM output (score=-2.854487): "q : P f h l q 22 ;"
I0123 14:19:50.129081 140277666050048 alphageometry.py:567] Translation: "q = on_pline q l f h"

I0123 14:19:50.129117 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_pline q l f h ? coll a m n"
I0123 14:19:50.129288 140277666050048 graph.py:498] 
I0123 14:19:50.129345 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_pline q l f h ? coll a m n
I0123 14:19:52.057701 140277666050048 ddar.py:60] Depth 1/1000 time = 1.877835988998413
I0123 14:19:55.518571 140277666050048 ddar.py:60] Depth 2/1000 time = 3.4607014656066895
I0123 14:20:04.357655 140277666050048 ddar.py:60] Depth 3/1000 time = 8.838877439498901
I0123 14:20:15.099575 140277666050048 ddar.py:60] Depth 4/1000 time = 10.741699934005737
I0123 14:20:26.381356 140277666050048 ddar.py:60] Depth 5/1000 time = 11.281562328338623
I0123 14:20:37.756061 140277666050048 ddar.py:60] Depth 6/1000 time = 11.374436140060425
I0123 14:20:49.031102 140277666050048 ddar.py:60] Depth 7/1000 time = 11.274406433105469
I0123 14:20:49.108701 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:20:49.108765 140277666050048 alphageometry.py:566] LM output (score=-2.857172): "q : T b p f q 22 ;"
I0123 14:20:49.108799 140277666050048 alphageometry.py:567] Translation: "q = on_tline q f b p"

I0123 14:20:49.108837 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q f b p ? coll a m n"
I0123 14:20:49.109009 140277666050048 graph.py:498] 
I0123 14:20:49.109071 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q f b p ? coll a m n
I0123 14:20:50.274976 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1167452335357666
I0123 14:20:54.482580 140277666050048 ddar.py:60] Depth 2/1000 time = 4.207382440567017
I0123 14:21:01.972894 140277666050048 ddar.py:60] Depth 3/1000 time = 7.490007400512695
I0123 14:21:12.090956 140277666050048 ddar.py:60] Depth 4/1000 time = 10.117825031280518
I0123 14:21:22.914539 140277666050048 ddar.py:60] Depth 5/1000 time = 10.823282241821289
I0123 14:21:33.863845 140277666050048 ddar.py:60] Depth 6/1000 time = 10.949050188064575
I0123 14:21:44.794988 140277666050048 ddar.py:60] Depth 7/1000 time = 10.9304678440094
I0123 14:21:44.873704 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:21:44.873770 140277666050048 alphageometry.py:566] LM output (score=-2.931488): "q : T b j j q 22 ;"
I0123 14:21:44.873807 140277666050048 alphageometry.py:567] Translation: "q = on_tline q j b j"

I0123 14:21:44.873846 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q j b j ? coll a m n"
I0123 14:21:44.874019 140277666050048 graph.py:498] 
I0123 14:21:44.874081 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q j b j ? coll a m n
I0123 14:21:46.833426 140277666050048 ddar.py:60] Depth 1/1000 time = 1.9086461067199707
I0123 14:21:50.612613 140277666050048 ddar.py:60] Depth 2/1000 time = 3.7789692878723145
I0123 14:21:59.279832 140277666050048 ddar.py:60] Depth 3/1000 time = 8.666908502578735
I0123 14:22:09.981145 140277666050048 ddar.py:60] Depth 4/1000 time = 10.701067686080933
I0123 14:22:21.962750 140277666050048 ddar.py:60] Depth 5/1000 time = 11.981293678283691
I0123 14:22:33.936425 140277666050048 ddar.py:60] Depth 6/1000 time = 11.973432064056396
I0123 14:22:45.821509 140277666050048 ddar.py:60] Depth 7/1000 time = 11.884487628936768
I0123 14:22:45.923224 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:22:45.923319 140277666050048 alphageometry.py:566] LM output (score=-2.955236): "q : T b j b q 22 ;"
I0123 14:22:45.923356 140277666050048 alphageometry.py:567] Translation: "q = on_tline q b b j"

I0123 14:22:45.923406 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b j ? coll a m n"
I0123 14:22:45.923596 140277666050048 graph.py:498] 
I0123 14:22:45.923654 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b j ? coll a m n
I0123 14:22:47.940349 140277666050048 ddar.py:60] Depth 1/1000 time = 1.9666023254394531
I0123 14:22:51.739126 140277666050048 ddar.py:60] Depth 2/1000 time = 3.798596143722534
I0123 14:23:01.255919 140277666050048 ddar.py:60] Depth 3/1000 time = 9.516616344451904
I0123 14:23:12.674542 140277666050048 ddar.py:60] Depth 4/1000 time = 11.418401956558228
I0123 14:23:23.831687 140277666050048 ddar.py:60] Depth 5/1000 time = 11.156870126724243
I0123 14:23:36.728220 140277666050048 ddar.py:60] Depth 6/1000 time = 12.89617133140564
I0123 14:23:48.959997 140277666050048 ddar.py:60] Depth 7/1000 time = 12.23131251335144
I0123 14:23:49.055404 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:23:49.055475 140277666050048 alphageometry.py:566] LM output (score=-2.999527): "q : T d g d q 22 ;"
I0123 14:23:49.055513 140277666050048 alphageometry.py:567] Translation: "q = on_tline q d d g"

I0123 14:23:49.055553 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q d d g ? coll a m n"
I0123 14:23:49.055733 140277666050048 graph.py:498] 
I0123 14:23:49.055792 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q d d g ? coll a m n
I0123 14:23:50.264105 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1578936576843262
I0123 14:23:53.786589 140277666050048 ddar.py:60] Depth 2/1000 time = 3.5223116874694824
I0123 14:24:02.756770 140277666050048 ddar.py:60] Depth 3/1000 time = 8.969913482666016
I0123 14:24:13.679395 140277666050048 ddar.py:60] Depth 4/1000 time = 10.922281980514526
I0123 14:24:25.306111 140277666050048 ddar.py:60] Depth 5/1000 time = 11.626479387283325
I0123 14:24:36.773012 140277666050048 ddar.py:60] Depth 6/1000 time = 11.466663122177124
I0123 14:24:48.196254 140277666050048 ddar.py:60] Depth 7/1000 time = 11.422739028930664
I0123 14:24:48.280074 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:24:48.280145 140277666050048 alphageometry.py:566] LM output (score=-3.026155): "q : T b p i q 22 ;"
I0123 14:24:48.280179 140277666050048 alphageometry.py:567] Translation: "q = on_tline q i b p"

I0123 14:24:48.280216 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q i b p ? coll a m n"
I0123 14:24:48.280385 140277666050048 graph.py:498] 
I0123 14:24:48.280444 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q i b p ? coll a m n
I0123 14:24:49.457278 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1230111122131348
I0123 14:24:52.910077 140277666050048 ddar.py:60] Depth 2/1000 time = 3.4526093006134033
I0123 14:25:01.585562 140277666050048 ddar.py:60] Depth 3/1000 time = 8.675282716751099
I0123 14:25:12.335458 140277666050048 ddar.py:60] Depth 4/1000 time = 10.749683856964111
I0123 14:25:22.674565 140277666050048 ddar.py:60] Depth 5/1000 time = 10.33889102935791
I0123 14:25:33.880848 140277666050048 ddar.py:60] Depth 6/1000 time = 11.205991506576538
I0123 14:25:45.899264 140277666050048 ddar.py:60] Depth 7/1000 time = 12.017681121826172
I0123 14:25:45.972720 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:25:45.972789 140277666050048 alphageometry.py:566] LM output (score=-3.076219): "q : T b d p q 22 ;"
I0123 14:25:45.972823 140277666050048 alphageometry.py:567] Translation: "q = on_tline q p b d"

I0123 14:25:45.972859 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q p b d ? coll a m n"
I0123 14:25:45.973039 140277666050048 graph.py:498] 
I0123 14:25:45.973099 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q p b d ? coll a m n
I0123 14:25:47.192872 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1790385246276855
I0123 14:25:50.772107 140277666050048 ddar.py:60] Depth 2/1000 time = 3.5789988040924072
I0123 14:25:59.474254 140277666050048 ddar.py:60] Depth 3/1000 time = 8.701846837997437
I0123 14:26:09.694315 140277666050048 ddar.py:60] Depth 4/1000 time = 10.219817638397217
I0123 14:26:20.763646 140277666050048 ddar.py:60] Depth 5/1000 time = 11.069030046463013
I0123 14:26:32.094429 140277666050048 ddar.py:60] Depth 6/1000 time = 11.33051872253418
I0123 14:26:43.208232 140277666050048 ddar.py:60] Depth 7/1000 time = 11.113199234008789
I0123 14:26:43.285776 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:26:43.285840 140277666050048 alphageometry.py:566] LM output (score=-3.098666): "q : T b i b q 22 ;"
I0123 14:26:43.285875 140277666050048 alphageometry.py:567] Translation: "q = on_tline q b b i"

I0123 14:26:43.285912 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b i ? coll a m n"
I0123 14:26:43.286086 140277666050048 graph.py:498] 
I0123 14:26:43.286145 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q b b i ? coll a m n
I0123 14:26:44.499344 140277666050048 ddar.py:60] Depth 1/1000 time = 1.163109302520752
I0123 14:26:48.269997 140277666050048 ddar.py:60] Depth 2/1000 time = 3.770447015762329
I0123 14:26:58.080741 140277666050048 ddar.py:60] Depth 3/1000 time = 9.810525178909302
I0123 14:27:09.902910 140277666050048 ddar.py:60] Depth 4/1000 time = 11.821908235549927
I0123 14:27:21.914997 140277666050048 ddar.py:60] Depth 5/1000 time = 12.011760711669922
I0123 14:27:35.037082 140277666050048 ddar.py:60] Depth 6/1000 time = 13.121844291687012
I0123 14:27:47.447569 140277666050048 ddar.py:60] Depth 7/1000 time = 12.410002708435059
I0123 14:27:47.540150 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:27:47.540213 140277666050048 alphageometry.py:566] LM output (score=-3.105717): "q : T b j f q 22 ;"
I0123 14:27:47.540248 140277666050048 alphageometry.py:567] Translation: "q = on_tline q f b j"

I0123 14:27:47.540284 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q f b j ? coll a m n"
I0123 14:27:47.540457 140277666050048 graph.py:498] 
I0123 14:27:47.540514 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q f b j ? coll a m n
I0123 14:27:48.750294 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1597909927368164
I0123 14:27:52.530596 140277666050048 ddar.py:60] Depth 2/1000 time = 3.780130624771118
I0123 14:28:02.184107 140277666050048 ddar.py:60] Depth 3/1000 time = 9.653265714645386
I0123 14:28:13.947065 140277666050048 ddar.py:60] Depth 4/1000 time = 11.762608528137207
I0123 14:28:25.460417 140277666050048 ddar.py:60] Depth 5/1000 time = 11.513079166412354
I0123 14:28:37.618518 140277666050048 ddar.py:60] Depth 6/1000 time = 12.157712459564209
I0123 14:28:49.780763 140277666050048 ddar.py:60] Depth 7/1000 time = 12.16161060333252
I0123 14:28:49.877255 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:28:49.877330 140277666050048 alphageometry.py:566] LM output (score=-3.118441): "q : T e g g q 22 ;"
I0123 14:28:49.877366 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g e g"

I0123 14:28:49.877403 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g e g ? coll a m n"
I0123 14:28:49.877574 140277666050048 graph.py:498] 
I0123 14:28:49.877633 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g e g ? coll a m n
I0123 14:28:51.885856 140277666050048 ddar.py:60] Depth 1/1000 time = 1.9538116455078125
I0123 14:28:55.740846 140277666050048 ddar.py:60] Depth 2/1000 time = 3.854750156402588
I0123 14:29:05.384873 140277666050048 ddar.py:60] Depth 3/1000 time = 9.643718004226685
I0123 14:29:18.279566 140277666050048 ddar.py:60] Depth 4/1000 time = 12.89447546005249
I0123 14:29:31.465535 140277666050048 ddar.py:60] Depth 5/1000 time = 13.185763120651245
I0123 14:29:43.522111 140277666050048 ddar.py:60] Depth 6/1000 time = 12.056322574615479
I0123 14:29:57.141102 140277666050048 ddar.py:60] Depth 7/1000 time = 13.618309020996094
I0123 14:29:57.243621 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:29:57.243710 140277666050048 alphageometry.py:566] LM output (score=-3.123051): "q : T b e h q 22 ;"
I0123 14:29:57.243748 140277666050048 alphageometry.py:567] Translation: "q = on_tline q h b e"

I0123 14:29:57.243801 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b e ? coll a m n"
I0123 14:29:57.243992 140277666050048 graph.py:498] 
I0123 14:29:57.244050 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q h b e ? coll a m n
I0123 14:29:58.435223 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1408038139343262
I0123 14:30:02.000425 140277666050048 ddar.py:60] Depth 2/1000 time = 3.5650296211242676
I0123 14:30:10.820447 140277666050048 ddar.py:60] Depth 3/1000 time = 8.819827318191528
I0123 14:30:21.214012 140277666050048 ddar.py:60] Depth 4/1000 time = 10.393316745758057
I0123 14:30:31.637621 140277666050048 ddar.py:60] Depth 5/1000 time = 10.423362493515015
I0123 14:30:43.008574 140277666050048 ddar.py:60] Depth 6/1000 time = 11.370619773864746
I0123 14:30:54.410534 140277666050048 ddar.py:60] Depth 7/1000 time = 11.401402711868286
I0123 14:30:54.491667 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:30:54.491730 140277666050048 alphageometry.py:566] LM output (score=-3.129958): "q : T b c g q 22 ;"
I0123 14:30:54.491766 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g b c"

I0123 14:30:54.491805 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g b c ? coll a m n"
I0123 14:30:54.491989 140277666050048 graph.py:498] 
I0123 14:30:54.492049 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_tline q g b c ? coll a m n
I0123 14:30:56.521161 140277666050048 ddar.py:60] Depth 1/1000 time = 1.9791648387908936
I0123 14:31:00.084294 140277666050048 ddar.py:60] Depth 2/1000 time = 3.5629653930664062
I0123 14:31:07.598447 140277666050048 ddar.py:60] Depth 3/1000 time = 7.513916730880737
I0123 14:31:18.134882 140277666050048 ddar.py:60] Depth 4/1000 time = 10.536117315292358
I0123 14:31:28.456186 140277666050048 ddar.py:60] Depth 5/1000 time = 10.32104206085205
I0123 14:31:39.709607 140277666050048 ddar.py:60] Depth 6/1000 time = 11.253071308135986
I0123 14:31:50.970435 140277666050048 ddar.py:60] Depth 7/1000 time = 11.260275602340698
I0123 14:31:51.056513 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:31:51.056579 140277666050048 alphageometry.py:566] LM output (score=-3.132021): "q : P f h k q 22 ;"
I0123 14:31:51.056613 140277666050048 alphageometry.py:567] Translation: "q = on_pline q k f h"

I0123 14:31:51.056655 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_pline q k f h ? coll a m n"
I0123 14:31:51.056823 140277666050048 graph.py:498] 
I0123 14:31:51.056880 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p b b d; q = on_pline q k f h ? coll a m n
I0123 14:31:53.057707 140277666050048 ddar.py:60] Depth 1/1000 time = 1.9516937732696533
I0123 14:31:56.614201 140277666050048 ddar.py:60] Depth 2/1000 time = 3.5563292503356934
I0123 14:32:05.370632 140277666050048 ddar.py:60] Depth 3/1000 time = 8.75618600845337
I0123 14:32:16.273409 140277666050048 ddar.py:60] Depth 4/1000 time = 10.90246868133545
I0123 14:32:27.000253 140277666050048 ddar.py:60] Depth 5/1000 time = 10.726621150970459
I0123 14:32:38.499787 140277666050048 ddar.py:60] Depth 6/1000 time = 11.499236106872559
I0123 14:32:49.963107 140277666050048 ddar.py:60] Depth 7/1000 time = 11.462656736373901
I0123 14:32:50.040382 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:32:50.040453 140277666050048 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T b c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C a c g 06 T a c d g 07 ; h : C d f h 08 ; i : C d g i 09 ; j : C d e j 10 ; k : C i j k 11 T c k i j 12 ; l : C h j l 13 T b l h j 14 ; m : C h i m 15 T a m h i 16 ; n : C b l n 17 C c k n 18 ; o : C a m o 19 D a m m o 20 ? C a m n {F1} x00 p : T b c c p 21 ; x00
I0123 14:32:57.196806 140277666050048 alphageometry.py:566] LM output (score=-0.969405): "q : T g i g q 22 ;"
I0123 14:32:57.196934 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g g i"

I0123 14:32:57.196982 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g i ? coll a m n"
I0123 14:32:57.197156 140277666050048 graph.py:498] 
I0123 14:32:57.197216 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g i ? coll a m n
I0123 14:32:59.583516 140277666050048 ddar.py:60] Depth 1/1000 time = 2.3457486629486084
I0123 14:33:03.326160 140277666050048 ddar.py:60] Depth 2/1000 time = 3.7424588203430176
I0123 14:33:11.000982 140277666050048 ddar.py:60] Depth 3/1000 time = 7.674606800079346
I0123 14:33:21.546327 140277666050048 ddar.py:60] Depth 4/1000 time = 10.545125246047974
I0123 14:33:33.724602 140277666050048 ddar.py:60] Depth 5/1000 time = 12.178090333938599
I0123 14:33:45.183980 140277666050048 ddar.py:60] Depth 6/1000 time = 11.4591646194458
I0123 14:33:57.565472 140277666050048 ddar.py:60] Depth 7/1000 time = 12.380914449691772
I0123 14:33:57.642377 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:33:57.642441 140277666050048 alphageometry.py:566] LM output (score=-1.007022): "q : P b q c p 22 ;"
I0123 14:33:57.642475 140277666050048 alphageometry.py:567] Translation: "q = on_pline q b c p"

I0123 14:33:57.642512 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q b c p ? coll a m n"
I0123 14:33:57.642684 140277666050048 graph.py:498] 
I0123 14:33:57.642740 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q b c p ? coll a m n
I0123 14:33:58.906028 140277666050048 ddar.py:60] Depth 1/1000 time = 1.2150068283081055
I0123 14:34:02.388170 140277666050048 ddar.py:60] Depth 2/1000 time = 3.48195219039917
I0123 14:34:09.808276 140277666050048 ddar.py:60] Depth 3/1000 time = 7.419862270355225
I0123 14:34:19.975312 140277666050048 ddar.py:60] Depth 4/1000 time = 10.166727781295776
I0123 14:34:31.274117 140277666050048 ddar.py:60] Depth 5/1000 time = 11.29859733581543
I0123 14:34:42.714794 140277666050048 ddar.py:60] Depth 6/1000 time = 11.440471649169922
I0123 14:34:53.025467 140277666050048 ddar.py:60] Depth 7/1000 time = 10.309999227523804
I0123 14:34:53.105962 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:34:53.106037 140277666050048 alphageometry.py:566] LM output (score=-1.457831): "q : T d g g q 22 ;"
I0123 14:34:53.106073 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g d g"

I0123 14:34:53.106113 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g d g ? coll a m n"
I0123 14:34:53.106285 140277666050048 graph.py:498] 
I0123 14:34:53.106342 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g d g ? coll a m n
I0123 14:34:55.406843 140277666050048 ddar.py:60] Depth 1/1000 time = 2.2595338821411133
I0123 14:34:59.977699 140277666050048 ddar.py:60] Depth 2/1000 time = 4.570693254470825
I0123 14:35:07.553138 140277666050048 ddar.py:60] Depth 3/1000 time = 7.575243711471558
I0123 14:35:17.916516 140277666050048 ddar.py:60] Depth 4/1000 time = 10.363163948059082
I0123 14:35:29.461193 140277666050048 ddar.py:60] Depth 5/1000 time = 11.544463634490967
I0123 14:35:40.760599 140277666050048 ddar.py:60] Depth 6/1000 time = 11.299142837524414
I0123 14:35:52.971660 140277666050048 ddar.py:60] Depth 7/1000 time = 12.210494041442871
I0123 14:35:53.047255 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:35:53.047323 140277666050048 alphageometry.py:566] LM output (score=-1.531383): "q : P b c p q 22 ;"
I0123 14:35:53.047359 140277666050048 alphageometry.py:567] Translation: "q = on_pline q p b c"

I0123 14:35:53.047396 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p b c ? coll a m n"
I0123 14:35:53.047569 140277666050048 graph.py:498] 
I0123 14:35:53.047627 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p b c ? coll a m n
I0123 14:35:55.188452 140277666050048 ddar.py:60] Depth 1/1000 time = 2.0918920040130615
I0123 14:35:58.715251 140277666050048 ddar.py:60] Depth 2/1000 time = 3.5265755653381348
I0123 14:36:05.198504 140277666050048 ddar.py:60] Depth 3/1000 time = 6.482936143875122
I0123 14:36:13.883258 140277666050048 ddar.py:60] Depth 4/1000 time = 8.6845703125
I0123 14:36:24.718295 140277666050048 ddar.py:60] Depth 5/1000 time = 10.834812641143799
I0123 14:36:35.656575 140277666050048 ddar.py:60] Depth 6/1000 time = 10.937998056411743
I0123 14:36:46.647959 140277666050048 ddar.py:60] Depth 7/1000 time = 10.990705013275146
I0123 14:36:46.729185 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:36:46.729268 140277666050048 alphageometry.py:566] LM output (score=-1.948395): "q : T b d b q 22 ;"
I0123 14:36:46.729304 140277666050048 alphageometry.py:567] Translation: "q = on_tline q b b d"

I0123 14:36:46.729341 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q b b d ? coll a m n"
I0123 14:36:46.729516 140277666050048 graph.py:498] 
I0123 14:36:46.729576 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q b b d ? coll a m n
I0123 14:36:48.805884 140277666050048 ddar.py:60] Depth 1/1000 time = 2.026743173599243
I0123 14:36:52.497961 140277666050048 ddar.py:60] Depth 2/1000 time = 3.6919045448303223
I0123 14:37:00.729493 140277666050048 ddar.py:60] Depth 3/1000 time = 8.231334924697876
I0123 14:37:11.329302 140277666050048 ddar.py:60] Depth 4/1000 time = 10.599563360214233
I0123 14:37:22.535148 140277666050048 ddar.py:60] Depth 5/1000 time = 11.205503702163696
I0123 14:37:34.245197 140277666050048 ddar.py:60] Depth 6/1000 time = 11.70966100692749
I0123 14:37:46.019001 140277666050048 ddar.py:60] Depth 7/1000 time = 11.773098945617676
I0123 14:37:46.105680 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:37:46.105773 140277666050048 alphageometry.py:566] LM output (score=-1.985206): "q : P b c p q 22 T b p c q 23 ;"
I0123 14:37:46.105810 140277666050048 alphageometry.py:567] Translation: "q = on_pline q p b c, on_tline q c b p"

I0123 14:37:46.105890 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p b c, on_tline q c b p ? coll a m n"
I0123 14:37:46.106101 140277666050048 graph.py:498] 
I0123 14:37:46.106160 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p b c, on_tline q c b p ? coll a m n
I0123 14:37:47.453032 140277666050048 ddar.py:60] Depth 1/1000 time = 1.278731107711792
I0123 14:37:51.434473 140277666050048 ddar.py:60] Depth 2/1000 time = 3.981265068054199
I0123 14:37:59.969477 140277666050048 ddar.py:60] Depth 3/1000 time = 8.534804105758667
I0123 14:38:10.400258 140277666050048 ddar.py:60] Depth 4/1000 time = 10.430543899536133
I0123 14:38:22.697223 140277666050048 ddar.py:60] Depth 5/1000 time = 12.29670524597168
I0123 14:38:35.149455 140277666050048 ddar.py:60] Depth 6/1000 time = 12.451882123947144
I0123 14:38:47.570901 140277666050048 ddar.py:60] Depth 7/1000 time = 12.420585632324219
I0123 14:38:47.665274 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:38:47.665368 140277666050048 alphageometry.py:566] LM output (score=-2.170414): "q : P f h l q 22 ;"
I0123 14:38:47.665406 140277666050048 alphageometry.py:567] Translation: "q = on_pline q l f h"

I0123 14:38:47.665457 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q l f h ? coll a m n"
I0123 14:38:47.665668 140277666050048 graph.py:498] 
I0123 14:38:47.665730 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q l f h ? coll a m n
I0123 14:38:49.871757 140277666050048 ddar.py:60] Depth 1/1000 time = 2.1572132110595703
I0123 14:38:53.342474 140277666050048 ddar.py:60] Depth 2/1000 time = 3.470534086227417
I0123 14:38:59.698275 140277666050048 ddar.py:60] Depth 3/1000 time = 6.35561466217041
I0123 14:39:09.770762 140277666050048 ddar.py:60] Depth 4/1000 time = 10.072264194488525
I0123 14:39:20.874715 140277666050048 ddar.py:60] Depth 5/1000 time = 11.103683948516846
I0123 14:39:30.959009 140277666050048 ddar.py:60] Depth 6/1000 time = 10.084086656570435
I0123 14:39:41.940628 140277666050048 ddar.py:60] Depth 7/1000 time = 10.981004238128662
I0123 14:39:42.023370 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:39:42.023460 140277666050048 alphageometry.py:566] LM output (score=-2.188305): "q : T d g d q 22 ;"
I0123 14:39:42.023496 140277666050048 alphageometry.py:567] Translation: "q = on_tline q d d g"

I0123 14:39:42.023545 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q d d g ? coll a m n"
I0123 14:39:42.023747 140277666050048 graph.py:498] 
I0123 14:39:42.023805 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q d d g ? coll a m n
I0123 14:39:44.308485 140277666050048 ddar.py:60] Depth 1/1000 time = 2.2359390258789062
I0123 14:39:47.844335 140277666050048 ddar.py:60] Depth 2/1000 time = 3.535674810409546
I0123 14:39:54.337718 140277666050048 ddar.py:60] Depth 3/1000 time = 6.493187665939331
I0123 14:40:03.096847 140277666050048 ddar.py:60] Depth 4/1000 time = 8.758870601654053
I0123 14:40:14.071238 140277666050048 ddar.py:60] Depth 5/1000 time = 10.974080562591553
I0123 14:40:25.277853 140277666050048 ddar.py:60] Depth 6/1000 time = 11.206398487091064
I0123 14:40:36.437116 140277666050048 ddar.py:60] Depth 7/1000 time = 11.158621549606323
I0123 14:40:36.517197 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:40:36.517285 140277666050048 alphageometry.py:566] LM output (score=-2.419427): "q : T b f g q 22 ;"
I0123 14:40:36.517320 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g b f"

I0123 14:40:36.517370 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g b f ? coll a m n"
I0123 14:40:36.517570 140277666050048 graph.py:498] 
I0123 14:40:36.517631 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g b f ? coll a m n
I0123 14:40:37.733731 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1678264141082764
I0123 14:40:41.307264 140277666050048 ddar.py:60] Depth 2/1000 time = 3.573330879211426
I0123 14:40:48.634459 140277666050048 ddar.py:60] Depth 3/1000 time = 7.327016115188599
I0123 14:40:57.826250 140277666050048 ddar.py:60] Depth 4/1000 time = 9.19153356552124
I0123 14:41:09.033636 140277666050048 ddar.py:60] Depth 5/1000 time = 11.20705246925354
I0123 14:41:20.242956 140277666050048 ddar.py:60] Depth 6/1000 time = 11.209083080291748
I0123 14:41:31.395702 140277666050048 ddar.py:60] Depth 7/1000 time = 11.152133703231812
I0123 14:41:31.475084 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:41:31.475175 140277666050048 alphageometry.py:566] LM output (score=-2.564824): "q : T g p g q 22 ;"
I0123 14:41:31.475213 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g g p"

I0123 14:41:31.475263 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g p ? coll a m n"
I0123 14:41:31.475462 140277666050048 graph.py:498] 
I0123 14:41:31.475523 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g p ? coll a m n
I0123 14:41:32.698860 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1726634502410889
I0123 14:41:36.453524 140277666050048 ddar.py:60] Depth 2/1000 time = 3.754462957382202
I0123 14:41:43.600466 140277666050048 ddar.py:60] Depth 3/1000 time = 7.146758317947388
I0123 14:41:54.074317 140277666050048 ddar.py:60] Depth 4/1000 time = 10.473629713058472
I0123 14:42:04.931278 140277666050048 ddar.py:60] Depth 5/1000 time = 10.856724500656128
I0123 14:42:17.010161 140277666050048 ddar.py:60] Depth 6/1000 time = 12.078624725341797
I0123 14:42:27.970130 140277666050048 ddar.py:60] Depth 7/1000 time = 10.959343194961548
I0123 14:42:28.064855 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:42:28.064949 140277666050048 alphageometry.py:566] LM output (score=-2.664298): "q : P b c p q 22 P b q c p 23 ;"
I0123 14:42:28.064985 140277666050048 alphageometry.py:567] Translation: "q = on_pline q p b c, on_pline q b c p"

I0123 14:42:28.065036 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p b c, on_pline q b c p ? coll a m n"
I0123 14:42:28.065236 140277666050048 graph.py:498] 
I0123 14:42:28.065296 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p b c, on_pline q b c p ? coll a m n
I0123 14:42:30.516828 140277666050048 ddar.py:60] Depth 1/1000 time = 2.393101930618286
I0123 14:42:34.751123 140277666050048 ddar.py:60] Depth 2/1000 time = 4.234113931655884
I0123 14:42:42.681307 140277666050048 ddar.py:60] Depth 3/1000 time = 7.929955720901489
I0123 14:42:56.243646 140277666050048 ddar.py:60] Depth 4/1000 time = 13.56203031539917
I0123 14:43:12.141283 140277666050048 ddar.py:60] Depth 5/1000 time = 15.89741563796997
I0123 14:43:29.483683 140277666050048 ddar.py:60] Depth 6/1000 time = 17.342139959335327
I0123 14:43:44.705403 140277666050048 ddar.py:60] Depth 7/1000 time = 15.220887899398804
I0123 14:44:01.276573 140277666050048 ddar.py:60] Depth 8/1000 time = 16.446675539016724
I0123 14:44:18.979691 140277666050048 ddar.py:60] Depth 9/1000 time = 17.672473907470703
I0123 14:44:18.979952 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:44:18.980006 140277666050048 alphageometry.py:566] LM output (score=-2.915850): "q : P g i p q 22 ;"
I0123 14:44:18.980042 140277666050048 alphageometry.py:567] Translation: "q = on_pline q p g i"

I0123 14:44:18.980079 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p g i ? coll a m n"
I0123 14:44:18.980255 140277666050048 graph.py:498] 
I0123 14:44:18.980314 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p g i ? coll a m n
I0123 14:44:20.192372 140277666050048 ddar.py:60] Depth 1/1000 time = 1.159311056137085
I0123 14:44:23.700549 140277666050048 ddar.py:60] Depth 2/1000 time = 3.508005380630493
I0123 14:44:30.109318 140277666050048 ddar.py:60] Depth 3/1000 time = 6.408578872680664
I0123 14:44:38.797561 140277666050048 ddar.py:60] Depth 4/1000 time = 8.688046932220459
I0123 14:44:49.626778 140277666050048 ddar.py:60] Depth 5/1000 time = 10.82895565032959
I0123 14:45:00.683699 140277666050048 ddar.py:60] Depth 6/1000 time = 11.056604146957397
I0123 14:45:11.787681 140277666050048 ddar.py:60] Depth 7/1000 time = 11.103329420089722
I0123 14:45:11.865206 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:45:11.865289 140277666050048 alphageometry.py:566] LM output (score=-2.942054): "q : T g k g q 22 ;"
I0123 14:45:11.865339 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g g k"

I0123 14:45:11.865393 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g k ? coll a m n"
I0123 14:45:11.865584 140277666050048 graph.py:498] 
I0123 14:45:11.865646 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g k ? coll a m n
I0123 14:45:13.093547 140277666050048 ddar.py:60] Depth 1/1000 time = 1.176708459854126
I0123 14:45:16.806831 140277666050048 ddar.py:60] Depth 2/1000 time = 3.7130942344665527
I0123 14:45:23.822975 140277666050048 ddar.py:60] Depth 3/1000 time = 7.01595401763916
I0123 14:45:33.189465 140277666050048 ddar.py:60] Depth 4/1000 time = 9.366294860839844
I0123 14:45:44.837987 140277666050048 ddar.py:60] Depth 5/1000 time = 11.648262977600098
I0123 14:45:56.707576 140277666050048 ddar.py:60] Depth 6/1000 time = 11.869239568710327
I0123 14:46:07.482469 140277666050048 ddar.py:60] Depth 7/1000 time = 10.774282932281494
I0123 14:46:07.569504 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:46:07.569586 140277666050048 alphageometry.py:566] LM output (score=-2.945112): "q : P d g p q 22 ;"
I0123 14:46:07.569622 140277666050048 alphageometry.py:567] Translation: "q = on_pline q p d g"

I0123 14:46:07.569684 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p d g ? coll a m n"
I0123 14:46:07.569870 140277666050048 graph.py:498] 
I0123 14:46:07.569927 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p d g ? coll a m n
I0123 14:46:09.816550 140277666050048 ddar.py:60] Depth 1/1000 time = 2.197509765625
I0123 14:46:13.365902 140277666050048 ddar.py:60] Depth 2/1000 time = 3.5491764545440674
I0123 14:46:19.891235 140277666050048 ddar.py:60] Depth 3/1000 time = 6.525096654891968
I0123 14:46:29.190871 140277666050048 ddar.py:60] Depth 4/1000 time = 9.299316883087158
I0123 14:46:40.520993 140277666050048 ddar.py:60] Depth 5/1000 time = 11.329870700836182
I0123 14:46:51.787439 140277666050048 ddar.py:60] Depth 6/1000 time = 11.26610803604126
I0123 14:47:02.027174 140277666050048 ddar.py:60] Depth 7/1000 time = 10.239166259765625
I0123 14:47:02.106551 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:47:02.106626 140277666050048 alphageometry.py:566] LM output (score=-2.965523): "q : P f h k q 22 ;"
I0123 14:47:02.106661 140277666050048 alphageometry.py:567] Translation: "q = on_pline q k f h"

I0123 14:47:02.106704 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q k f h ? coll a m n"
I0123 14:47:02.106883 140277666050048 graph.py:498] 
I0123 14:47:02.106937 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q k f h ? coll a m n
I0123 14:47:04.312397 140277666050048 ddar.py:60] Depth 1/1000 time = 2.157099485397339
I0123 14:47:07.874742 140277666050048 ddar.py:60] Depth 2/1000 time = 3.5621612071990967
I0123 14:47:14.425222 140277666050048 ddar.py:60] Depth 3/1000 time = 6.5502400398254395
I0123 14:47:23.172699 140277666050048 ddar.py:60] Depth 4/1000 time = 8.747172594070435
I0123 14:47:34.201405 140277666050048 ddar.py:60] Depth 5/1000 time = 11.028446435928345
I0123 14:47:45.494514 140277666050048 ddar.py:60] Depth 6/1000 time = 11.292753219604492
I0123 14:47:55.661521 140277666050048 ddar.py:60] Depth 7/1000 time = 10.16639256477356
I0123 14:47:55.740546 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:47:55.740632 140277666050048 alphageometry.py:566] LM output (score=-3.079073): "q : T f g g q 22 ;"
I0123 14:47:55.740668 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g f g"

I0123 14:47:55.740719 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g f g ? coll a m n"
I0123 14:47:55.740904 140277666050048 graph.py:498] 
I0123 14:47:55.740960 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g f g ? coll a m n
I0123 14:47:56.963432 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1725468635559082
I0123 14:48:00.666504 140277666050048 ddar.py:60] Depth 2/1000 time = 3.7028920650482178
I0123 14:48:08.748202 140277666050048 ddar.py:60] Depth 3/1000 time = 8.081453561782837
I0123 14:48:18.349879 140277666050048 ddar.py:60] Depth 4/1000 time = 9.601359844207764
I0123 14:48:30.584231 140277666050048 ddar.py:60] Depth 5/1000 time = 12.234102487564087
I0123 14:48:41.901011 140277666050048 ddar.py:60] Depth 6/1000 time = 11.316424369812012
I0123 14:48:54.304433 140277666050048 ddar.py:60] Depth 7/1000 time = 12.402814865112305
I0123 14:48:54.387941 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:48:54.388007 140277666050048 alphageometry.py:566] LM output (score=-3.107621): "q : P c p q r 22 ;"
I0123 14:48:54.388041 140277666050048 alphageometry.py:567] Translation: "ERROR: point r does not exist."

I0123 14:48:54.388075 140277666050048 alphageometry.py:566] LM output (score=-3.108949): "q : P b c p q 22 T b q c p 23 ;"
I0123 14:48:54.388101 140277666050048 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2637, in add_clause
    raise PointTooFarError()
graph.PointTooFarError
"

I0123 14:48:54.388129 140277666050048 alphageometry.py:566] LM output (score=-3.124881): "q : T b f f q 22 ;"
I0123 14:48:54.388153 140277666050048 alphageometry.py:567] Translation: "q = on_tline q f b f"

I0123 14:48:54.388182 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q f b f ? coll a m n"
I0123 14:48:54.388350 140277666050048 graph.py:498] 
I0123 14:48:54.388416 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q f b f ? coll a m n
I0123 14:48:56.880332 140277666050048 ddar.py:60] Depth 1/1000 time = 2.4517593383789062
I0123 14:49:00.742954 140277666050048 ddar.py:60] Depth 2/1000 time = 3.8624393939971924
I0123 14:49:08.723272 140277666050048 ddar.py:60] Depth 3/1000 time = 7.9801108837127686
I0123 14:49:20.452137 140277666050048 ddar.py:60] Depth 4/1000 time = 11.728674411773682
I0123 14:49:33.282062 140277666050048 ddar.py:60] Depth 5/1000 time = 12.82971739768982
I0123 14:49:45.106138 140277666050048 ddar.py:60] Depth 6/1000 time = 11.823787450790405
I0123 14:49:59.054155 140277666050048 ddar.py:60] Depth 7/1000 time = 13.94733715057373
I0123 14:49:59.150054 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:49:59.150169 140277666050048 alphageometry.py:566] LM output (score=-3.144203): "q : T g e g q 22 ;"
I0123 14:49:59.150216 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g g e"

I0123 14:49:59.150272 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g e ? coll a m n"
I0123 14:49:59.150569 140277666050048 graph.py:498] 
I0123 14:49:59.150650 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g e ? coll a m n
I0123 14:50:00.423738 140277666050048 ddar.py:60] Depth 1/1000 time = 1.198179006576538
I0123 14:50:04.297273 140277666050048 ddar.py:60] Depth 2/1000 time = 3.873323440551758
I0123 14:50:11.548174 140277666050048 ddar.py:60] Depth 3/1000 time = 7.250702381134033
I0123 14:50:23.018559 140277666050048 ddar.py:60] Depth 4/1000 time = 11.470184564590454
I0123 14:50:34.684970 140277666050048 ddar.py:60] Depth 5/1000 time = 11.66620945930481
I0123 14:50:47.451216 140277666050048 ddar.py:60] Depth 6/1000 time = 12.765995502471924
I0123 14:51:00.004643 140277666050048 ddar.py:60] Depth 7/1000 time = 12.552749156951904
I0123 14:51:00.097591 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:51:00.097697 140277666050048 alphageometry.py:566] LM output (score=-3.149661): "q : P e g j q 22 ;"
I0123 14:51:00.097735 140277666050048 alphageometry.py:567] Translation: "q = on_pline q j e g"

I0123 14:51:00.097787 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q j e g ? coll a m n"
I0123 14:51:00.098000 140277666050048 graph.py:498] 
I0123 14:51:00.098062 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q j e g ? coll a m n
I0123 14:51:01.315718 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1670770645141602
I0123 14:51:04.956325 140277666050048 ddar.py:60] Depth 2/1000 time = 3.640425443649292
I0123 14:51:11.386306 140277666050048 ddar.py:60] Depth 3/1000 time = 6.42979621887207
I0123 14:51:21.373173 140277666050048 ddar.py:60] Depth 4/1000 time = 9.986632585525513
I0123 14:51:31.587413 140277666050048 ddar.py:60] Depth 5/1000 time = 10.213980436325073
I0123 14:51:43.019828 140277666050048 ddar.py:60] Depth 6/1000 time = 11.43219542503357
I0123 14:51:53.322229 140277666050048 ddar.py:60] Depth 7/1000 time = 10.301890134811401
I0123 14:51:53.403940 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:51:53.404125 140277666050048 alphageometry.py:566] LM output (score=-3.164613): "q : T c g g q 22 ;"
I0123 14:51:53.404162 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g c g"

I0123 14:51:53.404210 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g c g ? coll a m n"
I0123 14:51:53.404416 140277666050048 graph.py:498] 
I0123 14:51:53.404478 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g c g ? coll a m n
I0123 14:51:56.000636 140277666050048 ddar.py:60] Depth 1/1000 time = 2.5553863048553467
I0123 14:51:59.942077 140277666050048 ddar.py:60] Depth 2/1000 time = 3.9412310123443604
I0123 14:52:07.954628 140277666050048 ddar.py:60] Depth 3/1000 time = 8.012384414672852
I0123 14:52:19.360229 140277666050048 ddar.py:60] Depth 4/1000 time = 11.405355453491211
I0123 14:52:32.282025 140277666050048 ddar.py:60] Depth 5/1000 time = 12.921428442001343
I0123 14:52:45.209078 140277666050048 ddar.py:60] Depth 6/1000 time = 12.926737308502197
I0123 14:52:58.297508 140277666050048 ddar.py:60] Depth 7/1000 time = 13.087852001190186
I0123 14:52:58.377506 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:52:58.377597 140277666050048 alphageometry.py:566] LM output (score=-3.187931): "q : P h i j q 22 ;"
I0123 14:52:58.377634 140277666050048 alphageometry.py:567] Translation: "q = on_pline q j h i"

I0123 14:52:58.377693 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q j h i ? coll a m n"
I0123 14:52:58.377881 140277666050048 graph.py:498] 
I0123 14:52:58.377938 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q j h i ? coll a m n
I0123 14:52:59.597220 140277666050048 ddar.py:60] Depth 1/1000 time = 1.167041301727295
I0123 14:53:03.272272 140277666050048 ddar.py:60] Depth 2/1000 time = 3.674868106842041
I0123 14:53:09.816062 140277666050048 ddar.py:60] Depth 3/1000 time = 6.543524980545044
I0123 14:53:19.760190 140277666050048 ddar.py:60] Depth 4/1000 time = 9.943856477737427
I0123 14:53:29.985765 140277666050048 ddar.py:60] Depth 5/1000 time = 10.225335359573364
I0123 14:53:40.260645 140277666050048 ddar.py:60] Depth 6/1000 time = 10.274567365646362
I0123 14:53:51.701649 140277666050048 ddar.py:60] Depth 7/1000 time = 11.440378904342651
I0123 14:53:51.780218 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:53:51.780316 140277666050048 alphageometry.py:566] LM output (score=-3.207968): "q : P h j o q 22 ;"
I0123 14:53:51.780354 140277666050048 alphageometry.py:567] Translation: "q = on_pline q o h j"

I0123 14:53:51.780417 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q o h j ? coll a m n"
I0123 14:53:51.780617 140277666050048 graph.py:498] 
I0123 14:53:51.780677 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q o h j ? coll a m n
I0123 14:53:54.172790 140277666050048 ddar.py:60] Depth 1/1000 time = 2.343264579772949
I0123 14:53:57.833271 140277666050048 ddar.py:60] Depth 2/1000 time = 3.6603119373321533
I0123 14:54:04.479524 140277666050048 ddar.py:60] Depth 3/1000 time = 6.646048069000244
I0123 14:54:13.837040 140277666050048 ddar.py:60] Depth 4/1000 time = 9.357270240783691
I0123 14:54:24.568390 140277666050048 ddar.py:60] Depth 5/1000 time = 10.731042623519897
I0123 14:54:35.927887 140277666050048 ddar.py:60] Depth 6/1000 time = 11.359280347824097
I0123 14:54:47.220488 140277666050048 ddar.py:60] Depth 7/1000 time = 11.29198408126831
I0123 14:54:47.297010 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:54:47.297103 140277666050048 alphageometry.py:566] LM output (score=-3.302495): "q : T e g g q 22 ;"
I0123 14:54:47.297139 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g e g"

I0123 14:54:47.297183 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g e g ? coll a m n"
I0123 14:54:47.297381 140277666050048 graph.py:498] 
I0123 14:54:47.297441 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g e g ? coll a m n
I0123 14:54:48.518616 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1706669330596924
I0123 14:54:52.452115 140277666050048 ddar.py:60] Depth 2/1000 time = 3.9333159923553467
I0123 14:54:59.584487 140277666050048 ddar.py:60] Depth 3/1000 time = 7.132180452346802
I0123 14:55:09.615583 140277666050048 ddar.py:60] Depth 4/1000 time = 10.030857801437378
I0123 14:55:21.964684 140277666050048 ddar.py:60] Depth 5/1000 time = 12.348785400390625
I0123 14:55:33.133402 140277666050048 ddar.py:60] Depth 6/1000 time = 11.168451070785522
I0123 14:55:45.364767 140277666050048 ddar.py:60] Depth 7/1000 time = 12.230724573135376
I0123 14:55:45.458459 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:55:45.458540 140277666050048 alphageometry.py:566] LM output (score=-3.319925): "q : T c p c q 22 ;"
I0123 14:55:45.458575 140277666050048 alphageometry.py:567] Translation: "q = on_tline q c c p"

I0123 14:55:45.458611 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q c c p ? coll a m n"
I0123 14:55:45.458799 140277666050048 graph.py:498] 
I0123 14:55:45.458863 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q c c p ? coll a m n
I0123 14:55:46.986921 140277666050048 ddar.py:60] Depth 1/1000 time = 1.4865577220916748
I0123 14:55:52.227515 140277666050048 ddar.py:60] Depth 2/1000 time = 5.24031662940979
I0123 14:55:59.220960 140277666050048 ddar.py:60] Depth 3/1000 time = 6.993255376815796
I0123 14:56:09.793421 140277666050048 ddar.py:60] Depth 4/1000 time = 10.572222471237183
I0123 14:56:21.668797 140277666050048 ddar.py:60] Depth 5/1000 time = 11.875041961669922
I0123 14:56:33.751022 140277666050048 ddar.py:60] Depth 6/1000 time = 12.081974983215332
I0123 14:56:45.682404 140277666050048 ddar.py:60] Depth 7/1000 time = 11.930804252624512
I0123 14:56:45.759555 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:56:45.759641 140277666050048 alphageometry.py:566] LM output (score=-3.332271): "q : T b f b q 22 ;"
I0123 14:56:45.759677 140277666050048 alphageometry.py:567] Translation: "q = on_tline q b b f"

I0123 14:56:45.759718 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q b b f ? coll a m n"
I0123 14:56:45.759909 140277666050048 graph.py:498] 
I0123 14:56:45.759968 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q b b f ? coll a m n
I0123 14:56:47.005659 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1969354152679443
I0123 14:56:50.780625 140277666050048 ddar.py:60] Depth 2/1000 time = 3.774780750274658
I0123 14:56:57.379120 140277666050048 ddar.py:60] Depth 3/1000 time = 6.598308086395264
I0123 14:57:07.361045 140277666050048 ddar.py:60] Depth 4/1000 time = 9.981699705123901
I0123 14:57:17.513059 140277666050048 ddar.py:60] Depth 5/1000 time = 10.151692628860474
I0123 14:57:28.923417 140277666050048 ddar.py:60] Depth 6/1000 time = 11.410096883773804
I0123 14:57:39.214641 140277666050048 ddar.py:60] Depth 7/1000 time = 10.290546894073486
I0123 14:57:39.292533 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:57:39.292600 140277666050048 alphageometry.py:566] LM output (score=-3.337247): "q : P h j j q 22 ;"
I0123 14:57:39.292636 140277666050048 alphageometry.py:567] Translation: "ERROR: Invalid predicate P h j j q"

I0123 14:57:39.292669 140277666050048 alphageometry.py:566] LM output (score=-3.401221): "q : P f h p q 22 ;"
I0123 14:57:39.292695 140277666050048 alphageometry.py:567] Translation: "q = on_pline q p f h"

I0123 14:57:39.292726 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p f h ? coll a m n"
I0123 14:57:39.292904 140277666050048 graph.py:498] 
I0123 14:57:39.292966 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_pline q p f h ? coll a m n
I0123 14:57:41.611718 140277666050048 ddar.py:60] Depth 1/1000 time = 2.2687714099884033
I0123 14:57:45.325689 140277666050048 ddar.py:60] Depth 2/1000 time = 3.7137460708618164
I0123 14:57:51.936802 140277666050048 ddar.py:60] Depth 3/1000 time = 6.610826015472412
I0123 14:58:01.169853 140277666050048 ddar.py:60] Depth 4/1000 time = 9.232839822769165
I0123 14:58:12.900279 140277666050048 ddar.py:60] Depth 5/1000 time = 11.730194568634033
I0123 14:58:23.050771 140277666050048 ddar.py:60] Depth 6/1000 time = 10.150243759155273
I0123 14:58:34.397876 140277666050048 ddar.py:60] Depth 7/1000 time = 11.346566200256348
I0123 14:58:34.477518 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:58:34.477581 140277666050048 alphageometry.py:566] LM output (score=-3.408571): "q : T g j g q 22 ;"
I0123 14:58:34.477618 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g g j"

I0123 14:58:34.477663 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g j ? coll a m n"
I0123 14:58:34.477835 140277666050048 graph.py:498] 
I0123 14:58:34.477891 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g g j ? coll a m n
I0123 14:58:35.711602 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1792676448822021
I0123 14:58:39.518538 140277666050048 ddar.py:60] Depth 2/1000 time = 3.8067641258239746
I0123 14:58:46.954914 140277666050048 ddar.py:60] Depth 3/1000 time = 7.43614935874939
I0123 14:58:58.207561 140277666050048 ddar.py:60] Depth 4/1000 time = 11.252333641052246
I0123 14:59:09.461414 140277666050048 ddar.py:60] Depth 5/1000 time = 11.253592252731323
I0123 14:59:20.585234 140277666050048 ddar.py:60] Depth 6/1000 time = 11.123472690582275
I0123 14:59:32.841161 140277666050048 ddar.py:60] Depth 7/1000 time = 12.255408763885498
I0123 14:59:32.933490 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:59:32.933555 140277666050048 alphageometry.py:566] LM output (score=-3.435608): "q : T b g g q 22 ;"
I0123 14:59:32.933590 140277666050048 alphageometry.py:567] Translation: "q = on_tline q g b g"

I0123 14:59:32.933629 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g b g ? coll a m n"
I0123 14:59:32.933811 140277666050048 graph.py:498] 
I0123 14:59:32.933868 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q g b g ? coll a m n
I0123 14:59:34.174034 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1890711784362793
I0123 14:59:38.018074 140277666050048 ddar.py:60] Depth 2/1000 time = 3.8438720703125
I0123 14:59:45.406626 140277666050048 ddar.py:60] Depth 3/1000 time = 7.388312101364136
I0123 14:59:55.037826 140277666050048 ddar.py:60] Depth 4/1000 time = 9.630895137786865
I0123 15:00:06.945782 140277666050048 ddar.py:60] Depth 5/1000 time = 11.907705307006836
I0123 15:00:17.910601 140277666050048 ddar.py:60] Depth 6/1000 time = 10.964462757110596
I0123 15:00:30.060278 140277666050048 ddar.py:60] Depth 7/1000 time = 12.149100303649902
I0123 15:00:30.153618 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:00:30.153707 140277666050048 alphageometry.py:566] LM output (score=-3.503703): "q : T d i d q 22 ;"
I0123 15:00:30.153755 140277666050048 alphageometry.py:567] Translation: "q = on_tline q d d i"

I0123 15:00:30.153804 140277666050048 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q d d i ? coll a m n"
I0123 15:00:30.153983 140277666050048 graph.py:498] 
I0123 15:00:30.154041 140277666050048 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e d c b; f = foot f d b a; g = foot g d c a; h = on_line h d f; i = on_line i d g; j = on_line j d e; k = foot k c i j; l = foot l b j h; m = foot m a h i; n = on_line n c k, on_line n b l; o = mirror o a m; p = on_tline p c b c; q = on_tline q d d i ? coll a m n
I0123 15:00:31.395499 140277666050048 ddar.py:60] Depth 1/1000 time = 1.1918327808380127
I0123 15:00:35.084602 140277666050048 ddar.py:60] Depth 2/1000 time = 3.6889336109161377
I0123 15:00:41.784769 140277666050048 ddar.py:60] Depth 3/1000 time = 6.699986457824707
I0123 15:00:51.937957 140277666050048 ddar.py:60] Depth 4/1000 time = 10.152979373931885
I0123 15:01:02.167136 140277666050048 ddar.py:60] Depth 5/1000 time = 10.228939294815063
I0123 15:01:13.797801 140277666050048 ddar.py:60] Depth 6/1000 time = 11.630367994308472
I0123 15:01:13.798485 140277666050048 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:01:13.798529 140277666050048 alphageometry.py:585] Timeout.
