I0123 20:04:23.906708 139860559134720 inference_utils.py:69] Parsing gin configuration.
I0123 20:04:23.906810 139860559134720 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 20:04:23.907010 139860559134720 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 20:04:23.907045 139860559134720 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 20:04:23.907077 139860559134720 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 20:04:23.907108 139860559134720 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 20:04:23.907136 139860559134720 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 20:04:23.907165 139860559134720 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 20:04:23.907193 139860559134720 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 20:04:23.907221 139860559134720 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 20:04:23.907247 139860559134720 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 20:04:23.907274 139860559134720 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 20:04:23.907320 139860559134720 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 20:04:23.907462 139860559134720 resource_reader.py:55] Path not found: base_htrans.gin
I0123 20:04:23.907666 139860559134720 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 20:04:23.907772 139860559134720 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 20:04:23.914201 139860559134720 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 20:04:23.914330 139860559134720 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 20:04:23.914661 139860559134720 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 20:04:23.914770 139860559134720 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 20:04:23.915058 139860559134720 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 20:04:23.915162 139860559134720 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 20:04:23.915580 139860559134720 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 20:04:23.915679 139860559134720 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 20:04:23.919348 139860559134720 training_loop.py:334] ==== Training loop: initializing model ====
I0123 20:04:24.030819 139860559134720 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 20:04:24.032272 139860559134720 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 20:04:24.046757 139860559134720 training_loop.py:335] Process 0 of 1
I0123 20:04:24.046848 139860559134720 training_loop.py:336] Local device count = 1
I0123 20:04:24.046915 139860559134720 training_loop.py:337] Number of replicas = 1
I0123 20:04:24.046969 139860559134720 training_loop.py:339] Using random number seed 42
I0123 20:04:24.530755 139860559134720 training_loop.py:359] Initializing the model.
I0123 20:04:24.953214 139860559134720 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:24.953442 139860559134720 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 20:04:24.953544 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.953624 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.953709 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.953792 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.953863 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.953934 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.954002 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.954071 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.954138 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.954206 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.954275 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.954343 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:04:24.954381 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:24.954425 139860559134720 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:04:24.954537 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:24.954575 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:24.954605 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:24.956620 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:24.961921 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:24.972966 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:24.973242 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:24.977598 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:24.988137 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:24.988193 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:24.988230 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:24.988262 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:24.988324 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:24.989498 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:24.989575 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:24.990288 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:24.992738 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:24.998939 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.000252 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.000336 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.000372 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.000433 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.000562 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.000888 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.000934 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.002840 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.002940 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.005843 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.005923 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.006417 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.016587 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.025349 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.025447 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.025757 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.025839 139860559134720 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:04:25.025949 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.025987 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.026018 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.027863 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.030323 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.035888 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.036152 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.038794 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.042565 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.042619 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.042655 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.042685 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.042748 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.043315 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.043395 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.043755 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.044509 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.046995 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.047616 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.047693 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.047727 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.047785 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.047910 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.048231 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.048272 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.050213 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.050306 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.052788 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.052866 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.053295 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.055612 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.057486 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.057578 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.057879 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.057960 139860559134720 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:04:25.058069 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.058107 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.058137 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.060372 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.062739 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.068251 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.068504 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.071236 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.075240 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.075295 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.075330 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.075360 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.075423 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.075982 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.076055 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.076413 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.077182 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.079704 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.080368 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.080444 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.080478 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.080537 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.080663 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.080987 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.081030 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.082922 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.083015 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.085521 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.085604 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.086111 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.088390 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.090314 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.090409 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.090707 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.090785 139860559134720 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:04:25.090893 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.090931 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.090960 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.092847 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.095244 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.100814 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.101077 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.103721 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.107483 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.107539 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.107574 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.107604 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.107664 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.108215 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.108289 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.108649 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.109415 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.111969 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.112591 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.112670 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.112705 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.112764 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.112894 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.113211 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.113253 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.115149 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.115245 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.117827 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.117914 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.118343 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.120575 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.122473 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.122571 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.122862 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.122941 139860559134720 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:04:25.123048 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.123085 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.123115 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.124990 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.127380 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.132967 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.133219 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.136229 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.140753 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.140859 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.140896 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.140927 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.140999 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.141583 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.141665 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.142029 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.142805 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.145365 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.145999 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.146076 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.146111 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.146173 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.146314 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.146656 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.146699 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.148620 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.148712 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.151269 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.151348 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.151781 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.154039 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.155992 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.156086 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.156378 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.156459 139860559134720 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:04:25.156567 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.156605 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.156635 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.158484 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.160846 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.166386 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.166645 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.169310 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.173278 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.173333 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.173368 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.173398 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.173457 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.174060 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.174137 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.174497 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.175272 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.177769 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.178393 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.178472 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.178506 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.178563 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.178691 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.179008 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.179049 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.180917 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.181011 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.183538 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.183618 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.184042 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.186326 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.188212 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.188306 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.188595 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.188673 139860559134720 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:04:25.188779 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.188816 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.188845 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.190706 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.193122 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.198710 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.198974 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.201622 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.205353 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.205406 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.205441 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.205470 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.205529 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.206098 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.206174 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.206537 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.207309 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.209803 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.210423 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.210501 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.210535 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.210593 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.210720 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.211033 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.211075 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.213318 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.213410 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.215860 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.215941 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.216357 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.357206 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.359407 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.359556 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.359876 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.359968 139860559134720 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:04:25.360083 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.360123 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.360155 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.362223 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.364716 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.370401 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.370674 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.373356 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.377239 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.377296 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.377332 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.377362 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.377424 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.378031 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.378112 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.378478 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.379256 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.381848 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.382484 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.382560 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.382597 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.382655 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.382782 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.383102 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.383145 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.385243 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.385337 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.387865 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.387945 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.388432 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.390904 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.392802 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.392902 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.393193 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.393272 139860559134720 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:04:25.393381 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.393419 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.393449 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.395361 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.397755 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.403342 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.403605 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.406297 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.410036 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.410091 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.410126 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.410156 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.410217 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.410778 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.410852 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.411206 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.411965 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.414495 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.415111 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.415186 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.415220 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.415278 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.415404 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.415724 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.415766 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.417637 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.417737 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.420277 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.420356 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.420787 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.423049 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.424917 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.425009 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.425299 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.425383 139860559134720 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:04:25.425493 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.425531 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.425560 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.427440 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.429816 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.435743 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.436004 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.438702 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.442423 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.442478 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.442514 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.442545 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.442607 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.443168 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.443246 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.443603 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.444413 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.446916 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.447527 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.447605 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.447638 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.447696 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.447823 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.448144 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.448187 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.450078 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.450170 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.452713 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.452793 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.453215 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.455461 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.457412 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.457509 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.457810 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.457895 139860559134720 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:04:25.458005 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.458044 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.458074 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.459898 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.462341 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.467827 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.468087 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.470752 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.474418 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.474476 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.474511 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.474540 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.474642 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.475207 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.475282 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.475635 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.476395 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.478870 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.479487 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.479562 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.479595 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.479654 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.479776 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.480096 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.480139 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.482104 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.482201 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.484932 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.485010 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.485430 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.487710 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.489650 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.489745 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.490038 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.490118 139860559134720 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:04:25.490233 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.490271 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.490301 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.492137 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.494557 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.500119 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.500382 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.502988 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:04:25.507054 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.507108 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.507143 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.507173 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.507233 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.507797 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.507873 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.508234 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.508994 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.511453 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.512075 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.512151 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.512185 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.512244 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.512368 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.512685 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.512727 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.514673 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.514765 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.517238 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.517316 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.517750 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.520021 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.521909 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.522010 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.522301 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.522592 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.522664 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.522728 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.522786 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.522841 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.522895 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.522948 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.523001 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.523054 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.523105 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.523156 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.523208 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:04:25.523244 139860559134720 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:04:25.526738 139860559134720 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:04:25.574064 139860559134720 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.574148 139860559134720 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:04:25.574201 139860559134720 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:04:25.574303 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.574340 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.574368 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.574431 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.576835 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.582269 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.582528 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.585152 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.601381 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.601437 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.601471 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.601501 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.601562 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.602684 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.602761 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.603462 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.605443 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.610170 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.611449 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.611534 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.611569 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.611628 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.611762 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.611873 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.611911 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.613810 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.613904 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.616314 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.616397 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.616504 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.618724 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.620655 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.620750 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.621040 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.621120 139860559134720 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:04:25.621227 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.621265 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.621295 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.621359 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.623613 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.629065 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.629319 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.632007 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.644870 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.644924 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.644959 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.644989 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.645048 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.645604 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.645685 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.646042 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.646728 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.649233 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.649868 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.649945 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.649983 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.650043 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.650173 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.650281 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.650318 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.652225 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.652318 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.654724 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.654804 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.654911 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.657114 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.659028 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.659122 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.659405 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.659484 139860559134720 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:04:25.659591 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.659628 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.659657 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.659721 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.661952 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.667341 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.667596 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.670277 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.682772 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.682828 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.682863 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.682893 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.682955 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.683513 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.683588 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.683944 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.684633 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.687098 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.687710 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.687784 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.687818 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.687881 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.688009 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.688119 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.688157 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.690081 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.690175 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.692597 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.692675 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.692782 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.694981 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.696878 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.696972 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.697257 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.697335 139860559134720 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:04:25.697443 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.697480 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.697510 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.697573 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.699808 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.705199 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.705455 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.708124 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.720569 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.720623 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.720658 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.720687 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.720749 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.721295 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.721370 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.721736 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.722421 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.724908 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.725519 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.725593 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.725627 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.725693 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.725830 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.725939 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.725975 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.728172 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.728264 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.730633 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.730712 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.730820 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.732988 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.734822 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.734916 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.735199 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.735278 139860559134720 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:04:25.735385 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.735422 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.735451 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.735512 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.737800 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.743137 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.743395 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.746079 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.758548 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.758602 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.758636 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.758666 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.758725 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.759280 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.759354 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.759709 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.760386 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.762908 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.763530 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.763606 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.763640 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.763697 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.763829 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.763936 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.763973 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.765835 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.765928 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.768309 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.768391 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.768497 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.770758 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.772579 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.772672 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.772952 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.773031 139860559134720 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:04:25.773137 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.773175 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.773205 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.773268 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.775506 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.780868 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.781123 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.783777 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.801676 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.801760 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.801798 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.801831 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.801906 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.802517 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.802592 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.802964 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.803666 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.806190 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.806812 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.806889 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.806925 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.806990 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.807115 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.807233 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.807271 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.809299 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.809392 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.811856 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.811935 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.812044 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.814304 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.816162 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.816254 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.816538 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.816619 139860559134720 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:04:25.816729 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.816769 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.816798 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.816864 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.819117 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.824603 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.824857 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.827460 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.840349 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.840404 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.840439 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.840470 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.840534 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.841085 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.841160 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.841516 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.842204 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.844690 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.845349 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.845424 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.845458 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.845516 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.845652 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.845762 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.845803 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.847665 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.847758 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.850152 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.850231 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.850335 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.852508 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.854433 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.854527 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.854811 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.854889 139860559134720 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:04:25.854995 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.855033 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.855062 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.855124 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.857376 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.862960 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.863230 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.865908 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.878463 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.878518 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.878553 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.878583 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.878645 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.879236 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.879312 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.879670 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.880371 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.882879 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.883501 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.883577 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.883610 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.883668 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.883796 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.883904 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.883947 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.885836 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.885930 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.888385 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.888464 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.888572 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.890803 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.892652 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.892746 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.893026 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.893105 139860559134720 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:04:25.893212 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.893249 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.893278 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.893340 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.895572 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.901050 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.901308 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.903922 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.916471 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.916525 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.916559 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.916589 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.916651 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.917202 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.917276 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.917634 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.918332 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.920817 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.921473 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.921550 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.921583 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.921648 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.921781 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.921891 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.921928 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.923818 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.923911 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.926316 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.926395 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.926499 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.928687 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.930606 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.930702 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.930986 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.931065 139860559134720 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:04:25.931173 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.931211 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.931239 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.931301 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.933511 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.938834 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.939085 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.942026 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.954295 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.954349 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.954384 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.954413 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.954473 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.955071 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.955146 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.955500 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.956179 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.958630 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.959239 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.959316 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.959349 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.959405 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.959527 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.959632 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.959668 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.961501 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.961601 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.964015 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.964099 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:25.964206 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:25.966391 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.968202 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.968296 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:25.968578 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.968657 139860559134720 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:04:25.968763 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:25.968800 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:25.968828 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:25.968890 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.971101 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:25.976500 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.976756 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:25.979361 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:25.991733 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:25.991786 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:25.991821 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:25.991849 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.991909 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.992450 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.992529 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.992876 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.993561 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.996028 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.996675 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.996750 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:25.996784 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:25.996840 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.996968 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:25.997076 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:25.997113 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:25.998970 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:25.999071 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.001461 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.001540 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.001651 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.003811 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.005711 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.005808 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.006089 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.006169 139860559134720 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:04:26.006273 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.006310 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.006339 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.006402 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.008598 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.013938 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.014189 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.016752 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.029033 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.029088 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.029124 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.029154 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.029214 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.029774 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.029850 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.030203 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.030882 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.033385 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.033999 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.034076 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.034110 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.034167 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.034296 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.034405 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.034444 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.036270 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.036361 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.038740 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.038819 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.038927 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.041478 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.043316 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.043410 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.043690 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.043779 139860559134720 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:04:26.046601 139860559134720 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:04:26.101485 139860559134720 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.101567 139860559134720 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:04:26.101620 139860559134720 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:04:26.101734 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.101771 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.101799 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.101860 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.104157 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.109419 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.109682 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.112179 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.124140 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.124194 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.124228 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.124258 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.124317 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.124856 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.124930 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.125279 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.125941 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.128390 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.128989 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.129063 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.129096 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.129153 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.129278 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.129393 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.129432 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.131222 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.131313 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.133625 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.133708 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.133812 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.136000 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.137792 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.137886 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.138164 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.138242 139860559134720 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:04:26.138345 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.138382 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.138411 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.138473 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.140637 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.145822 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.146072 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.148636 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.160455 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.160510 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.160544 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.160574 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.160634 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.161172 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.161245 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.161592 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.162260 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.164701 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.165299 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.165375 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.165408 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.165465 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.165588 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.165702 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.165746 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.167538 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.167629 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.169960 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.170038 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.170142 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.172335 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.174133 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.174226 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.174503 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.174582 139860559134720 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:04:26.174685 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.174722 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.174751 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.174813 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.176976 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.182184 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.182436 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.185004 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.196974 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.197029 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.197063 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.197092 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.197152 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.197695 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.197773 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.198122 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.198776 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.201662 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.202266 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.202341 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.202374 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.202431 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.202555 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.202661 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.202698 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.204493 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.204586 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.206929 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.207007 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.207112 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.209298 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.211099 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.211194 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.211474 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.211553 139860559134720 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:04:26.211657 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.211694 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.211723 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.211784 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.213957 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.219150 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.219401 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.221987 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.233959 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.234013 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.234049 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.234089 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.234151 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.234699 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.234772 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.235122 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.235788 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.238278 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.238882 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.238955 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.238987 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.239044 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.239166 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.239270 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.239312 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.241140 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.241230 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.243579 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.243655 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.243759 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.245980 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.247786 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.247877 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.248153 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.248231 139860559134720 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:04:26.248334 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.248369 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.248397 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.248456 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.250641 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.255875 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.256125 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.258742 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.270891 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.270942 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.270975 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.271003 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.271062 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.271596 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.271668 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.272018 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.272689 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.275175 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.275794 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.275867 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.275899 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.275955 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.276077 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.276183 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.276219 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.278044 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.278140 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.280480 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.280556 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.280660 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.282871 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.284672 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.284762 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.285039 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.285116 139860559134720 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:04:26.285218 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.285255 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.285283 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.285343 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.287534 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.292762 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.293012 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.295616 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.307696 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.307749 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.307782 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.307810 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.307869 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.308405 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.308477 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.308827 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.309490 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.312396 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.313000 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.313074 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.313107 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.313162 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.313285 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.313388 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.313424 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.315235 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.315330 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.317658 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.317734 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.317838 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.320044 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.321851 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.321943 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.322220 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.322298 139860559134720 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:04:26.322400 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.322436 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.322464 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.322524 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.324682 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.329947 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.330197 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.332790 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.344832 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.344886 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.344919 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.344947 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.345006 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.345550 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.345623 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.345984 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.346657 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.349120 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.349742 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.349816 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.349848 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.349905 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.350030 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.350134 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.350169 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.351978 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.352068 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.354402 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.354478 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.354586 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.356802 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.358601 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.358694 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.358973 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.359050 139860559134720 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:04:26.359152 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.359187 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.359215 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.359275 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.361447 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.366705 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.366956 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.369579 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.381852 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.381906 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.381939 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.381967 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.382027 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.382577 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.382650 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.382997 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.383666 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.386148 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.386754 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.386828 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.386860 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.386916 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.387037 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.387145 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.387181 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.389015 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.389104 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.391434 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.391519 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.391625 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.393848 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.395669 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.395760 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.396036 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.396114 139860559134720 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:04:26.396216 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.396252 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.396279 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.396339 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.398507 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.403785 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.404041 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.406681 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.418823 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.418876 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.418910 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.418938 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.418996 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.419547 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.419620 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.419969 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.420637 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.423511 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.424110 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.424184 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.424216 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.424271 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.424396 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.424502 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.424538 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.426368 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.426458 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.428788 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.428870 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.428974 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.431203 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.433007 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.433098 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.433376 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.433454 139860559134720 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:04:26.433556 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.433591 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.433619 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.433685 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.435856 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.441108 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.441357 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.443971 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.456122 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.456174 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.456207 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.456235 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.456295 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.456845 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.456919 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.457274 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.457961 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.460440 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.461045 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.461119 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.461152 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.461206 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.461328 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.461432 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.461468 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.463769 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.463860 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.466194 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.466270 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.466380 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.468554 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.470350 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.470443 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.470723 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.470801 139860559134720 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:04:26.470903 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.470939 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.470966 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.471026 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.473199 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.478483 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.478734 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.481329 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.493395 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.493448 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.493481 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.493510 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.493570 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.494121 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.494196 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.494549 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.495220 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.497700 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.498315 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.498389 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.498420 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.498475 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.498601 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.498706 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.498742 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.500566 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.500656 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.503021 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.503097 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.503201 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.505418 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.507219 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.507311 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.507591 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.507669 139860559134720 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:04:26.507773 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:04:26.507809 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:04:26.507837 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:04:26.507898 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.510082 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:04:26.515352 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.515603 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:04:26.518193 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:04:26.530227 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:04:26.530279 139860559134720 attention.py:418] Single window, no scan.
I0123 20:04:26.530312 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:04:26.530340 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.530399 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.530943 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.531016 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.531363 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.532031 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.534890 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.535493 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.535568 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:04:26.535600 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:04:26.535655 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.535777 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:04:26.535882 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:04:26.535917 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.537733 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.537824 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.540144 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.540220 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:04:26.540322 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:04:26.542549 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:04:26.544339 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.544430 139860559134720 nn_components.py:261] mlp: residual
I0123 20:04:26.544704 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:26.544784 139860559134720 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:04:26.547606 139860559134720 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:04:30.963453 139860559134720 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 20:04:31.484835 139860559134720 training_loop.py:409] No working directory specified.
I0123 20:04:31.484957 139860559134720 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 20:04:31.485736 139860559134720 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 20:04:34.776500 139860559134720 training_loop.py:447] Only restoring trainable parameters.
I0123 20:04:34.777112 139860559134720 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 20:04:34.777191 139860559134720 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.777241 139860559134720 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.777284 139860559134720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.777325 139860559134720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.777365 139860559134720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.777403 139860559134720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.777440 139860559134720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.777476 139860559134720 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.777512 139860559134720 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.777548 139860559134720 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.777584 139860559134720 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.777621 139860559134720 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.777664 139860559134720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.777701 139860559134720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.777738 139860559134720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.777773 139860559134720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.777809 139860559134720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.777844 139860559134720 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.777879 139860559134720 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.777926 139860559134720 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.777964 139860559134720 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.778000 139860559134720 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.778035 139860559134720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.778070 139860559134720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778106 139860559134720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.778141 139860559134720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778177 139860559134720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778211 139860559134720 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.778246 139860559134720 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.778280 139860559134720 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778315 139860559134720 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.778350 139860559134720 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.778383 139860559134720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.778418 139860559134720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778452 139860559134720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.778486 139860559134720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778521 139860559134720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778554 139860559134720 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.778588 139860559134720 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.778622 139860559134720 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778658 139860559134720 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.778692 139860559134720 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.778726 139860559134720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.778762 139860559134720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778797 139860559134720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.778837 139860559134720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778873 139860559134720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.778908 139860559134720 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.778942 139860559134720 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.778976 139860559134720 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779011 139860559134720 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.779045 139860559134720 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.779078 139860559134720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.779113 139860559134720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779148 139860559134720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.779182 139860559134720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779217 139860559134720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779251 139860559134720 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.779285 139860559134720 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.779319 139860559134720 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779354 139860559134720 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.779388 139860559134720 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.779423 139860559134720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.779457 139860559134720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779490 139860559134720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.779524 139860559134720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779558 139860559134720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779592 139860559134720 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.779626 139860559134720 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.779660 139860559134720 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779694 139860559134720 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.779728 139860559134720 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.779768 139860559134720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.779804 139860559134720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779839 139860559134720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.779875 139860559134720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779910 139860559134720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.779945 139860559134720 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.779980 139860559134720 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.780015 139860559134720 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780049 139860559134720 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.780084 139860559134720 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.780119 139860559134720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.780154 139860559134720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780189 139860559134720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.780223 139860559134720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780257 139860559134720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780291 139860559134720 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.780325 139860559134720 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.780359 139860559134720 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780393 139860559134720 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.780429 139860559134720 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.780464 139860559134720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.780498 139860559134720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780533 139860559134720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.780566 139860559134720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780601 139860559134720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780636 139860559134720 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.780669 139860559134720 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.780708 139860559134720 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780744 139860559134720 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.780779 139860559134720 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.780814 139860559134720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.780848 139860559134720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780882 139860559134720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.780916 139860559134720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780950 139860559134720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.780984 139860559134720 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.781018 139860559134720 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.781052 139860559134720 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.781086 139860559134720 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.781121 139860559134720 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:04:34.781155 139860559134720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:04:34.781188 139860559134720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.781222 139860559134720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.781256 139860559134720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.781290 139860559134720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.781324 139860559134720 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:04:34.781357 139860559134720 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:04:34.781391 139860559134720 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:04:34.781425 139860559134720 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:04:34.781453 139860559134720 training_loop.py:725] Total parameters: 152072288
I0123 20:04:34.781683 139860559134720 training_loop.py:739] Total state size: 0
I0123 20:04:34.805493 139860559134720 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 20:04:34.805746 139860559134720 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 20:04:34.806134 139860559134720 training_loop.py:652] Compiling mode beam_search with jit.
I0123 20:04:34.806460 139860559134720 training_loop.py:89] registering functions: dict_keys([])
I0123 20:04:34.823070 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q ? coll n r l
I0123 20:04:43.032076 139860559134720 ddar.py:60] Depth 1/1000 time = 8.108314990997314
I0123 20:05:03.341075 139860559134720 ddar.py:60] Depth 2/1000 time = 20.308741807937622
I0123 20:05:26.670177 139860559134720 ddar.py:60] Depth 3/1000 time = 23.32871651649475
I0123 20:05:53.998153 139860559134720 ddar.py:60] Depth 4/1000 time = 27.327613592147827
I0123 20:06:25.915662 139860559134720 ddar.py:60] Depth 5/1000 time = 31.917211771011353
I0123 20:06:58.695462 139860559134720 ddar.py:60] Depth 6/1000 time = 32.779470920562744
I0123 20:07:32.787942 139860559134720 ddar.py:60] Depth 7/1000 time = 34.0920844078064
I0123 20:08:07.039891 139860559134720 ddar.py:60] Depth 8/1000 time = 34.25065064430237
I0123 20:08:42.071954 139860559134720 ddar.py:60] Depth 9/1000 time = 34.90788650512695
I0123 20:09:17.565507 139860559134720 ddar.py:60] Depth 10/1000 time = 35.49321389198303
I0123 20:09:53.988193 139860559134720 ddar.py:60] Depth 11/1000 time = 36.23706936836243
I0123 20:10:30.470143 139860559134720 ddar.py:60] Depth 12/1000 time = 36.439409494400024
I0123 20:10:30.476897 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:10:30.477030 139860559134720 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 20:10:30.477070 139860559134720 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a e b e 02 D b d d e 03 ^ b a b e a e a b 04 ; f : D a d d f 05 ; g : C c f g 06 D c g f g 07 ; h : C d g h 08 T c d c h 09 ; i : C a b i 10 C e f i 11 ; j : C a b j 12 C c e j 13 ; k : C a d k 14 T a i i k 15 ; l : C d f l 16 C i k l 17 ; m : C b d m 18 T b j j m 19 ; n : C c d n 20 C j m n 21 ; o : D c d d o 22 ; p : C h o p 23 D d o d p 24 ; q : C a b q 25 C e p q 26 ; r : D o r p r 27 D o r q r 28 ? C n r l {F1} x00
I0123 20:10:30.477101 139860559134720 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a e b e 02 D b d d e 03 ^ b a b e a e a b 04 ; f : D a d d f 05 ; g : C c f g 06 D c g f g 07 ; h : C d g h 08 T c d c h 09 ; i : C a b i 10 C e f i 11 ; j : C a b j 12 C c e j 13 ; k : C a d k 14 T a i i k 15 ; l : C d f l 16 C i k l 17 ; m : C b d m 18 T b j j m 19 ; n : C c d n 20 C j m n 21 ; o : D c d d o 22 ; p : C h o p 23 D d o d p 24 ; q : C a b q 25 C e p q 26 ; r : D o r p r 27 D o r q r 28 ? C n r l {F1} x00
I0123 20:10:30.622071 139860559134720 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.622251 139860559134720 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 20:10:30.622351 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.622425 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.622493 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.622558 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.622624 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.622690 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.622754 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.622817 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.622892 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.622958 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.623023 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.623087 139860559134720 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:10:30.623124 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.623166 139860559134720 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:10:30.623268 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.623304 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.623332 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.625228 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.627666 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.633232 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.633496 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.636036 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.639784 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.639837 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.639872 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.639905 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.639966 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.640607 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.640681 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.641035 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.641790 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.644259 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.644861 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.644934 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.644966 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.645022 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.645147 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.645460 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.645500 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.647415 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.647507 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.649917 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.649995 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.650411 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.652667 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.654537 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.654629 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.654907 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.654984 139860559134720 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:10:30.655086 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.655121 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.655150 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.656961 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.659205 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.664610 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.664862 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.667410 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.670935 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.670987 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.671020 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.671049 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.671109 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.671653 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.671726 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.672078 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.672823 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.675221 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.675873 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.675947 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.675980 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.676035 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.676157 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.676465 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.676505 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.678361 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.678451 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.680826 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.680902 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.681320 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.683581 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.685432 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.685523 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.685806 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.685884 139860559134720 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:10:30.685988 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.686024 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.686052 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.687806 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.690046 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.695545 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.695797 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.698302 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.701841 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.701894 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.701927 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.701957 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.702017 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.702926 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.703000 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.703351 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.704097 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.706494 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.707099 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.707173 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.707207 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.707262 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.707385 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.707690 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.707730 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.709604 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.709699 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.712061 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.712137 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.712545 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.714694 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.716519 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.716614 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.716890 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.716966 139860559134720 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:10:30.717068 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.717103 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.717130 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.718943 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.721155 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.726500 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.726744 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.729187 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.732701 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.732753 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.732785 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.732813 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.732871 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.733403 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.733476 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.733829 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.734560 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.736920 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.737513 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.737587 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.737619 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.737679 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.737804 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.738158 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.738198 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.740008 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.740096 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.742453 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.742528 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.742935 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.745089 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.747018 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.747110 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.747396 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.747473 139860559134720 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:10:30.747575 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.747610 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.747638 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.749536 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.751774 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.757197 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.757442 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.759892 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.763379 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.763430 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.763463 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.763491 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.763551 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.764137 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.764210 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.764553 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.765289 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.767675 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.768267 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.768339 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.768372 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.768426 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.768547 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.768856 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.768897 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.770789 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.770877 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.773240 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.773315 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.773733 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.775899 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.777734 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.777826 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.778106 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.778188 139860559134720 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:10:30.778292 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.778328 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.778356 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.780170 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.782426 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.787809 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.788057 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.790525 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.794083 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.794135 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.794167 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.794196 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.794255 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.794791 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.794863 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.795207 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.795937 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.798321 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.798915 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.798987 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.799019 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.799073 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.799211 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.799566 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.799606 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.801461 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.801552 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.803951 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.804028 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.804435 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.806614 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.808867 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.808958 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.809245 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.809322 139860559134720 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:10:30.809432 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.809468 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.809497 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.811244 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.813485 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.818954 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.819204 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.821682 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.825181 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.825232 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.825264 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.825292 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.825352 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.825945 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.826018 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.826364 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.827091 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.829458 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.830059 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.830134 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.830166 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.830221 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.830343 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.830645 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.830684 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.832580 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.832669 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.835053 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.835130 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.835535 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.837689 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.839514 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.839606 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.839883 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.839960 139860559134720 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:10:30.840063 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.840105 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.840134 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.841940 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.844157 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.849614 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.849867 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.852304 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.855835 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.855886 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.855919 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.855947 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.856006 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.856537 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.856609 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.856949 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.857686 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.860058 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.860664 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.860740 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.860774 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.860830 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.860960 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.861451 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.861493 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.863397 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.863490 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.865872 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.865949 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.866368 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.868612 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.870564 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.870657 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.870949 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.871029 139860559134720 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:10:30.871137 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.871174 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.871210 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.872939 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.875186 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.880711 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.880954 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.883466 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.887009 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.887063 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.887096 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.887124 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.887185 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.887788 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.887861 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.888209 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.888942 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.891352 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.891950 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.892023 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.892055 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.892108 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.892228 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.892528 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.892567 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.894412 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.894504 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.896947 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.897023 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.897429 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.899709 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.901526 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.901617 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.901900 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.901981 139860559134720 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:10:30.902086 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.902123 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.902151 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.903930 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.906232 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.911701 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.911947 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.914446 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.917976 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.918027 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.918061 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.918090 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.918151 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.919126 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.919203 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.919562 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.920295 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.922713 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.923334 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.923411 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.923444 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.923501 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.923627 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.923931 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.923971 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.925795 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.925883 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.928344 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.928419 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.928824 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.931060 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.932911 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.933001 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.933279 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.933356 139860559134720 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:10:30.933457 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.933493 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.933520 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.935311 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.937637 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.943166 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.943426 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.945916 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.949462 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.949514 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.949548 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.949576 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.949635 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.950253 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.950328 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.950685 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.951442 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.953827 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.954437 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.954514 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.954547 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.954604 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.954732 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.955050 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.955091 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.956920 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.957009 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.959424 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.959501 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.959911 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.962272 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.964100 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.964190 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.964468 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.964545 139860559134720 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:10:30.964648 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:30.964685 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:30.964713 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:30.966441 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.968743 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:30.974064 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.974310 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:30.976810 139860559134720 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:10:30.980289 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:30.980340 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:30.980373 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:30.980401 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.980461 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.981051 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.981124 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.981470 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.982206 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.984564 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.985155 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.985229 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:30.985262 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:30.985316 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.985439 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:30.985752 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:30.985793 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.987615 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.987704 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.990127 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.990205 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:30.990610 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:30.992772 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:30.994605 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.994696 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:30.994970 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:30.995204 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995268 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995322 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995372 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995423 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995478 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995529 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995579 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995627 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995677 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995727 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995776 139860559134720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:10:30.995810 139860559134720 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:10:30.998618 139860559134720 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:10:31.042023 139860559134720 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.042105 139860559134720 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:10:31.042155 139860559134720 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:10:31.042253 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.042288 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.042315 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.042374 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.044681 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.049898 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.050148 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.052672 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.064990 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.065042 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.065075 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.065104 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.065164 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.065716 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.065791 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.066145 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.066825 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.069332 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.069947 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.070022 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.070055 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.070111 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.070234 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.070345 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.070381 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.072190 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.072281 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.074618 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.074695 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.074798 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.076975 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.078775 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.078868 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.079149 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.079226 139860559134720 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:10:31.079329 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.079366 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.079394 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.079455 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.081622 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.086854 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.087104 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.089711 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.102283 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.102335 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.102368 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.102395 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.102453 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.102988 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.103060 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.103404 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.104129 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.106540 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.107129 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.107202 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.107234 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.107288 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.107410 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.107514 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.107554 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.109340 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.109430 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.111759 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.111835 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.111937 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.114123 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.115912 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.116002 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.116283 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.116360 139860559134720 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:10:31.116462 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.116498 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.116526 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.116585 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.118761 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.123987 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.124237 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.126837 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.138889 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.138942 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.138974 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.139002 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.139061 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.139602 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.139675 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.140019 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.140741 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.143142 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.143749 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.143822 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.143854 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.143909 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.144032 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.144135 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.144176 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.145984 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.146073 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.148416 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.148493 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.148597 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.150795 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.152587 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.152679 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.152962 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.153040 139860559134720 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:10:31.153143 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.153179 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.153208 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.153269 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.155447 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.160654 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.160899 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.163500 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.175557 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.175609 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.175643 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.175671 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.175731 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.176275 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.176348 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.176698 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.177420 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.179840 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.180446 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.180521 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.180554 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.180610 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.180735 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.180840 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.180877 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.182708 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.182800 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.185132 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.185208 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.185312 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.187520 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.189315 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.189405 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.189694 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.189773 139860559134720 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:10:31.189877 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.189914 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.189943 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.190003 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.192183 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.197549 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.197804 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.200535 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.212955 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.213007 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.213040 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.213068 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.213126 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.213672 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.213748 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.214094 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.214807 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.217208 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.217820 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.217895 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.217927 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.217982 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.218104 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.218209 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.218244 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.220049 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.220144 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.222504 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.222581 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.222687 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.224886 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.226680 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.226773 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.227058 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.227136 139860559134720 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:10:31.227239 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.227276 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.227304 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.227364 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.229551 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.234786 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.235038 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.237631 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.249629 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.249686 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.249720 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.249748 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.249807 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.250350 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.250423 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.250769 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.251484 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.253902 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.254496 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.254570 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.254603 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.254658 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.254780 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.254883 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.254918 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.256704 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.256798 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.259146 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.259223 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.259327 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.261537 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.263327 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.263418 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.263700 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.263777 139860559134720 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:10:31.263880 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.263916 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.263943 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.264002 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.266155 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.271383 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.271632 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.274218 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.286128 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.286180 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.286213 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.286241 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.286300 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.286835 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.286906 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.287246 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.287960 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.290366 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.290966 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.291039 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.291071 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.291126 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.291248 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.291351 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.291387 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.293180 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.293270 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.295593 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.295670 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.295771 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.297963 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.299745 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.299967 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.300248 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.300325 139860559134720 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:10:31.300428 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.300464 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.300491 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.300550 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.302908 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.308122 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.308375 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.310956 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.323344 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.323397 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.323429 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.323457 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.323515 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.324053 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.324127 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.324473 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.325197 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.327585 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.328184 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.328258 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.328290 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.328345 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.328469 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.328575 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.328611 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.330390 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.330480 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.332791 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.332871 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.332977 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.335155 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.336934 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.337025 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.337309 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.337387 139860559134720 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:10:31.337491 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.337526 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.337554 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.337613 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.339771 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.344959 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.345210 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.347794 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.359580 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.359634 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.359667 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.359695 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.359753 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.360293 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.360367 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.360714 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.361372 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.363810 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.364409 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.364483 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.364515 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.364569 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.364691 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.364793 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.364828 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.366619 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.366708 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.369022 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.369106 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.369210 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.371397 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.373182 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.373272 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.373551 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.373627 139860559134720 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:10:31.373739 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.373775 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.373803 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.373861 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.376006 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.381196 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.381445 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.384010 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.395825 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.395877 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.395909 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.395938 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.395997 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.396538 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.396611 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.396954 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.397608 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.400048 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.400644 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.400718 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.400750 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.400804 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.400927 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.401031 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.401067 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.403009 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.403099 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.405427 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.405502 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.405613 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.407969 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.409760 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.409851 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.410132 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.410209 139860559134720 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:10:31.410311 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.410347 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.410375 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.410434 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.412576 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.417761 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.418010 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.420559 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.432866 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.432919 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.432952 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.432980 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.433040 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.433581 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.433658 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.434012 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.434678 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.437128 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.437729 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.437805 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.437837 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.437893 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.438016 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.438121 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.438157 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.439923 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.440011 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.442318 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.442394 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.442499 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.444671 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.446443 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.446534 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.446814 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.446889 139860559134720 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:10:31.446991 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.447027 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.447054 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.447113 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.449248 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.454441 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.454689 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.457238 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.468941 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.468994 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.469027 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.469056 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.469114 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.469655 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.469729 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.470072 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.470728 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.473153 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.473760 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.473835 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.473867 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.473922 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.474043 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.474147 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.474183 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.475944 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.476032 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.478332 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.478408 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.478509 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.480668 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.482462 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.482555 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.482837 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.482920 139860559134720 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:10:31.485613 139860559134720 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:10:31.535429 139860559134720 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.535511 139860559134720 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:10:31.535561 139860559134720 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:10:31.535660 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.535695 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.535723 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.535784 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.537993 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.543272 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.543519 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.546023 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.557961 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.558014 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.558048 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.558076 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.558136 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.558680 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.558753 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.559099 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.559756 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.562119 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.562707 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.562780 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.562812 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.562866 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.562988 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.563092 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.563128 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.564968 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.565063 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.567386 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.567462 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.567564 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.569648 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.571418 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.571508 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.571789 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.571866 139860559134720 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:10:31.571969 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.572006 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.572034 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.572093 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.574253 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.579516 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.579764 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.582276 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.594152 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.594205 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.594238 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.594266 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.594325 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.594859 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.594931 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.595275 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.595931 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.598326 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.598924 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.598998 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.599030 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.599085 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.599206 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.599311 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.599346 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.601651 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.601743 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.604068 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.604143 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.604246 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.606338 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.608107 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.608198 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.608482 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.608560 139860559134720 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:10:31.608661 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.608697 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.608725 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.608785 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.610939 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.616150 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.616397 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.618880 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.630690 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.630742 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.630775 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.630804 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.630863 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.631399 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.631472 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.631819 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.632470 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.634859 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.635446 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.635519 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.635551 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.635607 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.635730 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.635834 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.635870 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.637755 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.637845 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.640165 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.640247 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.640353 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.642480 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.644257 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.644348 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.644631 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.644708 139860559134720 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:10:31.644810 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.644846 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.644874 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.644933 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.647070 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.652342 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.652592 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.655101 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.666955 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.667007 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.667039 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.667068 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.667125 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.667664 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.667736 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.668081 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.668738 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.671126 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.671725 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.671798 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.671831 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.671885 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.672008 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.672112 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.672148 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.674026 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.674118 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.676448 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.676532 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.676639 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.678751 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.680525 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.680616 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.680901 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.680980 139860559134720 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:10:31.681085 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.681122 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.681150 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.681210 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.683360 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.688618 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.688868 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.691380 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.703355 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.703408 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.703441 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.703469 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.703528 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.704067 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.704139 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.704486 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.705147 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.707542 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.708134 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.708206 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.708239 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.708294 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.708415 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.708520 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.708557 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.710861 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.710955 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.713283 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.713360 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.713473 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.715612 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.717390 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.717480 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.717775 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.717854 139860559134720 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:10:31.717958 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.717994 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.718023 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.718083 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.720242 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.725534 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.725794 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.728307 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.740216 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.740269 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.740303 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.740332 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.740391 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.740931 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.741003 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.741348 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.742013 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.744398 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.744993 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.745067 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.745099 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.745156 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.745280 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.745385 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.745420 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.747286 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.747376 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.749703 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.749779 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.749886 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.751997 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.753797 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.753888 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.754172 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.754249 139860559134720 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:10:31.754353 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.754389 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.754417 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.754478 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.756644 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.761970 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.762225 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.764721 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.776669 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.776721 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.776755 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.776782 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.776841 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.777381 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.777454 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.777810 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.778474 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.780869 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.781465 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.781537 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.781569 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.781626 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.781756 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.781861 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.781897 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.783765 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.783854 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.786173 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.786250 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.786355 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.788461 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.790263 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.790355 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.790641 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.790719 139860559134720 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:10:31.790822 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.790859 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.790887 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.790947 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.793105 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.798484 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.798733 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.801238 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.813236 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.813289 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.813322 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.813350 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.813409 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.813960 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.814034 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.814382 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.815049 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.817441 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.818047 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.818121 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.818153 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.818208 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.818331 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.818437 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.818473 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.820744 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.820832 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.823180 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.823256 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.823361 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.825467 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.827263 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.827359 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.827643 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.827721 139860559134720 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:10:31.827824 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.827860 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.827888 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.827947 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.830117 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.835422 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.835671 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.838190 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.850207 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.850260 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.850293 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.850322 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.850379 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.850919 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.850990 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.851337 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.852003 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.854418 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.855019 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.855092 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.855125 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.855179 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.855300 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.855404 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.855440 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.857324 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.857411 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.859848 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.859938 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.860044 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.862185 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.864125 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.864222 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.864508 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.864585 139860559134720 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:10:31.864691 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.864727 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.864756 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.864816 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.866985 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.872311 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.872564 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.875092 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.887067 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.887120 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.887153 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.887181 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.887238 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.887781 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.887852 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.888197 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.888869 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.891275 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.891873 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.891945 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.891978 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.892034 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.892155 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.892259 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.892295 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.894159 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.894247 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.896565 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.896640 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.896745 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.898890 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.900679 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.900774 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.901062 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.901139 139860559134720 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:10:31.901241 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.901278 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.901305 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.901364 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.903527 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.908818 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.909066 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.911573 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.923579 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.923631 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.923664 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.923692 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.923749 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.924282 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.924352 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.924700 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.925353 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.927756 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.928355 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.928428 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.928461 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.928516 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.928638 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.928743 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.928779 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.931057 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.931147 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.933480 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.933554 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.933667 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.935795 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.937573 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.937668 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.937960 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.938036 139860559134720 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:10:31.938139 139860559134720 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:10:31.938175 139860559134720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:10:31.938203 139860559134720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:10:31.938262 139860559134720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.940406 139860559134720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:10:31.945703 139860559134720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.945953 139860559134720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:10:31.948456 139860559134720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:10:31.960448 139860559134720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:10:31.960500 139860559134720 attention.py:418] Single window, no scan.
I0123 20:10:31.960535 139860559134720 transformer_layer.py:389] tlayer: self-attention.
I0123 20:10:31.960562 139860559134720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.960620 139860559134720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.961158 139860559134720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.961228 139860559134720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.961572 139860559134720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.962240 139860559134720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.964771 139860559134720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.965369 139860559134720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.965442 139860559134720 transformer_layer.py:468] tlayer: End windows.
I0123 20:10:31.965474 139860559134720 transformer_layer.py:472] tlayer: final FFN.
I0123 20:10:31.965529 139860559134720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.965657 139860559134720 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:10:31.965763 139860559134720 nn_components.py:325] mlp: activation = None
I0123 20:10:31.965798 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.967650 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.967738 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.970065 139860559134720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.970141 139860559134720 transformer_base.py:443] tbase: final FFN
I0123 20:10:31.970246 139860559134720 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:10:31.972365 139860559134720 nn_components.py:329] mlp: final activation = None
I0123 20:10:31.974161 139860559134720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.974251 139860559134720 nn_components.py:261] mlp: residual
I0123 20:10:31.974533 139860559134720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:31.974620 139860559134720 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:10:31.977377 139860559134720 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:10:51.255878 139860559134720 alphageometry.py:566] LM output (score=-1.526707): "s : C f p s 29 D f s p s 30 ;"
I0123 20:10:51.256108 139860559134720 alphageometry.py:567] Translation: "s = on_line s f p, on_bline s p f"

I0123 20:10:51.256164 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s f p, on_bline s p f ? coll n r l"
I0123 20:10:51.256380 139860559134720 graph.py:498] 
I0123 20:10:51.256447 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s f p, on_bline s p f ? coll n r l
I0123 20:11:00.744206 139860559134720 ddar.py:60] Depth 1/1000 time = 9.364549398422241
I0123 20:11:21.609936 139860559134720 ddar.py:60] Depth 2/1000 time = 20.865536212921143
I0123 20:11:49.171518 139860559134720 ddar.py:60] Depth 3/1000 time = 27.561347007751465
I0123 20:12:21.142168 139860559134720 ddar.py:60] Depth 4/1000 time = 31.970327138900757
I0123 20:12:59.593291 139860559134720 ddar.py:60] Depth 5/1000 time = 38.45068860054016
I0123 20:13:39.228319 139860559134720 ddar.py:60] Depth 6/1000 time = 39.63456201553345
I0123 20:14:18.511242 139860559134720 ddar.py:60] Depth 7/1000 time = 39.28183937072754
I0123 20:14:59.049562 139860559134720 ddar.py:60] Depth 8/1000 time = 40.38513803482056
I0123 20:15:39.113502 139860559134720 ddar.py:60] Depth 9/1000 time = 40.063557147979736
I0123 20:16:21.022227 139860559134720 ddar.py:60] Depth 10/1000 time = 41.692089557647705
I0123 20:17:03.666293 139860559134720 ddar.py:60] Depth 11/1000 time = 42.592894077301025
I0123 20:17:03.676219 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:17:03.676429 139860559134720 alphageometry.py:566] LM output (score=-1.921930): "s : C c p s 29 D c s p s 30 ;"
I0123 20:17:03.676477 139860559134720 alphageometry.py:567] Translation: "s = on_line s c p, on_bline s p c"

I0123 20:17:03.677119 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s c p, on_bline s p c ? coll n r l"
I0123 20:17:03.677391 139860559134720 graph.py:498] 
I0123 20:17:03.677463 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s c p, on_bline s p c ? coll n r l
I0123 20:17:12.950845 139860559134720 ddar.py:60] Depth 1/1000 time = 9.168444395065308
I0123 20:17:33.358062 139860559134720 ddar.py:60] Depth 2/1000 time = 20.406922340393066
I0123 20:18:00.364597 139860559134720 ddar.py:60] Depth 3/1000 time = 27.006101846694946
I0123 20:18:32.313466 139860559134720 ddar.py:60] Depth 4/1000 time = 31.948418378829956
I0123 20:19:11.069252 139860559134720 ddar.py:60] Depth 5/1000 time = 38.75535988807678
I0123 20:19:50.492637 139860559134720 ddar.py:60] Depth 6/1000 time = 39.42303490638733
I0123 20:20:30.328351 139860559134720 ddar.py:60] Depth 7/1000 time = 39.834665298461914
I0123 20:21:10.605979 139860559134720 ddar.py:60] Depth 8/1000 time = 40.10733389854431
I0123 20:21:51.206725 139860559134720 ddar.py:60] Depth 9/1000 time = 40.60029888153076
I0123 20:22:32.336411 139860559134720 ddar.py:60] Depth 10/1000 time = 40.918272972106934
I0123 20:23:15.414251 139860559134720 ddar.py:60] Depth 11/1000 time = 43.02755689620972
I0123 20:23:15.424638 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:23:15.424770 139860559134720 alphageometry.py:566] LM output (score=-1.979182): "s : C m q s 29 D m s q s 30 ;"
I0123 20:23:15.424808 139860559134720 alphageometry.py:567] Translation: "s = on_line s m q, on_bline s q m"

I0123 20:23:15.425021 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s m q, on_bline s q m ? coll n r l"
I0123 20:23:15.425266 139860559134720 graph.py:498] 
I0123 20:23:15.425334 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s m q, on_bline s q m ? coll n r l
I0123 20:23:24.632683 139860559134720 ddar.py:60] Depth 1/1000 time = 9.10363245010376
I0123 20:23:46.992837 139860559134720 ddar.py:60] Depth 2/1000 time = 22.35996127128601
I0123 20:24:12.298346 139860559134720 ddar.py:60] Depth 3/1000 time = 25.30520009994507
I0123 20:24:42.060399 139860559134720 ddar.py:60] Depth 4/1000 time = 29.761667490005493
I0123 20:25:17.292571 139860559134720 ddar.py:60] Depth 5/1000 time = 35.23185873031616
I0123 20:25:52.950707 139860559134720 ddar.py:60] Depth 6/1000 time = 35.65776777267456
I0123 20:26:30.193236 139860559134720 ddar.py:60] Depth 7/1000 time = 37.24207949638367
I0123 20:27:07.567465 139860559134720 ddar.py:60] Depth 8/1000 time = 37.372735023498535
I0123 20:27:45.465163 139860559134720 ddar.py:60] Depth 9/1000 time = 37.76772880554199
I0123 20:28:23.467134 139860559134720 ddar.py:60] Depth 10/1000 time = 38.00151610374451
I0123 20:29:02.714348 139860559134720 ddar.py:60] Depth 11/1000 time = 39.16658067703247
I0123 20:29:42.119899 139860559134720 ddar.py:60] Depth 12/1000 time = 39.27678871154785
I0123 20:30:23.450915 139860559134720 ddar.py:60] Depth 13/1000 time = 41.28720498085022
I0123 20:30:23.460319 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:30:23.460450 139860559134720 alphageometry.py:566] LM output (score=-2.060580): "s : C f o s 29 D f s o s 30 ;"
I0123 20:30:23.460504 139860559134720 alphageometry.py:567] Translation: "s = on_line s f o, on_bline s o f"

I0123 20:30:23.460756 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s f o, on_bline s o f ? coll n r l"
I0123 20:30:23.461008 139860559134720 graph.py:498] 
I0123 20:30:23.461076 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s f o, on_bline s o f ? coll n r l
I0123 20:30:32.706590 139860559134720 ddar.py:60] Depth 1/1000 time = 9.13650369644165
I0123 20:30:55.939490 139860559134720 ddar.py:60] Depth 2/1000 time = 23.232527017593384
I0123 20:31:23.088380 139860559134720 ddar.py:60] Depth 3/1000 time = 27.148444175720215
I0123 20:31:55.768625 139860559134720 ddar.py:60] Depth 4/1000 time = 32.67978382110596
I0123 20:32:34.552860 139860559134720 ddar.py:60] Depth 5/1000 time = 38.78383803367615
I0123 20:33:14.182590 139860559134720 ddar.py:60] Depth 6/1000 time = 39.62936282157898
I0123 20:33:53.755547 139860559134720 ddar.py:60] Depth 7/1000 time = 39.571828842163086
I0123 20:34:34.509686 139860559134720 ddar.py:60] Depth 8/1000 time = 40.59710240364075
I0123 20:35:15.850365 139860559134720 ddar.py:60] Depth 9/1000 time = 41.34019875526428
I0123 20:35:57.421713 139860559134720 ddar.py:60] Depth 10/1000 time = 41.356574058532715
I0123 20:36:40.193810 139860559134720 ddar.py:60] Depth 11/1000 time = 42.726662397384644
I0123 20:36:40.204759 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:36:40.204895 139860559134720 alphageometry.py:566] LM output (score=-2.123924): "s : C f q s 29 D f s q s 30 ;"
I0123 20:36:40.204933 139860559134720 alphageometry.py:567] Translation: "s = on_line s f q, on_bline s q f"

I0123 20:36:40.205027 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s f q, on_bline s q f ? coll n r l"
I0123 20:36:40.205268 139860559134720 graph.py:498] 
I0123 20:36:40.205465 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s f q, on_bline s q f ? coll n r l
I0123 20:36:49.980703 139860559134720 ddar.py:60] Depth 1/1000 time = 9.65883755683899
I0123 20:37:12.375133 139860559134720 ddar.py:60] Depth 2/1000 time = 22.39421772956848
I0123 20:37:38.579535 139860559134720 ddar.py:60] Depth 3/1000 time = 26.204063892364502
I0123 20:38:08.655894 139860559134720 ddar.py:60] Depth 4/1000 time = 30.075926780700684
I0123 20:38:41.924743 139860559134720 ddar.py:60] Depth 5/1000 time = 33.26850938796997
I0123 20:39:16.835256 139860559134720 ddar.py:60] Depth 6/1000 time = 34.91002082824707
I0123 20:39:53.135183 139860559134720 ddar.py:60] Depth 7/1000 time = 36.29943513870239
I0123 20:40:28.991993 139860559134720 ddar.py:60] Depth 8/1000 time = 35.85551476478577
I0123 20:41:06.578147 139860559134720 ddar.py:60] Depth 9/1000 time = 37.45164155960083
I0123 20:41:43.472806 139860559134720 ddar.py:60] Depth 10/1000 time = 36.89417314529419
I0123 20:42:21.674735 139860559134720 ddar.py:60] Depth 11/1000 time = 38.004512786865234
I0123 20:43:00.614078 139860559134720 ddar.py:60] Depth 12/1000 time = 38.89699745178223
I0123 20:43:00.622717 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:43:00.622880 139860559134720 alphageometry.py:566] LM output (score=-2.237439): "s : C b e s 29 D b s e s 30 ;"
I0123 20:43:00.622922 139860559134720 alphageometry.py:567] Translation: "s = on_line s b e, on_bline s e b"

I0123 20:43:00.623442 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s b e, on_bline s e b ? coll n r l"
I0123 20:43:00.623709 139860559134720 graph.py:498] 
I0123 20:43:00.623779 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s b e, on_bline s e b ? coll n r l
I0123 20:43:10.271788 139860559134720 ddar.py:60] Depth 1/1000 time = 9.53575873374939
I0123 20:43:31.621584 139860559134720 ddar.py:60] Depth 2/1000 time = 21.349581718444824
I0123 20:43:59.412207 139860559134720 ddar.py:60] Depth 3/1000 time = 27.79030680656433
I0123 20:44:32.012982 139860559134720 ddar.py:60] Depth 4/1000 time = 32.60011577606201
I0123 20:45:09.283482 139860559134720 ddar.py:60] Depth 5/1000 time = 37.270039081573486
I0123 20:45:47.873418 139860559134720 ddar.py:60] Depth 6/1000 time = 38.58947396278381
I0123 20:46:27.050650 139860559134720 ddar.py:60] Depth 7/1000 time = 39.176769971847534
I0123 20:47:07.385809 139860559134720 ddar.py:60] Depth 8/1000 time = 40.33401894569397
I0123 20:47:47.751254 139860559134720 ddar.py:60] Depth 9/1000 time = 40.20772194862366
I0123 20:48:28.965393 139860559134720 ddar.py:60] Depth 10/1000 time = 41.21363711357117
I0123 20:49:10.606832 139860559134720 ddar.py:60] Depth 11/1000 time = 41.44290542602539
I0123 20:49:53.360396 139860559134720 ddar.py:60] Depth 12/1000 time = 42.71204996109009
I0123 20:49:53.368710 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:49:53.368841 139860559134720 alphageometry.py:566] LM output (score=-2.312369): "s : C b p s 29 D b s p s 30 ;"
I0123 20:49:53.368879 139860559134720 alphageometry.py:567] Translation: "s = on_line s b p, on_bline s p b"

I0123 20:49:53.369105 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s b p, on_bline s p b ? coll n r l"
I0123 20:49:53.369364 139860559134720 graph.py:498] 
I0123 20:49:53.369431 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s b p, on_bline s p b ? coll n r l
I0123 20:50:02.864967 139860559134720 ddar.py:60] Depth 1/1000 time = 9.365436792373657
I0123 20:50:26.319744 139860559134720 ddar.py:60] Depth 2/1000 time = 23.45457172393799
I0123 20:50:53.828990 139860559134720 ddar.py:60] Depth 3/1000 time = 27.50892162322998
I0123 20:51:25.730581 139860559134720 ddar.py:60] Depth 4/1000 time = 31.901100158691406
I0123 20:52:04.101486 139860559134720 ddar.py:60] Depth 5/1000 time = 38.370431661605835
I0123 20:52:43.924722 139860559134720 ddar.py:60] Depth 6/1000 time = 39.82276797294617
I0123 20:53:24.223358 139860559134720 ddar.py:60] Depth 7/1000 time = 40.29748749732971
I0123 20:54:04.742463 139860559134720 ddar.py:60] Depth 8/1000 time = 40.36241030693054
I0123 20:54:45.552086 139860559134720 ddar.py:60] Depth 9/1000 time = 40.809287786483765
I0123 20:55:26.858378 139860559134720 ddar.py:60] Depth 10/1000 time = 41.101303815841675
I0123 20:56:08.881720 139860559134720 ddar.py:60] Depth 11/1000 time = 41.976762533187866
I0123 20:56:08.892408 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:56:08.892589 139860559134720 alphageometry.py:566] LM output (score=-2.327406): "s : C b o s 29 D b s o s 30 ;"
I0123 20:56:08.892634 139860559134720 alphageometry.py:567] Translation: "s = on_line s b o, on_bline s o b"

I0123 20:56:08.892752 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s b o, on_bline s o b ? coll n r l"
I0123 20:56:08.893006 139860559134720 graph.py:498] 
I0123 20:56:08.893521 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s b o, on_bline s o b ? coll n r l
I0123 20:56:18.949772 139860559134720 ddar.py:60] Depth 1/1000 time = 9.944063901901245
I0123 20:56:40.090479 139860559134720 ddar.py:60] Depth 2/1000 time = 21.140501976013184
I0123 20:57:07.832268 139860559134720 ddar.py:60] Depth 3/1000 time = 27.74148917198181
I0123 20:57:40.746118 139860559134720 ddar.py:60] Depth 4/1000 time = 32.91341495513916
I0123 20:58:19.726222 139860559134720 ddar.py:60] Depth 5/1000 time = 38.97965407371521
I0123 20:58:59.290713 139860559134720 ddar.py:60] Depth 6/1000 time = 39.56403064727783
I0123 20:59:39.211328 139860559134720 ddar.py:60] Depth 7/1000 time = 39.919477701187134
I0123 21:00:19.674350 139860559134720 ddar.py:60] Depth 8/1000 time = 40.3050491809845
I0123 21:01:00.908912 139860559134720 ddar.py:60] Depth 9/1000 time = 41.23407459259033
I0123 21:01:42.489225 139860559134720 ddar.py:60] Depth 10/1000 time = 41.386417388916016
I0123 21:02:25.771078 139860559134720 ddar.py:60] Depth 11/1000 time = 43.23697209358215
I0123 21:02:25.777207 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:02:25.777378 139860559134720 alphageometry.py:566] LM output (score=-2.463798): "s : C m p s 29 D m s p s 30 ;"
I0123 21:02:25.777422 139860559134720 alphageometry.py:567] Translation: "s = on_line s m p, on_bline s p m"

I0123 21:02:25.778023 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s m p, on_bline s p m ? coll n r l"
I0123 21:02:25.778306 139860559134720 graph.py:498] 
I0123 21:02:25.778373 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_line s m p, on_bline s p m ? coll n r l
I0123 21:02:35.436834 139860559134720 ddar.py:60] Depth 1/1000 time = 9.545464754104614
I0123 21:02:57.519047 139860559134720 ddar.py:60] Depth 2/1000 time = 22.08201265335083
I0123 21:03:22.762267 139860559134720 ddar.py:60] Depth 3/1000 time = 25.242881536483765
I0123 21:03:52.280812 139860559134720 ddar.py:60] Depth 4/1000 time = 29.51805353164673
I0123 21:04:26.340901 139860559134720 ddar.py:60] Depth 5/1000 time = 34.05963945388794
I0123 21:05:01.648933 139860559134720 ddar.py:60] Depth 6/1000 time = 35.30764055252075
I0123 21:05:37.660320 139860559134720 ddar.py:60] Depth 7/1000 time = 36.010929107666016
I0123 21:06:14.761933 139860559134720 ddar.py:60] Depth 8/1000 time = 37.10055875778198
I0123 21:06:52.689155 139860559134720 ddar.py:60] Depth 9/1000 time = 37.79319524765015
I0123 21:07:30.302132 139860559134720 ddar.py:60] Depth 10/1000 time = 37.612529277801514
I0123 21:08:09.029104 139860559134720 ddar.py:60] Depth 11/1000 time = 38.53267288208008
I0123 21:08:48.567669 139860559134720 ddar.py:60] Depth 12/1000 time = 39.495065212249756
I0123 21:08:48.578587 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:08:48.578774 139860559134720 alphageometry.py:566] LM output (score=-2.713517): "s : T g h g s 29 ;"
I0123 21:08:48.578818 139860559134720 alphageometry.py:567] Translation: "s = on_tline s g g h"

I0123 21:08:48.578882 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_tline s g g h ? coll n r l"
I0123 21:08:48.579140 139860559134720 graph.py:498] 
I0123 21:08:48.579211 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_tline s g g h ? coll n r l
I0123 21:08:58.298539 139860559134720 ddar.py:60] Depth 1/1000 time = 9.618801832199097
I0123 21:09:20.133629 139860559134720 ddar.py:60] Depth 2/1000 time = 21.834720134735107
I0123 21:09:46.491898 139860559134720 ddar.py:60] Depth 3/1000 time = 26.357837677001953
I0123 21:10:17.549679 139860559134720 ddar.py:60] Depth 4/1000 time = 31.057335376739502
I0123 21:10:53.760239 139860559134720 ddar.py:60] Depth 5/1000 time = 36.21008563041687
I0123 21:11:30.444662 139860559134720 ddar.py:60] Depth 6/1000 time = 36.683926820755005
I0123 21:12:08.067819 139860559134720 ddar.py:60] Depth 7/1000 time = 37.62266278266907
I0123 21:12:45.799321 139860559134720 ddar.py:60] Depth 8/1000 time = 37.7299702167511
I0123 21:13:25.181013 139860559134720 ddar.py:60] Depth 9/1000 time = 39.23462414741516
I0123 21:14:04.080806 139860559134720 ddar.py:60] Depth 10/1000 time = 38.89945197105408
I0123 21:14:44.787325 139860559134720 ddar.py:60] Depth 11/1000 time = 40.51416254043579
I0123 21:15:25.722282 139860559134720 ddar.py:60] Depth 12/1000 time = 40.88827300071716
I0123 21:15:25.731813 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:15:25.731951 139860559134720 alphageometry.py:566] LM output (score=-3.073700): "s : T e i l s 29 ;"
I0123 21:15:25.731989 139860559134720 alphageometry.py:567] Translation: "s = on_tline s l e i"

I0123 21:15:25.732056 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_tline s l e i ? coll n r l"
I0123 21:15:25.732300 139860559134720 graph.py:498] 
I0123 21:15:25.732367 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_tline s l e i ? coll n r l
I0123 21:15:34.419867 139860559134720 ddar.py:60] Depth 1/1000 time = 8.580962181091309
I0123 21:15:57.168406 139860559134720 ddar.py:60] Depth 2/1000 time = 22.748278856277466
I0123 21:16:23.538637 139860559134720 ddar.py:60] Depth 3/1000 time = 26.36985969543457
I0123 21:16:55.650058 139860559134720 ddar.py:60] Depth 4/1000 time = 32.11107921600342
I0123 21:17:33.966200 139860559134720 ddar.py:60] Depth 5/1000 time = 38.315651655197144
I0123 21:18:11.916072 139860559134720 ddar.py:60] Depth 6/1000 time = 37.949424743652344
I0123 21:18:51.839319 139860559134720 ddar.py:60] Depth 7/1000 time = 39.922757148742676
I0123 21:19:31.499064 139860559134720 ddar.py:60] Depth 8/1000 time = 39.65783882141113
I0123 21:20:12.904419 139860559134720 ddar.py:60] Depth 9/1000 time = 41.241432666778564
I0123 21:20:53.176763 139860559134720 ddar.py:60] Depth 10/1000 time = 40.27188563346863
I0123 21:21:36.015338 139860559134720 ddar.py:60] Depth 11/1000 time = 42.62488627433777
I0123 21:22:19.134838 139860559134720 ddar.py:60] Depth 12/1000 time = 43.07271933555603
I0123 21:22:19.144295 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:22:19.144478 139860559134720 alphageometry.py:566] LM output (score=-3.110706): "s : T f g g s 29 ;"
I0123 21:22:19.144533 139860559134720 alphageometry.py:567] Translation: "s = on_tline s g f g"

I0123 21:22:19.144599 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_tline s g f g ? coll n r l"
I0123 21:22:19.144853 139860559134720 graph.py:498] 
I0123 21:22:19.144925 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_tline s g f g ? coll n r l
I0123 21:22:28.463652 139860559134720 ddar.py:60] Depth 1/1000 time = 9.230004787445068
I0123 21:22:51.699674 139860559134720 ddar.py:60] Depth 2/1000 time = 23.235806226730347
I0123 21:23:19.980914 139860559134720 ddar.py:60] Depth 3/1000 time = 28.280948162078857
I0123 21:23:53.186849 139860559134720 ddar.py:60] Depth 4/1000 time = 33.205488204956055
I0123 21:24:30.696656 139860559134720 ddar.py:60] Depth 5/1000 time = 37.509424448013306
I0123 21:25:09.798148 139860559134720 ddar.py:60] Depth 6/1000 time = 39.101163148880005
I0123 21:25:48.919566 139860559134720 ddar.py:60] Depth 7/1000 time = 39.120957136154175
I0123 21:26:29.154588 139860559134720 ddar.py:60] Depth 8/1000 time = 40.23315763473511
I0123 21:27:09.745318 139860559134720 ddar.py:60] Depth 9/1000 time = 40.43833589553833
I0123 21:27:51.112503 139860559134720 ddar.py:60] Depth 10/1000 time = 41.36690878868103
I0123 21:28:33.442079 139860559134720 ddar.py:60] Depth 11/1000 time = 42.12564277648926
I0123 21:29:15.851169 139860559134720 ddar.py:60] Depth 12/1000 time = 42.36434268951416
I0123 21:29:15.860643 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:29:15.860801 139860559134720 alphageometry.py:566] LM output (score=-3.139076): "s : P f s p q 29 ;"
I0123 21:29:15.860845 139860559134720 alphageometry.py:567] Translation: "s = on_pline s f p q"

I0123 21:29:15.860895 139860559134720 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_pline s f p q ? coll n r l"
I0123 21:29:15.861135 139860559134720 graph.py:498] 
I0123 21:29:15.861203 139860559134720 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b, on_bline e b a; f = on_circle f d a; g = midpoint g c f; h = lc_tangent h c d, on_line h g d; i = on_line i e f, on_line i b a; j = on_line j e c, on_line j b a; k = lc_tangent k i a, on_line k d a; l = on_line l i k, on_line l d f; m = lc_tangent m j b, on_line m d b; n = on_line n j m, on_line n d c; o = on_circle o d c; p = on_circle p d o, on_line p h o; q = on_line q e p, on_line q b a; r = circle r p o q; s = on_pline s f p q ? coll n r l
I0123 21:29:25.234462 139860559134720 ddar.py:60] Depth 1/1000 time = 9.271169424057007
I0123 21:29:45.205151 139860559134720 ddar.py:60] Depth 2/1000 time = 19.970486640930176
I0123 21:30:10.384911 139860559134720 ddar.py:60] Depth 3/1000 time = 25.179515838623047
I0123 21:30:39.279771 139860559134720 ddar.py:60] Depth 4/1000 time = 28.89452886581421
I0123 21:31:12.455106 139860559134720 ddar.py:60] Depth 5/1000 time = 33.17488694190979
I0123 21:31:47.471152 139860559134720 ddar.py:60] Depth 6/1000 time = 35.01558470726013
I0123 21:32:23.230453 139860559134720 ddar.py:60] Depth 7/1000 time = 35.75882697105408
I0123 21:32:59.380102 139860559134720 ddar.py:60] Depth 8/1000 time = 36.14796996116638
I0123 21:33:35.895133 139860559134720 ddar.py:60] Depth 9/1000 time = 36.37905836105347
I0123 21:34:13.127979 139860559134720 ddar.py:60] Depth 10/1000 time = 37.232544898986816
I0123 21:34:51.773081 139860559134720 ddar.py:60] Depth 11/1000 time = 38.456268072128296
I0123 21:34:51.822286 139860559134720 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:34:51.822390 139860559134720 alphageometry.py:585] Timeout.
