I0123 11:41:48.379150 140528121659392 inference_utils.py:69] Parsing gin configuration.
I0123 11:41:48.379242 140528121659392 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:41:48.379429 140528121659392 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:41:48.379462 140528121659392 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:41:48.379492 140528121659392 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:41:48.379519 140528121659392 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:41:48.379547 140528121659392 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:41:48.379574 140528121659392 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:41:48.379602 140528121659392 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:41:48.379629 140528121659392 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:41:48.379655 140528121659392 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:41:48.379681 140528121659392 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:41:48.379724 140528121659392 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:41:48.379843 140528121659392 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:41:48.380031 140528121659392 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:41:48.380134 140528121659392 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:41:48.386365 140528121659392 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:41:48.386487 140528121659392 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:41:48.386816 140528121659392 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:41:48.386922 140528121659392 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:41:48.387207 140528121659392 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:41:48.387309 140528121659392 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:41:48.387715 140528121659392 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:41:48.387815 140528121659392 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:41:48.391527 140528121659392 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:41:48.482130 140528121659392 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:41:48.482854 140528121659392 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:41:48.489446 140528121659392 training_loop.py:335] Process 0 of 1
I0123 11:41:48.489499 140528121659392 training_loop.py:336] Local device count = 1
I0123 11:41:48.489539 140528121659392 training_loop.py:337] Number of replicas = 1
I0123 11:41:48.489570 140528121659392 training_loop.py:339] Using random number seed 42
I0123 11:41:48.966658 140528121659392 training_loop.py:359] Initializing the model.
I0123 11:41:49.376996 140528121659392 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.377274 140528121659392 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:41:49.377383 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.377462 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.377540 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.377624 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.377710 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.377783 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.377855 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.377924 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.377994 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.378062 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.378130 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.378199 140528121659392 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:41:49.378238 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.378284 140528121659392 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:41:49.378400 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.378439 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.378469 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.380446 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.385744 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.396470 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.396744 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.401127 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.411764 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.411822 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.411860 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.411893 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.411956 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.413125 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.413203 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.413916 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.416369 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.422523 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.423836 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.423917 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.423953 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.424016 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.424143 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.424473 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.424519 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.426450 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.426550 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.429414 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.429492 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.429990 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.440091 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.448830 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.448929 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.449227 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.449307 140528121659392 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:41:49.449418 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.449457 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.449486 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.451314 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.453779 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.459308 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.459572 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.462321 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.466724 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.466833 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.466870 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.466901 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.466973 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.467610 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.467688 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.468063 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.468843 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.471354 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.471977 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.472054 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.472088 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.472149 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.472277 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.472625 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.472668 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.474629 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.474722 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.477234 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.477313 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.477749 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.480072 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.481971 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.482067 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.482364 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.482447 140528121659392 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:41:49.482556 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.482595 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.482626 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.484840 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.487206 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.492766 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.493031 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.495857 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.499724 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.499780 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.499816 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.499846 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.499912 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.500462 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.500537 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.500892 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.501658 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.504173 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.504838 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.504915 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.504952 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.505013 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.505142 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.505456 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.505498 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.507404 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.507499 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.510008 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.510092 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.510598 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.512928 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.514843 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.514938 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.515232 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.515312 140528121659392 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:41:49.515420 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.515459 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.515489 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.517366 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.519814 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.525396 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.525669 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.528321 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.532085 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.532140 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.532175 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.532205 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.532265 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.532815 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.532890 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.533252 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.534026 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.536568 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.537187 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.537263 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.537297 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.537355 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.537484 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.537806 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.537850 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.539746 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.539838 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.542400 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.542485 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.542913 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.545161 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.547062 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.547158 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.547450 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.547529 140528121659392 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:41:49.547640 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.547678 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.547709 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.549593 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.551987 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.557597 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.557864 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.560891 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.564587 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.564643 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.564678 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.564708 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.564769 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.565344 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.565421 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.565800 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.566570 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.569103 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.569743 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.569822 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.569858 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.569918 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.570055 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.570378 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.570420 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.572318 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.572411 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.574969 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.575050 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.575475 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.577724 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.579676 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.579772 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.580066 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.580146 140528121659392 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:41:49.580255 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.580294 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.580324 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.582151 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.584524 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.590088 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.590350 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.593025 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.596719 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.596774 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.596811 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.596841 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.596903 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.597504 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.597584 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.597952 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.598723 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.601198 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.601824 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.601901 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.601935 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.601993 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.602122 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.602438 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.602481 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.604354 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.604447 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.606998 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.607078 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.607496 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.609778 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.612010 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.612105 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.612398 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.612480 140528121659392 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:41:49.612588 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.612627 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.612655 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.614468 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.616882 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.622704 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.622968 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.625594 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.629294 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.629349 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.629385 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.629415 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.629476 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.630041 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.630117 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.630470 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.631232 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.633691 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.634313 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.634392 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.634426 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.634484 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.634614 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.634933 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.634976 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.637398 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.637494 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.639992 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.640075 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.640495 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.780227 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.782571 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.782767 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.783185 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.783289 140528121659392 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:41:49.783429 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.783472 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.783505 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.785624 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.788277 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.794120 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.794397 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.797080 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.800951 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.801010 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.801047 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.801078 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.801145 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.801760 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.801838 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.802207 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.802992 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.805576 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.806235 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.806313 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.806348 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.806409 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.806537 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.806861 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.806906 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.808795 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.808888 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.811396 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.811478 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.811958 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.814218 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.816109 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.816210 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.816506 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.816588 140528121659392 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:41:49.816699 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.816739 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.816769 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.818672 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.821189 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.826826 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.827087 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.829774 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.833685 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.833740 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.833776 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.833807 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.833874 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.834439 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.834514 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.834875 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.835644 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.838183 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.838800 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.838877 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.838912 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.838970 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.839095 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.839416 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.839459 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.841355 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.841448 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.843999 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.844079 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.844507 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.846780 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.848695 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.848793 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.849086 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.849173 140528121659392 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:41:49.849285 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.849324 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.849354 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.851246 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.853609 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.859529 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.859793 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.862509 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.866210 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.866266 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.866300 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.866331 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.866394 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.866951 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.867031 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.867389 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.868193 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.870665 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.871278 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.871358 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.871393 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.871452 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.871580 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.871898 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.871941 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.873833 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.873930 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.876467 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.876545 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.876969 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.879222 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.881181 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.881281 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.881574 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.881668 140528121659392 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:41:49.881783 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.881822 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.881852 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.883692 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.886123 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.891644 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.891907 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.894575 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.898280 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.898337 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.898373 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.898403 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.898505 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.899070 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.899147 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.899504 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.900278 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.902760 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.903380 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.903458 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.903493 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.903553 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.903679 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.903995 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.904039 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.905975 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.906068 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.908783 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.908862 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.909287 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.911582 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.913474 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.913568 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.913867 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.913949 140528121659392 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:41:49.914064 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.914104 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.914134 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.915935 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.918365 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:49.923970 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.924236 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:49.926881 140528121659392 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:41:49.931072 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:49.931129 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:49.931164 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:49.931194 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.931256 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.931824 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.931900 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.932259 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.933029 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.935512 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.936134 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.936209 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:49.936244 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:49.936304 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.936429 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:49.936755 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:49.936798 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.938734 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.938829 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.941312 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.941394 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:49.941826 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:49.944114 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:49.945996 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.946092 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:49.946385 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.946666 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.946736 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.946802 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.946859 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.946913 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.946966 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.947018 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.947069 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.947120 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.947170 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.947221 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.947271 140528121659392 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:41:49.947307 140528121659392 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:41:49.950792 140528121659392 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:41:49.997866 140528121659392 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:49.997951 140528121659392 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:41:49.998007 140528121659392 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:41:49.998112 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:49.998150 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:49.998179 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:49.998240 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.000662 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.006494 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.006752 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.009389 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.025725 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.025780 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.025815 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.025845 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.025908 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.027026 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.027104 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.027804 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.029806 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.034676 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.035974 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.036060 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.036096 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.036157 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.036290 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.036401 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.036440 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.038340 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.038432 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.040871 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.040950 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.041058 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.043349 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.045320 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.045415 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.045716 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.045799 140528121659392 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:41:50.045908 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.045947 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.045977 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.046050 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.048295 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.053735 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.053996 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.056681 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.069648 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.069705 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.069741 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.069772 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.069833 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.070385 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.070461 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.070816 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.071500 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.074003 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.074614 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.074689 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.074728 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.074790 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.074915 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.075023 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.075061 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.076963 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.077056 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.079479 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.079558 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.079667 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.081881 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.083802 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.083897 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.084188 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.084268 140528121659392 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:41:50.084376 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.084415 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.084445 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.084507 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.086757 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.092143 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.092401 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.095109 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.107646 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.107702 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.107738 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.107769 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.107830 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.108385 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.108460 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.108821 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.109517 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.112000 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.112613 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.112689 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.112723 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.112787 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.112915 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.113024 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.113062 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.114988 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.115082 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.117505 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.117583 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.117699 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.119902 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.126209 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.126354 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.126768 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.126858 140528121659392 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:41:50.126985 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.127027 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.127058 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.127127 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.129538 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.135077 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.135344 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.138087 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.151040 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.151099 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.151137 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.151169 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.151232 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.151822 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.151897 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.152266 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.152979 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.155525 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.156148 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.156223 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.156258 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.156316 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.156451 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.156562 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.156602 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.158832 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.158927 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.161475 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.161554 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.161669 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.163936 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.165825 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.165923 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.166213 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.166294 140528121659392 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:41:50.166402 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.166441 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.166472 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.166536 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.168836 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.174345 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.174608 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.177251 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.189968 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.190027 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.190062 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.190093 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.190154 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.190710 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.190787 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.191143 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.191854 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.194418 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.195042 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.195119 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.195154 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.195213 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.195349 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.195460 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.195498 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.197436 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.197529 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.199953 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.200034 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.200141 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.202438 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.204304 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.204398 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.204681 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.204762 140528121659392 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:41:50.204870 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.204909 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.204939 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.205000 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.207236 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.212656 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.212913 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.215618 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.228191 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.228246 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.228281 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.228312 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.228372 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.228927 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.229006 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.229365 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.230074 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.232549 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.233165 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.233242 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.233278 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.233337 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.233472 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.233588 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.233628 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.235679 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.235773 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.238175 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.238254 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.238362 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.240576 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.242440 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.242534 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.242820 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.242899 140528121659392 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:41:50.243007 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.243046 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.243077 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.243141 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.245371 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.250898 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.251159 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.253772 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.266698 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.266754 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.266789 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.266819 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.266880 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.267442 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.267518 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.267872 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.268558 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.271242 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.271904 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.271981 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.272015 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.272074 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.272208 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.272318 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.272361 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.274235 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.274330 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.276736 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.276813 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.276921 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.279133 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.281069 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.281164 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.281454 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.281535 140528121659392 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:41:50.281653 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.281693 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.281724 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.281787 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.284044 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.289483 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.289771 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.292447 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.304980 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.305036 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.305071 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.305102 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.305163 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.305776 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.305855 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.306209 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.306900 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.309390 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.310031 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.310109 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.310144 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.310203 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.310334 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.310443 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.310486 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.312355 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.312449 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.314938 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.315019 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.315128 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.317341 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.319228 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.319329 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.319626 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.319708 140528121659392 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:41:50.319818 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.319858 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.319889 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.319952 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.322217 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.327700 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.327960 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.330597 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.343147 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.343203 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.343238 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.343269 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.343333 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.343893 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.343970 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.344328 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.345010 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.347506 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.348174 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.348252 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.348286 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.348345 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.348478 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.348587 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.348626 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.350511 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.350606 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.353026 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.353104 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.353211 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.355417 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.357350 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.357444 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.357738 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.357820 140528121659392 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:41:50.357928 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.357966 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.357997 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.358058 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.360304 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.365684 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.365939 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.368920 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.381710 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.381766 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.381802 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.381833 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.381895 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.382498 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.382575 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.382930 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.383619 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.386095 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.386714 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.386790 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.386824 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.386882 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.387011 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.387119 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.387157 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.389016 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.389117 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.391607 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.391687 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.391795 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.394008 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.395865 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.395960 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.396243 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.396324 140528121659392 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:41:50.396433 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.396472 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.396503 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.396565 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.398810 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.404274 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.404536 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.407443 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.419880 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.419936 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.419971 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.420001 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.420060 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.420614 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.420690 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.421045 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.421742 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.424215 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.424880 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.424958 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.424993 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.425051 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.425181 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.425292 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.425330 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.427199 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.427299 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.429718 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.429797 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.429905 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.432108 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.434026 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.434121 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.434407 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.434487 140528121659392 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:41:50.434594 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.434633 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.434663 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.434724 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.436928 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.442312 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.442567 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.445198 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.457676 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.457732 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.457767 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.457797 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.457859 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.458416 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.458493 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.458851 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.459538 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.462091 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.462705 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.462781 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.462815 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.462873 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.463002 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.463111 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.463149 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.465025 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.465119 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.467549 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.467628 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.467735 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.470329 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.472202 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.472296 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.472585 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.472675 140528121659392 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:41:50.475550 140528121659392 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:41:50.531533 140528121659392 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.531619 140528121659392 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:41:50.531675 140528121659392 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:41:50.531779 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.531818 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.531848 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.531913 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.534289 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.539743 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.540000 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.542587 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.555602 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.555658 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.555693 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.555724 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.555787 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.556343 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.556420 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.556972 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.557657 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.560439 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.561045 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.561122 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.561157 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.561216 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.561343 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.561460 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.561499 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.563326 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.563421 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.565797 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.565877 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.565987 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.568220 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.570069 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.570165 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.570452 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.570532 140528121659392 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:41:50.570639 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.570678 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.570708 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.570771 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.572999 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.578348 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.578607 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.581265 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.593327 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.593384 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.593419 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.593450 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.593511 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.594070 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.594146 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.594501 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.595175 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.597696 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.598306 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.598383 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.598418 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.598478 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.598607 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.598717 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.598761 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.600583 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.600675 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.603075 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.603154 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.603264 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.605499 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.607342 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.607438 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.607728 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.607810 140528121659392 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:41:50.607918 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.607959 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.607990 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.608053 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.610285 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.615676 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.615934 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.618613 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.630818 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.630874 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.630910 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.630942 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.631004 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.631557 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.631632 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.631987 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.632659 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.635611 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.636230 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.636310 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.636345 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.636406 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.636534 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.636644 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.636682 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.638526 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.638621 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.641011 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.641088 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.641196 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.643423 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.645242 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.645338 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.645626 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.645715 140528121659392 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:41:50.645823 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.645862 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.645892 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.645955 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.648166 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.653466 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.653733 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.656380 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.668717 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.668774 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.668811 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.668859 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.668922 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.669477 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.669552 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.669914 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.670598 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.673121 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.673748 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.673824 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.673856 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.673914 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.674042 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.674149 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.674188 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.676034 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.676126 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.678506 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.678584 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.678691 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.680954 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.682796 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.682889 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.683175 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.683254 140528121659392 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:41:50.683362 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.683399 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.683428 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.683489 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.685716 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.691096 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.691351 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.694034 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.706420 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.706472 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.706506 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.706535 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.706595 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.707147 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.707221 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.707574 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.708258 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.710817 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.711433 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.711509 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.711542 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.711600 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.711728 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.711840 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.711877 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.713727 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.713826 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.716216 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.716295 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.716404 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.718668 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.720519 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.720612 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.720899 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.720980 140528121659392 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:41:50.721086 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.721124 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.721153 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.721215 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.723475 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.728829 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.729083 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.731765 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.744141 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.744194 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.744229 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.744257 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.744321 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.744871 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.744946 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.745297 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.745995 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.748933 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.749553 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.749629 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.749670 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.749730 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.749858 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.749966 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.750003 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.751846 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.751942 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.754332 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.754410 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.754518 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.756763 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.758630 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.758723 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.759010 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.759088 140528121659392 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:41:50.759195 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.759232 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.759261 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.759323 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.761535 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.766889 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.767141 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.769928 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.782360 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.782414 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.782448 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.782477 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.782537 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.783087 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.783161 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.783519 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.784203 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.786751 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.787375 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.787451 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.787483 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.787540 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.787664 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.787771 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.787808 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.789657 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.789748 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.792124 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.792201 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.792309 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.794580 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.796417 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.796510 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.796796 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.796876 140528121659392 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:41:50.796983 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.797021 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.797051 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.797114 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.799328 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.804690 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.804947 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.807637 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.819980 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.820035 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.820069 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.820099 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.820158 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.820714 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.820789 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.821147 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.821831 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.824357 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.824976 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.825052 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.825085 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.825144 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.825271 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.825379 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.825416 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.827276 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.827368 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.829755 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.829838 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.829947 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.832201 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.834055 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.834150 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.834436 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.834515 140528121659392 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:41:50.834621 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.834659 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.834688 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.834748 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.836957 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.842329 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.842588 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.845277 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.857676 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.857730 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.857764 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.857792 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.857852 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.858407 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.858482 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.858838 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.859524 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.862440 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.863057 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.863134 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.863166 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.863224 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.863353 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.863462 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.863499 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.865337 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.865428 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.867817 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.867899 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.868008 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.870269 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.872102 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.872196 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.872482 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.872562 140528121659392 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:41:50.872669 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.872707 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.872736 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.872797 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.875039 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.880568 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.880822 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.883502 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.895861 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.895915 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.895949 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.895977 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.896037 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.896592 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.896673 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.897028 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.897719 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.900243 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.900853 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.900928 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.900962 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.901020 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.901148 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.901260 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.901299 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.903604 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.903697 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.906068 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.906147 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.906262 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.908478 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.910284 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.910377 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.910661 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.910740 140528121659392 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:41:50.910845 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.910883 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.910912 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.910974 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.913181 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.918523 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.918775 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.921431 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.933744 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.933798 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.933831 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.933860 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.933920 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.934471 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.934545 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.934897 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.935575 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.938113 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.938726 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.938801 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.938834 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.938892 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.939019 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.939126 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.939163 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.941018 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.941110 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.943512 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.943591 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.943699 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.945960 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.947795 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.947888 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.948172 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.948250 140528121659392 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:41:50.948357 140528121659392 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:41:50.948394 140528121659392 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:41:50.948423 140528121659392 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:41:50.948485 140528121659392 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.950719 140528121659392 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:41:50.956051 140528121659392 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.956305 140528121659392 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:41:50.958989 140528121659392 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:41:50.971323 140528121659392 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:41:50.971377 140528121659392 attention.py:418] Single window, no scan.
I0123 11:41:50.971411 140528121659392 transformer_layer.py:389] tlayer: self-attention.
I0123 11:41:50.971440 140528121659392 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.971500 140528121659392 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.972052 140528121659392 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.972126 140528121659392 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.972479 140528121659392 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.973162 140528121659392 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.976052 140528121659392 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.976673 140528121659392 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.976750 140528121659392 transformer_layer.py:468] tlayer: End windows.
I0123 11:41:50.976784 140528121659392 transformer_layer.py:472] tlayer: final FFN.
I0123 11:41:50.976840 140528121659392 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.976970 140528121659392 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:41:50.977079 140528121659392 nn_components.py:325] mlp: activation = None
I0123 11:41:50.977115 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.978962 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.979053 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.981696 140528121659392 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.981771 140528121659392 transformer_base.py:443] tbase: final FFN
I0123 11:41:50.981877 140528121659392 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:41:50.984124 140528121659392 nn_components.py:329] mlp: final activation = None
I0123 11:41:50.985971 140528121659392 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.986064 140528121659392 nn_components.py:261] mlp: residual
I0123 11:41:50.986348 140528121659392 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:50.986432 140528121659392 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:41:50.989227 140528121659392 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:41:55.394967 140528121659392 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:41:55.933183 140528121659392 training_loop.py:409] No working directory specified.
I0123 11:41:55.933292 140528121659392 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:41:55.934032 140528121659392 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:41:58.866490 140528121659392 training_loop.py:447] Only restoring trainable parameters.
I0123 11:41:58.867183 140528121659392 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:41:58.867242 140528121659392 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.867287 140528121659392 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.867329 140528121659392 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.867369 140528121659392 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.867407 140528121659392 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.867445 140528121659392 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.867483 140528121659392 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.867520 140528121659392 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.867557 140528121659392 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.867594 140528121659392 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.867632 140528121659392 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.867669 140528121659392 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.867705 140528121659392 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.867741 140528121659392 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.867778 140528121659392 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.867815 140528121659392 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.867850 140528121659392 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.867886 140528121659392 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.867920 140528121659392 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.867969 140528121659392 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868006 140528121659392 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.868042 140528121659392 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.868078 140528121659392 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.868114 140528121659392 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868151 140528121659392 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.868186 140528121659392 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868222 140528121659392 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868258 140528121659392 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.868294 140528121659392 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.868330 140528121659392 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868365 140528121659392 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.868401 140528121659392 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.868437 140528121659392 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.868472 140528121659392 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868507 140528121659392 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.868542 140528121659392 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868577 140528121659392 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868613 140528121659392 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.868648 140528121659392 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.868684 140528121659392 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868720 140528121659392 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.868756 140528121659392 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.868790 140528121659392 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.868825 140528121659392 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868860 140528121659392 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.868901 140528121659392 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868937 140528121659392 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.868973 140528121659392 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.869008 140528121659392 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.869043 140528121659392 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869077 140528121659392 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.869112 140528121659392 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.869148 140528121659392 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.869183 140528121659392 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869219 140528121659392 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.869254 140528121659392 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869289 140528121659392 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869324 140528121659392 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.869360 140528121659392 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.869394 140528121659392 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869429 140528121659392 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.869465 140528121659392 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.869500 140528121659392 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.869535 140528121659392 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869571 140528121659392 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.869606 140528121659392 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869648 140528121659392 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869688 140528121659392 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.869724 140528121659392 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.869759 140528121659392 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869795 140528121659392 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.869830 140528121659392 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.869871 140528121659392 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.869907 140528121659392 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.869944 140528121659392 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.869979 140528121659392 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870014 140528121659392 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870050 140528121659392 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.870085 140528121659392 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.870120 140528121659392 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870156 140528121659392 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.870190 140528121659392 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.870226 140528121659392 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.870260 140528121659392 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870295 140528121659392 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.870330 140528121659392 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870365 140528121659392 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870399 140528121659392 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.870434 140528121659392 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.870469 140528121659392 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870503 140528121659392 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.870538 140528121659392 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.870572 140528121659392 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.870607 140528121659392 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870641 140528121659392 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.870676 140528121659392 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870711 140528121659392 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870745 140528121659392 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.870780 140528121659392 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.870821 140528121659392 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870858 140528121659392 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.870894 140528121659392 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.870929 140528121659392 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.870964 140528121659392 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.870998 140528121659392 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.871033 140528121659392 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.871068 140528121659392 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.871103 140528121659392 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.871137 140528121659392 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.871172 140528121659392 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.871207 140528121659392 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.871242 140528121659392 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:41:58.871277 140528121659392 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:41:58.871312 140528121659392 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.871347 140528121659392 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.871381 140528121659392 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.871416 140528121659392 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.871450 140528121659392 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:41:58.871485 140528121659392 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:41:58.871520 140528121659392 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:41:58.871554 140528121659392 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:41:58.871582 140528121659392 training_loop.py:725] Total parameters: 152072288
I0123 11:41:58.871787 140528121659392 training_loop.py:739] Total state size: 0
I0123 11:41:58.892377 140528121659392 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:41:58.892613 140528121659392 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:41:58.892972 140528121659392 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:41:58.893286 140528121659392 training_loop.py:89] registering functions: dict_keys([])
I0123 11:41:58.909821 140528121659392 graph.py:499] a b c = triangle a b c; d = circle d a c b; e = on_circle e d c; f = midpoint f a c; g = midpoint g a b; h = on_line h a e, on_line h d f; i = on_line i a e, on_line i d g; j = on_line j c h, on_line j b i; k = on_circle k d c, on_line k j c; l = on_circle l d b, on_line l j b ? cong a e c k
I0123 11:42:01.281840 140528121659392 ddar.py:60] Depth 1/1000 time = 2.333078622817993
I0123 11:42:07.266593 140528121659392 ddar.py:60] Depth 2/1000 time = 5.984565258026123
I0123 11:42:15.549592 140528121659392 ddar.py:60] Depth 3/1000 time = 8.282799482345581
I0123 11:42:15.552425 140528121659392 alphageometry.py:191] 
==========================
 * From theorem premises:
A C D E F H J K : Points
DA = DC [00]
DE = DC [01]
FA = FC [02]
A,C,F are collinear [03]
D,F,H are collinear [04]
A,E,H are collinear [05]
J,C,H are collinear [06]
C,J,K are collinear [07]
DK = DC [08]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. FA = FC [02] & DA = DC [00] ⇒  FA:FC = DA:DC [09]
002. FA:FC = DA:DC [09] & A,C,F are collinear [03] ⇒  ∠ADF = ∠FDC [10]
003. D,F,H are collinear [04] & ∠ADF = ∠FDC [10] ⇒  ∠ADH = ∠HDC [11]
004. DA = DC [00] & ∠ADH = ∠HDC [11] (SAS)⇒  ∠DAH = ∠HCD [12]
005. DK = DC [08] ⇒  ∠DKC = ∠KCD [13]
006. C,J,K are collinear [07] & J,C,H are collinear [06] & ∠DAH = ∠HCD [12] & A,E,H are collinear [05] & ∠DKC = ∠KCD [13] ⇒  ∠DKC = ∠DAE [14]
007. DE = DC [01] & DA = DC [00] ⇒  DA = DE [15]
008. DA = DE [15] ⇒  ∠EAD = ∠DEA [16]
009. C,J,K are collinear [07] & J,C,H are collinear [06] & ∠EAD = ∠DEA [16] & ∠DAH = ∠HCD [12] & A,E,H are collinear [05] ⇒  ∠DCK = ∠DEA [17]
010. ∠DKC = ∠DAE [14] & ∠DCK = ∠DEA [17] (Similar Triangles)⇒  KD:AD = KC:AE [18]
011. KD:AD = KC:AE [18] & DK = DC [08] & DA = DC [00] ⇒  KC = AE
==========================

