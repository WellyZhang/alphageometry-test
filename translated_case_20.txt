I0123 12:29:38.170024 140427613868032 inference_utils.py:69] Parsing gin configuration.
I0123 12:29:38.170136 140427613868032 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:29:38.170365 140427613868032 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:29:38.170400 140427613868032 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:29:38.170430 140427613868032 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:29:38.170458 140427613868032 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:29:38.170486 140427613868032 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:29:38.170512 140427613868032 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:29:38.170538 140427613868032 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:29:38.170564 140427613868032 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:29:38.170590 140427613868032 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:29:38.170616 140427613868032 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:29:38.170663 140427613868032 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:29:38.170805 140427613868032 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:29:38.171028 140427613868032 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:29:38.171136 140427613868032 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:29:38.177568 140427613868032 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:29:38.177701 140427613868032 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:29:38.178028 140427613868032 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:29:38.178132 140427613868032 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:29:38.178412 140427613868032 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:29:38.178513 140427613868032 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:29:38.178920 140427613868032 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:29:38.179019 140427613868032 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:29:38.182791 140427613868032 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:29:38.289736 140427613868032 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:29:38.290483 140427613868032 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:29:38.297158 140427613868032 training_loop.py:335] Process 0 of 1
I0123 12:29:38.297211 140427613868032 training_loop.py:336] Local device count = 1
I0123 12:29:38.297250 140427613868032 training_loop.py:337] Number of replicas = 1
I0123 12:29:38.297280 140427613868032 training_loop.py:339] Using random number seed 42
I0123 12:29:38.759613 140427613868032 training_loop.py:359] Initializing the model.
I0123 12:29:39.124153 140427613868032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.124445 140427613868032 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:29:39.124552 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.124631 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.124705 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.124783 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.124854 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.124923 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.124990 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.125057 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.125123 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.125190 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.125258 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.125325 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:29:39.125364 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.125408 140427613868032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:29:39.125524 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.125563 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.125594 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.127599 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.132809 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.143723 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.143999 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.148303 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.158971 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.159030 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.159069 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.159103 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.159169 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.160394 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.160472 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.161182 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.163640 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.169307 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.171030 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.171111 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.171146 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.171207 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.171336 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.171674 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.171721 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.173616 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.173720 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.176550 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.176632 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.177127 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.187114 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.195761 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.195860 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.196153 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.196234 140427613868032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:29:39.196344 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.196383 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.196415 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.198270 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.200701 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.206265 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.206529 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.209131 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.212987 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.213042 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.213077 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.213107 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.213168 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.213736 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.213816 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.214174 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.214939 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.217383 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.218009 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.218086 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.218121 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.218178 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.218303 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.218632 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.218675 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.220618 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.220710 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.223220 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.223302 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.223740 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.226061 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.227963 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.228056 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.228341 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.228422 140427613868032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:29:39.228532 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.228571 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.228601 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.230515 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.232822 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.238800 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.239058 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.241693 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.245762 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.245817 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.245852 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.245882 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.245942 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.246505 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.246580 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.246936 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.247699 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.250278 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.250944 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.251021 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.251055 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.251113 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.251242 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.251569 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.251610 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.253509 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.253600 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.256101 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.256184 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.256674 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.258936 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.260828 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.260925 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.261219 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.261299 140427613868032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:29:39.261408 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.261446 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.261476 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.263374 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.265739 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.271308 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.271566 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.274172 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.277954 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.278008 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.278045 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.278075 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.278134 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.278691 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.278770 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.279128 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.279895 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.282429 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.283056 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.283133 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.283171 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.283229 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.283359 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.283682 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.283724 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.285588 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.285688 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.288191 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.288274 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.288704 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.290930 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.292790 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.292886 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.293170 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.293250 140427613868032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:29:39.293357 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.293395 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.293426 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.295329 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.297675 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.303260 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.303518 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.306174 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.309933 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.309987 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.310021 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.310051 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.310112 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.310668 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.310743 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.311094 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.311851 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.314688 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.315313 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.315393 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.315428 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.315487 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.315628 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.315952 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.315994 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.317870 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.317965 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.320476 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.320554 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.320985 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.323241 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.325174 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.325271 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.325558 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.325639 140427613868032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:29:39.325757 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.325796 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.325826 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.327677 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.330023 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.336326 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.336640 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.339284 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.343036 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.343092 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.343127 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.343156 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.343216 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.343831 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.343909 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.344263 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.345042 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.347481 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.348098 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.348176 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.348211 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.348269 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.348395 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.348721 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.348763 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.350655 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.350747 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.353511 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.353591 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.354026 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.356330 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.358242 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.358338 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.358624 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.358706 140427613868032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:29:39.358814 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.358853 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.358883 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.360734 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.363156 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.368712 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.368978 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.371555 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.375320 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.375373 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.375407 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.375436 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.375496 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.376057 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.376135 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.376485 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.377237 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.379658 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.380279 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.380355 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.380389 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.380445 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.380570 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.380894 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.380936 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.382864 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.382956 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.385422 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.385500 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.385940 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.388551 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.390457 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.390557 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.390841 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.390924 140427613868032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:29:39.391034 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.391072 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.391103 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.529854 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.533021 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.538999 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.539315 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.542279 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.546361 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.546420 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.546459 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.546493 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.546565 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.547201 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.547279 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.547650 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.548449 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.551106 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.551756 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.551835 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.551872 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.551934 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.552070 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.552423 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.552468 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.554408 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.554502 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.557114 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.557193 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.557651 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.560023 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.561965 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.562075 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.562369 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.562452 140427613868032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:29:39.562563 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.562602 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.562634 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.564617 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.567028 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.572730 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.572999 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.575667 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.579506 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.579560 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.579596 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.579626 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.579687 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.580254 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.580329 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.580684 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.581441 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.583983 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.584613 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.584690 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.584725 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.584782 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.584911 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.585237 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.585279 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.587171 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.587265 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.589770 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.589852 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.590286 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.592550 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.594512 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.594610 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.594897 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.594986 140427613868032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:29:39.595098 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.595137 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.595167 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.596983 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.599399 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.604933 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.605192 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.608198 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.611983 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.612039 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.612075 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.612105 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.612166 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.612773 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.612851 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.613216 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.613991 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.616424 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.617050 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.617125 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.617160 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.617217 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.617346 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.617674 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.617717 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.619587 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.619678 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.622215 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.622295 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.622716 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.625002 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.626899 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.626994 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.627278 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.627363 140427613868032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:29:39.627472 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.627511 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.627542 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.629367 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.631774 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.637311 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.637569 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.640321 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.644114 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.644169 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.644204 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.644235 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.644297 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.644851 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.644927 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.645286 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.646064 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.648478 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.649104 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.649179 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.649214 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.649271 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.649395 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.649727 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.649775 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.651701 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.651793 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.654550 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.654629 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.655057 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.657379 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.659280 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.659375 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.659657 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.659737 140427613868032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:29:39.659852 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.659891 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.659921 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.661837 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.664170 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.669694 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.669958 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.672549 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:29:39.676331 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.676386 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.676421 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.676451 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.676512 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.677073 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.677150 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.677500 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.678272 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.680679 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.681657 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.681736 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.681771 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.681840 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.681970 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.682287 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.682330 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.684200 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.684292 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.686740 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.686819 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.687299 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.689502 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.691370 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.691464 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.691747 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.692028 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692097 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692161 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692217 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692269 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692321 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692371 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692421 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692471 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692523 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692574 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692624 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:29:39.692661 140427613868032 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:29:39.696146 140427613868032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:29:39.743460 140427613868032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.743544 140427613868032 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:29:39.743597 140427613868032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:29:39.743701 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.743739 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.743769 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.743832 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.746209 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.751620 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.751879 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.754509 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:39.771043 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.771100 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.771136 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.771167 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.771228 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.772355 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.772437 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.773140 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.775139 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.779812 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.781110 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.781197 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.781233 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.781291 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.781426 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.781537 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.781576 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.783472 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.783566 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.785993 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.786074 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.786186 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.788432 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.790388 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.790485 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.790770 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.790852 140427613868032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:29:39.790960 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.791000 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.791033 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.791097 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.793328 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.798783 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.799043 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.801718 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:39.815096 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.815152 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.815189 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.815221 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.815283 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.815844 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.815920 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.816282 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.816968 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.819450 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.820073 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.820151 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.820193 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.820255 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.820384 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.820493 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.820532 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.822466 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.822561 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.824962 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.825041 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.825153 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.827375 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.829293 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.829388 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.829683 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.829767 140427613868032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:29:39.829876 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.829916 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.829948 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.830013 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.832260 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.837759 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.838025 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.840717 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:39.853631 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.853701 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.853743 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.853778 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.853842 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.854413 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.854492 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.854859 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.855554 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.858033 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.858661 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.858742 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.858779 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.858846 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.858981 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.859094 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.859135 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.861167 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.861264 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.863780 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.863864 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.863974 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.866232 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.868241 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.868341 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.868640 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.868726 140427613868032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:29:39.868842 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.868884 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.868917 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.868984 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.871321 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.876806 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.877069 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.879805 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:39.893053 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.893110 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.893146 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.893178 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.893241 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.893815 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.893893 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.894260 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.894994 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.897455 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.898093 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.898175 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.898211 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.898272 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.898413 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.898526 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.898567 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.900538 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.900630 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.903071 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.903153 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.903267 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.905461 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.907381 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.907475 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.907754 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.907836 140427613868032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:29:39.907944 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.907983 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.908014 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.908077 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.910661 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.916467 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.916732 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.919427 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:39.932453 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.932509 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.932544 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.932575 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.932637 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.933207 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.933285 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.933647 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.934355 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.936898 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.937525 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.937605 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.937648 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.937712 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.937848 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.937959 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.938000 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.939934 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.940027 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.942470 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.942553 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.942668 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.944992 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.946884 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.946979 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.947265 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.947347 140427613868032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:29:39.947458 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.947497 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.947530 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.947594 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.949850 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.955323 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.955586 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:39.958282 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:39.971182 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:39.971239 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:39.971275 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:39.971307 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.971373 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.971939 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.972017 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.972375 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.973084 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.975555 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.976175 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.976253 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:39.976288 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:39.976345 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.976477 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:39.976594 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:39.976635 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.978579 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.978673 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.981077 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.981155 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:39.981266 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:39.983514 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:39.985368 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.985462 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:39.985755 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.985839 140427613868032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:29:39.985948 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:39.985987 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:39.986019 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:39.986083 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.988329 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:39.998460 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:39.998785 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.001631 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.015081 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.015139 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.015177 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.015210 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.015274 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.015869 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.015945 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.016310 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.017018 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.019529 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.020530 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.020609 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.020644 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.020703 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.020836 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.020952 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.020998 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.022998 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.023092 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.025532 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.025610 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.025728 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.028009 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.029950 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.030046 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.030333 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.030419 140427613868032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:29:40.030535 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.030578 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.030611 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.030674 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.032941 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.038440 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.038714 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.041410 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.054411 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.054468 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.054504 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.054536 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.054597 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.055211 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.055288 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.055647 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.056349 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.058864 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.059498 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.059576 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.059611 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.059670 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.059800 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.059911 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.059957 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.061856 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.061949 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.064411 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.064490 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.064599 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.066845 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.068796 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.068895 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.069192 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.069274 140427613868032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:29:40.069386 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.069426 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.069458 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.069522 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.071793 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.077380 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.077649 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.080285 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.093351 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.093408 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.093444 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.093475 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.093540 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.094113 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.094190 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.094550 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.095243 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.097729 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.098401 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.098478 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.098514 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.098573 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.098704 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.098814 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.098853 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.100763 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.100856 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.103272 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.103353 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.103463 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.105729 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.107678 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.107774 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.108059 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.108143 140427613868032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:29:40.108254 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.108293 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.108325 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.108390 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.110641 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.116138 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.116402 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.119078 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.132294 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.132349 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.132390 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.132424 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.132486 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.133093 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.133169 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.133526 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.134228 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.136707 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.137335 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.137412 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.137447 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.137507 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.137646 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.137760 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.137800 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.139706 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.139806 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.142307 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.142389 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.142500 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.144764 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.146660 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.146756 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.147044 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.147127 140427613868032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:29:40.147237 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.147277 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.147309 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.147373 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.149649 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.155252 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.155520 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.158202 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.171224 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.171279 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.171316 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.171348 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.171409 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.171968 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.172045 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.172402 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.173099 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.175592 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.176254 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.176332 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.176368 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.176428 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.176571 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.176683 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.176723 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.178651 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.178748 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.181185 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.181265 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.181375 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.183615 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.185562 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.185663 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.185950 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.186033 140427613868032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:29:40.186145 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.186185 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.186218 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.186281 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.188522 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.194071 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.194336 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.197046 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.210141 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.210198 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.210235 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.210266 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.210328 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.210895 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.210971 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.211325 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.212077 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.214587 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.215228 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.215307 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.215344 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.215404 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.215535 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.215647 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.215686 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.217594 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.217696 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.220129 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.220209 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.220319 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.222631 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.224504 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.224599 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.224885 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.224975 140427613868032 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:29:40.227873 140427613868032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:29:40.283054 140427613868032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.283141 140427613868032 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:29:40.283195 140427613868032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:29:40.283300 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.283338 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.283368 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.283431 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.286045 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.291369 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.291627 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.294164 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.306428 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.306483 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.306517 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.306548 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.306609 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.307162 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.307238 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.307595 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.308261 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.310834 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.311456 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.311533 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.311568 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.311629 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.311763 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.311879 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.311918 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.313767 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.313860 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.316237 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.316316 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.316426 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.318665 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.320511 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.320606 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.320888 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.320969 140427613868032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:29:40.321077 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.321117 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.321150 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.321214 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.323473 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.328822 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.329081 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.331722 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.344146 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.344202 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.344238 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.344269 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.344330 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.344889 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.344965 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.345319 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.346004 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.348495 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.349111 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.349189 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.349225 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.349286 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.349415 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.349524 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.349569 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.351423 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.351515 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.353887 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.353967 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.354078 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.356313 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.358162 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.358256 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.358537 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.358619 140427613868032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:29:40.358726 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.358766 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.358798 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.358860 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.361056 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.366380 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.366640 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.369251 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.381589 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.381650 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.381688 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.381720 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.381782 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.382337 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.382413 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.382769 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.383438 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.385916 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.386527 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.386603 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.386638 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.386698 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.386829 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.386937 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.386976 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.388808 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.388901 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.391266 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.391345 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.391454 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.394127 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.395972 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.396067 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.396350 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.396432 140427613868032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:29:40.396540 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.396579 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.396612 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.396674 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.398900 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.404266 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.404531 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.407190 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.419885 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.419941 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.419985 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.420026 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.420089 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.420661 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.420736 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.421090 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.421788 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.424288 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.424909 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.424984 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.425019 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.425079 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.425207 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.425315 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.425355 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.427236 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.427327 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.429741 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.429818 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.429926 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.432219 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.434088 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.434182 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.434462 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.434541 140427613868032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:29:40.434648 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.434687 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.434718 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.434782 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.436990 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.442455 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.442709 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.445400 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.458088 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.458142 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.458177 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.458206 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.458265 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.458814 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.458887 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.459239 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.459920 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.462425 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.463049 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.463125 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.463160 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.463218 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.463346 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.463454 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.463491 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.465357 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.465454 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.467858 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.467936 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.468044 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.470334 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.472199 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.472295 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.472580 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.472661 140427613868032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:29:40.472769 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.472806 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.472837 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.472900 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.475141 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.480538 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.480795 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.483481 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.496201 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.496255 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.496289 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.496320 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.496381 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.496934 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.497008 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.497359 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.498053 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.500560 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.501175 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.501249 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.501283 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.501339 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.501465 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.501572 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.501609 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.503463 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.503559 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.505939 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.506016 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.506123 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.508804 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.510675 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.510769 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.511051 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.511131 140427613868032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:29:40.511238 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.511276 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.511306 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.511368 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.513586 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.519127 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.519387 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.522050 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.534931 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.534985 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.535020 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.535051 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.535111 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.535673 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.535747 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.536105 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.536782 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.539294 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.539929 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.540006 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.540040 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.540100 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.540228 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.540336 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.540377 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.542257 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.542350 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.544728 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.544810 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.544922 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.547206 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.549058 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.549152 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.549436 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.549516 140427613868032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:29:40.549622 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.549666 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.549696 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.549759 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.551975 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.557390 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.557658 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.560338 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.573147 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.573201 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.573235 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.573265 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.573326 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.573894 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.573969 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.574326 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.575013 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.577527 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.578159 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.578236 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.578270 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.578328 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.578457 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.578566 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.578605 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.580464 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.580555 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.582946 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.583028 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.583137 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.585413 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.587280 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.587373 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.587655 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.587735 140427613868032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:29:40.587843 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.587882 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.587913 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.587976 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.590199 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.595597 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.595858 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.598530 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.611168 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.611222 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.611257 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.611287 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.611348 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.611905 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.611979 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.612330 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.613008 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.615543 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.616161 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.616237 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.616271 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.616328 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.616453 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.616560 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.616598 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.618473 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.618564 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.620932 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.621012 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.621119 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.623772 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.625634 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.625734 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.626029 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.626108 140427613868032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:29:40.626219 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.626258 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.626289 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.626352 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.628587 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.634158 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.634416 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.637067 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.649746 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.649799 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.649833 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.649864 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.649924 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.650488 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.650563 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.650913 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.651598 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.654093 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.654708 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.654783 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.654817 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.654874 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.655002 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.655109 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.655146 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.657504 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.657596 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.659984 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.660062 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.660176 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.662408 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.664241 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.664333 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.664612 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.664691 140427613868032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:29:40.664798 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.664836 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.664866 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.664928 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.667153 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.672557 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.672814 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.675485 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.688049 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.688103 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.688137 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.688168 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.688229 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.688783 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.688857 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.689207 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.689897 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.692391 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.693014 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.693089 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.693123 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.693179 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.693304 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.693409 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.693447 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.695314 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.695406 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.697793 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.697870 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.697977 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.700237 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.702087 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.702181 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.702460 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.702539 140427613868032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:29:40.702645 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:29:40.702683 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:29:40.702713 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:29:40.702773 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.704980 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:29:40.710364 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.710623 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:29:40.713289 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:29:40.725794 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:29:40.725847 140427613868032 attention.py:418] Single window, no scan.
I0123 12:29:40.725880 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:29:40.725909 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.725969 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.726516 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.726591 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.726939 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.727612 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.730062 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.730677 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.730755 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:29:40.730788 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:29:40.730844 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.730968 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:29:40.731074 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:29:40.731111 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.733106 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.733197 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.735591 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.735668 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:29:40.735774 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:29:40.738584 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:29:40.740444 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.740536 140427613868032 nn_components.py:261] mlp: residual
I0123 12:29:40.740818 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:40.740901 140427613868032 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:29:40.743727 140427613868032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:29:45.137516 140427613868032 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:29:45.620967 140427613868032 training_loop.py:409] No working directory specified.
I0123 12:29:45.621084 140427613868032 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:29:45.621863 140427613868032 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:29:48.924087 140427613868032 training_loop.py:447] Only restoring trainable parameters.
I0123 12:29:48.924718 140427613868032 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:29:48.924800 140427613868032 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.924851 140427613868032 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.924895 140427613868032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.924937 140427613868032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.924976 140427613868032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.925014 140427613868032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925053 140427613868032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925092 140427613868032 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.925130 140427613868032 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.925168 140427613868032 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925206 140427613868032 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.925244 140427613868032 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.925282 140427613868032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.925318 140427613868032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925355 140427613868032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.925392 140427613868032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925428 140427613868032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925464 140427613868032 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.925500 140427613868032 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.925548 140427613868032 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925585 140427613868032 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.925621 140427613868032 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.925664 140427613868032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.925702 140427613868032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925741 140427613868032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.925776 140427613868032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925813 140427613868032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925849 140427613868032 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.925884 140427613868032 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.925922 140427613868032 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.925959 140427613868032 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.925996 140427613868032 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.926033 140427613868032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.926069 140427613868032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926105 140427613868032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.926141 140427613868032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926177 140427613868032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926213 140427613868032 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.926248 140427613868032 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.926285 140427613868032 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926321 140427613868032 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.926356 140427613868032 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.926391 140427613868032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.926426 140427613868032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926460 140427613868032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.926501 140427613868032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926542 140427613868032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926577 140427613868032 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.926611 140427613868032 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.926646 140427613868032 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926680 140427613868032 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.926714 140427613868032 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.926749 140427613868032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.926784 140427613868032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926820 140427613868032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.926858 140427613868032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926893 140427613868032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.926927 140427613868032 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.926962 140427613868032 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.926996 140427613868032 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927031 140427613868032 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.927065 140427613868032 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.927098 140427613868032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.927132 140427613868032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927166 140427613868032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.927200 140427613868032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927233 140427613868032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927267 140427613868032 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.927301 140427613868032 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.927335 140427613868032 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927370 140427613868032 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.927412 140427613868032 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.927453 140427613868032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.927489 140427613868032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927524 140427613868032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.927559 140427613868032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927593 140427613868032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927628 140427613868032 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.927662 140427613868032 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.927697 140427613868032 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927732 140427613868032 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.927767 140427613868032 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.927801 140427613868032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.927835 140427613868032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927869 140427613868032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.927903 140427613868032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927937 140427613868032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.927971 140427613868032 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.928005 140427613868032 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.928039 140427613868032 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928073 140427613868032 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.928107 140427613868032 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.928141 140427613868032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.928175 140427613868032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928209 140427613868032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.928242 140427613868032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928275 140427613868032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928308 140427613868032 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.928342 140427613868032 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.928380 140427613868032 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928416 140427613868032 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.928450 140427613868032 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.928484 140427613868032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.928517 140427613868032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928551 140427613868032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.928584 140427613868032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928617 140427613868032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928651 140427613868032 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.928684 140427613868032 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.928718 140427613868032 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928751 140427613868032 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.928784 140427613868032 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:29:48.928817 140427613868032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:29:48.928851 140427613868032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928885 140427613868032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.928919 140427613868032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928953 140427613868032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.928987 140427613868032 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:29:48.929021 140427613868032 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:29:48.929055 140427613868032 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:29:48.929089 140427613868032 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:29:48.929116 140427613868032 training_loop.py:725] Total parameters: 152072288
I0123 12:29:48.929343 140427613868032 training_loop.py:739] Total state size: 0
I0123 12:29:48.954640 140427613868032 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:29:48.954874 140427613868032 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:29:48.955261 140427613868032 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:29:48.955597 140427613868032 training_loop.py:89] registering functions: dict_keys([])
I0123 12:29:48.971977 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l ? perp m c c a
I0123 12:29:51.001748 140427613868032 ddar.py:60] Depth 1/1000 time = 1.9743318557739258
I0123 12:29:54.618128 140427613868032 ddar.py:60] Depth 2/1000 time = 3.6162118911743164
I0123 12:29:58.964587 140427613868032 ddar.py:60] Depth 3/1000 time = 4.346274137496948
I0123 12:30:03.940550 140427613868032 ddar.py:60] Depth 4/1000 time = 4.975720643997192
I0123 12:30:08.787595 140427613868032 ddar.py:60] Depth 5/1000 time = 4.8468592166900635
I0123 12:30:13.812711 140427613868032 ddar.py:60] Depth 6/1000 time = 5.024879455566406
I0123 12:30:19.291487 140427613868032 ddar.py:60] Depth 7/1000 time = 5.473978519439697
I0123 12:30:29.094047 140427613868032 ddar.py:60] Depth 8/1000 time = 9.80229377746582
I0123 12:30:37.639766 140427613868032 ddar.py:60] Depth 9/1000 time = 8.545405387878418
I0123 12:30:46.018909 140427613868032 ddar.py:60] Depth 10/1000 time = 8.378880500793457
I0123 12:30:54.572242 140427613868032 ddar.py:60] Depth 11/1000 time = 8.395971775054932
I0123 12:30:54.594027 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:30:54.594132 140427613868032 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 12:30:54.594167 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00
I0123 12:30:54.594198 140427613868032 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00
I0123 12:30:54.734997 140427613868032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.735189 140427613868032 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:30:54.735290 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735366 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735438 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735505 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735573 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735641 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735708 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735774 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735840 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735907 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.735974 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.736040 140427613868032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:30:54.736082 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:54.736137 140427613868032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:30:54.736251 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:54.736291 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:54.736322 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:54.738249 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.740781 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:54.746650 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.746927 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:54.749541 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:54.753585 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:54.753649 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:54.753689 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:54.753726 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.753788 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.754429 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.754504 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.754870 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.755649 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.758182 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.758815 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.758892 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:54.758925 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:54.758987 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.759116 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:54.759442 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:54.759485 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.761468 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.761559 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.764027 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.764105 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:54.764543 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:54.766910 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.768854 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.768948 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.769235 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.769315 140427613868032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:30:54.769423 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:54.769467 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:54.769498 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:54.771787 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.774107 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:54.779891 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.780151 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:54.782791 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:54.786544 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:54.786598 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:54.786633 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:54.786664 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.786727 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.787292 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.787368 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.787726 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.788499 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.790968 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.791649 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.791726 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:54.791760 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:54.791819 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.791945 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:54.792263 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:54.792304 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.794243 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.794336 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.796785 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.796863 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:54.797292 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:54.799662 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.801586 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.801685 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.801977 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.802058 140427613868032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:30:54.802171 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:54.802209 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:54.802245 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:54.804053 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.806422 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:54.812314 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.812577 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:54.815210 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:54.818972 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:54.819026 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:54.819061 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:54.819091 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.819153 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.819777 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.819853 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.820210 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.820973 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.823423 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.824042 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.824118 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:54.824153 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:54.824211 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.824342 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:54.824663 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:54.824704 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.826683 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.826774 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.829211 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.829290 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:54.829732 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:54.832002 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.833930 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.834023 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.834313 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.834393 140427613868032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:30:54.834500 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:54.834537 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:54.834568 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:54.836447 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.838768 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:54.844404 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.844664 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:54.847275 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:54.851041 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:54.851095 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:54.851130 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:54.851161 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.851223 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.851784 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.851860 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.852220 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.852996 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.855444 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.856063 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.856140 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:54.856173 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:54.856232 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.856360 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:54.856731 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:54.856773 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.858704 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.858797 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.861255 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.861333 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:54.861773 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:54.864102 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.866123 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.866217 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.866508 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.866588 140427613868032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:30:54.866697 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:54.866735 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:54.866765 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:54.868573 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.870969 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:54.876675 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.876933 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:54.879529 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:54.883275 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:54.883328 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:54.883364 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:54.883395 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.883458 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.884451 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.884531 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.884907 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.885710 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.888268 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.888901 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.888978 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:54.889013 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:54.889072 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.889207 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:54.889538 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:54.889580 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.891642 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.891735 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.894232 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.894315 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:54.894764 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:54.897077 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.899060 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.899157 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.899458 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.899543 140427613868032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:30:54.899653 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:54.899691 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:54.899721 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:54.901612 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.904023 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:54.909684 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.909947 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:54.912611 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:54.916317 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:54.916371 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:54.916406 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:54.916436 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.916497 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.917053 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.917129 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.917486 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.918279 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.920813 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.921494 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.921572 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:54.921607 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:54.921674 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.921826 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:54.922145 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:54.922188 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.924127 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.924219 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.926712 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.926867 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:54.927304 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:54.929652 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.931649 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.931742 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.932031 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.932112 140427613868032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:30:54.932218 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:54.932256 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:54.932286 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:54.934103 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.936464 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:54.942324 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.942592 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:54.945197 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:54.948946 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:54.949000 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:54.949034 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:54.949065 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.949178 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.949749 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.949825 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.950194 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.950990 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.953464 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.954101 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.954182 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:54.954217 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:54.954278 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.954411 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:54.954741 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:54.954784 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.956794 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.956886 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.959419 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.959500 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:54.959925 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:54.962214 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.964199 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.964292 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.964581 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.964661 140427613868032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:30:54.964769 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:54.964807 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:54.964838 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:54.966737 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.969085 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:54.974894 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.975172 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:54.977838 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:54.981617 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:54.981679 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:54.981715 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:54.981746 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.981810 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.982389 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.982469 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.982838 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.983628 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.986132 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.986840 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.986922 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:54.986957 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:54.987018 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.987154 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:54.987485 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:54.987527 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.989469 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.989562 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.992104 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.992184 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:54.992615 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:54.995389 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:54.997333 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.997427 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:54.997725 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:54.997807 140427613868032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:30:54.997915 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:54.997953 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:54.997984 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:54.999782 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.002123 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.007832 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.008089 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.010659 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:55.014336 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.014389 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.014423 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.014454 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.014516 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.015137 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.015213 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.015576 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.016349 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.018820 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.019454 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.019531 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.019565 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.019624 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.019753 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.020072 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.020114 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.022036 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.022128 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.024670 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.024748 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.025185 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.027466 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.029380 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.029474 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.029770 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.029852 140427613868032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:30:55.029966 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.030005 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.030035 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.031827 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.034248 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.039860 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.040121 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.042709 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:55.046423 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.046478 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.046513 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.046543 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.046660 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.047222 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.047297 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.047653 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.048424 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.050881 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.051505 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.051582 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.051617 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.051676 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.051805 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.052129 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.052170 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.054098 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.054188 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.056715 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.056794 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.057225 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.059503 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.061432 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.061527 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.061822 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.061904 140427613868032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:30:55.062013 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.062052 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.062084 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.063895 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.066307 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.071906 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.072166 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.074755 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:55.078511 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.078569 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.078605 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.078635 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.078749 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.079319 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.079396 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.079760 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.080547 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.083045 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.083679 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.083756 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.083791 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.083850 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.083979 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.084300 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.084342 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.086307 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.086403 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.088932 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.089010 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.089437 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.091718 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.093647 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.093742 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.094033 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.094115 140427613868032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:30:55.094223 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.094261 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.094291 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.096102 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.098526 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.104147 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.104406 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.106988 140427613868032 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:30:55.110701 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.110755 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.110795 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.110827 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.110944 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.111512 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.111589 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.111948 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.112723 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.115183 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.115804 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.115880 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.115914 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.115971 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.116099 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.116418 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.116459 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.118393 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.118486 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.121472 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.121551 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.121997 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.124271 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.126203 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.126297 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.126589 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.126834 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.126901 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.126958 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127012 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127063 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127115 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127167 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127219 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127270 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127321 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127379 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127433 140427613868032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:30:55.127469 140427613868032 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:30:55.130399 140427613868032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:30:55.175426 140427613868032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.175510 140427613868032 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:30:55.175562 140427613868032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:30:55.175668 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.175705 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.175735 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.175796 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.178173 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.183602 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.183865 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.186454 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.199869 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.199922 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.199958 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.199989 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.200050 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.200610 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.200686 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.201047 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.201751 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.204304 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.204926 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.205002 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.205036 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.205094 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.205222 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.205329 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.205367 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.207229 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.207322 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.209735 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.209819 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.209931 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.212198 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.214084 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.214178 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.214468 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.214550 140427613868032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:30:55.214658 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.214696 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.214726 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.214788 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.217040 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.222559 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.222825 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.225503 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.238173 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.238228 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.238263 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.238293 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.238355 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.238917 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.238992 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.239349 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.240096 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.242554 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.243180 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.243257 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.243292 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.243352 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.243484 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.243593 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.243631 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.245492 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.245583 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.248159 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.248239 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.248354 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.250643 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.252653 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.252746 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.253035 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.253116 140427613868032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:30:55.253223 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.253262 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.253293 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.253355 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.255605 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.261004 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.261266 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.263959 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.276586 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.276640 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.276675 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.276706 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.276770 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.277330 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.277406 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.277767 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.278504 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.280935 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.281557 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.281634 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.281673 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.281733 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.281863 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.281970 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.282009 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.283875 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.283968 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.286388 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.286467 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.286577 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.288845 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.290726 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.290822 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.291112 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.291193 140427613868032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:30:55.291301 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.291338 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.291369 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.291431 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.293659 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.299104 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.299367 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.302488 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.315152 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.315206 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.315241 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.315272 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.315334 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.315890 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.315964 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.316316 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.317056 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.319523 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.320136 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.320212 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.320246 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.320304 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.320433 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.320541 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.320579 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.322526 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.322619 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.325045 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.325124 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.325233 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.327540 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.329418 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.329511 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.329814 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.329897 140427613868032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:30:55.330006 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.330044 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.330074 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.330137 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.332387 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.337900 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.338173 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.340916 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.353771 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.353825 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.353861 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.353892 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.353954 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.354514 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.354589 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.354949 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.355696 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.358162 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.358783 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.358863 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.358898 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.358957 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.359087 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.359199 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.359237 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.361105 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.361197 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.363612 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.363691 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.363801 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.366079 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.367949 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.368043 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.368329 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.368409 140427613868032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:30:55.368518 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.368556 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.368586 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.368649 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.370898 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.376311 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.376575 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.379237 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.391848 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.391902 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.391937 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.391969 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.392030 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.392596 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.392670 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.393024 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.393772 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.396227 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.396865 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.396944 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.396979 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.397040 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.397175 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.397287 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.397327 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.399236 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.399327 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.401735 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.401813 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.401921 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.404167 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.406029 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.406129 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.406419 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.406500 140427613868032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:30:55.406610 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.406648 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.406679 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.406742 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.408983 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.414480 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.414747 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.417832 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.430500 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.430555 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.430591 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.430622 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.430683 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.431251 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.431328 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.431689 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.432441 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.434941 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.435567 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.435642 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.435676 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.435734 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.435863 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.435971 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.436009 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.437903 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.437997 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.440430 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.440508 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.440618 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.442923 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.444783 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.444883 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.445176 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.445258 140427613868032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:30:55.445368 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.445407 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.445438 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.445501 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.447747 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.453171 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.453435 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.456134 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.468897 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.468950 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.468984 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.469015 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.469076 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.469631 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.469716 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.470078 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.470824 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.473334 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.473994 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.474075 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.474111 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.474172 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.474306 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.474420 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.474460 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.476322 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.476413 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.478816 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.478894 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.479002 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.481269 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.483150 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.483245 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.483543 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.483625 140427613868032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:30:55.483733 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.483771 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.483801 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.483863 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.486122 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.491760 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.492021 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.494712 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.507364 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.507419 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.507454 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.507486 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.507550 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.508112 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.508188 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.508545 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.509238 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.511762 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.512389 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.512466 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.512501 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.512558 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.512687 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.512797 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.512835 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.514720 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.514814 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.517232 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.517310 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.517419 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.519708 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.521610 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.521715 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.522010 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.522099 140427613868032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:30:55.522210 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.522248 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.522279 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.522342 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.524588 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.530048 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.530309 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.533393 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.546173 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.546231 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.546266 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.546297 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.546359 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.546917 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.546992 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.547350 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.548038 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.550757 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.551370 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.551446 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.551480 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.551539 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.551669 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.551779 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.551817 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.553674 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.553768 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.556174 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.556252 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.556361 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.558642 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.560533 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.560626 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.560921 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.561009 140427613868032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:30:55.561120 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.561160 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.561191 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.561255 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.563539 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.569047 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.569311 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.571987 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.584586 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.584639 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.584674 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.584705 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.584766 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.585325 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.585401 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.585765 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.586465 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.588987 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.589614 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.589697 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.589732 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.589790 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.589919 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.590026 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.590064 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.591929 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.592020 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.594460 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.594540 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.594651 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.596931 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.598811 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.598906 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.599195 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.599277 140427613868032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:30:55.599392 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.599431 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.599462 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.599526 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.601917 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.607357 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.607619 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.610292 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.622878 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.622932 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.622967 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.622998 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.623059 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.623619 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.623693 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.624051 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.624743 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.627272 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.627894 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.627970 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.628006 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.628065 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.628195 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.628305 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.628344 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.630216 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.630309 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.632732 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.632810 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.632920 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.635205 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.637076 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.637168 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.637459 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.637545 140427613868032 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:30:55.640404 140427613868032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:30:55.690822 140427613868032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.690907 140427613868032 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:30:55.690961 140427613868032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:30:55.691066 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.691104 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.691134 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.691197 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.693500 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.699021 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.699282 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.701879 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.715035 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.715090 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.715124 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.715155 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.715216 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.715777 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.715854 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.716214 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.716904 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.719344 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.719963 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.720039 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.720073 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.720132 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.720263 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.720371 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.720408 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.722433 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.722528 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.724930 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.725008 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.725117 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.727349 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.729214 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.729315 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.729610 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.729702 140427613868032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:30:55.729811 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.729849 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.729878 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.729940 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.732176 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.737716 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.737982 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.740596 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.753854 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.753907 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.753942 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.753973 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.754033 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.754590 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.754666 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.755210 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.755895 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.758367 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.758992 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.759069 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.759103 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.759162 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.759291 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.759401 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.759439 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.761386 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.761477 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.763886 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.763965 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.764076 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.766281 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.768161 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.768260 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.768555 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.768636 140427613868032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:30:55.768744 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.768782 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.768813 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.768875 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.771130 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.776713 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.776979 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.779605 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.792270 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.792325 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.792360 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.792390 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.792453 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.793008 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.793083 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.793439 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.794147 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.796591 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.797204 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.797281 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.797315 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.797373 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.797502 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.797610 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.797655 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.799591 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.799684 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.802076 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.802155 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.802265 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.804454 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.806295 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.806389 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.806685 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.806765 140427613868032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:30:55.806873 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.806911 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.806941 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.807002 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.809226 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.814700 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.814962 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.817545 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.830646 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.830699 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.830734 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.830765 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.830828 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.831385 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.831459 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.831817 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.832503 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.834964 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.835582 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.835659 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.835694 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.835753 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.835882 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.835991 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.836028 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.837973 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.838066 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.840448 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.840527 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.840634 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.842829 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.844677 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.844770 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.845059 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.845146 140427613868032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:30:55.845255 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.845293 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.845324 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.845386 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.847621 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.853113 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.853375 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.856273 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.869392 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.869446 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.869481 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.869513 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.869574 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.870150 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.870229 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.870607 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.871328 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.873828 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.874444 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.874522 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.874557 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.874614 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.874743 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.874853 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.874890 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.876830 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.876922 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.879389 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.879478 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.879608 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.881810 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.883681 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.883774 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.884065 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.884152 140427613868032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:30:55.884264 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.884302 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.884332 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.884394 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.886720 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.892332 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.892595 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.895213 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.908017 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.908072 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.908108 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.908138 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.908199 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.908763 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.908838 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.909196 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.909893 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.912383 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.913000 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.913076 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.913110 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.913169 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.913299 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.913408 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.913445 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.915461 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.915555 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.917983 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.918062 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.918171 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.920407 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.922329 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.922424 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.922717 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.922799 140427613868032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:30:55.922916 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.922955 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.922985 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.923048 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.925299 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.930842 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.931108 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.933719 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.946898 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.946953 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.946988 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.947019 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.947081 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.947641 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.947716 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.948074 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.948758 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.951270 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.951894 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.951972 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.952006 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.952066 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.952196 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.952306 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.952344 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.954298 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.954392 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.956792 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.956869 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.956978 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.959190 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.961230 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.961324 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.961614 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.961707 140427613868032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:30:55.961816 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:55.961862 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:55.961894 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:55.962116 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.964338 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:55.969847 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.970108 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:55.972672 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:55.985307 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:55.985362 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:55.985397 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:55.985427 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.985489 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.986055 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.986131 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.986492 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.987177 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.989662 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.990283 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.990357 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:55.990392 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:55.990450 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.990582 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:55.990691 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:55.990729 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.992657 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.992747 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.995158 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.995236 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:55.995347 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:55.997526 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:55.999392 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.999486 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:55.999782 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:55.999863 140427613868032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:30:55.999971 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:56.000009 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:56.000046 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:56.000109 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.002359 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:56.007840 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.008104 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:56.010690 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:56.023306 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:56.023361 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:56.023397 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:56.023428 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.023490 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.024047 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.024122 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.024478 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.025173 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.027655 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.028278 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.028353 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:56.028388 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:56.028446 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.028575 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:56.028686 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:56.028724 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:56.030668 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.030761 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:56.033161 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.033237 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:56.033346 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:56.035565 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:56.037411 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.037503 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:56.037801 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.037880 140427613868032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:30:56.037989 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:56.038027 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:56.038058 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:56.038125 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.040363 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:56.045870 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.046130 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:56.048697 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:56.061885 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:56.061940 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:56.061975 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:56.062006 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.062067 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.062628 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.062701 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.063058 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.063742 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.066196 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.066819 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.066893 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:56.067091 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:56.067149 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.067276 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:56.067384 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:56.067421 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:56.069348 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.069438 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:56.071854 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.071932 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:56.072042 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:56.074275 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:56.076157 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.076249 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:56.076541 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.076619 140427613868032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:30:56.076728 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:56.076765 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:56.076794 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:56.076862 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.079219 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:56.084886 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.085147 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:56.087755 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:56.100386 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:56.100440 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:56.100474 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:56.100505 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.100565 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.101125 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.101200 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.101560 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.102252 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.104701 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.105319 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.105393 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:56.105428 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:56.105486 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.105614 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:56.105731 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:56.105771 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:56.107695 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.107785 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:56.110200 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.110278 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:56.110388 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:56.112592 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:56.114437 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.114530 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:56.114817 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.114895 140427613868032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:30:56.115002 140427613868032 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:30:56.115040 140427613868032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:30:56.115069 140427613868032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:30:56.115131 140427613868032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.117372 140427613868032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:30:56.122941 140427613868032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.123206 140427613868032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:30:56.125811 140427613868032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:30:56.138428 140427613868032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:30:56.138483 140427613868032 attention.py:418] Single window, no scan.
I0123 12:30:56.138518 140427613868032 transformer_layer.py:389] tlayer: self-attention.
I0123 12:30:56.138548 140427613868032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.138608 140427613868032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.139169 140427613868032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.139243 140427613868032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.139601 140427613868032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.140285 140427613868032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.142766 140427613868032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.143390 140427613868032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.143465 140427613868032 transformer_layer.py:468] tlayer: End windows.
I0123 12:30:56.143500 140427613868032 transformer_layer.py:472] tlayer: final FFN.
I0123 12:30:56.143558 140427613868032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.143687 140427613868032 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:30:56.143796 140427613868032 nn_components.py:325] mlp: activation = None
I0123 12:30:56.143834 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:56.145767 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.145860 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:56.148273 140427613868032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.148351 140427613868032 transformer_base.py:443] tbase: final FFN
I0123 12:30:56.148461 140427613868032 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:30:56.150677 140427613868032 nn_components.py:329] mlp: final activation = None
I0123 12:30:56.152554 140427613868032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.152647 140427613868032 nn_components.py:261] mlp: residual
I0123 12:30:56.152939 140427613868032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:30:56.153025 140427613868032 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:30:56.155901 140427613868032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:08.285366 140427613868032 alphageometry.py:566] LM output (score=-1.494427): "n : P c g d n 19 ;"
I0123 12:31:08.285516 140427613868032 alphageometry.py:567] Translation: "n = on_pline n d c g"

I0123 12:31:08.285561 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g ? perp m c c a"
I0123 12:31:08.285742 140427613868032 graph.py:498] 
I0123 12:31:08.285802 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g ? perp m c c a
I0123 12:31:10.630270 140427613868032 ddar.py:60] Depth 1/1000 time = 2.3004913330078125
I0123 12:31:14.821151 140427613868032 ddar.py:60] Depth 2/1000 time = 4.190704345703125
I0123 12:31:19.615759 140427613868032 ddar.py:60] Depth 3/1000 time = 4.794431924819946
I0123 12:31:24.891743 140427613868032 ddar.py:60] Depth 4/1000 time = 5.275810718536377
I0123 12:31:30.187123 140427613868032 ddar.py:60] Depth 5/1000 time = 5.295200824737549
I0123 12:31:35.471081 140427613868032 ddar.py:60] Depth 6/1000 time = 5.283766746520996
I0123 12:31:41.450312 140427613868032 ddar.py:60] Depth 7/1000 time = 5.974308729171753
I0123 12:31:52.816018 140427613868032 ddar.py:60] Depth 8/1000 time = 11.365509510040283
I0123 12:32:02.005650 140427613868032 ddar.py:60] Depth 9/1000 time = 9.189396142959595
I0123 12:32:11.399485 140427613868032 ddar.py:60] Depth 10/1000 time = 9.393570899963379
I0123 12:32:20.691736 140427613868032 ddar.py:60] Depth 11/1000 time = 9.129541397094727
I0123 12:32:20.714386 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:32:20.714488 140427613868032 alphageometry.py:566] LM output (score=-1.797754): "n : P a g e n 19 ;"
I0123 12:32:20.714525 140427613868032 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2570, in add_clause
    raise DepCheckFailError(
graph.DepCheckFailError: ncoll e a g
"

I0123 12:32:20.714559 140427613868032 alphageometry.py:566] LM output (score=-1.839113): "n : P c h d n 19 ;"
I0123 12:32:20.714585 140427613868032 alphageometry.py:567] Translation: "n = on_pline n d c h"

I0123 12:32:20.714615 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h ? perp m c c a"
I0123 12:32:20.714778 140427613868032 graph.py:498] 
I0123 12:32:20.714829 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h ? perp m c c a
I0123 12:32:22.866972 140427613868032 ddar.py:60] Depth 1/1000 time = 2.110692024230957
I0123 12:32:27.189024 140427613868032 ddar.py:60] Depth 2/1000 time = 4.32188868522644
I0123 12:32:31.844111 140427613868032 ddar.py:60] Depth 3/1000 time = 4.654860258102417
I0123 12:32:37.174636 140427613868032 ddar.py:60] Depth 4/1000 time = 5.330246210098267
I0123 12:32:42.651284 140427613868032 ddar.py:60] Depth 5/1000 time = 5.476439476013184
I0123 12:32:47.978638 140427613868032 ddar.py:60] Depth 6/1000 time = 5.327171564102173
I0123 12:32:53.933745 140427613868032 ddar.py:60] Depth 7/1000 time = 5.95011568069458
I0123 12:33:03.245489 140427613868032 ddar.py:60] Depth 8/1000 time = 9.311501026153564
I0123 12:33:12.269135 140427613868032 ddar.py:60] Depth 9/1000 time = 9.023405075073242
I0123 12:33:21.462125 140427613868032 ddar.py:60] Depth 10/1000 time = 9.192551851272583
I0123 12:33:30.493408 140427613868032 ddar.py:60] Depth 11/1000 time = 8.872472524642944
I0123 12:33:30.516419 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:33:30.516529 140427613868032 alphageometry.py:566] LM output (score=-1.973750): "n : C b c n 19 D b n c n 20 ;"
I0123 12:33:30.516566 140427613868032 alphageometry.py:567] Translation: "n = on_line n b c, on_bline n c b"

I0123 12:33:30.516604 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_line n b c, on_bline n c b ? perp m c c a"
I0123 12:33:30.516775 140427613868032 graph.py:498] 
I0123 12:33:30.516834 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_line n b c, on_bline n c b ? perp m c c a
I0123 12:33:33.485020 140427613868032 ddar.py:60] Depth 1/1000 time = 2.9122045040130615
I0123 12:33:38.993170 140427613868032 ddar.py:60] Depth 2/1000 time = 5.507928848266602
I0123 12:33:47.730406 140427613868032 ddar.py:60] Depth 3/1000 time = 8.736940383911133
I0123 12:33:56.893605 140427613868032 ddar.py:60] Depth 4/1000 time = 9.163000345230103
I0123 12:34:07.399354 140427613868032 ddar.py:60] Depth 5/1000 time = 10.505491495132446
I0123 12:34:19.453778 140427613868032 ddar.py:60] Depth 6/1000 time = 12.054163694381714
I0123 12:34:31.126548 140427613868032 ddar.py:60] Depth 7/1000 time = 11.672492980957031
I0123 12:34:42.806448 140427613868032 ddar.py:60] Depth 8/1000 time = 11.678771257400513
I0123 12:34:54.034660 140427613868032 ddar.py:60] Depth 9/1000 time = 11.040998935699463
I0123 12:34:54.044306 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:34:54.044407 140427613868032 alphageometry.py:566] LM output (score=-2.278347): "n : P e l k n 19 ;"
I0123 12:34:54.044446 140427613868032 alphageometry.py:567] Translation: "n = on_pline n k e l"

I0123 12:34:54.044484 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k e l ? perp m c c a"
I0123 12:34:54.044661 140427613868032 graph.py:498] 
I0123 12:34:54.044718 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k e l ? perp m c c a
I0123 12:34:56.281167 140427613868032 ddar.py:60] Depth 1/1000 time = 2.1926932334899902
I0123 12:35:00.771428 140427613868032 ddar.py:60] Depth 2/1000 time = 4.490086078643799
I0123 12:35:05.361808 140427613868032 ddar.py:60] Depth 3/1000 time = 4.590161323547363
I0123 12:35:10.782801 140427613868032 ddar.py:60] Depth 4/1000 time = 5.420770883560181
I0123 12:35:16.315129 140427613868032 ddar.py:60] Depth 5/1000 time = 5.532125234603882
I0123 12:35:21.548549 140427613868032 ddar.py:60] Depth 6/1000 time = 5.233224391937256
I0123 12:35:27.753627 140427613868032 ddar.py:60] Depth 7/1000 time = 6.200057506561279
I0123 12:35:39.417568 140427613868032 ddar.py:60] Depth 8/1000 time = 11.663678407669067
I0123 12:35:48.842393 140427613868032 ddar.py:60] Depth 9/1000 time = 9.42448878288269
I0123 12:35:58.232837 140427613868032 ddar.py:60] Depth 10/1000 time = 9.390074253082275
I0123 12:36:07.731675 140427613868032 ddar.py:60] Depth 11/1000 time = 9.340399265289307
I0123 12:36:07.754433 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:36:07.754559 140427613868032 alphageometry.py:566] LM output (score=-2.283123): "n : C b e n 19 D b n e n 20 ;"
I0123 12:36:07.754596 140427613868032 alphageometry.py:567] Translation: "n = on_line n b e, on_bline n e b"

I0123 12:36:07.754645 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_line n b e, on_bline n e b ? perp m c c a"
I0123 12:36:07.754836 140427613868032 graph.py:498] 
I0123 12:36:07.754893 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_line n b e, on_bline n e b ? perp m c c a
I0123 12:36:10.601376 140427613868032 ddar.py:60] Depth 1/1000 time = 2.7898569107055664
I0123 12:36:15.924534 140427613868032 ddar.py:60] Depth 2/1000 time = 5.322979211807251
I0123 12:36:22.531599 140427613868032 ddar.py:60] Depth 3/1000 time = 6.606873989105225
I0123 12:36:30.449162 140427613868032 ddar.py:60] Depth 4/1000 time = 7.9172844886779785
I0123 12:36:38.107881 140427613868032 ddar.py:60] Depth 5/1000 time = 7.658376693725586
I0123 12:36:46.043920 140427613868032 ddar.py:60] Depth 6/1000 time = 7.935830593109131
I0123 12:36:53.676498 140427613868032 ddar.py:60] Depth 7/1000 time = 7.631863832473755
I0123 12:37:02.176269 140427613868032 ddar.py:60] Depth 8/1000 time = 8.495019912719727
I0123 12:37:16.417431 140427613868032 ddar.py:60] Depth 9/1000 time = 14.240904331207275
I0123 12:37:28.819164 140427613868032 ddar.py:60] Depth 10/1000 time = 12.401447057723999
I0123 12:37:41.199327 140427613868032 ddar.py:60] Depth 11/1000 time = 12.379887104034424
I0123 12:37:53.391812 140427613868032 ddar.py:60] Depth 12/1000 time = 12.001220464706421
I0123 12:37:53.414581 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:37:53.414696 140427613868032 alphageometry.py:566] LM output (score=-2.361213): "n : P a e e n 19 ;"
I0123 12:37:53.414733 140427613868032 alphageometry.py:567] Translation: "ERROR: Invalid predicate P a e e n"

I0123 12:37:53.414769 140427613868032 alphageometry.py:566] LM output (score=-2.440739): "n : C b f n 19 D b n f n 20 ;"
I0123 12:37:53.414795 140427613868032 alphageometry.py:567] Translation: "n = on_line n b f, on_bline n f b"

I0123 12:37:53.414825 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_line n b f, on_bline n f b ? perp m c c a"
I0123 12:37:53.415004 140427613868032 graph.py:498] 
I0123 12:37:53.415065 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_line n b f, on_bline n f b ? perp m c c a
I0123 12:37:56.279289 140427613868032 ddar.py:60] Depth 1/1000 time = 2.8056938648223877
I0123 12:38:01.702456 140427613868032 ddar.py:60] Depth 2/1000 time = 5.4229676723480225
I0123 12:38:08.553526 140427613868032 ddar.py:60] Depth 3/1000 time = 6.850900411605835
I0123 12:38:16.526573 140427613868032 ddar.py:60] Depth 4/1000 time = 7.972795486450195
I0123 12:38:24.369102 140427613868032 ddar.py:60] Depth 5/1000 time = 7.842196702957153
I0123 12:38:32.104700 140427613868032 ddar.py:60] Depth 6/1000 time = 7.735387086868286
I0123 12:38:39.872366 140427613868032 ddar.py:60] Depth 7/1000 time = 7.766911268234253
I0123 12:38:48.558423 140427613868032 ddar.py:60] Depth 8/1000 time = 8.681159734725952
I0123 12:39:03.069621 140427613868032 ddar.py:60] Depth 9/1000 time = 14.510879516601562
I0123 12:39:16.022165 140427613868032 ddar.py:60] Depth 10/1000 time = 12.952111721038818
I0123 12:39:29.058385 140427613868032 ddar.py:60] Depth 11/1000 time = 13.035930395126343
I0123 12:39:41.220857 140427613868032 ddar.py:60] Depth 12/1000 time = 11.968502521514893
I0123 12:39:41.244368 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:39:41.244495 140427613868032 alphageometry.py:566] LM output (score=-2.457901): "n : P a b c n 19 ;"
I0123 12:39:41.244533 140427613868032 alphageometry.py:567] Translation: "n = on_pline n c a b"

I0123 12:39:41.244585 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n c a b ? perp m c c a"
I0123 12:39:41.244780 140427613868032 graph.py:498] 
I0123 12:39:41.244838 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n c a b ? perp m c c a
I0123 12:39:43.686967 140427613868032 ddar.py:60] Depth 1/1000 time = 2.3974952697753906
I0123 12:39:48.107390 140427613868032 ddar.py:60] Depth 2/1000 time = 4.420255899429321
I0123 12:39:52.798382 140427613868032 ddar.py:60] Depth 3/1000 time = 4.69081974029541
I0123 12:39:58.106689 140427613868032 ddar.py:60] Depth 4/1000 time = 5.308104991912842
I0123 12:40:03.827068 140427613868032 ddar.py:60] Depth 5/1000 time = 5.720119476318359
I0123 12:40:09.635527 140427613868032 ddar.py:60] Depth 6/1000 time = 5.808122634887695
I0123 12:40:15.456226 140427613868032 ddar.py:60] Depth 7/1000 time = 5.815732955932617
I0123 12:40:25.396814 140427613868032 ddar.py:60] Depth 8/1000 time = 9.940324306488037
I0123 12:40:35.001765 140427613868032 ddar.py:60] Depth 9/1000 time = 9.604589462280273
I0123 12:40:44.687880 140427613868032 ddar.py:60] Depth 10/1000 time = 9.685827016830444
I0123 12:40:54.058677 140427613868032 ddar.py:60] Depth 11/1000 time = 9.210126399993896
I0123 12:40:54.081086 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:40:54.081193 140427613868032 alphageometry.py:566] LM output (score=-2.458336): "n : P a g d n 19 ;"
I0123 12:40:54.081231 140427613868032 alphageometry.py:567] Translation: "n = on_pline n d a g"

I0123 12:40:54.081270 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d a g ? perp m c c a"
I0123 12:40:54.081455 140427613868032 graph.py:498] 
I0123 12:40:54.081514 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d a g ? perp m c c a
I0123 12:40:56.487621 140427613868032 ddar.py:60] Depth 1/1000 time = 2.3616628646850586
I0123 12:41:00.702441 140427613868032 ddar.py:60] Depth 2/1000 time = 4.214653491973877
I0123 12:41:05.446888 140427613868032 ddar.py:60] Depth 3/1000 time = 4.744268178939819
I0123 12:41:11.232554 140427613868032 ddar.py:60] Depth 4/1000 time = 5.78547215461731
I0123 12:41:16.702949 140427613868032 ddar.py:60] Depth 5/1000 time = 5.470195770263672
I0123 12:41:22.540618 140427613868032 ddar.py:60] Depth 6/1000 time = 5.837482929229736
I0123 12:41:28.921927 140427613868032 ddar.py:60] Depth 7/1000 time = 6.376239061355591
I0123 12:41:38.449424 140427613868032 ddar.py:60] Depth 8/1000 time = 9.527168035507202
I0123 12:41:48.507199 140427613868032 ddar.py:60] Depth 9/1000 time = 10.057548522949219
I0123 12:41:58.039455 140427613868032 ddar.py:60] Depth 10/1000 time = 9.532002210617065
I0123 12:42:07.565140 140427613868032 ddar.py:60] Depth 11/1000 time = 9.368544816970825
I0123 12:42:07.588017 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:07.588124 140427613868032 alphageometry.py:566] LM output (score=-2.517314): "n : P e g k n 19 ;"
I0123 12:42:07.588162 140427613868032 alphageometry.py:567] Translation: "n = on_pline n k e g"

I0123 12:42:07.588200 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k e g ? perp m c c a"
I0123 12:42:07.588376 140427613868032 graph.py:498] 
I0123 12:42:07.588437 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k e g ? perp m c c a
I0123 12:42:09.711884 140427613868032 ddar.py:60] Depth 1/1000 time = 2.0795981884002686
I0123 12:42:14.011335 140427613868032 ddar.py:60] Depth 2/1000 time = 4.299280405044556
I0123 12:42:18.822496 140427613868032 ddar.py:60] Depth 3/1000 time = 4.8109660148620605
I0123 12:42:24.224374 140427613868032 ddar.py:60] Depth 4/1000 time = 5.401703119277954
I0123 12:42:30.123573 140427613868032 ddar.py:60] Depth 5/1000 time = 5.899016380310059
I0123 12:42:35.652843 140427613868032 ddar.py:60] Depth 6/1000 time = 5.529061794281006
I0123 12:42:41.951788 140427613868032 ddar.py:60] Depth 7/1000 time = 6.293714284896851
I0123 12:42:52.868623 140427613868032 ddar.py:60] Depth 8/1000 time = 10.916646242141724
I0123 12:43:02.642345 140427613868032 ddar.py:60] Depth 9/1000 time = 9.77348518371582
I0123 12:43:12.645853 140427613868032 ddar.py:60] Depth 10/1000 time = 10.003256797790527
I0123 12:43:22.445556 140427613868032 ddar.py:60] Depth 11/1000 time = 9.636608839035034
I0123 12:43:22.468543 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:43:22.468646 140427613868032 alphageometry.py:566] LM output (score=-2.524383): "n : P a e c n 19 ;"
I0123 12:43:22.468683 140427613868032 alphageometry.py:567] Translation: "n = on_pline n c a e"

I0123 12:43:22.468721 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n c a e ? perp m c c a"
I0123 12:43:22.469107 140427613868032 graph.py:498] 
I0123 12:43:22.469166 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n c a e ? perp m c c a
I0123 12:43:24.951830 140427613868032 ddar.py:60] Depth 1/1000 time = 2.4358413219451904
I0123 12:43:28.941903 140427613868032 ddar.py:60] Depth 2/1000 time = 3.9898929595947266
I0123 12:43:33.855647 140427613868032 ddar.py:60] Depth 3/1000 time = 4.913553237915039
I0123 12:43:39.371208 140427613868032 ddar.py:60] Depth 4/1000 time = 5.515326499938965
I0123 12:43:45.317311 140427613868032 ddar.py:60] Depth 5/1000 time = 5.9458746910095215
I0123 12:43:50.914823 140427613868032 ddar.py:60] Depth 6/1000 time = 5.597308397293091
I0123 12:43:56.956518 140427613868032 ddar.py:60] Depth 7/1000 time = 6.036684036254883
I0123 12:44:07.043945 140427613868032 ddar.py:60] Depth 8/1000 time = 10.087212562561035
I0123 12:44:16.751951 140427613868032 ddar.py:60] Depth 9/1000 time = 9.70777177810669
I0123 12:44:26.510081 140427613868032 ddar.py:60] Depth 10/1000 time = 9.757853746414185
I0123 12:44:36.105026 140427613868032 ddar.py:60] Depth 11/1000 time = 9.434247732162476
I0123 12:44:36.127904 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:44:36.128008 140427613868032 alphageometry.py:566] LM output (score=-2.606225): "n : P e l l n 19 ;"
I0123 12:44:36.128046 140427613868032 alphageometry.py:567] Translation: "ERROR: Invalid predicate P e l l n"

I0123 12:44:36.128079 140427613868032 alphageometry.py:566] LM output (score=-2.688791): "n : D g h h n 19 ;"
I0123 12:44:36.128106 140427613868032 alphageometry.py:567] Translation: "n = on_circle n h g"

I0123 12:44:36.128135 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_circle n h g ? perp m c c a"
I0123 12:44:36.128311 140427613868032 graph.py:498] 
I0123 12:44:36.128369 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_circle n h g ? perp m c c a
I0123 12:44:38.674886 140427613868032 ddar.py:60] Depth 1/1000 time = 2.482536554336548
I0123 12:44:43.070763 140427613868032 ddar.py:60] Depth 2/1000 time = 4.395678520202637
I0123 12:44:48.130767 140427613868032 ddar.py:60] Depth 3/1000 time = 5.059787034988403
I0123 12:44:53.831403 140427613868032 ddar.py:60] Depth 4/1000 time = 5.70046067237854
I0123 12:44:59.626525 140427613868032 ddar.py:60] Depth 5/1000 time = 5.794931411743164
I0123 12:45:05.059583 140427613868032 ddar.py:60] Depth 6/1000 time = 5.432824611663818
I0123 12:45:11.726536 140427613868032 ddar.py:60] Depth 7/1000 time = 6.662150144577026
I0123 12:45:22.311520 140427613868032 ddar.py:60] Depth 8/1000 time = 10.58476972579956
I0123 12:45:32.263262 140427613868032 ddar.py:60] Depth 9/1000 time = 9.951495885848999
I0123 12:45:42.151692 140427613868032 ddar.py:60] Depth 10/1000 time = 9.888189554214478
I0123 12:45:52.377823 140427613868032 ddar.py:60] Depth 11/1000 time = 10.062701940536499
I0123 12:45:52.400874 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:45:52.400972 140427613868032 alphageometry.py:566] LM output (score=-2.783534): "n : D g h h n 19 T g h h n 20 ;"
I0123 12:45:52.401022 140427613868032 alphageometry.py:567] Translation: "n = on_circle n h g, on_tline n h g h"

I0123 12:45:52.401062 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_circle n h g, on_tline n h g h ? perp m c c a"
I0123 12:45:52.401244 140427613868032 graph.py:498] 
I0123 12:45:52.401304 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_circle n h g, on_tline n h g h ? perp m c c a
I0123 12:45:54.570001 140427613868032 ddar.py:60] Depth 1/1000 time = 2.111567974090576
I0123 12:45:59.061542 140427613868032 ddar.py:60] Depth 2/1000 time = 4.491357326507568
I0123 12:46:04.362376 140427613868032 ddar.py:60] Depth 3/1000 time = 5.3006508350372314
I0123 12:46:10.315039 140427613868032 ddar.py:60] Depth 4/1000 time = 5.952439546585083
I0123 12:46:16.374929 140427613868032 ddar.py:60] Depth 5/1000 time = 6.0596466064453125
I0123 12:46:22.439455 140427613868032 ddar.py:60] Depth 6/1000 time = 6.064336061477661
I0123 12:46:28.608031 140427613868032 ddar.py:60] Depth 7/1000 time = 6.162611722946167
I0123 12:46:39.353014 140427613868032 ddar.py:60] Depth 8/1000 time = 10.744646072387695
I0123 12:46:49.510579 140427613868032 ddar.py:60] Depth 9/1000 time = 10.157292127609253
I0123 12:46:59.702556 140427613868032 ddar.py:60] Depth 10/1000 time = 10.191604375839233
I0123 12:47:09.679328 140427613868032 ddar.py:60] Depth 11/1000 time = 9.795969724655151
I0123 12:47:09.702164 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:47:09.702287 140427613868032 alphageometry.py:566] LM output (score=-2.793203): "n : D b d d n 19 ;"
I0123 12:47:09.702325 140427613868032 alphageometry.py:567] Translation: "n = on_circle n d b"

I0123 12:47:09.702374 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_circle n d b ? perp m c c a"
I0123 12:47:09.702564 140427613868032 graph.py:498] 
I0123 12:47:09.702622 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_circle n d b ? perp m c c a
I0123 12:47:12.619652 140427613868032 ddar.py:60] Depth 1/1000 time = 2.8647823333740234
I0123 12:47:18.811783 140427613868032 ddar.py:60] Depth 2/1000 time = 6.191927909851074
I0123 12:47:24.647913 140427613868032 ddar.py:60] Depth 3/1000 time = 5.835940361022949
I0123 12:47:31.759994 140427613868032 ddar.py:60] Depth 4/1000 time = 7.1118481159210205
I0123 12:47:38.983204 140427613868032 ddar.py:60] Depth 5/1000 time = 7.222892999649048
I0123 12:47:46.527071 140427613868032 ddar.py:60] Depth 6/1000 time = 7.543677568435669
I0123 12:47:54.349664 140427613868032 ddar.py:60] Depth 7/1000 time = 7.817222833633423
I0123 12:48:05.375285 140427613868032 ddar.py:60] Depth 8/1000 time = 11.025360584259033
I0123 12:48:17.376073 140427613868032 ddar.py:60] Depth 9/1000 time = 12.000482320785522
I0123 12:48:29.560834 140427613868032 ddar.py:60] Depth 10/1000 time = 12.184362888336182
I0123 12:48:41.251844 140427613868032 ddar.py:60] Depth 11/1000 time = 11.48102331161499
I0123 12:48:41.275329 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:48:41.275461 140427613868032 alphageometry.py:566] LM output (score=-2.793392): "n : P d e i n 19 ;"
I0123 12:48:41.275498 140427613868032 alphageometry.py:567] Translation: "n = on_pline n i d e"

I0123 12:48:41.275550 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n i d e ? perp m c c a"
I0123 12:48:41.275746 140427613868032 graph.py:498] 
I0123 12:48:41.275805 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n i d e ? perp m c c a
I0123 12:48:43.832619 140427613868032 ddar.py:60] Depth 1/1000 time = 2.508730173110962
I0123 12:48:47.972532 140427613868032 ddar.py:60] Depth 2/1000 time = 4.139750719070435
I0123 12:48:52.856157 140427613868032 ddar.py:60] Depth 3/1000 time = 4.883453130722046
I0123 12:48:57.956154 140427613868032 ddar.py:60] Depth 4/1000 time = 5.0997538566589355
I0123 12:49:03.523229 140427613868032 ddar.py:60] Depth 5/1000 time = 5.566776275634766
I0123 12:49:09.114238 140427613868032 ddar.py:60] Depth 6/1000 time = 5.590821266174316
I0123 12:49:14.654859 140427613868032 ddar.py:60] Depth 7/1000 time = 5.535943508148193
I0123 12:49:24.688013 140427613868032 ddar.py:60] Depth 8/1000 time = 10.03294038772583
I0123 12:49:33.844466 140427613868032 ddar.py:60] Depth 9/1000 time = 9.156217336654663
I0123 12:49:43.326493 140427613868032 ddar.py:60] Depth 10/1000 time = 9.481715202331543
I0123 12:49:52.402503 140427613868032 ddar.py:60] Depth 11/1000 time = 8.91612696647644
I0123 12:49:52.424177 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:49:52.424278 140427613868032 alphageometry.py:566] LM output (score=-2.830305): "n : P d e k n 19 ;"
I0123 12:49:52.424315 140427613868032 alphageometry.py:567] Translation: "n = on_pline n k d e"

I0123 12:49:52.424353 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k d e ? perp m c c a"
I0123 12:49:52.424533 140427613868032 graph.py:498] 
I0123 12:49:52.424592 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k d e ? perp m c c a
I0123 12:49:54.603000 140427613868032 ddar.py:60] Depth 1/1000 time = 2.133948564529419
I0123 12:49:59.242453 140427613868032 ddar.py:60] Depth 2/1000 time = 4.639297008514404
I0123 12:50:03.795856 140427613868032 ddar.py:60] Depth 3/1000 time = 4.553215503692627
I0123 12:50:08.886219 140427613868032 ddar.py:60] Depth 4/1000 time = 5.090153455734253
I0123 12:50:14.486271 140427613868032 ddar.py:60] Depth 5/1000 time = 5.599856853485107
I0123 12:50:20.123460 140427613868032 ddar.py:60] Depth 6/1000 time = 5.636995553970337
I0123 12:50:26.240740 140427613868032 ddar.py:60] Depth 7/1000 time = 6.112401008605957
I0123 12:50:35.830810 140427613868032 ddar.py:60] Depth 8/1000 time = 9.589852333068848
I0123 12:50:45.360980 140427613868032 ddar.py:60] Depth 9/1000 time = 9.529890298843384
I0123 12:50:55.272120 140427613868032 ddar.py:60] Depth 10/1000 time = 9.910754442214966
I0123 12:51:04.423832 140427613868032 ddar.py:60] Depth 11/1000 time = 8.991332054138184
I0123 12:51:04.447172 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:51:04.447279 140427613868032 alphageometry.py:566] LM output (score=-2.851993): "n : P a e b n 19 ;"
I0123 12:51:04.447317 140427613868032 alphageometry.py:567] Translation: "n = on_pline n b a e"

I0123 12:51:04.447354 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n b a e ? perp m c c a"
I0123 12:51:04.447528 140427613868032 graph.py:498] 
I0123 12:51:04.447585 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n b a e ? perp m c c a
I0123 12:51:07.185559 140427613868032 ddar.py:60] Depth 1/1000 time = 2.692446231842041
I0123 12:51:11.562454 140427613868032 ddar.py:60] Depth 2/1000 time = 4.376706123352051
I0123 12:51:16.224978 140427613868032 ddar.py:60] Depth 3/1000 time = 4.662353515625
I0123 12:51:21.977364 140427613868032 ddar.py:60] Depth 4/1000 time = 5.752192497253418
I0123 12:51:27.344409 140427613868032 ddar.py:60] Depth 5/1000 time = 5.366807222366333
I0123 12:51:33.173666 140427613868032 ddar.py:60] Depth 6/1000 time = 5.8289289474487305
I0123 12:51:38.864684 140427613868032 ddar.py:60] Depth 7/1000 time = 5.686007499694824
I0123 12:51:49.735867 140427613868032 ddar.py:60] Depth 8/1000 time = 10.870916843414307
I0123 12:51:59.130781 140427613868032 ddar.py:60] Depth 9/1000 time = 9.39456820487976
I0123 12:52:08.660413 140427613868032 ddar.py:60] Depth 10/1000 time = 9.52941107749939
I0123 12:52:17.913915 140427613868032 ddar.py:60] Depth 11/1000 time = 9.093038320541382
I0123 12:52:17.936759 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:52:17.936854 140427613868032 alphageometry.py:566] LM output (score=-2.884297): "n : P a b c n 19 T a b a n 20 ;"
I0123 12:52:17.936890 140427613868032 alphageometry.py:567] Translation: "n = on_pline n c a b, on_tline n a a b"

I0123 12:52:17.936928 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n c a b, on_tline n a a b ? perp m c c a"
I0123 12:52:17.937102 140427613868032 graph.py:498] 
I0123 12:52:17.937159 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n c a b, on_tline n a a b ? perp m c c a
I0123 12:52:20.672907 140427613868032 ddar.py:60] Depth 1/1000 time = 2.6885993480682373
I0123 12:52:24.986092 140427613868032 ddar.py:60] Depth 2/1000 time = 4.31301736831665
I0123 12:52:29.960456 140427613868032 ddar.py:60] Depth 3/1000 time = 4.974189519882202
I0123 12:52:36.071861 140427613868032 ddar.py:60] Depth 4/1000 time = 6.111214876174927
I0123 12:52:41.775916 140427613868032 ddar.py:60] Depth 5/1000 time = 5.703850269317627
I0123 12:52:47.913130 140427613868032 ddar.py:60] Depth 6/1000 time = 6.137004613876343
I0123 12:52:54.724888 140427613868032 ddar.py:60] Depth 7/1000 time = 6.804055452346802
I0123 12:53:07.651159 140427613868032 ddar.py:60] Depth 8/1000 time = 12.925921440124512
I0123 12:53:17.903273 140427613868032 ddar.py:60] Depth 9/1000 time = 10.251813650131226
I0123 12:53:28.502653 140427613868032 ddar.py:60] Depth 10/1000 time = 10.599015951156616
I0123 12:53:38.535051 140427613868032 ddar.py:60] Depth 11/1000 time = 9.849469661712646
I0123 12:53:38.557664 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:53:38.557796 140427613868032 alphageometry.py:566] LM output (score=-2.980378): "n : P c j g n 19 ;"
I0123 12:53:38.557834 140427613868032 alphageometry.py:567] Translation: "n = on_pline n g c j"

I0123 12:53:38.557887 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n g c j ? perp m c c a"
I0123 12:53:38.558084 140427613868032 graph.py:498] 
I0123 12:53:38.558144 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n g c j ? perp m c c a
I0123 12:53:40.784911 140427613868032 ddar.py:60] Depth 1/1000 time = 2.1824662685394287
I0123 12:53:45.362037 140427613868032 ddar.py:60] Depth 2/1000 time = 4.576958894729614
I0123 12:53:50.147881 140427613868032 ddar.py:60] Depth 3/1000 time = 4.78565239906311
I0123 12:53:55.407436 140427613868032 ddar.py:60] Depth 4/1000 time = 5.259358882904053
I0123 12:54:00.774191 140427613868032 ddar.py:60] Depth 5/1000 time = 5.366543292999268
I0123 12:54:06.569853 140427613868032 ddar.py:60] Depth 6/1000 time = 5.795433282852173
I0123 12:54:12.776101 140427613868032 ddar.py:60] Depth 7/1000 time = 6.201541423797607
I0123 12:54:21.323414 140427613868032 ddar.py:60] Depth 8/1000 time = 8.547044515609741
I0123 12:54:31.502619 140427613868032 ddar.py:60] Depth 9/1000 time = 10.178858518600464
I0123 12:54:40.999209 140427613868032 ddar.py:60] Depth 10/1000 time = 9.496285676956177
I0123 12:54:50.406867 140427613868032 ddar.py:60] Depth 11/1000 time = 9.252535104751587
I0123 12:54:50.429783 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:54:50.429882 140427613868032 alphageometry.py:566] LM output (score=-3.015671): "n : P a j e n 19 ;"
I0123 12:54:50.429919 140427613868032 alphageometry.py:567] Translation: "n = on_pline n e a j"

I0123 12:54:50.429956 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n e a j ? perp m c c a"
I0123 12:54:50.430135 140427613868032 graph.py:498] 
I0123 12:54:50.430192 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n e a j ? perp m c c a
I0123 12:54:52.712115 140427613868032 ddar.py:60] Depth 1/1000 time = 2.2350573539733887
I0123 12:54:56.965018 140427613868032 ddar.py:60] Depth 2/1000 time = 4.252740859985352
I0123 12:55:01.748677 140427613868032 ddar.py:60] Depth 3/1000 time = 4.783438444137573
I0123 12:55:07.670223 140427613868032 ddar.py:60] Depth 4/1000 time = 5.921228885650635
I0123 12:55:13.123880 140427613868032 ddar.py:60] Depth 5/1000 time = 5.453449726104736
I0123 12:55:19.123739 140427613868032 ddar.py:60] Depth 6/1000 time = 5.999667406082153
I0123 12:55:25.011487 140427613868032 ddar.py:60] Depth 7/1000 time = 5.882752180099487
I0123 12:55:34.001896 140427613868032 ddar.py:60] Depth 8/1000 time = 8.9900803565979
I0123 12:55:43.917650 140427613868032 ddar.py:60] Depth 9/1000 time = 9.91547155380249
I0123 12:55:53.515055 140427613868032 ddar.py:60] Depth 10/1000 time = 9.597005128860474
I0123 12:56:03.534356 140427613868032 ddar.py:60] Depth 11/1000 time = 9.859122514724731
I0123 12:56:03.557163 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:56:03.557270 140427613868032 alphageometry.py:566] LM output (score=-3.021131): "n : P a e f n 19 ;"
I0123 12:56:03.557307 140427613868032 alphageometry.py:567] Translation: "n = on_pline n f a e"

I0123 12:56:03.557343 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n f a e ? perp m c c a"
I0123 12:56:03.557514 140427613868032 graph.py:498] 
I0123 12:56:03.557571 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n f a e ? perp m c c a
I0123 12:56:05.896687 140427613868032 ddar.py:60] Depth 1/1000 time = 2.291337490081787
I0123 12:56:10.217167 140427613868032 ddar.py:60] Depth 2/1000 time = 4.320296764373779
I0123 12:56:15.137665 140427613868032 ddar.py:60] Depth 3/1000 time = 4.920300722122192
I0123 12:56:20.970225 140427613868032 ddar.py:60] Depth 4/1000 time = 5.832377672195435
I0123 12:56:26.949194 140427613868032 ddar.py:60] Depth 5/1000 time = 5.978711128234863
I0123 12:56:33.500705 140427613868032 ddar.py:60] Depth 6/1000 time = 6.551188945770264
I0123 12:56:40.393743 140427613868032 ddar.py:60] Depth 7/1000 time = 6.888672351837158
I0123 12:56:50.975985 140427613868032 ddar.py:60] Depth 8/1000 time = 10.581976175308228
I0123 12:57:00.598357 140427613868032 ddar.py:60] Depth 9/1000 time = 9.622024297714233
I0123 12:57:09.356127 140427613868032 ddar.py:60] Depth 10/1000 time = 8.757471084594727
I0123 12:57:19.482580 140427613868032 ddar.py:60] Depth 11/1000 time = 9.967460632324219
I0123 12:57:19.505142 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:57:19.505253 140427613868032 alphageometry.py:566] LM output (score=-3.051740): "n : P e l j n 19 ;"
I0123 12:57:19.505290 140427613868032 alphageometry.py:567] Translation: "n = on_pline n j e l"

I0123 12:57:19.505328 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n j e l ? perp m c c a"
I0123 12:57:19.505506 140427613868032 graph.py:498] 
I0123 12:57:19.505563 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n j e l ? perp m c c a
I0123 12:57:21.281373 140427613868032 ddar.py:60] Depth 1/1000 time = 1.7303838729858398
I0123 12:57:26.324536 140427613868032 ddar.py:60] Depth 2/1000 time = 5.042974233627319
I0123 12:57:31.277565 140427613868032 ddar.py:60] Depth 3/1000 time = 4.952836990356445
I0123 12:57:36.759787 140427613868032 ddar.py:60] Depth 4/1000 time = 5.4820075035095215
I0123 12:57:42.336970 140427613868032 ddar.py:60] Depth 5/1000 time = 5.576974153518677
I0123 12:57:47.879191 140427613868032 ddar.py:60] Depth 6/1000 time = 5.542024374008179
I0123 12:57:54.461728 140427613868032 ddar.py:60] Depth 7/1000 time = 6.577415704727173
I0123 12:58:04.244359 140427613868032 ddar.py:60] Depth 8/1000 time = 9.782397747039795
I0123 12:58:14.117288 140427613868032 ddar.py:60] Depth 9/1000 time = 9.872653722763062
I0123 12:58:23.967346 140427613868032 ddar.py:60] Depth 10/1000 time = 9.849685907363892
I0123 12:58:33.848983 140427613868032 ddar.py:60] Depth 11/1000 time = 9.72017502784729
I0123 12:58:33.871976 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:58:33.872085 140427613868032 alphageometry.py:566] LM output (score=-3.068160): "n : P i j k n 19 ;"
I0123 12:58:33.872121 140427613868032 alphageometry.py:567] Translation: "n = on_pline n k i j"

I0123 12:58:33.872158 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k i j ? perp m c c a"
I0123 12:58:33.872339 140427613868032 graph.py:498] 
I0123 12:58:33.872396 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k i j ? perp m c c a
I0123 12:58:36.170598 140427613868032 ddar.py:60] Depth 1/1000 time = 2.2545053958892822
I0123 12:58:40.050342 140427613868032 ddar.py:60] Depth 2/1000 time = 3.8795559406280518
I0123 12:58:44.716449 140427613868032 ddar.py:60] Depth 3/1000 time = 4.665921449661255
I0123 12:58:50.577022 140427613868032 ddar.py:60] Depth 4/1000 time = 5.860381603240967
I0123 12:58:56.038315 140427613868032 ddar.py:60] Depth 5/1000 time = 5.461044549942017
I0123 12:59:01.481535 140427613868032 ddar.py:60] Depth 6/1000 time = 5.442900657653809
I0123 12:59:07.269597 140427613868032 ddar.py:60] Depth 7/1000 time = 5.7833051681518555
I0123 12:59:17.135101 140427613868032 ddar.py:60] Depth 8/1000 time = 9.865258693695068
I0123 12:59:26.944964 140427613868032 ddar.py:60] Depth 9/1000 time = 9.809525728225708
I0123 12:59:36.542961 140427613868032 ddar.py:60] Depth 10/1000 time = 9.597769260406494
I0123 12:59:46.658669 140427613868032 ddar.py:60] Depth 11/1000 time = 9.952564716339111
I0123 12:59:46.681311 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:59:46.681412 140427613868032 alphageometry.py:566] LM output (score=-3.144558): "n : P b j d n 19 ;"
I0123 12:59:46.681450 140427613868032 alphageometry.py:567] Translation: "n = on_pline n d b j"

I0123 12:59:46.681488 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d b j ? perp m c c a"
I0123 12:59:46.681674 140427613868032 graph.py:498] 
I0123 12:59:46.681734 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d b j ? perp m c c a
I0123 12:59:48.999763 140427613868032 ddar.py:60] Depth 1/1000 time = 2.272918939590454
I0123 12:59:53.344969 140427613868032 ddar.py:60] Depth 2/1000 time = 4.3450469970703125
I0123 12:59:58.310925 140427613868032 ddar.py:60] Depth 3/1000 time = 4.965728521347046
I0123 13:00:03.823217 140427613868032 ddar.py:60] Depth 4/1000 time = 5.511987209320068
I0123 13:00:09.439998 140427613868032 ddar.py:60] Depth 5/1000 time = 5.61659836769104
I0123 13:00:15.007102 140427613868032 ddar.py:60] Depth 6/1000 time = 5.566903829574585
I0123 13:00:20.985097 140427613868032 ddar.py:60] Depth 7/1000 time = 5.973000526428223
I0123 13:00:30.969329 140427613868032 ddar.py:60] Depth 8/1000 time = 9.98390245437622
I0123 13:00:40.764649 140427613868032 ddar.py:60] Depth 9/1000 time = 9.795048713684082
I0123 13:00:50.465179 140427613868032 ddar.py:60] Depth 10/1000 time = 9.700145959854126
I0123 13:00:59.509500 140427613868032 ddar.py:60] Depth 11/1000 time = 8.888269662857056
I0123 13:00:59.532382 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:00:59.532489 140427613868032 alphageometry.py:566] LM output (score=-3.151047): "n : P d e e n 19 ;"
I0123 13:00:59.532526 140427613868032 alphageometry.py:567] Translation: "ERROR: Invalid predicate P d e e n"

I0123 13:00:59.532560 140427613868032 alphageometry.py:566] LM output (score=-3.154401): "n : P e n h j 19 ;"
I0123 13:00:59.532585 140427613868032 alphageometry.py:567] Translation: "n = on_pline n e h j"

I0123 13:00:59.532614 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n e h j ? perp m c c a"
I0123 13:00:59.532788 140427613868032 graph.py:498] 
I0123 13:00:59.532844 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n e h j ? perp m c c a
I0123 13:01:02.532456 140427613868032 ddar.py:60] Depth 1/1000 time = 2.954890012741089
I0123 13:01:06.340029 140427613868032 ddar.py:60] Depth 2/1000 time = 3.807382345199585
I0123 13:01:11.227551 140427613868032 ddar.py:60] Depth 3/1000 time = 4.887347936630249
I0123 13:01:17.334733 140427613868032 ddar.py:60] Depth 4/1000 time = 6.106988191604614
I0123 13:01:23.040272 140427613868032 ddar.py:60] Depth 5/1000 time = 5.705271244049072
I0123 13:01:28.730021 140427613868032 ddar.py:60] Depth 6/1000 time = 5.689425468444824
I0123 13:01:34.769862 140427613868032 ddar.py:60] Depth 7/1000 time = 6.034879684448242
I0123 13:01:44.205417 140427613868032 ddar.py:60] Depth 8/1000 time = 9.435287714004517
I0123 13:01:54.212004 140427613868032 ddar.py:60] Depth 9/1000 time = 10.006251811981201
I0123 13:02:03.983244 140427613868032 ddar.py:60] Depth 10/1000 time = 9.770953178405762
I0123 13:02:13.654826 140427613868032 ddar.py:60] Depth 11/1000 time = 9.511887073516846
I0123 13:02:13.676600 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:02:13.676702 140427613868032 alphageometry.py:566] LM output (score=-3.205504): "n : P c i h n 19 ;"
I0123 13:02:13.676738 140427613868032 alphageometry.py:567] Translation: "n = on_pline n h c i"

I0123 13:02:13.676776 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n h c i ? perp m c c a"
I0123 13:02:13.676967 140427613868032 graph.py:498] 
I0123 13:02:13.677026 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n h c i ? perp m c c a
I0123 13:02:15.971837 140427613868032 ddar.py:60] Depth 1/1000 time = 2.2506697177886963
I0123 13:02:19.655733 140427613868032 ddar.py:60] Depth 2/1000 time = 3.6837334632873535
I0123 13:02:24.488816 140427613868032 ddar.py:60] Depth 3/1000 time = 4.832853078842163
I0123 13:02:30.539708 140427613868032 ddar.py:60] Depth 4/1000 time = 6.050573110580444
I0123 13:02:36.072708 140427613868032 ddar.py:60] Depth 5/1000 time = 5.532791614532471
I0123 13:02:40.949171 140427613868032 ddar.py:60] Depth 6/1000 time = 4.876269102096558
I0123 13:02:47.532997 140427613868032 ddar.py:60] Depth 7/1000 time = 6.579099416732788
I0123 13:02:55.837287 140427613868032 ddar.py:60] Depth 8/1000 time = 8.304067373275757
I0123 13:03:05.759676 140427613868032 ddar.py:60] Depth 9/1000 time = 9.922152757644653
I0123 13:03:15.530735 140427613868032 ddar.py:60] Depth 10/1000 time = 9.770766258239746
I0123 13:03:24.606925 140427613868032 ddar.py:60] Depth 11/1000 time = 8.915033102035522
I0123 13:03:24.629674 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:03:24.629788 140427613868032 alphageometry.py:566] LM output (score=-3.236145): "n : P d n j m 19 ;"
I0123 13:03:24.629826 140427613868032 alphageometry.py:567] Translation: "n = on_pline n d j m"

I0123 13:03:24.629864 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d j m ? perp m c c a"
I0123 13:03:24.630044 140427613868032 graph.py:498] 
I0123 13:03:24.630101 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d j m ? perp m c c a
I0123 13:03:27.001309 140427613868032 ddar.py:60] Depth 1/1000 time = 2.3272106647491455
I0123 13:03:31.619124 140427613868032 ddar.py:60] Depth 2/1000 time = 4.617609977722168
I0123 13:03:36.536480 140427613868032 ddar.py:60] Depth 3/1000 time = 4.917181730270386
I0123 13:03:42.060185 140427613868032 ddar.py:60] Depth 4/1000 time = 5.523516416549683
I0123 13:03:47.766151 140427613868032 ddar.py:60] Depth 5/1000 time = 5.705685138702393
I0123 13:03:54.059476 140427613868032 ddar.py:60] Depth 6/1000 time = 6.292992353439331
I0123 13:04:00.190546 140427613868032 ddar.py:60] Depth 7/1000 time = 6.126316785812378
I0123 13:04:10.336751 140427613868032 ddar.py:60] Depth 8/1000 time = 10.14592170715332
I0123 13:04:19.546346 140427613868032 ddar.py:60] Depth 9/1000 time = 9.20906114578247
I0123 13:04:29.326806 140427613868032 ddar.py:60] Depth 10/1000 time = 9.780171871185303
I0123 13:04:38.630415 140427613868032 ddar.py:60] Depth 11/1000 time = 9.149402856826782
I0123 13:04:38.652989 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:04:38.653095 140427613868032 alphageometry.py:566] LM output (score=-3.265152): "n : P b e j n 19 ;"
I0123 13:04:38.653130 140427613868032 alphageometry.py:567] Translation: "n = on_pline n j b e"

I0123 13:04:38.653168 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n j b e ? perp m c c a"
I0123 13:04:38.653346 140427613868032 graph.py:498] 
I0123 13:04:38.653403 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n j b e ? perp m c c a
I0123 13:04:40.996267 140427613868032 ddar.py:60] Depth 1/1000 time = 2.2959985733032227
I0123 13:04:45.619042 140427613868032 ddar.py:60] Depth 2/1000 time = 4.622598171234131
I0123 13:04:50.665651 140427613868032 ddar.py:60] Depth 3/1000 time = 5.046355724334717
I0123 13:04:56.294799 140427613868032 ddar.py:60] Depth 4/1000 time = 5.628844738006592
I0123 13:05:01.990658 140427613868032 ddar.py:60] Depth 5/1000 time = 5.695680379867554
I0123 13:05:08.302236 140427613868032 ddar.py:60] Depth 6/1000 time = 6.311395883560181
I0123 13:05:14.412778 140427613868032 ddar.py:60] Depth 7/1000 time = 6.1058080196380615
I0123 13:05:23.164497 140427613868032 ddar.py:60] Depth 8/1000 time = 8.751510858535767
I0123 13:05:33.400694 140427613868032 ddar.py:60] Depth 9/1000 time = 10.235907077789307
I0123 13:05:42.970717 140427613868032 ddar.py:60] Depth 10/1000 time = 9.569648027420044
I0123 13:05:52.965240 140427613868032 ddar.py:60] Depth 11/1000 time = 9.833222389221191
I0123 13:05:52.988021 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:05:52.988151 140427613868032 alphageometry.py:566] LM output (score=-3.332221): "n : P k n c e 19 ;"
I0123 13:05:52.988188 140427613868032 alphageometry.py:567] Translation: "n = on_pline n k c e"

I0123 13:05:52.988239 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k c e ? perp m c c a"
I0123 13:05:52.988428 140427613868032 graph.py:498] 
I0123 13:05:52.988486 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n k c e ? perp m c c a
I0123 13:05:55.413084 140427613868032 ddar.py:60] Depth 1/1000 time = 2.3783390522003174
I0123 13:05:59.887117 140427613868032 ddar.py:60] Depth 2/1000 time = 4.473864555358887
I0123 13:06:04.930512 140427613868032 ddar.py:60] Depth 3/1000 time = 5.043224096298218
I0123 13:06:10.546827 140427613868032 ddar.py:60] Depth 4/1000 time = 5.616125822067261
I0123 13:06:16.340290 140427613868032 ddar.py:60] Depth 5/1000 time = 5.793237209320068
I0123 13:06:22.084184 140427613868032 ddar.py:60] Depth 6/1000 time = 5.743685245513916
I0123 13:06:27.497613 140427613868032 ddar.py:60] Depth 7/1000 time = 5.408602952957153
I0123 13:06:37.126080 140427613868032 ddar.py:60] Depth 8/1000 time = 9.628186464309692
I0123 13:06:46.275885 140427613868032 ddar.py:60] Depth 9/1000 time = 9.149471759796143
I0123 13:06:56.044634 140427613868032 ddar.py:60] Depth 10/1000 time = 9.76845407485962
I0123 13:07:05.880223 140427613868032 ddar.py:60] Depth 11/1000 time = 9.678492546081543
I0123 13:07:05.902908 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:07:05.903026 140427613868032 alphageometry.py:540] Depth 1. There are 28 nodes to expand:
I0123 13:07:05.903064 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P c g d n 19 ; x00
I0123 13:07:05.903113 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P c h d n 19 ; x00
I0123 13:07:05.903141 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : C b c n 19 D b n c n 20 ; x00
I0123 13:07:05.903166 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P e l k n 19 ; x00
I0123 13:07:05.903190 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : C b e n 19 D b n e n 20 ; x00
I0123 13:07:05.903214 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : C b f n 19 D b n f n 20 ; x00
I0123 13:07:05.903239 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P a b c n 19 ; x00
I0123 13:07:05.903263 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P a g d n 19 ; x00
I0123 13:07:05.903286 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P e g k n 19 ; x00
I0123 13:07:05.903309 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P a e c n 19 ; x00
I0123 13:07:05.903337 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : D g h h n 19 ; x00
I0123 13:07:05.903362 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : D g h h n 19 T g h h n 20 ; x00
I0123 13:07:05.903386 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : D b d d n 19 ; x00
I0123 13:07:05.903410 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P d e i n 19 ; x00
I0123 13:07:05.903434 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P d e k n 19 ; x00
I0123 13:07:05.903456 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P a e b n 19 ; x00
I0123 13:07:05.903480 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P a b c n 19 T a b a n 20 ; x00
I0123 13:07:05.903502 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P c j g n 19 ; x00
I0123 13:07:05.903526 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P a j e n 19 ; x00
I0123 13:07:05.903550 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P a e f n 19 ; x00
I0123 13:07:05.903577 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P e l j n 19 ; x00
I0123 13:07:05.903602 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P i j k n 19 ; x00
I0123 13:07:05.903625 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P b j d n 19 ; x00
I0123 13:07:05.903649 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P e n h j 19 ; x00
I0123 13:07:05.903673 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P c i h n 19 ; x00
I0123 13:07:05.903697 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P d n j m 19 ; x00
I0123 13:07:05.903719 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P b e j n 19 ; x00
I0123 13:07:05.903742 140427613868032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P k n c e 19 ; x00
I0123 13:07:05.903770 140427613868032 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P c g d n 19 ; x00
I0123 13:07:11.023182 140427613868032 alphageometry.py:566] LM output (score=-1.469279): "o : T a g n o 20 ;"
I0123 13:07:11.023321 140427613868032 alphageometry.py:567] Translation: "o = on_tline o n a g"

I0123 13:07:11.023364 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n a g ? perp m c c a"
I0123 13:07:11.023550 140427613868032 graph.py:498] 
I0123 13:07:11.023613 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n a g ? perp m c c a
I0123 13:07:12.894219 140427613868032 ddar.py:60] Depth 1/1000 time = 1.821690559387207
I0123 13:07:17.670181 140427613868032 ddar.py:60] Depth 2/1000 time = 4.775740146636963
I0123 13:07:23.126816 140427613868032 ddar.py:60] Depth 3/1000 time = 5.456339597702026
I0123 13:07:29.208573 140427613868032 ddar.py:60] Depth 4/1000 time = 6.081560134887695
I0123 13:07:35.369875 140427613868032 ddar.py:60] Depth 5/1000 time = 6.161109209060669
I0123 13:07:41.539866 140427613868032 ddar.py:60] Depth 6/1000 time = 6.169797897338867
I0123 13:07:48.247430 140427613868032 ddar.py:60] Depth 7/1000 time = 6.699702978134155
I0123 13:08:00.208359 140427613868032 ddar.py:60] Depth 8/1000 time = 11.96071457862854
I0123 13:08:11.481493 140427613868032 ddar.py:60] Depth 9/1000 time = 11.27285885810852
I0123 13:08:22.287956 140427613868032 ddar.py:60] Depth 10/1000 time = 10.806178092956543
I0123 13:08:33.268258 140427613868032 ddar.py:60] Depth 11/1000 time = 10.8012855052948
I0123 13:08:33.290358 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:08:33.290424 140427613868032 alphageometry.py:566] LM output (score=-1.536346): "o : T a b b o 20 ;"
I0123 13:08:33.290458 140427613868032 alphageometry.py:567] Translation: "o = on_tline o b a b"

I0123 13:08:33.290495 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o b a b ? perp m c c a"
I0123 13:08:33.290678 140427613868032 graph.py:498] 
I0123 13:08:33.290736 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o b a b ? perp m c c a
I0123 13:08:35.824640 140427613868032 ddar.py:60] Depth 1/1000 time = 2.486480474472046
I0123 13:08:39.925320 140427613868032 ddar.py:60] Depth 2/1000 time = 4.100500106811523
I0123 13:08:45.376801 140427613868032 ddar.py:60] Depth 3/1000 time = 5.4512810707092285
I0123 13:08:51.431658 140427613868032 ddar.py:60] Depth 4/1000 time = 6.054643154144287
I0123 13:08:58.246940 140427613868032 ddar.py:60] Depth 5/1000 time = 6.815072536468506
I0123 13:09:04.502348 140427613868032 ddar.py:60] Depth 6/1000 time = 6.2551844120025635
I0123 13:09:11.352642 140427613868032 ddar.py:60] Depth 7/1000 time = 6.842353343963623
I0123 13:09:24.533949 140427613868032 ddar.py:60] Depth 8/1000 time = 13.1810302734375
I0123 13:09:36.007032 140427613868032 ddar.py:60] Depth 9/1000 time = 11.472719669342041
I0123 13:09:46.173806 140427613868032 ddar.py:60] Depth 10/1000 time = 10.166491031646729
I0123 13:09:56.822546 140427613868032 ddar.py:60] Depth 11/1000 time = 10.4663987159729
I0123 13:09:56.844635 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:09:56.844724 140427613868032 alphageometry.py:566] LM output (score=-1.661062): "o : T a j a o 20 ;"
I0123 13:09:56.844771 140427613868032 alphageometry.py:567] Translation: "o = on_tline o a a j"

I0123 13:09:56.844824 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o a a j ? perp m c c a"
I0123 13:09:56.845031 140427613868032 graph.py:498] 
I0123 13:09:56.845090 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o a a j ? perp m c c a
I0123 13:09:59.460324 140427613868032 ddar.py:60] Depth 1/1000 time = 2.566696882247925
I0123 13:10:04.457316 140427613868032 ddar.py:60] Depth 2/1000 time = 4.996825218200684
I0123 13:10:09.900698 140427613868032 ddar.py:60] Depth 3/1000 time = 5.443201780319214
I0123 13:10:16.067112 140427613868032 ddar.py:60] Depth 4/1000 time = 6.166168451309204
I0123 13:10:21.580220 140427613868032 ddar.py:60] Depth 5/1000 time = 5.512821674346924
I0123 13:10:28.434300 140427613868032 ddar.py:60] Depth 6/1000 time = 6.8538665771484375
I0123 13:10:35.266189 140427613868032 ddar.py:60] Depth 7/1000 time = 6.824016094207764
I0123 13:10:46.974511 140427613868032 ddar.py:60] Depth 8/1000 time = 11.708091020584106
I0123 13:10:57.928978 140427613868032 ddar.py:60] Depth 9/1000 time = 10.954246044158936
I0123 13:11:07.751778 140427613868032 ddar.py:60] Depth 10/1000 time = 9.82252550125122
I0123 13:11:18.424155 140427613868032 ddar.py:60] Depth 11/1000 time = 10.490156412124634
I0123 13:11:18.446825 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:11:18.446883 140427613868032 alphageometry.py:566] LM output (score=-1.704474): "o : T b h d o 20 ;"
I0123 13:11:18.446917 140427613868032 alphageometry.py:567] Translation: "o = on_tline o d b h"

I0123 13:11:18.446954 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o d b h ? perp m c c a"
I0123 13:11:18.447131 140427613868032 graph.py:498] 
I0123 13:11:18.447190 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o d b h ? perp m c c a
I0123 13:11:21.039716 140427613868032 ddar.py:60] Depth 1/1000 time = 2.54764461517334
I0123 13:11:25.179125 140427613868032 ddar.py:60] Depth 2/1000 time = 4.13923454284668
I0123 13:11:30.727952 140427613868032 ddar.py:60] Depth 3/1000 time = 5.548611879348755
I0123 13:11:36.863558 140427613868032 ddar.py:60] Depth 4/1000 time = 6.1353864669799805
I0123 13:11:42.330562 140427613868032 ddar.py:60] Depth 5/1000 time = 5.4668073654174805
I0123 13:11:48.543701 140427613868032 ddar.py:60] Depth 6/1000 time = 6.2128496170043945
I0123 13:11:56.099701 140427613868032 ddar.py:60] Depth 7/1000 time = 7.548063039779663
I0123 13:12:07.034890 140427613868032 ddar.py:60] Depth 8/1000 time = 10.93491530418396
I0123 13:12:17.328950 140427613868032 ddar.py:60] Depth 9/1000 time = 10.29372239112854
I0123 13:12:28.381906 140427613868032 ddar.py:60] Depth 10/1000 time = 11.052648782730103
I0123 13:12:38.645348 140427613868032 ddar.py:60] Depth 11/1000 time = 10.082121849060059
I0123 13:12:49.547007 140427613868032 ddar.py:60] Depth 12/1000 time = 10.86807632446289
I0123 13:12:49.547448 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:12:49.547515 140427613868032 alphageometry.py:566] LM output (score=-1.767761): "o : T b h h o 20 ;"
I0123 13:12:49.547550 140427613868032 alphageometry.py:567] Translation: "o = on_tline o h b h"

I0123 13:12:49.547588 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o h b h ? perp m c c a"
I0123 13:12:49.547779 140427613868032 graph.py:498] 
I0123 13:12:49.547840 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o h b h ? perp m c c a
I0123 13:12:52.152822 140427613868032 ddar.py:60] Depth 1/1000 time = 2.556861639022827
I0123 13:12:56.282037 140427613868032 ddar.py:60] Depth 2/1000 time = 4.129044771194458
I0123 13:13:01.976425 140427613868032 ddar.py:60] Depth 3/1000 time = 5.694112777709961
I0123 13:13:08.139120 140427613868032 ddar.py:60] Depth 4/1000 time = 6.16238808631897
I0123 13:13:14.435342 140427613868032 ddar.py:60] Depth 5/1000 time = 6.2960333824157715
I0123 13:13:20.004212 140427613868032 ddar.py:60] Depth 6/1000 time = 5.5686023235321045
I0123 13:13:26.890919 140427613868032 ddar.py:60] Depth 7/1000 time = 6.878719091415405
I0123 13:13:41.659123 140427613868032 ddar.py:60] Depth 8/1000 time = 14.767908096313477
I0123 13:13:53.086890 140427613868032 ddar.py:60] Depth 9/1000 time = 11.427397012710571
I0123 13:14:03.123686 140427613868032 ddar.py:60] Depth 10/1000 time = 10.036529302597046
I0123 13:14:13.960407 140427613868032 ddar.py:60] Depth 11/1000 time = 10.656321048736572
I0123 13:14:13.982542 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:14:13.982640 140427613868032 alphageometry.py:566] LM output (score=-1.829217): "o : T a j j o 20 ;"
I0123 13:14:13.982676 140427613868032 alphageometry.py:567] Translation: "o = on_tline o j a j"

I0123 13:14:13.982727 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o j a j ? perp m c c a"
I0123 13:14:13.982931 140427613868032 graph.py:498] 
I0123 13:14:13.982989 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o j a j ? perp m c c a
I0123 13:14:15.832695 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8038289546966553
I0123 13:14:21.062666 140427613868032 ddar.py:60] Depth 2/1000 time = 5.229772567749023
I0123 13:14:25.819536 140427613868032 ddar.py:60] Depth 3/1000 time = 4.756680727005005
I0123 13:14:31.971766 140427613868032 ddar.py:60] Depth 4/1000 time = 6.151984930038452
I0123 13:14:38.318808 140427613868032 ddar.py:60] Depth 5/1000 time = 6.346659183502197
I0123 13:14:44.583165 140427613868032 ddar.py:60] Depth 6/1000 time = 6.264120817184448
I0123 13:14:51.474081 140427613868032 ddar.py:60] Depth 7/1000 time = 6.8832902908325195
I0123 13:15:05.847334 140427613868032 ddar.py:60] Depth 8/1000 time = 14.37291669845581
I0123 13:15:16.538447 140427613868032 ddar.py:60] Depth 9/1000 time = 10.690822839736938
I0123 13:15:27.409532 140427613868032 ddar.py:60] Depth 10/1000 time = 10.870805501937866
I0123 13:15:38.402984 140427613868032 ddar.py:60] Depth 11/1000 time = 10.811642169952393
I0123 13:15:38.425131 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:15:38.425198 140427613868032 alphageometry.py:566] LM output (score=-1.985313): "o : T a h n o 20 ;"
I0123 13:15:38.425233 140427613868032 alphageometry.py:567] Translation: "o = on_tline o n a h"

I0123 13:15:38.425270 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n a h ? perp m c c a"
I0123 13:15:38.425449 140427613868032 graph.py:498] 
I0123 13:15:38.425508 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n a h ? perp m c c a
I0123 13:15:40.271871 140427613868032 ddar.py:60] Depth 1/1000 time = 1.801192045211792
I0123 13:15:45.421006 140427613868032 ddar.py:60] Depth 2/1000 time = 5.148970603942871
I0123 13:15:51.111392 140427613868032 ddar.py:60] Depth 3/1000 time = 5.690196514129639
I0123 13:15:56.521068 140427613868032 ddar.py:60] Depth 4/1000 time = 5.409464597702026
I0123 13:16:02.805275 140427613868032 ddar.py:60] Depth 5/1000 time = 6.283995866775513
I0123 13:16:09.148183 140427613868032 ddar.py:60] Depth 6/1000 time = 6.342664480209351
I0123 13:16:15.987713 140427613868032 ddar.py:60] Depth 7/1000 time = 6.832042217254639
I0123 13:16:25.392782 140427613868032 ddar.py:60] Depth 8/1000 time = 9.404781341552734
I0123 13:16:36.536548 140427613868032 ddar.py:60] Depth 9/1000 time = 11.143402814865112
I0123 13:16:46.791960 140427613868032 ddar.py:60] Depth 10/1000 time = 10.255069255828857
I0123 13:16:56.868591 140427613868032 ddar.py:60] Depth 11/1000 time = 9.896233081817627
I0123 13:16:56.891911 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:16:56.891979 140427613868032 alphageometry.py:566] LM output (score=-2.094461): "o : D g h h o 20 ;"
I0123 13:16:56.892013 140427613868032 alphageometry.py:567] Translation: "o = on_circle o h g"

I0123 13:16:56.892051 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_circle o h g ? perp m c c a"
I0123 13:16:56.892235 140427613868032 graph.py:498] 
I0123 13:16:56.892299 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_circle o h g ? perp m c c a
I0123 13:16:59.624174 140427613868032 ddar.py:60] Depth 1/1000 time = 2.675623893737793
I0123 13:17:04.795649 140427613868032 ddar.py:60] Depth 2/1000 time = 5.171287536621094
I0123 13:17:09.674784 140427613868032 ddar.py:60] Depth 3/1000 time = 4.878920555114746
I0123 13:17:15.966497 140427613868032 ddar.py:60] Depth 4/1000 time = 6.291518688201904
I0123 13:17:22.446840 140427613868032 ddar.py:60] Depth 5/1000 time = 6.480100393295288
I0123 13:17:28.897454 140427613868032 ddar.py:60] Depth 6/1000 time = 6.450346946716309
I0123 13:17:35.786078 140427613868032 ddar.py:60] Depth 7/1000 time = 6.883745431900024
I0123 13:17:46.801198 140427613868032 ddar.py:60] Depth 8/1000 time = 11.014906406402588
I0123 13:17:56.978038 140427613868032 ddar.py:60] Depth 9/1000 time = 10.176577091217041
I0123 13:18:07.946912 140427613868032 ddar.py:60] Depth 10/1000 time = 10.968500137329102
I0123 13:18:18.166051 140427613868032 ddar.py:60] Depth 11/1000 time = 10.058375835418701
I0123 13:18:18.188926 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:18:18.189015 140427613868032 alphageometry.py:566] LM output (score=-2.180028): "o : T a h m o 20 ;"
I0123 13:18:18.189052 140427613868032 alphageometry.py:567] Translation: "o = on_tline o m a h"

I0123 13:18:18.189103 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o m a h ? perp m c c a"
I0123 13:18:18.189300 140427613868032 graph.py:498] 
I0123 13:18:18.189360 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o m a h ? perp m c c a
I0123 13:18:20.908468 140427613868032 ddar.py:60] Depth 1/1000 time = 2.6683249473571777
I0123 13:18:25.345317 140427613868032 ddar.py:60] Depth 2/1000 time = 4.436671257019043
I0123 13:18:31.045480 140427613868032 ddar.py:60] Depth 3/1000 time = 5.699989318847656
I0123 13:18:36.650946 140427613868032 ddar.py:60] Depth 4/1000 time = 5.605256795883179
I0123 13:18:43.038279 140427613868032 ddar.py:60] Depth 5/1000 time = 6.387106895446777
I0123 13:18:49.481711 140427613868032 ddar.py:60] Depth 6/1000 time = 6.443236351013184
I0123 13:18:56.660674 140427613868032 ddar.py:60] Depth 7/1000 time = 7.171489477157593
I0123 13:19:11.794417 140427613868032 ddar.py:60] Depth 8/1000 time = 15.133422613143921
I0123 13:19:22.575107 140427613868032 ddar.py:60] Depth 9/1000 time = 10.780305862426758
I0123 13:19:33.612847 140427613868032 ddar.py:60] Depth 10/1000 time = 11.037469387054443
I0123 13:19:43.985046 140427613868032 ddar.py:60] Depth 11/1000 time = 10.190717458724976
I0123 13:19:44.007946 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:19:44.008020 140427613868032 alphageometry.py:566] LM output (score=-2.229860): "o : T a h h o 20 ;"
I0123 13:19:44.008055 140427613868032 alphageometry.py:567] Translation: "o = on_tline o h a h"

I0123 13:19:44.008094 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o h a h ? perp m c c a"
I0123 13:19:44.008273 140427613868032 graph.py:498] 
I0123 13:19:44.008332 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o h a h ? perp m c c a
I0123 13:19:46.715362 140427613868032 ddar.py:60] Depth 1/1000 time = 2.658998489379883
I0123 13:19:50.912336 140427613868032 ddar.py:60] Depth 2/1000 time = 4.196686267852783
I0123 13:19:56.569745 140427613868032 ddar.py:60] Depth 3/1000 time = 5.657214164733887
I0123 13:20:02.905265 140427613868032 ddar.py:60] Depth 4/1000 time = 6.335322618484497
I0123 13:20:09.454573 140427613868032 ddar.py:60] Depth 5/1000 time = 6.549095392227173
I0123 13:20:15.051844 140427613868032 ddar.py:60] Depth 6/1000 time = 5.597052335739136
I0123 13:20:22.166920 140427613868032 ddar.py:60] Depth 7/1000 time = 7.107375621795654
I0123 13:20:34.924672 140427613868032 ddar.py:60] Depth 8/1000 time = 12.757505655288696
I0123 13:20:46.250125 140427613868032 ddar.py:60] Depth 9/1000 time = 11.32518482208252
I0123 13:20:56.710408 140427613868032 ddar.py:60] Depth 10/1000 time = 10.460026979446411
I0123 13:21:07.206541 140427613868032 ddar.py:60] Depth 11/1000 time = 10.313173294067383
I0123 13:21:07.228622 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:21:07.228689 140427613868032 alphageometry.py:566] LM output (score=-2.267234): "o : T a h f o 20 ;"
I0123 13:21:07.228724 140427613868032 alphageometry.py:567] Translation: "o = on_tline o f a h"

I0123 13:21:07.228765 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o f a h ? perp m c c a"
I0123 13:21:07.228941 140427613868032 graph.py:498] 
I0123 13:21:07.229001 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o f a h ? perp m c c a
I0123 13:21:09.915935 140427613868032 ddar.py:60] Depth 1/1000 time = 2.6410374641418457
I0123 13:21:14.989374 140427613868032 ddar.py:60] Depth 2/1000 time = 5.073264122009277
I0123 13:21:19.894185 140427613868032 ddar.py:60] Depth 3/1000 time = 4.904574632644653
I0123 13:21:26.328467 140427613868032 ddar.py:60] Depth 4/1000 time = 6.4339599609375
I0123 13:21:31.931585 140427613868032 ddar.py:60] Depth 5/1000 time = 5.6029253005981445
I0123 13:21:38.421274 140427613868032 ddar.py:60] Depth 6/1000 time = 6.489410400390625
I0123 13:21:45.678591 140427613868032 ddar.py:60] Depth 7/1000 time = 7.248920202255249
I0123 13:21:58.626776 140427613868032 ddar.py:60] Depth 8/1000 time = 12.947905778884888
I0123 13:22:10.490394 140427613868032 ddar.py:60] Depth 9/1000 time = 11.863259553909302
I0123 13:22:20.974988 140427613868032 ddar.py:60] Depth 10/1000 time = 10.48431134223938
I0123 13:22:31.370080 140427613868032 ddar.py:60] Depth 11/1000 time = 10.216212272644043
I0123 13:22:31.392381 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:22:31.392461 140427613868032 alphageometry.py:566] LM output (score=-2.379285): "o : T a e n o 20 ;"
I0123 13:22:31.392499 140427613868032 alphageometry.py:567] Translation: "o = on_tline o n a e"

I0123 13:22:31.392538 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n a e ? perp m c c a"
I0123 13:22:31.392738 140427613868032 graph.py:498] 
I0123 13:22:31.392808 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n a e ? perp m c c a
I0123 13:22:34.164361 140427613868032 ddar.py:60] Depth 1/1000 time = 2.7251789569854736
I0123 13:22:39.285038 140427613868032 ddar.py:60] Depth 2/1000 time = 5.120441436767578
I0123 13:22:44.156275 140427613868032 ddar.py:60] Depth 3/1000 time = 4.871057510375977
I0123 13:22:50.585585 140427613868032 ddar.py:60] Depth 4/1000 time = 6.429114580154419
I0123 13:22:56.332292 140427613868032 ddar.py:60] Depth 5/1000 time = 5.746477842330933
I0123 13:23:02.818767 140427613868032 ddar.py:60] Depth 6/1000 time = 6.4862425327301025
I0123 13:23:10.029069 140427613868032 ddar.py:60] Depth 7/1000 time = 7.202390909194946
I0123 13:23:23.091554 140427613868032 ddar.py:60] Depth 8/1000 time = 13.062138557434082
I0123 13:23:33.758490 140427613868032 ddar.py:60] Depth 9/1000 time = 10.666619777679443
I0123 13:23:44.235035 140427613868032 ddar.py:60] Depth 10/1000 time = 10.476146221160889
I0123 13:23:55.668732 140427613868032 ddar.py:60] Depth 11/1000 time = 11.260043859481812
I0123 13:23:55.691953 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:23:55.692015 140427613868032 alphageometry.py:566] LM output (score=-2.508320): "o : T b h n o 20 ;"
I0123 13:23:55.692050 140427613868032 alphageometry.py:567] Translation: "o = on_tline o n b h"

I0123 13:23:55.692088 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n b h ? perp m c c a"
I0123 13:23:55.692269 140427613868032 graph.py:498] 
I0123 13:23:55.692330 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n b h ? perp m c c a
I0123 13:23:57.604969 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8634700775146484
I0123 13:24:02.751166 140427613868032 ddar.py:60] Depth 2/1000 time = 5.146029233932495
I0123 13:24:07.713988 140427613868032 ddar.py:60] Depth 3/1000 time = 4.962567329406738
I0123 13:24:14.133749 140427613868032 ddar.py:60] Depth 4/1000 time = 6.419443130493164
I0123 13:24:19.732630 140427613868032 ddar.py:60] Depth 5/1000 time = 5.598682880401611
I0123 13:24:26.313356 140427613868032 ddar.py:60] Depth 6/1000 time = 6.580459117889404
I0123 13:24:33.464236 140427613868032 ddar.py:60] Depth 7/1000 time = 7.142975568771362
I0123 13:24:44.191000 140427613868032 ddar.py:60] Depth 8/1000 time = 10.72649884223938
I0123 13:24:54.599293 140427613868032 ddar.py:60] Depth 9/1000 time = 10.407949924468994
I0123 13:25:04.806988 140427613868032 ddar.py:60] Depth 10/1000 time = 10.207386493682861
I0123 13:25:15.980537 140427613868032 ddar.py:60] Depth 11/1000 time = 10.990003108978271
I0123 13:25:16.003611 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:25:16.003678 140427613868032 alphageometry.py:566] LM output (score=-2.514519): "o : T a j m o 20 ;"
I0123 13:25:16.003713 140427613868032 alphageometry.py:567] Translation: "o = on_tline o m a j"

I0123 13:25:16.003749 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o m a j ? perp m c c a"
I0123 13:25:16.003947 140427613868032 graph.py:498] 
I0123 13:25:16.004008 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o m a j ? perp m c c a
I0123 13:25:17.872889 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8235599994659424
I0123 13:25:23.068192 140427613868032 ddar.py:60] Depth 2/1000 time = 5.195120334625244
I0123 13:25:28.163477 140427613868032 ddar.py:60] Depth 3/1000 time = 5.095098972320557
I0123 13:25:34.893938 140427613868032 ddar.py:60] Depth 4/1000 time = 6.730275392532349
I0123 13:25:40.633290 140427613868032 ddar.py:60] Depth 5/1000 time = 5.739095211029053
I0123 13:25:47.221525 140427613868032 ddar.py:60] Depth 6/1000 time = 6.587918758392334
I0123 13:25:53.472802 140427613868032 ddar.py:60] Depth 7/1000 time = 6.24349045753479
I0123 13:26:07.345900 140427613868032 ddar.py:60] Depth 8/1000 time = 13.872868061065674
I0123 13:26:18.097485 140427613868032 ddar.py:60] Depth 9/1000 time = 10.7512366771698
I0123 13:26:28.546958 140427613868032 ddar.py:60] Depth 10/1000 time = 10.449085235595703
I0123 13:26:39.217445 140427613868032 ddar.py:60] Depth 11/1000 time = 10.489603757858276
I0123 13:26:39.239538 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:26:39.239610 140427613868032 alphageometry.py:566] LM output (score=-2.596191): "o : T a l a o 20 ;"
I0123 13:26:39.239647 140427613868032 alphageometry.py:567] Translation: "o = on_tline o a a l"

I0123 13:26:39.239691 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o a a l ? perp m c c a"
I0123 13:26:39.239879 140427613868032 graph.py:498] 
I0123 13:26:39.239940 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o a a l ? perp m c c a
I0123 13:26:41.991042 140427613868032 ddar.py:60] Depth 1/1000 time = 2.7046399116516113
I0123 13:26:46.564250 140427613868032 ddar.py:60] Depth 2/1000 time = 4.57304835319519
I0123 13:26:52.546921 140427613868032 ddar.py:60] Depth 3/1000 time = 5.98225736618042
I0123 13:26:58.186689 140427613868032 ddar.py:60] Depth 4/1000 time = 5.6395368576049805
I0123 13:27:04.887859 140427613868032 ddar.py:60] Depth 5/1000 time = 6.700960397720337
I0123 13:27:10.661601 140427613868032 ddar.py:60] Depth 6/1000 time = 5.773471117019653
I0123 13:27:17.974912 140427613868032 ddar.py:60] Depth 7/1000 time = 7.305348873138428
I0123 13:27:30.010641 140427613868032 ddar.py:60] Depth 8/1000 time = 12.035454750061035
I0123 13:27:40.592623 140427613868032 ddar.py:60] Depth 9/1000 time = 10.581620931625366
I0123 13:27:50.859906 140427613868032 ddar.py:60] Depth 10/1000 time = 10.26696491241455
I0123 13:28:01.241656 140427613868032 ddar.py:60] Depth 11/1000 time = 10.202423095703125
I0123 13:28:01.264638 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:28:01.264707 140427613868032 alphageometry.py:566] LM output (score=-2.658035): "o : T c l l o 20 ;"
I0123 13:28:01.264752 140427613868032 alphageometry.py:567] Translation: "o = on_tline o l c l"

I0123 13:28:01.264791 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l c l ? perp m c c a"
I0123 13:28:01.264971 140427613868032 graph.py:498] 
I0123 13:28:01.265028 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l c l ? perp m c c a
I0123 13:28:04.088798 140427613868032 ddar.py:60] Depth 1/1000 time = 2.7784583568573
I0123 13:28:08.698035 140427613868032 ddar.py:60] Depth 2/1000 time = 4.609033584594727
I0123 13:28:13.658756 140427613868032 ddar.py:60] Depth 3/1000 time = 4.9605183601379395
I0123 13:28:20.089966 140427613868032 ddar.py:60] Depth 4/1000 time = 6.4310314655303955
I0123 13:28:25.769993 140427613868032 ddar.py:60] Depth 5/1000 time = 5.679744005203247
I0123 13:28:32.320075 140427613868032 ddar.py:60] Depth 6/1000 time = 6.549734115600586
I0123 13:28:38.361017 140427613868032 ddar.py:60] Depth 7/1000 time = 6.035337209701538
I0123 13:28:49.220958 140427613868032 ddar.py:60] Depth 8/1000 time = 10.859657526016235
I0123 13:28:59.899195 140427613868032 ddar.py:60] Depth 9/1000 time = 10.677677154541016
I0123 13:29:10.500088 140427613868032 ddar.py:60] Depth 10/1000 time = 10.600607633590698
I0123 13:29:21.008193 140427613868032 ddar.py:60] Depth 11/1000 time = 10.327750205993652
I0123 13:29:21.031486 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:21.031579 140427613868032 alphageometry.py:566] LM output (score=-2.666868): "o : T a j f o 20 ;"
I0123 13:29:21.031614 140427613868032 alphageometry.py:567] Translation: "o = on_tline o f a j"

I0123 13:29:21.031663 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o f a j ? perp m c c a"
I0123 13:29:21.031868 140427613868032 graph.py:498] 
I0123 13:29:21.031926 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o f a j ? perp m c c a
I0123 13:29:23.870608 140427613868032 ddar.py:60] Depth 1/1000 time = 2.792088031768799
I0123 13:29:29.294165 140427613868032 ddar.py:60] Depth 2/1000 time = 5.423373222351074
I0123 13:29:34.217308 140427613868032 ddar.py:60] Depth 3/1000 time = 4.922974348068237
I0123 13:29:39.771554 140427613868032 ddar.py:60] Depth 4/1000 time = 5.5539870262146
I0123 13:29:46.332676 140427613868032 ddar.py:60] Depth 5/1000 time = 6.560794353485107
I0123 13:29:52.948342 140427613868032 ddar.py:60] Depth 6/1000 time = 6.6154680252075195
I0123 13:29:59.409741 140427613868032 ddar.py:60] Depth 7/1000 time = 6.453265905380249
I0123 13:30:12.269198 140427613868032 ddar.py:60] Depth 8/1000 time = 12.859131336212158
I0123 13:30:23.405000 140427613868032 ddar.py:60] Depth 9/1000 time = 11.135562181472778
I0123 13:30:34.399513 140427613868032 ddar.py:60] Depth 10/1000 time = 10.994187831878662
I0123 13:30:45.350959 140427613868032 ddar.py:60] Depth 11/1000 time = 10.771430015563965
I0123 13:30:45.372873 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:30:45.372937 140427613868032 alphageometry.py:566] LM output (score=-2.723201): "o : T j l l o 20 ;"
I0123 13:30:45.372972 140427613868032 alphageometry.py:567] Translation: "o = on_tline o l j l"

I0123 13:30:45.373011 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l j l ? perp m c c a"
I0123 13:30:45.373200 140427613868032 graph.py:498] 
I0123 13:30:45.373260 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l j l ? perp m c c a
I0123 13:30:48.174769 140427613868032 ddar.py:60] Depth 1/1000 time = 2.755143165588379
I0123 13:30:52.590341 140427613868032 ddar.py:60] Depth 2/1000 time = 4.4153430461883545
I0123 13:30:57.667951 140427613868032 ddar.py:60] Depth 3/1000 time = 5.07729959487915
I0123 13:31:04.184720 140427613868032 ddar.py:60] Depth 4/1000 time = 6.516578674316406
I0123 13:31:09.928393 140427613868032 ddar.py:60] Depth 5/1000 time = 5.743483304977417
I0123 13:31:16.751868 140427613868032 ddar.py:60] Depth 6/1000 time = 6.8232526779174805
I0123 13:31:23.116878 140427613868032 ddar.py:60] Depth 7/1000 time = 6.357909679412842
I0123 13:31:36.529469 140427613868032 ddar.py:60] Depth 8/1000 time = 13.412299394607544
I0123 13:31:47.744917 140427613868032 ddar.py:60] Depth 9/1000 time = 11.214998006820679
I0123 13:31:59.674437 140427613868032 ddar.py:60] Depth 10/1000 time = 11.929121971130371
I0123 13:32:10.800754 140427613868032 ddar.py:60] Depth 11/1000 time = 10.947041511535645
I0123 13:32:10.823182 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:32:10.823247 140427613868032 alphageometry.py:566] LM output (score=-2.785016): "o : D b d d o 20 ;"
I0123 13:32:10.823280 140427613868032 alphageometry.py:567] Translation: "o = on_circle o d b"

I0123 13:32:10.823318 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_circle o d b ? perp m c c a"
I0123 13:32:10.823499 140427613868032 graph.py:498] 
I0123 13:32:10.823559 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_circle o d b ? perp m c c a
I0123 13:32:13.154145 140427613868032 ddar.py:60] Depth 1/1000 time = 2.2691802978515625
I0123 13:32:19.861130 140427613868032 ddar.py:60] Depth 2/1000 time = 6.706786870956421
I0123 13:32:26.119179 140427613868032 ddar.py:60] Depth 3/1000 time = 6.257772922515869
I0123 13:32:34.056465 140427613868032 ddar.py:60] Depth 4/1000 time = 7.9369871616363525
I0123 13:32:41.111702 140427613868032 ddar.py:60] Depth 5/1000 time = 7.055015563964844
I0123 13:32:49.151717 140427613868032 ddar.py:60] Depth 6/1000 time = 8.0397789478302
I0123 13:32:57.790414 140427613868032 ddar.py:60] Depth 7/1000 time = 8.63298487663269
I0123 13:33:09.133054 140427613868032 ddar.py:60] Depth 8/1000 time = 11.34236764907837
I0123 13:33:22.533941 140427613868032 ddar.py:60] Depth 9/1000 time = 13.400493144989014
I0123 13:33:35.217936 140427613868032 ddar.py:60] Depth 10/1000 time = 12.683637380599976
I0123 13:33:47.966288 140427613868032 ddar.py:60] Depth 11/1000 time = 12.52942180633545
I0123 13:33:47.989277 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:33:47.989342 140427613868032 alphageometry.py:566] LM output (score=-2.848548): "o : T c j k o 20 ;"
I0123 13:33:47.989376 140427613868032 alphageometry.py:567] Translation: "o = on_tline o k c j"

I0123 13:33:47.989413 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o k c j ? perp m c c a"
I0123 13:33:47.989592 140427613868032 graph.py:498] 
I0123 13:33:47.989696 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o k c j ? perp m c c a
I0123 13:33:49.867667 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8333346843719482
I0123 13:33:54.474901 140427613868032 ddar.py:60] Depth 2/1000 time = 4.607050657272339
I0123 13:34:00.429691 140427613868032 ddar.py:60] Depth 3/1000 time = 5.954538106918335
I0123 13:34:06.106066 140427613868032 ddar.py:60] Depth 4/1000 time = 5.676063776016235
I0123 13:34:12.808230 140427613868032 ddar.py:60] Depth 5/1000 time = 6.701955556869507
I0123 13:34:18.576614 140427613868032 ddar.py:60] Depth 6/1000 time = 5.768120050430298
I0123 13:34:25.866895 140427613868032 ddar.py:60] Depth 7/1000 time = 7.284428834915161
I0123 13:34:39.923760 140427613868032 ddar.py:60] Depth 8/1000 time = 14.056570529937744
I0123 13:34:51.014114 140427613868032 ddar.py:60] Depth 9/1000 time = 11.089966773986816
I0123 13:35:01.989116 140427613868032 ddar.py:60] Depth 10/1000 time = 10.974738597869873
I0123 13:35:12.865312 140427613868032 ddar.py:60] Depth 11/1000 time = 10.692753553390503
I0123 13:35:12.888649 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:35:12.888739 140427613868032 alphageometry.py:566] LM output (score=-2.915913): "o : T a e l o 20 ;"
I0123 13:35:12.888774 140427613868032 alphageometry.py:567] Translation: "o = on_tline o l a e"

I0123 13:35:12.888827 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l a e ? perp m c c a"
I0123 13:35:12.889027 140427613868032 graph.py:498] 
I0123 13:35:12.889085 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l a e ? perp m c c a
I0123 13:35:14.740751 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8059287071228027
I0123 13:35:19.144850 140427613868032 ddar.py:60] Depth 2/1000 time = 4.403902053833008
I0123 13:35:25.155709 140427613868032 ddar.py:60] Depth 3/1000 time = 6.01068902015686
I0123 13:35:30.873603 140427613868032 ddar.py:60] Depth 4/1000 time = 5.717663288116455
I0123 13:35:36.718863 140427613868032 ddar.py:60] Depth 5/1000 time = 5.844919681549072
I0123 13:35:43.444234 140427613868032 ddar.py:60] Depth 6/1000 time = 6.725160360336304
I0123 13:35:50.943219 140427613868032 ddar.py:60] Depth 7/1000 time = 7.491353511810303
I0123 13:36:01.882565 140427613868032 ddar.py:60] Depth 8/1000 time = 10.939108848571777
I0123 13:36:13.675043 140427613868032 ddar.py:60] Depth 9/1000 time = 11.792152643203735
I0123 13:36:25.657903 140427613868032 ddar.py:60] Depth 10/1000 time = 11.982476472854614
I0123 13:36:36.801418 140427613868032 ddar.py:60] Depth 11/1000 time = 10.95704460144043
I0123 13:36:36.824046 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:36:36.824111 140427613868032 alphageometry.py:566] LM output (score=-2.938578): "o : T k l l o 20 ;"
I0123 13:36:36.824147 140427613868032 alphageometry.py:567] Translation: "o = on_tline o l k l"

I0123 13:36:36.824185 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l k l ? perp m c c a"
I0123 13:36:36.824368 140427613868032 graph.py:498] 
I0123 13:36:36.824427 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l k l ? perp m c c a
I0123 13:36:38.692251 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8187847137451172
I0123 13:36:43.382101 140427613868032 ddar.py:60] Depth 2/1000 time = 4.689670562744141
I0123 13:36:49.528187 140427613868032 ddar.py:60] Depth 3/1000 time = 6.14590048789978
I0123 13:36:55.923454 140427613868032 ddar.py:60] Depth 4/1000 time = 6.395069122314453
I0123 13:37:03.401094 140427613868032 ddar.py:60] Depth 5/1000 time = 7.477375268936157
I0123 13:37:09.924313 140427613868032 ddar.py:60] Depth 6/1000 time = 6.522871255874634
I0123 13:37:18.165692 140427613868032 ddar.py:60] Depth 7/1000 time = 8.236371517181396
I0123 13:37:31.075011 140427613868032 ddar.py:60] Depth 8/1000 time = 12.909077644348145
I0123 13:37:42.810046 140427613868032 ddar.py:60] Depth 9/1000 time = 11.734733819961548
I0123 13:37:54.296806 140427613868032 ddar.py:60] Depth 10/1000 time = 11.486464738845825
I0123 13:38:05.517638 140427613868032 ddar.py:60] Depth 11/1000 time = 11.035455226898193
I0123 13:38:05.540492 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:38:05.540556 140427613868032 alphageometry.py:566] LM output (score=-2.979696): "o : T a h l o 20 ;"
I0123 13:38:05.540591 140427613868032 alphageometry.py:567] Translation: "o = on_tline o l a h"

I0123 13:38:05.540629 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l a h ? perp m c c a"
I0123 13:38:05.540806 140427613868032 graph.py:498] 
I0123 13:38:05.540866 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o l a h ? perp m c c a
I0123 13:38:07.404973 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8192155361175537
I0123 13:38:11.778551 140427613868032 ddar.py:60] Depth 2/1000 time = 4.373406648635864
I0123 13:38:17.950303 140427613868032 ddar.py:60] Depth 3/1000 time = 6.171492576599121
I0123 13:38:23.700853 140427613868032 ddar.py:60] Depth 4/1000 time = 5.750218152999878
I0123 13:38:29.443876 140427613868032 ddar.py:60] Depth 5/1000 time = 5.74281644821167
I0123 13:38:36.283203 140427613868032 ddar.py:60] Depth 6/1000 time = 6.839046955108643
I0123 13:38:42.682032 140427613868032 ddar.py:60] Depth 7/1000 time = 6.391059637069702
I0123 13:38:56.229426 140427613868032 ddar.py:60] Depth 8/1000 time = 13.547084331512451
I0123 13:39:07.628176 140427613868032 ddar.py:60] Depth 9/1000 time = 11.398319482803345
I0123 13:39:18.655434 140427613868032 ddar.py:60] Depth 10/1000 time = 11.026951789855957
I0123 13:39:29.500646 140427613868032 ddar.py:60] Depth 11/1000 time = 10.66097617149353
I0123 13:39:29.522343 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:39:29.522444 140427613868032 alphageometry.py:566] LM output (score=-2.994693): "o : T a f d o 20 ;"
I0123 13:39:29.522481 140427613868032 alphageometry.py:567] Translation: "o = on_tline o d a f"

I0123 13:39:29.522535 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o d a f ? perp m c c a"
I0123 13:39:29.522741 140427613868032 graph.py:498] 
I0123 13:39:29.522801 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o d a f ? perp m c c a
I0123 13:39:31.384495 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8167624473571777
I0123 13:39:37.056278 140427613868032 ddar.py:60] Depth 2/1000 time = 5.671585559844971
I0123 13:39:41.982419 140427613868032 ddar.py:60] Depth 3/1000 time = 4.925954818725586
I0123 13:39:47.497171 140427613868032 ddar.py:60] Depth 4/1000 time = 5.514509677886963
I0123 13:39:54.208366 140427613868032 ddar.py:60] Depth 5/1000 time = 6.710856914520264
I0123 13:39:59.844964 140427613868032 ddar.py:60] Depth 6/1000 time = 5.636387825012207
I0123 13:40:05.871742 140427613868032 ddar.py:60] Depth 7/1000 time = 6.021820306777954
I0123 13:40:16.953604 140427613868032 ddar.py:60] Depth 8/1000 time = 11.081534624099731
I0123 13:40:28.033029 140427613868032 ddar.py:60] Depth 9/1000 time = 11.079130172729492
I0123 13:40:38.970906 140427613868032 ddar.py:60] Depth 10/1000 time = 10.937498331069946
I0123 13:40:48.860292 140427613868032 ddar.py:60] Depth 11/1000 time = 9.712616443634033
I0123 13:40:59.630518 140427613868032 ddar.py:60] Depth 12/1000 time = 10.72908091545105
I0123 13:40:59.630781 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:40:59.630853 140427613868032 alphageometry.py:566] LM output (score=-3.003621): "o : T a h d o 20 ;"
I0123 13:40:59.630890 140427613868032 alphageometry.py:567] Translation: "o = on_tline o d a h"

I0123 13:40:59.630929 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o d a h ? perp m c c a"
I0123 13:40:59.631123 140427613868032 graph.py:498] 
I0123 13:40:59.631185 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o d a h ? perp m c c a
I0123 13:41:02.575790 140427613868032 ddar.py:60] Depth 1/1000 time = 2.900369167327881
I0123 13:41:07.273105 140427613868032 ddar.py:60] Depth 2/1000 time = 4.697047233581543
I0123 13:41:13.301404 140427613868032 ddar.py:60] Depth 3/1000 time = 6.028103351593018
I0123 13:41:19.055904 140427613868032 ddar.py:60] Depth 4/1000 time = 5.754279136657715
I0123 13:41:24.953329 140427613868032 ddar.py:60] Depth 5/1000 time = 5.897189140319824
I0123 13:41:31.835019 140427613868032 ddar.py:60] Depth 6/1000 time = 6.881427764892578
I0123 13:41:38.289729 140427613868032 ddar.py:60] Depth 7/1000 time = 6.446978330612183
I0123 13:41:49.122170 140427613868032 ddar.py:60] Depth 8/1000 time = 10.832114219665527
I0123 13:42:01.200631 140427613868032 ddar.py:60] Depth 9/1000 time = 12.078155755996704
I0123 13:42:12.686498 140427613868032 ddar.py:60] Depth 10/1000 time = 11.48546552658081
I0123 13:42:24.200979 140427613868032 ddar.py:60] Depth 11/1000 time = 11.329672574996948
I0123 13:42:34.452768 140427613868032 ddar.py:60] Depth 12/1000 time = 10.21895432472229
I0123 13:42:34.453278 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:42:34.453371 140427613868032 alphageometry.py:566] LM output (score=-3.021557): "o : T e g n o 20 ;"
I0123 13:42:34.453408 140427613868032 alphageometry.py:567] Translation: "o = on_tline o n e g"

I0123 13:42:34.453456 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n e g ? perp m c c a"
I0123 13:42:34.453668 140427613868032 graph.py:498] 
I0123 13:42:34.453732 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n e g ? perp m c c a
I0123 13:42:37.477951 140427613868032 ddar.py:60] Depth 1/1000 time = 2.9731638431549072
I0123 13:42:41.974201 140427613868032 ddar.py:60] Depth 2/1000 time = 4.49602198600769
I0123 13:42:47.099206 140427613868032 ddar.py:60] Depth 3/1000 time = 5.1248252391815186
I0123 13:42:53.983625 140427613868032 ddar.py:60] Depth 4/1000 time = 6.8841776847839355
I0123 13:42:59.945950 140427613868032 ddar.py:60] Depth 5/1000 time = 5.962002754211426
I0123 13:43:05.797276 140427613868032 ddar.py:60] Depth 6/1000 time = 5.851132154464722
I0123 13:43:13.428502 140427613868032 ddar.py:60] Depth 7/1000 time = 7.623181581497192
I0123 13:43:24.891978 140427613868032 ddar.py:60] Depth 8/1000 time = 11.463147163391113
I0123 13:43:35.731419 140427613868032 ddar.py:60] Depth 9/1000 time = 10.839174747467041
I0123 13:43:46.320025 140427613868032 ddar.py:60] Depth 10/1000 time = 10.588233470916748
I0123 13:43:57.022053 140427613868032 ddar.py:60] Depth 11/1000 time = 10.521496057510376
I0123 13:43:57.044641 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:43:57.044703 140427613868032 alphageometry.py:566] LM output (score=-3.030156): "o : D a g a o 20 ;"
I0123 13:43:57.044738 140427613868032 alphageometry.py:567] Translation: "o = on_circle o a g"

I0123 13:43:57.044776 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_circle o a g ? perp m c c a"
I0123 13:43:57.044969 140427613868032 graph.py:498] 
I0123 13:43:57.045029 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_circle o a g ? perp m c c a
I0123 13:44:00.042954 140427613868032 ddar.py:60] Depth 1/1000 time = 2.9416277408599854
I0123 13:44:04.919744 140427613868032 ddar.py:60] Depth 2/1000 time = 4.8766186237335205
I0123 13:44:10.060890 140427613868032 ddar.py:60] Depth 3/1000 time = 5.140909671783447
I0123 13:44:16.923717 140427613868032 ddar.py:60] Depth 4/1000 time = 6.8625195026397705
I0123 13:44:22.765626 140427613868032 ddar.py:60] Depth 5/1000 time = 5.841724872589111
I0123 13:44:28.610376 140427613868032 ddar.py:60] Depth 6/1000 time = 5.8444764614105225
I0123 13:44:36.017777 140427613868032 ddar.py:60] Depth 7/1000 time = 7.402199029922485
I0123 13:44:46.376695 140427613868032 ddar.py:60] Depth 8/1000 time = 10.358633518218994
I0123 13:44:56.967789 140427613868032 ddar.py:60] Depth 9/1000 time = 10.590747833251953
I0123 13:45:07.383962 140427613868032 ddar.py:60] Depth 10/1000 time = 10.4159095287323
I0123 13:45:16.670540 140427613868032 ddar.py:60] Depth 11/1000 time = 9.12073016166687
I0123 13:45:16.692935 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:45:16.693000 140427613868032 alphageometry.py:566] LM output (score=-3.048955): "o : T j h h o 20 ;"
I0123 13:45:16.693033 140427613868032 alphageometry.py:567] Translation: "o = on_tline o h j h"

I0123 13:45:16.693070 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o h j h ? perp m c c a"
I0123 13:45:16.693251 140427613868032 graph.py:498] 
I0123 13:45:16.693309 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o h j h ? perp m c c a
I0123 13:45:19.677996 140427613868032 ddar.py:60] Depth 1/1000 time = 2.9403607845306396
I0123 13:45:24.474400 140427613868032 ddar.py:60] Depth 2/1000 time = 4.796170711517334
I0123 13:45:29.603685 140427613868032 ddar.py:60] Depth 3/1000 time = 5.129068613052368
I0123 13:45:36.563594 140427613868032 ddar.py:60] Depth 4/1000 time = 6.959734916687012
I0123 13:45:42.608465 140427613868032 ddar.py:60] Depth 5/1000 time = 6.044623851776123
I0123 13:45:48.543832 140427613868032 ddar.py:60] Depth 6/1000 time = 5.935073614120483
I0123 13:45:56.244272 140427613868032 ddar.py:60] Depth 7/1000 time = 7.6927313804626465
I0123 13:46:08.053568 140427613868032 ddar.py:60] Depth 8/1000 time = 11.809051990509033
I0123 13:46:19.212889 140427613868032 ddar.py:60] Depth 9/1000 time = 11.158972024917603
I0123 13:46:30.083194 140427613868032 ddar.py:60] Depth 10/1000 time = 10.869929552078247
I0123 13:46:39.765605 140427613868032 ddar.py:60] Depth 11/1000 time = 9.49726676940918
I0123 13:46:39.788090 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:46:39.788170 140427613868032 alphageometry.py:566] LM output (score=-3.061550): "o : T a e a o 20 ;"
I0123 13:46:39.788217 140427613868032 alphageometry.py:567] Translation: "o = on_tline o a a e"

I0123 13:46:39.788264 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o a a e ? perp m c c a"
I0123 13:46:39.788456 140427613868032 graph.py:498] 
I0123 13:46:39.788516 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o a a e ? perp m c c a
I0123 13:46:42.750981 140427613868032 ddar.py:60] Depth 1/1000 time = 2.912205696105957
I0123 13:46:47.544054 140427613868032 ddar.py:60] Depth 2/1000 time = 4.79289698600769
I0123 13:46:52.720898 140427613868032 ddar.py:60] Depth 3/1000 time = 5.176657438278198
I0123 13:46:59.748094 140427613868032 ddar.py:60] Depth 4/1000 time = 7.027000188827515
I0123 13:47:05.719536 140427613868032 ddar.py:60] Depth 5/1000 time = 5.971236228942871
I0123 13:47:11.702321 140427613868032 ddar.py:60] Depth 6/1000 time = 5.982569217681885
I0123 13:47:18.321515 140427613868032 ddar.py:60] Depth 7/1000 time = 6.611565351486206
I0123 13:47:30.227290 140427613868032 ddar.py:60] Depth 8/1000 time = 11.905462265014648
I0123 13:47:41.811271 140427613868032 ddar.py:60] Depth 9/1000 time = 11.58369755744934
I0123 13:47:52.901631 140427613868032 ddar.py:60] Depth 10/1000 time = 11.090052127838135
I0123 13:48:03.944354 140427613868032 ddar.py:60] Depth 11/1000 time = 10.862646579742432
I0123 13:48:03.966916 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:48:03.966978 140427613868032 alphageometry.py:566] LM output (score=-3.083095): "o : T a j n o 20 ;"
I0123 13:48:03.967012 140427613868032 alphageometry.py:567] Translation: "o = on_tline o n a j"

I0123 13:48:03.967050 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n a j ? perp m c c a"
I0123 13:48:03.967230 140427613868032 graph.py:498] 
I0123 13:48:03.967288 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o n a j ? perp m c c a
I0123 13:48:05.832109 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8193788528442383
I0123 13:48:10.585106 140427613868032 ddar.py:60] Depth 2/1000 time = 4.752820014953613
I0123 13:48:15.780095 140427613868032 ddar.py:60] Depth 3/1000 time = 5.194765567779541
I0123 13:48:22.603007 140427613868032 ddar.py:60] Depth 4/1000 time = 6.822649955749512
I0123 13:48:28.462643 140427613868032 ddar.py:60] Depth 5/1000 time = 5.859443664550781
I0123 13:48:34.358472 140427613868032 ddar.py:60] Depth 6/1000 time = 5.895565986633301
I0123 13:48:41.960792 140427613868032 ddar.py:60] Depth 7/1000 time = 7.594238519668579
I0123 13:48:53.955597 140427613868032 ddar.py:60] Depth 8/1000 time = 11.994540214538574
I0123 13:49:05.127853 140427613868032 ddar.py:60] Depth 9/1000 time = 11.171883583068848
I0123 13:49:15.860673 140427613868032 ddar.py:60] Depth 10/1000 time = 10.732489347457886
I0123 13:49:26.453017 140427613868032 ddar.py:60] Depth 11/1000 time = 10.413344144821167
I0123 13:49:26.474886 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:49:26.474954 140427613868032 alphageometry.py:566] LM output (score=-3.108035): "o : T a j b o 20 ;"
I0123 13:49:26.474990 140427613868032 alphageometry.py:567] Translation: "o = on_tline o b a j"

I0123 13:49:26.475029 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o b a j ? perp m c c a"
I0123 13:49:26.475216 140427613868032 graph.py:498] 
I0123 13:49:26.475275 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o b a j ? perp m c c a
I0123 13:49:28.352792 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8322107791900635
I0123 13:49:33.174816 140427613868032 ddar.py:60] Depth 2/1000 time = 4.821791887283325
I0123 13:49:39.496202 140427613868032 ddar.py:60] Depth 3/1000 time = 6.321108818054199
I0123 13:49:45.283294 140427613868032 ddar.py:60] Depth 4/1000 time = 5.786912679672241
I0123 13:49:51.214127 140427613868032 ddar.py:60] Depth 5/1000 time = 5.930628538131714
I0123 13:49:57.116535 140427613868032 ddar.py:60] Depth 6/1000 time = 5.902154922485352
I0123 13:50:04.754283 140427613868032 ddar.py:60] Depth 7/1000 time = 7.630016326904297
I0123 13:50:16.289554 140427613868032 ddar.py:60] Depth 8/1000 time = 11.534968852996826
I0123 13:50:27.698065 140427613868032 ddar.py:60] Depth 9/1000 time = 11.408148050308228
I0123 13:50:38.686388 140427613868032 ddar.py:60] Depth 10/1000 time = 10.988032817840576
I0123 13:50:48.427046 140427613868032 ddar.py:60] Depth 11/1000 time = 9.554630041122437
I0123 13:50:48.448679 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:50:48.448759 140427613868032 alphageometry.py:566] LM output (score=-3.261118): "o : T a b c o 20 ;"
I0123 13:50:48.448794 140427613868032 alphageometry.py:567] Translation: "o = on_tline o c a b"

I0123 13:50:48.448832 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o c a b ? perp m c c a"
I0123 13:50:48.449020 140427613868032 graph.py:498] 
I0123 13:50:48.449080 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c g; o = on_tline o c a b ? perp m c c a
I0123 13:50:51.538960 140427613868032 ddar.py:60] Depth 1/1000 time = 3.0458128452301025
I0123 13:50:56.102098 140427613868032 ddar.py:60] Depth 2/1000 time = 4.562882900238037
I0123 13:51:01.284496 140427613868032 ddar.py:60] Depth 3/1000 time = 5.182217597961426
I0123 13:51:07.138510 140427613868032 ddar.py:60] Depth 4/1000 time = 5.853815078735352
I0123 13:51:14.311521 140427613868032 ddar.py:60] Depth 5/1000 time = 7.172787427902222
I0123 13:51:20.293736 140427613868032 ddar.py:60] Depth 6/1000 time = 5.9819769859313965
I0123 13:51:26.855931 140427613868032 ddar.py:60] Depth 7/1000 time = 6.554481506347656
I0123 13:51:40.226897 140427613868032 ddar.py:60] Depth 8/1000 time = 13.370702743530273
I0123 13:51:51.582592 140427613868032 ddar.py:60] Depth 9/1000 time = 11.355320930480957
I0123 13:52:01.521119 140427613868032 ddar.py:60] Depth 10/1000 time = 9.938110113143921
I0123 13:52:12.565224 140427613868032 ddar.py:60] Depth 11/1000 time = 10.869388103485107
I0123 13:52:12.588047 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:52:12.588173 140427613868032 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a c f 03 C b e f 04 ; g : C a e g 05 C b c g 06 ; h : C a b h 07 C c e h 08 ; i : D b i c i 09 D b i f i 10 ; j : C a b j 11 D b i i j 12 ; k : D c k e k 13 D e k f k 14 ; l : C a e l 15 D e k k l 16 ; m : C g j m 17 C h l m 18 ? T m c c a {F1} x00 n : P c h d n 19 ; x00
I0123 13:52:17.343152 140427613868032 alphageometry.py:566] LM output (score=-1.105487): "o : T a g n o 20 ;"
I0123 13:52:17.343287 140427613868032 alphageometry.py:567] Translation: "o = on_tline o n a g"

I0123 13:52:17.343328 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o n a g ? perp m c c a"
I0123 13:52:17.343508 140427613868032 graph.py:498] 
I0123 13:52:17.343568 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o n a g ? perp m c c a
I0123 13:52:20.429526 140427613868032 ddar.py:60] Depth 1/1000 time = 3.0418803691864014
I0123 13:52:25.264871 140427613868032 ddar.py:60] Depth 2/1000 time = 4.835155487060547
I0123 13:52:30.456443 140427613868032 ddar.py:60] Depth 3/1000 time = 5.191398859024048
I0123 13:52:36.292458 140427613868032 ddar.py:60] Depth 4/1000 time = 5.835827589035034
I0123 13:52:43.431047 140427613868032 ddar.py:60] Depth 5/1000 time = 7.138396739959717
I0123 13:52:49.383820 140427613868032 ddar.py:60] Depth 6/1000 time = 5.952582359313965
I0123 13:52:55.964029 140427613868032 ddar.py:60] Depth 7/1000 time = 6.57267951965332
I0123 13:53:07.753785 140427613868032 ddar.py:60] Depth 8/1000 time = 11.789432287216187
I0123 13:53:17.264514 140427613868032 ddar.py:60] Depth 9/1000 time = 9.51054048538208
I0123 13:53:28.140112 140427613868032 ddar.py:60] Depth 10/1000 time = 10.875313758850098
I0123 13:53:39.207185 140427613868032 ddar.py:60] Depth 11/1000 time = 10.894500017166138
I0123 13:53:39.229084 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:53:39.229176 140427613868032 alphageometry.py:566] LM output (score=-1.480997): "o : T a j a o 20 ;"
I0123 13:53:39.229212 140427613868032 alphageometry.py:567] Translation: "o = on_tline o a a j"

I0123 13:53:39.229264 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o a a j ? perp m c c a"
I0123 13:53:39.229477 140427613868032 graph.py:498] 
I0123 13:53:39.229536 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o a a j ? perp m c c a
I0123 13:53:41.114817 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8366742134094238
I0123 13:53:46.037012 140427613868032 ddar.py:60] Depth 2/1000 time = 4.9220194816589355
I0123 13:53:51.227957 140427613868032 ddar.py:60] Depth 3/1000 time = 5.190774202346802
I0123 13:53:58.298507 140427613868032 ddar.py:60] Depth 4/1000 time = 7.070361137390137
I0123 13:54:04.360370 140427613868032 ddar.py:60] Depth 5/1000 time = 6.061617851257324
I0123 13:54:10.377822 140427613868032 ddar.py:60] Depth 6/1000 time = 6.017244100570679
I0123 13:54:16.984024 140427613868032 ddar.py:60] Depth 7/1000 time = 6.5983171463012695
I0123 13:54:31.505052 140427613868032 ddar.py:60] Depth 8/1000 time = 14.520782470703125
I0123 13:54:42.992434 140427613868032 ddar.py:60] Depth 9/1000 time = 11.487026929855347
I0123 13:54:52.933576 140427613868032 ddar.py:60] Depth 10/1000 time = 9.940775871276855
I0123 13:55:03.916189 140427613868032 ddar.py:60] Depth 11/1000 time = 10.805304050445557
I0123 13:55:03.939450 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:55:03.939525 140427613868032 alphageometry.py:566] LM output (score=-1.488439): "o : T a b b o 20 ;"
I0123 13:55:03.939562 140427613868032 alphageometry.py:567] Translation: "o = on_tline o b a b"

I0123 13:55:03.939603 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o b a b ? perp m c c a"
I0123 13:55:03.939796 140427613868032 graph.py:498] 
I0123 13:55:03.939856 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o b a b ? perp m c c a
I0123 13:55:07.017410 140427613868032 ddar.py:60] Depth 1/1000 time = 3.0300023555755615
I0123 13:55:11.663272 140427613868032 ddar.py:60] Depth 2/1000 time = 4.6457014083862305
I0123 13:55:16.990733 140427613868032 ddar.py:60] Depth 3/1000 time = 5.3272669315338135
I0123 13:55:22.962703 140427613868032 ddar.py:60] Depth 4/1000 time = 5.971736669540405
I0123 13:55:28.941543 140427613868032 ddar.py:60] Depth 5/1000 time = 5.9786293506622314
I0123 13:55:34.947895 140427613868032 ddar.py:60] Depth 6/1000 time = 6.006152629852295
I0123 13:55:42.802058 140427613868032 ddar.py:60] Depth 7/1000 time = 7.846408367156982
I0123 13:55:55.767329 140427613868032 ddar.py:60] Depth 8/1000 time = 12.965022087097168
I0123 13:56:07.154481 140427613868032 ddar.py:60] Depth 9/1000 time = 11.386861801147461
I0123 13:56:18.116285 140427613868032 ddar.py:60] Depth 10/1000 time = 10.96149754524231
I0123 13:56:27.799259 140427613868032 ddar.py:60] Depth 11/1000 time = 9.498876571655273
I0123 13:56:27.821754 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:56:27.821816 140427613868032 alphageometry.py:566] LM output (score=-1.956320): "o : T b h d o 20 ;"
I0123 13:56:27.821850 140427613868032 alphageometry.py:567] Translation: "o = on_tline o d b h"

I0123 13:56:27.821887 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o d b h ? perp m c c a"
I0123 13:56:27.822075 140427613868032 graph.py:498] 
I0123 13:56:27.822145 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o d b h ? perp m c c a
I0123 13:56:30.920364 140427613868032 ddar.py:60] Depth 1/1000 time = 3.0529842376708984
I0123 13:56:35.898901 140427613868032 ddar.py:60] Depth 2/1000 time = 4.978354215621948
I0123 13:56:41.290811 140427613868032 ddar.py:60] Depth 3/1000 time = 5.3916850090026855
I0123 13:56:47.206695 140427613868032 ddar.py:60] Depth 4/1000 time = 5.915696144104004
I0123 13:56:53.278186 140427613868032 ddar.py:60] Depth 5/1000 time = 6.071290731430054
I0123 13:56:59.379004 140427613868032 ddar.py:60] Depth 6/1000 time = 6.100559949874878
I0123 13:57:05.967365 140427613868032 ddar.py:60] Depth 7/1000 time = 6.580389022827148
I0123 13:57:19.231148 140427613868032 ddar.py:60] Depth 8/1000 time = 13.263505220413208
I0123 13:57:29.406879 140427613868032 ddar.py:60] Depth 9/1000 time = 10.175390243530273
I0123 13:57:40.636511 140427613868032 ddar.py:60] Depth 10/1000 time = 11.229357481002808
I0123 13:57:52.002953 140427613868032 ddar.py:60] Depth 11/1000 time = 11.185354948043823
I0123 13:58:01.998426 140427613868032 ddar.py:60] Depth 12/1000 time = 9.963415384292603
I0123 13:58:01.998841 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:58:01.998898 140427613868032 alphageometry.py:566] LM output (score=-2.117496): "o : T a e n o 20 ;"
I0123 13:58:01.998931 140427613868032 alphageometry.py:567] Translation: "o = on_tline o n a e"

I0123 13:58:01.998970 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o n a e ? perp m c c a"
I0123 13:58:01.999151 140427613868032 graph.py:498] 
I0123 13:58:01.999209 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o n a e ? perp m c c a
I0123 13:58:05.104264 140427613868032 ddar.py:60] Depth 1/1000 time = 3.0540122985839844
I0123 13:58:10.041690 140427613868032 ddar.py:60] Depth 2/1000 time = 4.937263011932373
I0123 13:58:15.379521 140427613868032 ddar.py:60] Depth 3/1000 time = 5.337605237960815
I0123 13:58:21.357161 140427613868032 ddar.py:60] Depth 4/1000 time = 5.977328062057495
I0123 13:58:27.393951 140427613868032 ddar.py:60] Depth 5/1000 time = 6.036589622497559
I0123 13:58:33.492741 140427613868032 ddar.py:60] Depth 6/1000 time = 6.098583698272705
I0123 13:58:40.142299 140427613868032 ddar.py:60] Depth 7/1000 time = 6.642060279846191
I0123 13:58:51.881223 140427613868032 ddar.py:60] Depth 8/1000 time = 11.73869800567627
I0123 13:59:02.084440 140427613868032 ddar.py:60] Depth 9/1000 time = 10.202967643737793
I0123 13:59:13.195842 140427613868032 ddar.py:60] Depth 10/1000 time = 11.111095190048218
I0123 13:59:24.612219 140427613868032 ddar.py:60] Depth 11/1000 time = 11.240351915359497
I0123 13:59:24.634083 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:59:24.634144 140427613868032 alphageometry.py:566] LM output (score=-2.151391): "o : T a j j o 20 ;"
I0123 13:59:24.634178 140427613868032 alphageometry.py:567] Translation: "o = on_tline o j a j"

I0123 13:59:24.634216 140427613868032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o j a j ? perp m c c a"
I0123 13:59:24.634408 140427613868032 graph.py:498] 
I0123 13:59:24.634469 140427613868032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c a, on_line f b e; g = on_line g c b, on_line g a e; h = on_line h e c, on_line h b a; i = circle i c b f; j = on_circle j i b, on_line j a b; k = circle k c e f; l = on_circle l k e, on_line l a e; m = on_line m g j, on_line m h l; n = on_pline n d c h; o = on_tline o j a j ? perp m c c a
I0123 13:59:26.501714 140427613868032 ddar.py:60] Depth 1/1000 time = 1.8227448463439941
I0123 13:59:31.178061 140427613868032 ddar.py:60] Depth 2/1000 time = 4.676187753677368
I0123 13:59:36.512005 140427613868032 ddar.py:60] Depth 3/1000 time = 5.3337180614471436
I0123 13:59:42.400959 140427613868032 ddar.py:60] Depth 4/1000 time = 5.888685941696167
I0123 13:59:49.708759 140427613868032 ddar.py:60] Depth 5/1000 time = 7.3075950145721436
I0123 13:59:55.879913 140427613868032 ddar.py:60] Depth 6/1000 time = 6.170894384384155
I0123 13:59:55.887919 140427613868032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:59:55.887970 140427613868032 alphageometry.py:585] Timeout.
