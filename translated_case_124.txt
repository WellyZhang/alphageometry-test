I0123 15:32:05.262262 139799132389376 inference_utils.py:69] Parsing gin configuration.
I0123 15:32:05.262419 139799132389376 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 15:32:05.262639 139799132389376 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 15:32:05.262672 139799132389376 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 15:32:05.262702 139799132389376 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 15:32:05.262730 139799132389376 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 15:32:05.262756 139799132389376 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 15:32:05.262783 139799132389376 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 15:32:05.262808 139799132389376 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 15:32:05.262835 139799132389376 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 15:32:05.262861 139799132389376 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 15:32:05.262888 139799132389376 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 15:32:05.262941 139799132389376 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 15:32:05.263121 139799132389376 resource_reader.py:55] Path not found: base_htrans.gin
I0123 15:32:05.263381 139799132389376 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 15:32:05.263491 139799132389376 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 15:32:05.269984 139799132389376 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 15:32:05.270101 139799132389376 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 15:32:05.270437 139799132389376 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 15:32:05.270544 139799132389376 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 15:32:05.270838 139799132389376 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 15:32:05.270941 139799132389376 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 15:32:05.271357 139799132389376 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 15:32:05.271459 139799132389376 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 15:32:05.275178 139799132389376 training_loop.py:334] ==== Training loop: initializing model ====
I0123 15:32:05.379455 139799132389376 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 15:32:05.380494 139799132389376 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 15:32:05.387225 139799132389376 training_loop.py:335] Process 0 of 1
I0123 15:32:05.387283 139799132389376 training_loop.py:336] Local device count = 1
I0123 15:32:05.387327 139799132389376 training_loop.py:337] Number of replicas = 1
I0123 15:32:05.387361 139799132389376 training_loop.py:339] Using random number seed 42
I0123 15:32:05.894648 139799132389376 training_loop.py:359] Initializing the model.
I0123 15:32:06.286891 139799132389376 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.287248 139799132389376 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 15:32:06.287360 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.287496 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.287582 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.287669 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.287745 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.287817 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.287887 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.287956 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.288026 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.288097 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.288166 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.288236 139799132389376 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:32:06.288276 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.288322 139799132389376 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:32:06.288439 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.288480 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.288512 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.290567 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.296188 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.307395 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.307697 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.312079 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.323192 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.323257 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.323298 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.323333 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.323400 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.324664 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.324744 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.325455 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.327934 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.334149 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.335407 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.335494 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.335531 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.335594 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.335724 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.336065 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.336112 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.338064 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.338172 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.341107 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.341192 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.341632 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.351746 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.360549 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.360649 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.360947 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.361031 139799132389376 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:32:06.361145 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.361185 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.361217 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.363212 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.365620 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.371340 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.371616 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.374274 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.378172 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.378231 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.378268 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.378300 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.378363 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.378941 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.379019 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.379383 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.380162 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.382696 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.383388 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.383468 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.383505 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.383567 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.383696 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.384031 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.384076 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.386010 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.386111 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.388686 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.388766 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.389268 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.391576 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.393490 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.393585 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.393894 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.393978 139799132389376 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:32:06.394093 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.394133 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.394165 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.396124 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.398541 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.404618 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.404888 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.407572 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.411473 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.411531 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.411568 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.411601 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.411668 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.412233 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.412310 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.412669 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.413445 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.416032 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.416667 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.416745 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.416781 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.416841 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.416977 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.417304 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.417348 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.419288 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.419387 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.421968 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.422056 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.422504 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.424799 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.426745 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.426843 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.427136 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.427218 139799132389376 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:32:06.427331 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.427372 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.427403 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.429348 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.431755 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.437430 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.437705 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.440407 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.444236 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.444294 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.444331 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.444365 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.444427 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.444995 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.445073 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.445431 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.446223 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.448776 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.449404 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.449485 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.449522 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.449583 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.449721 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.450048 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.450093 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.452064 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.452158 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.454731 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.454820 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.455259 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.457524 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.459520 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.459618 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.459912 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.459994 139799132389376 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:32:06.460106 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.460146 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.460179 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.462057 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.464488 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.470225 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.470494 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.473219 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.477065 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.477123 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.477160 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.477192 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.477255 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.478203 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.478284 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.478653 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.479431 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.481938 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.482578 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.482656 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.482692 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.482752 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.482886 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.483223 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.483269 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.485208 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.485303 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.487921 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.488004 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.488443 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.490833 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.492784 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.492880 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.493175 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.493258 139799132389376 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:32:06.493371 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.493413 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.493446 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.495361 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.497875 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.503592 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.503859 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.506539 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.510401 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.510458 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.510495 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.510527 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.510589 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.511154 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.511230 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.511584 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.512364 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.514865 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.515506 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.515584 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.515620 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.515681 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.515810 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.516142 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.516186 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.518209 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.518305 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.520843 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.520923 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.521362 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.523721 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.525685 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.525784 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.526077 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.526158 139799132389376 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:32:06.526269 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.526309 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.526341 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.528309 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.530725 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.536445 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.536708 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.539383 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.543230 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.543287 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.543325 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.543357 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.543421 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.543997 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.544074 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.544435 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.545217 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.547721 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.548403 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.548481 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.548517 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.548583 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.548714 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.549042 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.549086 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.551051 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.551148 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.553675 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.553757 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.554561 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.556849 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.558780 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.558884 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.559176 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.559258 139799132389376 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:32:06.559371 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.559411 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.559444 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.705369 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.708415 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.714411 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.714718 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.717498 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.721524 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.721586 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.721625 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.721669 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.721740 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.722386 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.722465 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.722831 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.723622 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.726302 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.726966 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.727046 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.727082 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.727144 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.727273 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.727612 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.727659 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.729595 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.729708 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.732343 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.732425 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.732873 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.735464 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.737673 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.737804 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.738104 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.738191 139799132389376 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:32:06.738306 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.738347 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.738380 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.740304 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.742814 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.748544 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.748816 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.751558 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.755423 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.755482 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.755519 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.755552 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.755614 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.756239 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.756318 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.756679 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.757467 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.759981 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.760610 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.760689 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.760725 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.760787 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.760915 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.761249 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.761294 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.763241 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.763335 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.765947 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.766029 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.766481 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.768862 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.770801 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.770899 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.771193 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.771282 139799132389376 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:32:06.771397 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.771438 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.771470 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.773351 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.775848 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.781892 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.782164 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.784820 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.788675 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.788734 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.788772 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.788804 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.788867 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.789444 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.789521 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.789896 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.790677 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.793157 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.793805 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.793886 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.793922 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.793984 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.794117 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.794449 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.794494 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.796464 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.796559 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.799073 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.799157 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.799598 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.801946 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.803854 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.803949 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.804240 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.804327 139799132389376 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:32:06.804440 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.804481 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.804512 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.806470 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.808881 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.814610 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.814882 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.817519 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.821372 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.821430 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.821468 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.821501 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.821564 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.822158 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.822236 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.822593 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.823372 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.825865 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.826547 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.826626 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.826663 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.826723 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.826854 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.827185 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.827229 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.829159 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.829258 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.832004 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.832086 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.832581 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.834867 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.836798 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.836894 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.837182 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.837263 139799132389376 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:32:06.837383 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.837423 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.837455 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.839392 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.841796 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.847453 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.847723 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.850363 139799132389376 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:32:06.854183 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.854240 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.854277 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.854310 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.854373 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.854938 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.855016 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.855379 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.856152 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.859004 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.859642 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.859722 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.859758 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.859818 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.859949 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.860272 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.860317 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.862323 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.862426 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.865045 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.865125 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.865566 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.867938 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.869873 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.869973 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.870266 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.870560 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.870634 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.870704 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.870765 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.870823 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.870880 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.870937 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.870993 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.871049 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.871105 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.871160 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.871215 139799132389376 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:32:06.871255 139799132389376 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:32:06.874882 139799132389376 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:32:06.924049 139799132389376 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.924136 139799132389376 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:32:06.924190 139799132389376 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:32:06.924295 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.924334 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.924364 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.924428 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.926864 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.932460 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.932722 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.935391 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:06.952616 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.952673 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.952710 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.952743 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.952806 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.953958 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.954040 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.954764 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.956818 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.961684 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.963067 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.963158 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:06.963198 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:06.963263 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.963406 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:06.963531 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:06.963575 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.965508 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.965605 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.968104 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.968192 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:06.968304 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:06.970591 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:06.972613 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.972710 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:06.973002 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.973084 139799132389376 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:32:06.973197 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:06.973238 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:06.973271 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:06.973337 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.975660 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:06.981274 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.981541 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:06.984296 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:06.997823 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:06.997880 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:06.997918 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:06.997951 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.998020 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.998599 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.998680 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.999057 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:06.999779 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.002302 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.002963 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.003044 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.003087 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.003151 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.003285 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.003401 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.003443 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.005401 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.005497 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.007979 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.008061 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.008171 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.010436 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.012436 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.012534 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.012821 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.012904 139799132389376 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:32:07.013016 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.013056 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.013088 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.013153 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.015465 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.021020 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.021283 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.024056 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.037177 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.037235 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.037274 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.037307 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.037370 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.037950 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.038029 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.038398 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.039124 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.041634 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.042280 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.042359 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.042398 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.042470 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.042608 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.042724 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.042765 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.044770 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.044866 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.047371 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.047457 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.047574 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.049864 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.051870 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.051969 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.052255 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.052338 139799132389376 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:32:07.052451 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.052490 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.052523 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.052590 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.054886 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.060464 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.060728 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.063478 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.076591 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.076651 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.076689 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.076720 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.076782 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.077347 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.077426 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.077792 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.078500 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.081031 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.081684 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.081765 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.081802 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.081864 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.082001 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.082113 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.082153 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.084172 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.084268 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.086701 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.086791 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.086909 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.089164 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.091082 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.091184 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.091480 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.091566 139799132389376 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:32:07.091684 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.091726 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.091760 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.091829 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.094442 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.100085 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.100354 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.102998 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.116078 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.116137 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.116175 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.116207 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.116270 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.116835 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.116912 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.117267 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.117980 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.120575 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.121207 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.121286 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.121322 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.121383 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.121520 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.121637 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.121689 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.123619 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.123718 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.126147 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.126229 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.126346 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.128681 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.130575 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.130676 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.130971 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.131056 139799132389376 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:32:07.131173 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.131214 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.131248 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.131317 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.133577 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.139147 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.139417 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.142112 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.155072 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.155132 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.155171 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.155205 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.155271 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.155862 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.155938 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.156294 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.156985 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.159487 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.160128 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.160208 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.160244 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.160306 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.160438 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.160555 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.160595 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.162570 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.162671 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.165118 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.165199 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.165310 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.167596 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.169444 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.169541 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.169833 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.169917 139799132389376 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:32:07.170028 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.170068 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.170099 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.170166 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.172464 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.178106 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.178377 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.181039 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.194096 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.194155 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.194192 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.194225 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.194292 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.194893 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.194973 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.195343 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.196054 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.198632 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.199679 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.199763 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.199802 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.199865 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.199998 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.200110 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.200155 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.202079 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.202178 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.204652 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.204733 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.204848 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.207130 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.209112 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.209209 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.209496 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.209580 139799132389376 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:32:07.209709 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.209752 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.209784 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.209849 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.212161 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.217721 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.217997 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.220725 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.233817 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.233876 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.233914 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.233947 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.234011 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.234653 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.234735 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.235109 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.235833 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.238318 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.238981 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.239063 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.239100 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.239162 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.239294 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.239408 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.239454 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.241356 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.241452 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.243999 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.244081 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.244201 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.246540 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.248463 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.248560 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.248846 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.248930 139799132389376 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:32:07.249042 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.249082 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.249114 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.249180 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.251483 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.257102 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.257367 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.260061 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.273080 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.273139 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.273175 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.273208 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.273271 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.273853 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.273932 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.274293 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.275027 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.277540 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.278238 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.278323 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.278361 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.278424 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.278563 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.278680 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.278722 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.280666 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.280762 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.283221 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.283310 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.283426 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.285701 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.287713 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.287814 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.288104 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.288187 139799132389376 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:32:07.288300 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.288339 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.288371 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.288436 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.290702 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.296264 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.296525 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.299238 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.312581 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.312639 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.312676 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.312708 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.312771 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.313385 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.313462 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.313832 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.314541 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.317070 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.317719 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.317800 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.317835 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.317897 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.318030 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.318142 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.318182 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.320136 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.320238 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.322739 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.322830 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.322947 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.325220 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.327133 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.327235 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.327533 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.327619 139799132389376 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:32:07.327737 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.327779 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.327812 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.327879 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.330119 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.335797 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.336063 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.338727 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.351791 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.351851 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.351888 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.351920 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.351989 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.352553 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.352630 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.352982 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.353682 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.356241 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.356913 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.356992 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.357028 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.357088 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.357216 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.357328 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.357367 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.359321 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.359429 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.361900 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.361983 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.362093 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.364354 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.366309 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.366410 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.366708 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.366794 139799132389376 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:32:07.366912 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.366954 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.366988 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.367055 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.369324 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.374881 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.375156 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.377862 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.390809 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.390869 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.390907 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.390941 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.391006 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.391588 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.391668 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.392030 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.392761 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.395336 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.396148 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.396240 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.396287 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.396370 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.396556 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.396704 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.396763 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.398873 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.398975 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.401410 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.401491 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.401601 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.403968 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.405837 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.405934 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.406219 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.406310 139799132389376 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:32:07.409209 139799132389376 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:32:07.465038 139799132389376 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.465125 139799132389376 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:32:07.465181 139799132389376 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:32:07.465287 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.465326 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.465357 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.465421 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.468061 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.473441 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.473717 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.476273 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.488857 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.488914 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.488952 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.488985 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.489049 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.489615 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.489701 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.490059 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.490739 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.493226 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.493858 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.493938 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.493975 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.494037 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.494166 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.494286 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.494327 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.496178 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.496274 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.498682 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.498767 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.498883 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.501168 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.503026 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.503130 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.503417 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.503498 139799132389376 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:32:07.503610 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.503651 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.503683 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.503748 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.505980 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.511345 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.511609 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.514246 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.526623 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.526679 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.526716 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.526749 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.526813 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.527369 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.527447 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.527801 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.528471 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.530984 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.531607 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.531685 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.531723 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.531784 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.531912 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.532024 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.532072 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.533949 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.534046 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.536493 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.536573 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.536683 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.538974 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.540846 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.540943 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.541228 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.541309 139799132389376 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:32:07.541420 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.541460 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.541492 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.541557 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.543792 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.549133 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.549396 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.552051 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.564468 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.564525 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.564563 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.564596 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.564659 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.565220 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.565295 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.565653 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.566331 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.568806 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.569425 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.569503 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.569539 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.569602 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.569741 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.569851 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.569890 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.571840 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.571935 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.574310 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.574391 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.574502 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.577162 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.579016 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.579113 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.579399 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.579480 139799132389376 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:32:07.579591 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.579630 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.579662 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.579727 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.581947 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.587311 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.587574 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.590237 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.602708 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.602765 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.602803 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.602849 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.602914 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.603477 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.603551 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.603908 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.604589 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.607104 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.607723 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.607800 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.607835 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.607895 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.608026 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.608136 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.608177 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.610061 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.610155 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.612537 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.612615 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.612721 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.615001 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.616858 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.616953 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.617237 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.617317 139799132389376 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:32:07.617427 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.617465 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.617496 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.617560 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.619804 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.625200 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.625460 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.628149 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.640781 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.640836 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.640871 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.640901 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.640962 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.641518 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.641593 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.641951 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.642633 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.645139 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.645780 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.645858 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.645893 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.645953 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.646081 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.646190 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.646227 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.648112 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.648210 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.650612 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.650691 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.650801 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.653087 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.654949 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.655045 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.655327 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.655408 139799132389376 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:32:07.655518 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.655556 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.655587 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.655652 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.657886 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.663277 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.663535 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.666190 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.679091 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.679146 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.679181 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.679212 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.679273 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.679833 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.679909 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.680265 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.680949 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.683478 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.684101 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.684177 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.684212 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.684272 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.684398 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.684505 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.684543 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.686426 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.686523 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.688912 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.688991 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.689100 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.691790 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.693655 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.693749 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.694032 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.694113 139799132389376 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:32:07.694221 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.694259 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.694290 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.694355 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.696574 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.701969 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.702227 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.704885 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.717490 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.717545 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.717581 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.717611 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.717680 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.718249 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.718323 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.718677 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.719357 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.721885 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.722512 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.722588 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.722623 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.722681 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.722809 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.722920 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.722959 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.724826 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.724919 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.727300 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.727380 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.727488 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.729779 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.731635 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.731730 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.732013 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.732094 139799132389376 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:32:07.732203 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.732241 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.732271 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.732334 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.734555 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.739997 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.740257 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.742941 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.755552 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.755606 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.755641 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.755671 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.755733 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.756299 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.756374 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.756720 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.757404 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.759922 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.760552 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.760628 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.760663 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.760723 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.760850 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.760958 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.760996 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.762870 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.762965 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.765330 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.765417 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.765527 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.767814 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.769665 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.769760 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.770040 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.770120 139799132389376 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:32:07.770230 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.770268 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.770298 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.770361 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.772585 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.778131 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.778391 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.781059 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.793662 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.793716 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.793751 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.793782 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.793843 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.794411 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.794486 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.794842 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.795519 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.798081 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.798696 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.798771 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.798806 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.798864 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.798990 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.799101 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.799139 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.800993 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.801087 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.803475 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.803560 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.803670 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.806343 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.808218 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.808312 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.808593 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.808674 139799132389376 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:32:07.808782 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.808820 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.808851 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.808915 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.811166 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.816579 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.816839 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.819499 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.832050 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.832104 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.832139 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.832170 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.832235 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.832795 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.832870 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.833222 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.833901 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.836404 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.837029 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.837105 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.837140 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.837198 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.837326 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.837434 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.837471 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.839631 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.839725 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.842092 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.842171 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.842286 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.844538 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.846384 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.846479 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.846762 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.846843 139799132389376 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:32:07.846952 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.846990 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.847020 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.847084 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.849289 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.854687 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.854949 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.857603 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.870150 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.870204 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.870239 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.870269 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.870330 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.870892 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.870967 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.871321 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.872001 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.874543 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.875173 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.875249 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.875284 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.875343 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.875470 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.875576 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.875613 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.877627 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.877731 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.880219 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.880296 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.880403 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.882866 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.884726 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.884819 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.885102 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.885182 139799132389376 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:32:07.885292 139799132389376 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:32:07.885330 139799132389376 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:32:07.885361 139799132389376 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:32:07.885424 139799132389376 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.887646 139799132389376 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:32:07.893063 139799132389376 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.893323 139799132389376 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:32:07.895999 139799132389376 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:32:07.908617 139799132389376 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:32:07.908672 139799132389376 attention.py:418] Single window, no scan.
I0123 15:32:07.908707 139799132389376 transformer_layer.py:389] tlayer: self-attention.
I0123 15:32:07.908737 139799132389376 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.908799 139799132389376 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.909358 139799132389376 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.909435 139799132389376 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.909801 139799132389376 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.910486 139799132389376 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.913013 139799132389376 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.913648 139799132389376 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.913730 139799132389376 transformer_layer.py:468] tlayer: End windows.
I0123 15:32:07.913766 139799132389376 transformer_layer.py:472] tlayer: final FFN.
I0123 15:32:07.913826 139799132389376 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.913953 139799132389376 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:32:07.914062 139799132389376 nn_components.py:325] mlp: activation = None
I0123 15:32:07.914100 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.915966 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.916058 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.918466 139799132389376 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.918545 139799132389376 transformer_base.py:443] tbase: final FFN
I0123 15:32:07.918653 139799132389376 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:32:07.921303 139799132389376 nn_components.py:329] mlp: final activation = None
I0123 15:32:07.923210 139799132389376 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.923305 139799132389376 nn_components.py:261] mlp: residual
I0123 15:32:07.923587 139799132389376 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:07.923671 139799132389376 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:32:07.926500 139799132389376 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:32:12.342605 139799132389376 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 15:32:12.913638 139799132389376 training_loop.py:409] No working directory specified.
I0123 15:32:12.913800 139799132389376 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 15:32:12.914640 139799132389376 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 15:32:16.019620 139799132389376 training_loop.py:447] Only restoring trainable parameters.
I0123 15:32:16.020543 139799132389376 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 15:32:16.020609 139799132389376 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.020656 139799132389376 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.020697 139799132389376 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.020737 139799132389376 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.020776 139799132389376 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.020815 139799132389376 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.020853 139799132389376 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.020890 139799132389376 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.020927 139799132389376 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.020966 139799132389376 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021003 139799132389376 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.021040 139799132389376 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.021076 139799132389376 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.021113 139799132389376 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021149 139799132389376 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.021185 139799132389376 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021221 139799132389376 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021257 139799132389376 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.021294 139799132389376 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.021359 139799132389376 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021398 139799132389376 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.021437 139799132389376 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.021474 139799132389376 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.021511 139799132389376 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021548 139799132389376 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.021584 139799132389376 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021621 139799132389376 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021666 139799132389376 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.021704 139799132389376 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.021742 139799132389376 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021779 139799132389376 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.021815 139799132389376 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.021850 139799132389376 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.021886 139799132389376 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021922 139799132389376 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.021957 139799132389376 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.021994 139799132389376 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022031 139799132389376 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.022067 139799132389376 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.022103 139799132389376 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022138 139799132389376 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.022172 139799132389376 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.022208 139799132389376 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.022244 139799132389376 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022280 139799132389376 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.022322 139799132389376 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022359 139799132389376 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022395 139799132389376 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.022430 139799132389376 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.022465 139799132389376 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022501 139799132389376 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.022537 139799132389376 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.022572 139799132389376 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.022607 139799132389376 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022644 139799132389376 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.022679 139799132389376 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022714 139799132389376 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022750 139799132389376 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.022786 139799132389376 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.022822 139799132389376 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.022859 139799132389376 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.022894 139799132389376 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.022930 139799132389376 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.022965 139799132389376 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023001 139799132389376 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.023037 139799132389376 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023074 139799132389376 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023110 139799132389376 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.023145 139799132389376 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.023180 139799132389376 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023217 139799132389376 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.023253 139799132389376 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.023295 139799132389376 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.023333 139799132389376 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023370 139799132389376 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.023405 139799132389376 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023440 139799132389376 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023475 139799132389376 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.023509 139799132389376 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.023544 139799132389376 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023578 139799132389376 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.023613 139799132389376 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.023648 139799132389376 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.023684 139799132389376 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023719 139799132389376 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.023755 139799132389376 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023790 139799132389376 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023826 139799132389376 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.023862 139799132389376 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.023898 139799132389376 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.023934 139799132389376 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.023969 139799132389376 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.024004 139799132389376 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.024039 139799132389376 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024075 139799132389376 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.024110 139799132389376 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024146 139799132389376 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024181 139799132389376 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.024216 139799132389376 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.024256 139799132389376 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024293 139799132389376 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.024328 139799132389376 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.024364 139799132389376 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.024399 139799132389376 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024435 139799132389376 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.024470 139799132389376 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024505 139799132389376 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024540 139799132389376 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.024576 139799132389376 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.024611 139799132389376 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024646 139799132389376 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.024681 139799132389376 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:32:16.024716 139799132389376 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:32:16.024750 139799132389376 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024784 139799132389376 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.024819 139799132389376 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024854 139799132389376 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024889 139799132389376 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:32:16.024924 139799132389376 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:32:16.024958 139799132389376 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:32:16.024994 139799132389376 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:32:16.025022 139799132389376 training_loop.py:725] Total parameters: 152072288
I0123 15:32:16.025244 139799132389376 training_loop.py:739] Total state size: 0
I0123 15:32:16.046170 139799132389376 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 15:32:16.046523 139799132389376 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 15:32:16.046857 139799132389376 training_loop.py:652] Compiling mode beam_search with jit.
I0123 15:32:16.047234 139799132389376 training_loop.py:89] registering functions: dict_keys([])
I0123 15:32:16.069842 139799132389376 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d a, on_bline e a b; f = on_line f b a; g = on_pline g f b e, on_line g e a; h = on_pline h f a e, on_line h e b; i = circle i e g h; j = foot j e d i; k = mirror k e j; l = on_circle l i g, on_line l f g; m = lc_tangent m k l, on_line m c l; n = on_circle n d k, on_line n m k; o = lc_tangent o k a, on_line o c a; p = on_circle p i k, on_line p o k; q = circle q f k n; r = circle r f k p ? cyclic e k q r
