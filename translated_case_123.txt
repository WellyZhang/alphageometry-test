I0123 19:03:35.673558 140169189924864 inference_utils.py:69] Parsing gin configuration.
I0123 19:03:35.673676 140169189924864 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 19:03:35.673877 140169189924864 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 19:03:35.673910 140169189924864 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 19:03:35.673940 140169189924864 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 19:03:35.673967 140169189924864 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 19:03:35.673995 140169189924864 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 19:03:35.674021 140169189924864 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 19:03:35.674047 140169189924864 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 19:03:35.674074 140169189924864 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 19:03:35.674099 140169189924864 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 19:03:35.674124 140169189924864 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 19:03:35.674172 140169189924864 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 19:03:35.674311 140169189924864 resource_reader.py:55] Path not found: base_htrans.gin
I0123 19:03:35.674546 140169189924864 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 19:03:35.674652 140169189924864 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 19:03:35.680993 140169189924864 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 19:03:35.681118 140169189924864 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 19:03:35.681430 140169189924864 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 19:03:35.681534 140169189924864 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 19:03:35.681819 140169189924864 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 19:03:35.681919 140169189924864 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 19:03:35.682319 140169189924864 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 19:03:35.682416 140169189924864 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 19:03:35.686073 140169189924864 training_loop.py:334] ==== Training loop: initializing model ====
I0123 19:03:35.782928 140169189924864 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 19:03:35.783735 140169189924864 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 19:03:35.790134 140169189924864 training_loop.py:335] Process 0 of 1
I0123 19:03:35.790189 140169189924864 training_loop.py:336] Local device count = 1
I0123 19:03:35.790229 140169189924864 training_loop.py:337] Number of replicas = 1
I0123 19:03:35.790261 140169189924864 training_loop.py:339] Using random number seed 42
I0123 19:03:36.304395 140169189924864 training_loop.py:359] Initializing the model.
I0123 19:03:36.741968 140169189924864 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.742322 140169189924864 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 19:03:36.742429 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.742507 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.742583 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.742668 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.742740 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.742809 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.742878 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.742945 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.743013 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.743081 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.743149 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.743216 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:03:36.743257 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:36.743303 140169189924864 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:03:36.743419 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:36.743459 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:36.743489 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:36.745505 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.750797 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:36.761656 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.761933 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:36.766205 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:36.776739 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:36.776798 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:36.776835 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:36.776867 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.776935 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.778126 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.778205 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.778899 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.781328 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.787435 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.788738 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.788823 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:36.788859 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:36.788919 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.789049 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:36.789386 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:36.789434 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.791330 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.791430 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.794272 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.794356 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:36.794854 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:36.804779 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.813365 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.813464 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.813764 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.813847 140169189924864 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:03:36.813957 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:36.813996 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:36.814027 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:36.815863 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.818294 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:36.823776 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.824039 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:36.826610 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:36.830403 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:36.830459 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:36.830495 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:36.830524 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.830586 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.831150 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.831227 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.831587 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.832339 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.834785 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.835402 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.835479 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:36.835514 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:36.835571 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.835697 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:36.836026 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:36.836071 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.838008 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.838102 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.840598 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.840678 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:36.841117 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:36.843445 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.845355 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.845449 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.845749 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.845831 140169189924864 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:03:36.845940 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:36.845979 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:36.846012 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:36.848277 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.850646 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:36.856237 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.856504 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:36.859150 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:36.863028 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:36.863083 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:36.863120 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:36.863151 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.863213 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.863779 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.863856 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.864211 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.864982 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.867593 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.868270 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.868348 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:36.868383 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:36.868441 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.868567 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:36.868903 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:36.868948 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.870883 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.870977 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.873488 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.873575 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:36.874097 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:36.876390 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.878332 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.878429 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.878727 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.878809 140169189924864 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:03:36.878921 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:36.878960 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:36.878992 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:36.880929 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.883436 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:36.889150 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.889415 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:36.892057 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:36.895899 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:36.895957 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:36.895995 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:36.896026 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.896093 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.896664 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.896741 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.897099 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.897887 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.900427 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.901053 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.901132 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:36.901168 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:36.901228 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.901360 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:36.901701 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:36.901748 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.903664 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.903759 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.906327 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.906415 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:36.906851 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:36.909113 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.911029 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.911124 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.911413 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.911494 140169189924864 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:03:36.911604 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:36.911643 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:36.911675 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:36.913596 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.915999 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:36.921674 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.921940 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:36.924950 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:36.928731 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:36.928787 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:36.928824 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:36.928858 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.928924 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.929494 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.929571 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.929937 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.930711 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.933274 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.933918 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.933998 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:36.934034 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:36.934098 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.934230 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:36.934558 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:36.934602 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.936503 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.936597 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.939151 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.939232 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:36.939663 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:36.941944 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.943911 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.944006 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.944296 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.944378 140169189924864 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:03:36.944488 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:36.944528 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:36.944560 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:36.946432 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.948828 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:36.954469 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.954732 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:36.957422 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:36.961185 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:36.961241 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:36.961278 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:36.961310 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.961374 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.961997 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.962078 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.962436 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.963213 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.965707 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.966331 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.966408 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:36.966444 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:36.966503 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.966628 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:36.966950 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:36.966999 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.968898 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.968996 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.971526 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.971607 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:36.972037 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:36.974358 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:36.976255 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.976353 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:36.976640 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.976722 140169189924864 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:03:36.976832 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:36.976872 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:36.976904 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:36.978783 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.981212 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:36.986845 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.987110 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:36.989726 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:36.993515 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:36.993569 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:36.993606 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:36.993638 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.993714 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.994281 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.994359 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.994717 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.995490 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.997960 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.998597 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.998676 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:36.998711 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:36.998771 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:36.998897 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:36.999229 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:36.999274 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.001543 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.001637 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.004126 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.004206 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.004640 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.147662 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.149952 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.150132 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.150448 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.150545 140169189924864 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:03:37.150661 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.150702 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.150735 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.152807 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.155508 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.161188 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.161465 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.164129 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:37.168097 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.168156 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.168195 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.168229 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.168294 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.168922 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.169003 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.169363 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.170162 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.172739 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.173387 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.173467 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.173503 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.173563 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.173708 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.174046 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.174092 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.176008 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.176103 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.178631 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.178713 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.179203 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.181492 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.183413 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.183519 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.183813 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.183895 140169189924864 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:03:37.184005 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.184044 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.184077 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.186006 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.188374 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.194046 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.194310 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.196980 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:37.200800 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.200857 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.200894 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.200926 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.200987 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.201565 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.201652 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.202010 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.202786 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.205329 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.205959 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.206039 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.206075 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.206134 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.206260 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.206589 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.206634 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.208534 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.208632 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.211199 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.211279 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.211720 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.213993 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.215903 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.215999 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.216284 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.216372 140169189924864 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:03:37.216485 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.216525 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.216557 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.218472 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.220839 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.226760 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.227027 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.229702 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:37.233453 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.233508 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.233546 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.233578 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.233651 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.234226 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.234303 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.234656 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.235471 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.237935 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.238566 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.238645 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.238681 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.238741 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.238866 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.239194 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.239238 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.241135 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.241228 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.243755 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.243836 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.244275 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.246547 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.248493 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.248588 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.248879 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.248968 140169189924864 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:03:37.249081 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.249120 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.249152 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.251000 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.253442 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.259009 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.259281 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.261940 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:37.265672 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.265729 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.265766 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.265797 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.265901 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.266479 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.266556 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.266914 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.267688 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.270135 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.270761 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.270840 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.270876 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.270935 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.271063 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.271390 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.271434 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.273383 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.273478 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.276207 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.276289 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.276728 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.279037 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.280924 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.281020 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.281306 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.281388 140169189924864 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:03:37.281506 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.281546 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.281578 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.283425 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.285865 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.291474 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.291739 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.294355 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:03:37.298462 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.298518 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.298555 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.298587 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.298649 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.299212 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.299292 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.299646 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.300413 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.306980 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.307725 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.307807 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.307845 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.307919 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.308055 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.308434 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.308480 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.310546 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.310641 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.313179 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.313259 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.313708 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.316063 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.317996 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.318093 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.318379 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.318690 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.318766 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.318835 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.318894 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.318950 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.319004 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.319057 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.319112 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.319166 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.319219 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.319271 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.319325 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:03:37.319363 140169189924864 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:03:37.322926 140169189924864 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:03:37.371127 140169189924864 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.371235 140169189924864 decoder_stack.py:333] dstack: autoregressive generator.
I0123 19:03:37.371290 140169189924864 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:03:37.371395 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.371437 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.371469 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.371534 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.373981 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.379506 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.379772 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.382439 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.399248 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.399307 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.399344 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.399376 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.399441 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.400582 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.400662 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.401366 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.403383 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.408134 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.409444 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.409537 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.409573 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.409634 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.409776 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.409888 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.409927 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.411828 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.411925 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.414343 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.414425 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.414542 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.416779 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.418743 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.418840 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.419127 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.419209 140169189924864 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:03:37.419318 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.419358 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.419389 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.419455 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.421707 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.427165 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.427427 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.430131 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.443381 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.443437 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.443474 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.443506 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.443568 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.444129 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.444210 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.444572 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.445261 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.447756 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.448379 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.448458 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.448499 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.448560 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.448693 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.448804 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.448843 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.450782 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.450876 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.453267 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.453351 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.453461 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.455700 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.457653 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.457753 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.458040 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.458123 140169189924864 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:03:37.458235 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.458275 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.458307 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.458373 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.460614 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.466069 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.466329 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.468989 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.481756 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.481812 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.481854 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.481886 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.481950 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.482510 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.482589 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.482942 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.483633 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.486095 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.486717 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.486795 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.486831 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.486897 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.487024 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.487137 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.487178 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.489117 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.489211 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.491657 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.491739 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.491848 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.494077 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.496003 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.496098 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.496383 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.496466 140169189924864 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:03:37.496576 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.496616 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.496649 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.496715 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.498952 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.504407 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.504668 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.507324 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.520126 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.520183 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.520220 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.520252 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.520314 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.520872 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.520951 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.521312 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.522012 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.524489 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.525112 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.525191 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.525227 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.525286 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.525425 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.525536 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.525575 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.527812 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.527909 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.530324 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.530405 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.530517 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.532748 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.534626 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.534724 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.535007 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.535089 140169189924864 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:03:37.535199 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.535239 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.535271 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.535337 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.537652 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.543087 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.543354 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.545952 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.558812 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.558870 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.558907 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.558938 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.559000 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.559560 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.559642 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.560004 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.560704 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.563248 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.563886 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.563966 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.564002 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.564062 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.564199 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.564311 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.564351 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.566256 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.566351 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.568762 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.568842 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.568950 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.571256 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.573125 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.573221 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.573501 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.573583 140169189924864 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:03:37.573698 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.573740 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.573773 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.573838 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.576086 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.581512 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.581782 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.584464 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.597328 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.597385 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.597422 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.597454 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.597517 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.598091 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.598169 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.598524 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.599218 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.601679 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.602306 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.602384 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.602420 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.602482 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.602612 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.602728 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.602768 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.604712 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.604806 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.607204 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.607285 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.607397 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.609628 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.611503 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.611598 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.611879 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.611959 140169189924864 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:03:37.612069 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.612109 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.612141 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.612205 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.614447 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.620004 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.620266 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.622861 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.635926 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.635981 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.636017 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.636047 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.636109 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.636671 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.636748 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.637099 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.637792 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.640246 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.640908 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.640986 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.641021 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.641081 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.641213 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.641322 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.641366 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.643252 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.643347 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.645718 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.645798 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.645905 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.648117 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.650049 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.650146 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.650430 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.650512 140169189924864 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:03:37.650620 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.650660 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.650693 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.650757 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.652979 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.658436 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.658710 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.661372 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.674200 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.674256 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.674293 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.674325 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.674388 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.674991 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.675070 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.675427 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.676112 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.678583 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.679222 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.679301 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.679337 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.679400 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.679529 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.679640 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.679686 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.681583 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.681687 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.684151 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.684232 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.684343 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.686570 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.688455 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.688553 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.688836 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.688920 140169189924864 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:03:37.689029 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.689069 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.689102 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.689166 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.691409 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.696948 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.697210 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.699821 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.712616 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.712674 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.712712 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.712745 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.712808 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.713370 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.713449 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.713816 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.714508 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.716979 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.717661 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.717741 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.717778 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.717838 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.717972 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.718084 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.718124 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.720029 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.720124 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.722502 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.722583 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.722691 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.724895 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.726813 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.726910 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.727194 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.727276 140169189924864 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:03:37.727385 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.727425 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.727458 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.727523 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.729755 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.735169 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.735429 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.738418 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.751242 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.751299 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.751336 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.751367 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.751430 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.752045 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.752123 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.752477 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.753174 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.755650 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.756282 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.756361 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.756397 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.756455 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.756587 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.756698 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.756738 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.758618 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.758719 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.761149 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.761229 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.761337 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.763569 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.765413 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.765509 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.765799 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.765883 140169189924864 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:03:37.765991 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.766030 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.766063 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.766128 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.768353 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.773884 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.774151 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.776771 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.789842 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.789903 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.789940 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.789973 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.790036 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.790597 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.790675 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.791038 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.791727 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.794239 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.794907 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.794986 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.795023 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.795082 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.795214 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.795324 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.795364 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.797255 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.797355 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.799772 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.799853 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.799962 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.802181 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.804241 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.804337 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.804616 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.804699 140169189924864 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:03:37.804808 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.804847 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.804879 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.804942 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.807236 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.812652 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.812913 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.815516 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.828241 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.828298 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.828335 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.828368 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.828432 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.829021 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.829100 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.829456 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.830155 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.832697 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.833320 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.833398 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.833434 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.833493 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.833625 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.833747 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.833787 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.835669 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.835762 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.838182 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.838263 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.838374 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.840986 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.842848 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.842946 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.843228 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.843315 140169189924864 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:03:37.846163 140169189924864 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:03:37.901034 140169189924864 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.901123 140169189924864 decoder_stack.py:333] dstack: autoregressive generator.
I0123 19:03:37.901178 140169189924864 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:03:37.901285 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.901324 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.901355 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.901418 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.903764 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.909209 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.909469 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.912019 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.924408 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.924465 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.924502 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.924535 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.924598 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.925152 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.925230 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.925579 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.926259 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.928724 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.929337 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.929416 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.929453 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.929513 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.929651 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.929772 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.929812 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.931651 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.931744 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.934116 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.934196 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.934305 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.936526 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.938364 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.938462 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.938743 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.938824 140169189924864 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:03:37.938932 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.938972 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.939003 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.939067 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.941278 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.946591 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.946852 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.949450 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.961816 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.961872 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.961909 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.961942 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.962006 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.962560 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.962636 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.962989 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.963660 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.966140 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.966753 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.966831 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:37.966867 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:37.966927 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.967054 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:37.967162 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:37.967208 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.969049 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.969144 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.971523 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.971604 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:37.971714 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:37.973957 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:37.975780 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.975878 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:37.976156 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.976237 140169189924864 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:03:37.976343 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:37.976382 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:37.976413 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:37.976475 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.978672 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:37.983988 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.984250 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:37.986894 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:37.999230 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:37.999286 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:37.999323 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:37.999355 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.999417 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:37.999972 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.000049 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.000399 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.001070 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.003983 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.004602 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.004680 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.004717 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.004777 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.004905 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.005016 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.005056 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.006911 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.007008 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.009385 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.009464 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.009573 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.011815 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.013661 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.013759 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.014043 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.014125 140169189924864 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:03:38.014234 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:38.014275 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:38.014307 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:38.014371 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.016598 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:38.021931 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.022192 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:38.024822 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:38.037399 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:38.037456 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:38.037495 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:38.037537 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.037602 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.038166 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.038241 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.038593 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.039269 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.041795 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.042415 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.042492 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.042527 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.042587 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.042716 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.042824 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.042864 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.044740 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.044832 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.047225 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.047304 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.047413 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.049696 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.051537 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.051632 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.051910 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.051990 140169189924864 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:03:38.052097 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:38.052135 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:38.052165 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:38.052228 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.054447 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:38.059788 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.060049 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:38.062716 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:38.075372 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:38.075429 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:38.075465 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:38.075496 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.075563 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.076125 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.076200 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.076555 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.077242 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.079764 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.080385 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.080466 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.080502 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.080560 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.080690 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.080800 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.080838 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.082738 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.082847 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.085230 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.085309 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.085418 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.087710 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.089561 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.089664 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.089950 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.090030 140169189924864 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:03:38.090139 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:38.090178 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:38.090209 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:38.090272 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.092509 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:38.097950 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.098213 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:38.100900 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:38.113573 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:38.113628 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:38.113672 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:38.113704 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.113767 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.114330 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.114406 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.114759 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.115447 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.118385 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.119003 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.119079 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.119114 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.119171 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.119304 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.119414 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.119452 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.121303 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.121401 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.123784 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.123865 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.123973 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.126264 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.128141 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.128236 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.128520 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.128600 140169189924864 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:03:38.128707 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:38.128745 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:38.128775 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:38.128837 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.131063 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:38.136491 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.136749 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:38.139438 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:38.152151 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:38.152207 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:38.152242 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:38.152272 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.152338 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.152906 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.152982 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.153336 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.154028 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.156542 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.157178 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.157256 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.157291 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.157349 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.157476 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.157585 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.157624 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.159504 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.159598 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.161995 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.162074 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.162182 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.164460 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.166318 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.166414 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.166697 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.166778 140169189924864 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:03:38.166884 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:38.166922 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:38.166953 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:38.167015 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.169229 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:38.174624 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.174888 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:38.177583 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:38.190251 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:38.190305 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:38.190340 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:38.190371 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.190434 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.191006 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.191082 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.191437 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.192118 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.194689 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.195316 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.195394 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.195429 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.195488 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.195618 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.195745 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.195786 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.197677 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.197771 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.200146 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.200229 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.200337 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.202614 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.204463 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.204558 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.204837 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.204918 140169189924864 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:03:38.205026 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:38.205065 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:38.205096 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:38.205159 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.207392 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:38.212765 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.213028 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:38.215721 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:38.228363 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:38.228418 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:38.228454 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:38.228484 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.228546 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.229106 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.229182 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.229535 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.230222 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.233130 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.233761 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.233839 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.233874 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.233931 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.234059 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.234166 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.234203 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.236071 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.236164 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.238534 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.238623 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.238735 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.241012 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.242879 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.242975 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.243257 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.243338 140169189924864 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:03:38.243445 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:38.243483 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:38.243514 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:38.243575 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.245800 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:38.251210 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.251474 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:38.254170 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:38.266810 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:38.266865 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:38.266900 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:38.266930 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.266992 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.267553 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.267628 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.267984 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.268669 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.271206 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.271830 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.271908 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.271943 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.272000 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.272126 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.272233 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.272271 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.274573 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.274670 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.277035 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.277113 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.277227 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.279496 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.281332 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.281426 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.281715 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.281795 140169189924864 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:03:38.281902 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:38.281940 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:38.281970 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:38.282033 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.284253 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:38.289632 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.289898 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:38.292545 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:38.305096 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:38.305150 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:38.305186 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:38.305217 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.305279 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.305845 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.305921 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.306275 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.306951 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.309482 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.310110 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.310187 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.310222 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.310278 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.310406 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.310513 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.310552 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.312398 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.312490 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.314857 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.314936 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.315043 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.317309 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.319177 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.319273 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.319556 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.319637 140169189924864 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:03:38.319744 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:03:38.319783 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:03:38.319814 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:03:38.319876 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.322105 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:03:38.327497 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.327758 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:03:38.330426 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:03:38.343056 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:03:38.343111 140169189924864 attention.py:418] Single window, no scan.
I0123 19:03:38.343147 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 19:03:38.343178 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.343240 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.343799 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.343875 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.344231 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.344922 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.347812 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.348435 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.348513 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 19:03:38.348548 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 19:03:38.348604 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.348731 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:03:38.348839 140169189924864 nn_components.py:325] mlp: activation = None
I0123 19:03:38.348877 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.350749 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.350843 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.353203 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.353280 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 19:03:38.353387 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:03:38.355672 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 19:03:38.357530 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.357624 140169189924864 nn_components.py:261] mlp: residual
I0123 19:03:38.357915 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:38.358000 140169189924864 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:03:38.360782 140169189924864 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:03:42.796757 140169189924864 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 19:03:43.320787 140169189924864 training_loop.py:409] No working directory specified.
I0123 19:03:43.320929 140169189924864 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 19:03:43.321784 140169189924864 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 19:03:46.849025 140169189924864 training_loop.py:447] Only restoring trainable parameters.
I0123 19:03:46.849784 140169189924864 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 19:03:46.849873 140169189924864 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.849928 140169189924864 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.849976 140169189924864 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.850020 140169189924864 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850061 140169189924864 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.850101 140169189924864 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850141 140169189924864 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850180 140169189924864 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.850219 140169189924864 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.850258 140169189924864 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850298 140169189924864 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.850336 140169189924864 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.850374 140169189924864 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.850413 140169189924864 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850450 140169189924864 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.850487 140169189924864 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850524 140169189924864 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850560 140169189924864 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.850597 140169189924864 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.850654 140169189924864 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850693 140169189924864 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.850731 140169189924864 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.850767 140169189924864 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.850805 140169189924864 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850842 140169189924864 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.850880 140169189924864 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850917 140169189924864 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.850955 140169189924864 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.850992 140169189924864 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.851029 140169189924864 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851066 140169189924864 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.851103 140169189924864 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.851139 140169189924864 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.851176 140169189924864 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851212 140169189924864 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.851249 140169189924864 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851285 140169189924864 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851321 140169189924864 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.851358 140169189924864 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.851395 140169189924864 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851431 140169189924864 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.851469 140169189924864 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.851506 140169189924864 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.851543 140169189924864 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851580 140169189924864 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.851622 140169189924864 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851660 140169189924864 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851697 140169189924864 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.851734 140169189924864 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.851770 140169189924864 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851806 140169189924864 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.851841 140169189924864 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.851878 140169189924864 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.851913 140169189924864 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.851950 140169189924864 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.851985 140169189924864 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852021 140169189924864 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852058 140169189924864 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.852094 140169189924864 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.852130 140169189924864 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852166 140169189924864 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.852202 140169189924864 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.852239 140169189924864 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.852275 140169189924864 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852312 140169189924864 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.852348 140169189924864 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852384 140169189924864 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852419 140169189924864 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.852454 140169189924864 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.852492 140169189924864 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852528 140169189924864 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.852564 140169189924864 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.852606 140169189924864 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.852643 140169189924864 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852680 140169189924864 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.852717 140169189924864 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852752 140169189924864 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852788 140169189924864 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.852824 140169189924864 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.852860 140169189924864 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.852895 140169189924864 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.852930 140169189924864 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.852966 140169189924864 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.853002 140169189924864 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853038 140169189924864 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.853074 140169189924864 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853110 140169189924864 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853147 140169189924864 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.853183 140169189924864 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.853219 140169189924864 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853256 140169189924864 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.853292 140169189924864 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.853328 140169189924864 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.853364 140169189924864 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853401 140169189924864 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.853438 140169189924864 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853474 140169189924864 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853510 140169189924864 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.853546 140169189924864 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.853586 140169189924864 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853623 140169189924864 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.853668 140169189924864 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.853705 140169189924864 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.853741 140169189924864 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853776 140169189924864 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.853812 140169189924864 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853847 140169189924864 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853883 140169189924864 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.853919 140169189924864 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.853954 140169189924864 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.853990 140169189924864 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.854027 140169189924864 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:03:46.854063 140169189924864 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:03:46.854099 140169189924864 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.854135 140169189924864 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.854170 140169189924864 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.854206 140169189924864 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.854241 140169189924864 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:03:46.854277 140169189924864 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:03:46.854313 140169189924864 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:03:46.854349 140169189924864 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:03:46.854377 140169189924864 training_loop.py:725] Total parameters: 152072288
I0123 19:03:46.854607 140169189924864 training_loop.py:739] Total state size: 0
I0123 19:03:46.878495 140169189924864 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 19:03:46.878783 140169189924864 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 19:03:46.879511 140169189924864 training_loop.py:652] Compiling mode beam_search with jit.
I0123 19:03:46.879866 140169189924864 training_loop.py:89] registering functions: dict_keys([])
I0123 19:03:46.900245 140169189924864 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e b a; f = on_line f e c; g = foot g f b a; h = mirror h f g; i = foot i f c a; j = mirror j f i; k = foot k f c b; l = mirror l f k; m = on_circle m d c, on_line m f c; n = on_circle n d b, on_line n f b; o = on_circle o d a, on_line o f a; p = circle p f h m; q = circle q f j n; r = circle r f l o; s = foot s b c a; t = foot t c b a; u = on_line u b s, on_line u c t; v = mirror v u t; w = mirror w u s; x = on_line x h v, on_line x w j ? cong r f r x
I0123 19:04:07.877561 140169189924864 ddar.py:60] Depth 1/1000 time = 20.772220373153687
I0123 19:05:37.709060 140169189924864 ddar.py:60] Depth 2/1000 time = 89.83102631568909
I0123 19:07:49.289114 140169189924864 ddar.py:60] Depth 3/1000 time = 131.57948064804077
I0123 19:11:21.683107 140169189924864 ddar.py:60] Depth 4/1000 time = 212.39335584640503
I0123 19:14:49.648975 140169189924864 ddar.py:60] Depth 5/1000 time = 207.96521520614624
I0123 19:19:50.236970 140169189924864 ddar.py:60] Depth 6/1000 time = 300.58727192878723
I0123 19:24:58.039355 140169189924864 ddar.py:60] Depth 7/1000 time = 307.80170226097107
I0123 19:30:14.648000 140169189924864 ddar.py:60] Depth 8/1000 time = 316.6079065799713
I0123 19:35:33.183437 140169189924864 ddar.py:60] Depth 9/1000 time = 318.5347728729248
I0123 19:40:55.819478 140169189924864 ddar.py:60] Depth 10/1000 time = 322.6353039741516
I0123 19:46:22.919353 140169189924864 ddar.py:60] Depth 11/1000 time = 327.09921169281006
I0123 19:51:47.400734 140169189924864 ddar.py:60] Depth 12/1000 time = 324.4806652069092
I0123 19:57:14.590579 140169189924864 ddar.py:60] Depth 13/1000 time = 327.1891827583313
I0123 20:02:45.266097 140169189924864 ddar.py:60] Depth 14/1000 time = 330.6748483181
I0123 20:08:16.952287 140169189924864 ddar.py:60] Depth 15/1000 time = 331.6842477321625
I0123 20:13:59.030443 140169189924864 ddar.py:60] Depth 16/1000 time = 342.06727409362793
I0123 20:19:54.646090 140169189924864 ddar.py:60] Depth 17/1000 time = 355.6150462627411
I0123 20:26:11.886656 140169189924864 ddar.py:60] Depth 18/1000 time = 377.2398202419281
I0123 20:32:28.742649 140169189924864 ddar.py:60] Depth 19/1000 time = 376.85533237457275
I0123 20:39:28.197368 140169189924864 ddar.py:60] Depth 20/1000 time = 419.45400381088257
I0123 20:46:19.823421 140169189924864 ddar.py:60] Depth 21/1000 time = 411.6253869533539
I0123 20:53:10.558274 140169189924864 ddar.py:60] Depth 22/1000 time = 410.73418045043945
I0123 21:00:08.688356 140169189924864 ddar.py:60] Depth 23/1000 time = 418.1292996406555
I0123 21:07:58.579483 140169189924864 ddar.py:60] Depth 24/1000 time = 469.89046239852905
I0123 21:17:00.398964 140169189924864 ddar.py:60] Depth 25/1000 time = 541.818826675415
I0123 21:26:00.114070 140169189924864 ddar.py:60] Depth 26/1000 time = 539.7144310474396
I0123 21:35:06.596005 140169189924864 ddar.py:60] Depth 27/1000 time = 546.481288433075
I0123 21:44:13.405180 140169189924864 ddar.py:60] Depth 28/1000 time = 546.8084232807159
I0123 21:44:15.234205 140169189924864 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:44:15.234379 140169189924864 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 21:44:15.234418 140169189924864 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 ; f : C c e f 03 ; g : C a b g 04 T a b f g 05 ; h : C f g h 06 D f g g h 07 ; i : C a c i 08 T a c f i 09 ; j : C f i j 10 D f i i j 11 ; k : C b c k 12 T b c f k 13 ; l : C f k l 14 D f k k l 15 ; m : C c f m 16 D c d d m 17 ; n : C b f n 18 D b d d n 19 ; o : C a f o 20 D a d d o 21 ; p : D f p h p 22 D h p m p 23 ; q : D f q j q 24 D j q n q 25 ; r : D f r l r 26 D l r o r 27 ; s : C a c s 28 T a c b s 29 ; t : C a b t 30 T a b c t 31 ; u : C b s u 32 C c t u 33 ; v : C t u v 34 D t u t v 35 ; w : C s u w 36 D s u s w 37 ; x : C h v x 38 C j w x 39 ? D r f r x {F1} x00
I0123 21:44:15.234453 140169189924864 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 ; f : C c e f 03 ; g : C a b g 04 T a b f g 05 ; h : C f g h 06 D f g g h 07 ; i : C a c i 08 T a c f i 09 ; j : C f i j 10 D f i i j 11 ; k : C b c k 12 T b c f k 13 ; l : C f k l 14 D f k k l 15 ; m : C c f m 16 D c d d m 17 ; n : C b f n 18 D b d d n 19 ; o : C a f o 20 D a d d o 21 ; p : D f p h p 22 D h p m p 23 ; q : D f q j q 24 D j q n q 25 ; r : D f r l r 26 D l r o r 27 ; s : C a c s 28 T a c b s 29 ; t : C a b t 30 T a b c t 31 ; u : C b s u 32 C c t u 33 ; v : C t u v 34 D t u t v 35 ; w : C s u w 36 D s u s w 37 ; x : C h v x 38 C j w x 39 ? D r f r x {F1} x00
I0123 21:44:15.402674 140169189924864 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.402914 140169189924864 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 21:44:15.403015 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403091 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403162 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403230 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403298 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403366 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403434 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403500 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403566 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403633 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403699 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403764 140169189924864 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:44:15.403805 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.403851 140169189924864 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:44:15.403959 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.404000 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.404031 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.406028 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.408616 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.414434 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.414712 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.417379 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.421420 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.421477 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.421517 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.421550 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.421614 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.422320 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.422396 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.422775 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.423579 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.426165 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.426827 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.426906 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.426941 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.427003 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.427139 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.427490 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.427534 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.429454 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.429549 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.431982 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.432061 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.432490 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.434939 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.436872 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.436966 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.437249 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.437330 140169189924864 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:44:15.437438 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.437476 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.437507 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.439290 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.441566 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.447230 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.447491 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.450060 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.453735 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.453789 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.453825 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.453856 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.453919 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.454532 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.454609 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.454962 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.455727 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.458173 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.458797 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.458874 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.458909 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.458966 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.459097 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.459415 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.459457 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.462167 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.462262 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.464708 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.464788 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.465219 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.467471 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.469400 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.469494 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.469790 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.469872 140169189924864 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:44:15.469978 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.470017 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.470049 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.471937 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.474267 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.479811 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.480067 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.482623 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.486425 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.486480 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.486516 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.486548 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.486612 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.487174 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.487250 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.487605 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.488386 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.490821 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.491437 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.491514 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.491549 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.491606 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.491733 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.492105 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.492148 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.494066 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.494160 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.496563 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.496641 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.497066 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.499310 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.501284 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.501378 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.501672 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.501753 140169189924864 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:44:15.501860 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.501899 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.501929 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.503704 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.506014 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.511641 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.511901 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.514428 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.518100 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.518154 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.518190 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.518221 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.518283 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.518901 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.518978 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.519331 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.520085 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.522572 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.523201 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.523277 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.523312 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.523370 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.523498 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.523815 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.523857 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.525867 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.525961 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.528397 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.528476 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.528907 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.531164 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.533075 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.533169 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.533453 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.533533 140169189924864 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:44:15.533648 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.533688 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.533720 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.535595 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.537886 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.543416 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.543673 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.546201 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.549925 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.549979 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.550015 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.550046 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.550109 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.550672 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.550749 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.551101 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.551858 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.554301 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.554927 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.555003 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.555037 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.555101 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.555228 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.555598 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.555641 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.557513 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.557605 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.560039 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.560118 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.560540 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.562814 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.564812 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.564906 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.565193 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.565275 140169189924864 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:44:15.565382 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.565421 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.565452 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.567268 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.569581 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.575638 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.575899 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.578456 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.582117 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.582171 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.582207 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.582237 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.582301 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.582914 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.582992 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.583348 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.584104 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.586556 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.587188 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.587265 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.587300 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.587357 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.587503 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.587826 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.587869 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.589831 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.589924 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.592371 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.592450 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.592876 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.595121 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.597025 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.597119 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.597404 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.597485 140169189924864 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:44:15.597592 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.597630 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.597669 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.599547 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.601860 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.607478 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.607736 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.610265 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.613959 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.614014 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.614050 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.614080 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.614142 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.614702 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.614778 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.615139 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.615899 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.618356 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.618978 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.619061 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.619096 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.619155 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.619284 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.619658 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.619701 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.621618 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.621716 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.624154 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.624233 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.624659 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.626910 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.628908 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.629002 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.629287 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.629367 140169189924864 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:44:15.629475 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.629514 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.629544 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.631343 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.633760 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.639428 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.639688 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.642248 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.645948 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.646003 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.646039 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.646070 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.646132 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.646754 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.646832 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.647187 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.647943 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.650404 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.651032 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.651109 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.651149 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.651209 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.651340 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.651658 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.651700 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.653686 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.653779 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.656185 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.656264 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.656684 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.658911 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.660806 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.660900 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.661187 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.661267 140169189924864 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:44:15.661375 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.661414 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.661445 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.663312 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.665606 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.671218 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.671475 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.674033 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.677737 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.677791 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.677827 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.677858 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.677920 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.678476 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.678552 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.678904 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.679656 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.682079 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.682701 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.682779 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.682814 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.682878 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.683007 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.683326 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.683369 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.685711 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.685806 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.688230 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.688309 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.688732 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.690986 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.692882 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.692976 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.693259 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.693340 140169189924864 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:44:15.693447 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.693485 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.693516 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.695404 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.697706 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.703245 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.703503 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.706082 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.709831 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.709887 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.709922 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.709953 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.710017 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.710578 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.710654 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.711004 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.711768 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.714205 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.714825 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.714902 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.714937 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.714994 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.715134 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.715456 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.715499 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.717468 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.717561 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.720008 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.720088 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.720516 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.722788 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.724709 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.724804 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.725090 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.725172 140169189924864 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:44:15.725281 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.725320 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.725352 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.727230 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.729544 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.735170 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.735427 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.737993 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.741752 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.741807 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.741843 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.741875 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.741939 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.742503 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.742579 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.742933 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.743695 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.746128 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.746748 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.746825 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.746860 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.746918 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.747053 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.747376 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.747419 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.749401 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.749494 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.751938 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.752017 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.752475 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.754730 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.756637 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.756731 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.757019 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.757099 140169189924864 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:44:15.757206 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.757244 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.757275 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.759168 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.761485 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.767101 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.767362 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.769935 140169189924864 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:44:15.773718 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.773772 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.773808 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.773839 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.773902 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.774459 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.774534 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.774888 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.775649 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.778112 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.778736 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.778813 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.778849 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.778906 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.779034 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.779363 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.779406 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.781362 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.781454 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.783934 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.784013 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.784440 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.786737 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.788627 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.788722 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.789008 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.789255 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789323 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789381 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789436 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789490 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789543 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789596 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789655 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789710 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789763 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789816 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789869 140169189924864 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:44:15.789906 140169189924864 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:44:15.792819 140169189924864 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:44:15.837876 140169189924864 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.837963 140169189924864 decoder_stack.py:333] dstack: autoregressive generator.
I0123 21:44:15.838016 140169189924864 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:44:15.838118 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.838155 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.838185 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.838247 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.840590 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.845976 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.846237 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.848799 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:15.861879 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.861942 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.861979 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.862010 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.862073 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.862649 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.862726 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.863085 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.863774 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.866321 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.866951 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.867028 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.867063 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.867121 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.867249 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.867358 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.867397 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.869256 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.869349 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.871752 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.871832 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.871940 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.874540 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.876391 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.876485 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.876773 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.876855 140169189924864 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:44:15.876961 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.877000 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.877031 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.877094 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.879330 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.884744 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.885010 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.887687 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:15.900274 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.900330 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.900373 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.900405 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.900468 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.901031 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.901108 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.901463 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.902218 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.904663 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.905282 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.905360 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.905396 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.905455 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.905586 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.905705 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.905746 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.907601 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.907695 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.910100 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.910181 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.910289 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.912544 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.914412 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.914507 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.914798 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.914879 140169189924864 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:44:15.914987 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.915025 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.915055 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.915118 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.917332 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.922733 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.922993 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.925636 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:15.938270 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.938326 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.938361 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.938399 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.938463 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.939027 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.939103 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.939457 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.940192 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.942661 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.943280 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.943358 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.943393 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.943452 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.943583 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.943691 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.943729 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.945571 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.945671 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.948084 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.948163 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.948270 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.950533 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.952383 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.952478 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.952768 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.952850 140169189924864 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:44:15.952958 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.952997 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.953028 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.953091 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.955343 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.960771 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.961032 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:15.963689 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:15.976181 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:15.976237 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:15.976273 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:15.976310 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.976376 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.976936 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.977013 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.977364 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.978105 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.980534 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.981157 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.981235 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:15.981271 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:15.981329 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.981457 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:15.981564 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:15.981602 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.983453 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.983547 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.985931 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.986011 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:15.986118 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:15.988766 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:15.990633 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.990730 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:15.991015 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.991096 140169189924864 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:44:15.991202 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:15.991241 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:15.991272 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:15.991335 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.993530 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:15.998897 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:15.999160 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.001827 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.014315 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.014370 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.014407 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.014438 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.014509 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.015069 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.015146 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.015497 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.016226 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.018655 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.019271 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.019348 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.019383 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.019440 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.019567 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.019675 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.019713 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.021542 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.021634 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.024010 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.024089 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.024195 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.026437 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.028283 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.028377 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.028664 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.028745 140169189924864 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:44:16.028853 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.028891 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.028921 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.028983 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.031187 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.036567 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.036830 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.039491 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.051980 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.052039 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.052075 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.052106 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.052174 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.052733 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.052809 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.053159 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.053902 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.056310 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.056923 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.057001 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.057036 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.057096 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.057224 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.057333 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.057374 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.059220 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.059319 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.061698 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.061778 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.061888 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.064140 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.065978 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.066073 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.066359 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.066440 140169189924864 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:44:16.066549 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.066587 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.066617 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.066679 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.068889 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.074318 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.074582 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.077214 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.089638 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.089701 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.089736 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.089770 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.089836 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.090406 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.090483 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.090839 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.091579 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.094012 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.094627 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.094704 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.094739 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.094797 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.094925 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.095031 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.095069 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.096890 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.096982 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.099359 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.099439 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.099546 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.102228 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.104075 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.104170 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.104456 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.104539 140169189924864 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:44:16.104645 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.104683 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.104715 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.104777 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.106995 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.112409 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.112683 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.115343 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.127809 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.127863 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.127899 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.127930 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.127993 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.128558 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.128639 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.128988 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.129724 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.132156 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.132775 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.132852 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.132887 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.132944 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.133073 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.133182 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.133220 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.135066 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.135159 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.137527 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.137606 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.137719 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.139968 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.141825 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.141920 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.142207 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.142289 140169189924864 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:44:16.142396 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.142434 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.142465 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.142528 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.144745 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.150150 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.150409 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.153045 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.165522 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.165577 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.165612 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.165649 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.165714 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.166271 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.166356 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.166715 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.167401 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.169918 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.170538 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.170616 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.170651 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.170709 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.170836 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.170943 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.170982 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.172819 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.172912 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.175284 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.175363 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.175471 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.177717 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.179563 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.179656 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.179944 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.180026 140169189924864 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:44:16.180132 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.180170 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.180201 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.180263 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.182478 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.187832 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.188093 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.190749 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.203168 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.203222 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.203258 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.203290 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.203354 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.203915 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.203998 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.204358 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.205044 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.207545 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.208168 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.208245 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.208280 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.208338 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.208468 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.208578 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.208617 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.210465 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.210559 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.212930 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.213009 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.213119 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.215795 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.217670 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.217786 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.218075 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.218156 140169189924864 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:44:16.218264 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.218302 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.218333 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.218395 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.220618 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.225978 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.226241 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.228867 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.241353 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.241408 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.241443 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.241474 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.241537 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.242100 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.242179 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.242539 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.243214 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.245693 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.246306 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.246383 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.246418 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.246475 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.246611 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.246719 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.246756 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.248577 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.248670 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.251073 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.251152 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.251258 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.253508 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.255369 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.255464 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.255753 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.255834 140169189924864 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:44:16.255942 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.255980 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.256011 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.256080 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.258302 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.263670 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.263932 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.266598 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.279134 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.279189 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.279225 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.279257 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.279320 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.279877 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.279954 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.280328 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.281004 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.283504 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.284122 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.284200 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.284235 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.284292 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.284419 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.284526 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.284564 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.286418 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.286512 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.288881 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.288960 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.289068 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.291319 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.293155 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.293249 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.293532 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.293618 140169189924864 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:44:16.296422 140169189924864 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:44:16.346967 140169189924864 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.347055 140169189924864 decoder_stack.py:333] dstack: autoregressive generator.
I0123 21:44:16.347109 140169189924864 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:44:16.347215 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.347254 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.347285 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.347348 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.349631 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.355279 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.355541 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.358123 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.370754 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.370810 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.370846 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.370878 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.370941 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.371513 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.371590 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.371941 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.372614 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.375047 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.375665 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.375742 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.375783 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.375841 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.375970 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.376080 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.376120 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.378046 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.378141 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.380517 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.380596 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.380704 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.382896 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.384736 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.384829 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.385115 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.385197 140169189924864 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:44:16.385303 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.385342 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.385374 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.385436 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.387670 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.393178 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.393440 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.396041 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.408897 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.408953 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.408989 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.409020 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.409083 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.409653 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.409732 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.410084 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.410753 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.413174 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.413797 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.413875 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.413909 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.413970 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.414099 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.414206 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.414245 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.416167 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.416260 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.418656 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.418736 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.418843 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.421002 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.422836 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.422931 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.423218 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.423301 140169189924864 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:44:16.423407 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.423446 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.423477 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.423539 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.425756 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.431148 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.431406 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.433969 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.446453 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.446508 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.446544 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.446576 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.446638 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.447199 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.447276 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.447630 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.448324 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.450769 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.451378 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.451455 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.451490 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.451548 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.451675 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.451783 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.451821 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.453743 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.453837 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.456216 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.456295 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.456402 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.458583 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.460444 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.460539 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.460825 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.460906 140169189924864 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:44:16.461011 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.461049 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.461081 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.461144 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.463351 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.468829 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.469092 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.471686 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.484159 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.484214 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.484249 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.484281 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.484344 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.484902 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.484984 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.485340 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.486023 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.488452 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.489068 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.489147 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.489182 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.489242 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.489371 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.489480 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.489518 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.491441 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.491537 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.493930 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.494010 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.494116 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.496294 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.498148 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.498242 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.498528 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.498610 140169189924864 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:44:16.498718 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.498757 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.498788 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.498852 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.501063 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.506537 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.506799 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.509356 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.522243 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.522298 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.522333 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.522365 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.522427 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.522980 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.523062 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.523450 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.524126 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.526591 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.527221 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.527304 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.527339 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.527396 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.527522 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.527628 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.527667 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.529564 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.529662 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.532012 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.532092 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.532199 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.534358 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.536180 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.536274 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.536561 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.536642 140169189924864 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:44:16.536749 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.536787 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.536817 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.536880 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.539080 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.544486 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.544748 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.547329 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.559754 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.559810 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.559847 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.559878 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.559942 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.560499 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.560575 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.560937 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.561613 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.564051 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.564662 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.564739 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.564775 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.564833 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.564959 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.565067 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.565105 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.567018 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.567111 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.569485 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.569564 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.569679 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.571852 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.573685 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.573781 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.574071 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.574153 140169189924864 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:44:16.574262 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.574301 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.574331 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.574395 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.576605 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.582056 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.582320 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.584871 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.597399 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.597454 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.597491 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.597522 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.597585 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.598148 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.598228 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.598591 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.599264 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.601704 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.602324 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.602401 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.602436 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.602494 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.602620 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.602729 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.602768 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.604680 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.604772 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.607132 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.607212 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.607318 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.609488 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.611337 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.611433 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.611721 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.611804 140169189924864 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:44:16.611912 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.611950 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.611981 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.612043 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.614277 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.619721 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.619984 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.622554 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.635453 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.635510 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.635546 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.635576 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.635639 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.636194 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.636270 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.636626 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.637317 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.639764 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.640383 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.640460 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.640495 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.640554 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.640683 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.640792 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.640830 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.642777 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.642869 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.645229 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.645308 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.645416 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.647601 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.649441 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.649535 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.649831 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.649913 140169189924864 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:44:16.650021 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.650059 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.650090 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.650152 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.652361 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.657808 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.658066 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.660622 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.673083 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.673138 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.673174 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.673205 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.673266 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.673830 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.673907 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.674258 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.674938 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.677357 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.677979 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.678056 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.678092 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.678148 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.678274 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.678382 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.678421 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.680331 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.680424 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.682812 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.682891 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.682997 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.685166 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.686992 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.687087 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.687373 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.687453 140169189924864 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:44:16.687561 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.687599 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.687630 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.687692 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.689890 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.695308 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.695566 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.698133 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.710564 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.710620 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.710655 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.710685 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.710746 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.711300 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.711374 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.711726 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.712398 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.714840 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.715465 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.715541 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.715577 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.715634 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.715760 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.715867 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.715907 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.717822 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.717916 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.720280 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.720356 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.720463 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.722699 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.724544 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.724637 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.724922 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.725002 140169189924864 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:44:16.725108 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.725147 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.725177 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.725238 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.727447 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.732868 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.733129 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.735685 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.748608 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.748663 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.748699 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.748730 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.748791 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.749343 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.749418 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.749779 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.750462 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.752901 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.753549 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.753624 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.753668 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.753726 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.753858 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.753967 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.754006 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.755951 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.756043 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.758434 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.758512 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.758621 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.760798 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.762641 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.762735 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.763021 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.763101 140169189924864 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:44:16.763208 140169189924864 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:44:16.763247 140169189924864 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:44:16.763278 140169189924864 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:44:16.763340 140169189924864 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.765558 140169189924864 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:44:16.771045 140169189924864 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.771307 140169189924864 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:44:16.773900 140169189924864 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:44:16.786500 140169189924864 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:44:16.786555 140169189924864 attention.py:418] Single window, no scan.
I0123 21:44:16.786590 140169189924864 transformer_layer.py:389] tlayer: self-attention.
I0123 21:44:16.786622 140169189924864 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.786683 140169189924864 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.787269 140169189924864 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.787344 140169189924864 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.787703 140169189924864 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.788381 140169189924864 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.790821 140169189924864 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.791450 140169189924864 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.791528 140169189924864 transformer_layer.py:468] tlayer: End windows.
I0123 21:44:16.791562 140169189924864 transformer_layer.py:472] tlayer: final FFN.
I0123 21:44:16.791620 140169189924864 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.791746 140169189924864 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:44:16.791855 140169189924864 nn_components.py:325] mlp: activation = None
I0123 21:44:16.791893 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.793825 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.793918 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.796310 140169189924864 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.796388 140169189924864 transformer_base.py:443] tbase: final FFN
I0123 21:44:16.796495 140169189924864 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:44:16.798707 140169189924864 nn_components.py:329] mlp: final activation = None
I0123 21:44:16.800548 140169189924864 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.800645 140169189924864 nn_components.py:261] mlp: residual
I0123 21:44:16.800935 140169189924864 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:44:16.801020 140169189924864 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:44:16.803873 140169189924864 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
