I0123 12:32:19.894724 140147660288000 inference_utils.py:69] Parsing gin configuration.
I0123 12:32:19.894839 140147660288000 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:32:19.895036 140147660288000 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:32:19.895068 140147660288000 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:32:19.895095 140147660288000 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:32:19.895120 140147660288000 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:32:19.895145 140147660288000 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:32:19.895170 140147660288000 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:32:19.895195 140147660288000 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:32:19.895220 140147660288000 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:32:19.895244 140147660288000 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:32:19.895268 140147660288000 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:32:19.895314 140147660288000 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:32:19.895454 140147660288000 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:32:19.895688 140147660288000 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:32:19.895794 140147660288000 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:32:19.902218 140147660288000 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:32:19.902339 140147660288000 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:32:19.902661 140147660288000 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:32:19.902765 140147660288000 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:32:19.903044 140147660288000 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:32:19.903143 140147660288000 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:32:19.903546 140147660288000 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:32:19.903645 140147660288000 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:32:19.907302 140147660288000 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:32:20.012639 140147660288000 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:32:20.013445 140147660288000 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:32:20.019942 140147660288000 training_loop.py:335] Process 0 of 1
I0123 12:32:20.019995 140147660288000 training_loop.py:336] Local device count = 1
I0123 12:32:20.020032 140147660288000 training_loop.py:337] Number of replicas = 1
I0123 12:32:20.020062 140147660288000 training_loop.py:339] Using random number seed 42
I0123 12:32:20.532909 140147660288000 training_loop.py:359] Initializing the model.
I0123 12:32:20.941664 140147660288000 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.942028 140147660288000 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:32:20.942134 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942214 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942293 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942377 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942448 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942518 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942589 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942658 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942728 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942796 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942864 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942933 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:20.942974 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:20.943021 140147660288000 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:20.943135 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:20.943176 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:20.943206 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:20.945278 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.950714 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:20.961557 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.961859 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:20.966328 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:20.977394 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:20.977452 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:20.977490 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:20.977522 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.977584 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.978876 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.978971 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.979680 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.982155 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.988427 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.989741 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.989825 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:20.989860 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:20.989920 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.990050 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:20.990377 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:20.990426 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:20.992350 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.992452 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:20.995372 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:20.995456 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:20.995977 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.006235 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.015143 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.015241 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.015537 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.015619 140147660288000 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:21.015728 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.015767 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.015803 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.017673 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.020176 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.025709 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.025974 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.028638 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.032450 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.032506 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.032540 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.032569 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.032629 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.033195 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.033270 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.033626 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.034392 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.036867 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.037502 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.037577 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.037611 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.037676 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.037804 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.038126 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.038169 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.040085 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.040177 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.042660 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.042739 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.043167 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.045436 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.047306 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.047400 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.047696 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.047776 140147660288000 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:21.047883 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.047920 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.047950 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.050185 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.052532 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.057996 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.058258 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.060883 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.064617 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.064672 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.064705 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.064735 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.064795 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.065350 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.065423 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.065787 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.066552 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.069080 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.069745 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.069825 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.069860 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.069918 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.070043 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.070369 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.070412 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.072367 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.072460 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.074954 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.075044 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.075524 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.077772 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.079667 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.079766 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.080059 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.080140 140147660288000 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:21.080248 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.080287 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.080317 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.082216 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.084597 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.090181 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.090439 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.093080 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.096804 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.096858 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.096892 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.096921 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.096981 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.097527 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.097601 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.097963 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.098716 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.101230 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.101851 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.101927 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.101962 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.102019 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.102145 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.102458 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.102501 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.104368 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.104461 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.107003 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.107088 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.107509 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.109753 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.111639 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.111732 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.112019 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.112098 140147660288000 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:21.112204 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.112242 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.112272 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.114161 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.116501 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.122033 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.122291 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.125254 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.128914 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.128969 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.129003 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.129033 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.129093 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.129658 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.129733 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.130089 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.130842 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.133376 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.133996 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.134073 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.134107 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.134165 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.134298 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.134614 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.134656 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.136520 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.136616 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.139128 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.139211 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.139637 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.141857 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.143789 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.143886 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.144176 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.144257 140147660288000 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:21.144364 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.144401 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.144430 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.146273 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.148632 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.154235 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.154495 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.157139 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.160788 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.160843 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.160878 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.160907 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.160966 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.161556 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.161632 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.162001 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.162774 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.165230 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.165851 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.165931 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.165966 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.166024 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.166151 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.166469 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.166511 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.168395 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.168488 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.171026 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.171109 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.171539 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.173814 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.175695 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.175788 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.176078 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.176162 140147660288000 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:21.176272 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.176310 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.176340 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.178363 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.180799 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.186327 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.186587 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.189200 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.192934 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.192989 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.193024 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.193054 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.193115 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.193675 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.193752 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.194108 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.194874 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.197322 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.197946 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.198023 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.198057 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.198114 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.198240 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.198563 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.198607 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.200840 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.200933 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.203416 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.203495 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.203922 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.351543 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.353884 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.354043 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.354373 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.354470 140147660288000 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:21.354587 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.354629 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.354663 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.356772 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.359326 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.365072 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.365342 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.368117 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.372135 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.372193 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.372229 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.372260 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.372322 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.372946 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.373021 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.373386 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.374172 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.376855 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.377495 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.377572 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.377606 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.377674 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.377805 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.378133 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.378176 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.380149 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.380243 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.382769 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.382850 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.383325 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.385597 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.387501 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.387602 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.387890 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.387970 140147660288000 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:21.388077 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.388115 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.388144 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.390052 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.392408 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.397981 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.398243 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.400911 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.404659 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.404714 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.404750 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.404779 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.404839 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.405397 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.405477 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.405847 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.406616 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.409152 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.409780 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.409857 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.409892 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.409950 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.410075 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.410398 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.410441 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.412687 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.412781 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.415329 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.415410 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.415842 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.418121 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.420013 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.420108 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.420397 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.420486 140147660288000 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:21.420598 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.420636 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.420666 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.422568 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.424928 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.430773 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.431037 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.433745 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.437447 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.437502 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.437538 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.437568 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.437629 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.438191 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.438266 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.438630 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.439439 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.441926 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.442548 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.442625 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.442658 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.442715 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.442840 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.443154 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.443201 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.445081 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.445173 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.447699 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.447780 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.448208 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.450456 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.452397 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.452491 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.452782 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.452870 140147660288000 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:21.452981 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.453019 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.453050 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.454903 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.457327 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.462827 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.463087 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.465745 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.469420 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.469475 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.469510 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.469539 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.469646 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.470203 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.470281 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.470638 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.471394 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.473885 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.474507 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.474583 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.474617 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.474674 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.474799 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.475117 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.475161 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.477089 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.477181 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.479925 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.480006 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.480436 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.482717 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.484605 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.484698 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.484986 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.485067 140147660288000 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:21.485183 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.485222 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.485253 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.487082 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.489514 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.495042 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.495302 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.497931 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:21.502007 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.502062 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.502096 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.502125 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.502186 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.502738 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.502814 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.503171 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.503930 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.506404 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.507020 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.507101 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.507134 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.507192 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.507317 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.507632 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.507674 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.509592 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.509692 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.512164 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.512242 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.512814 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.515174 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.517048 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.517145 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.517433 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.517721 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.517790 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.517854 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.517910 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.517962 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.518014 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.518065 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.518115 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.518166 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.518216 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.518266 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.518316 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:21.518352 140147660288000 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:21.521854 140147660288000 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:21.573861 140147660288000 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.573981 140147660288000 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:21.574036 140147660288000 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:21.574145 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.574186 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.574216 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.574286 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.576746 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.582388 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.582643 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.585355 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.602103 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.602162 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.602198 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.602228 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.602288 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.603464 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.603543 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.604273 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.606273 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.611113 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.612406 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.612491 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.612526 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.612587 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.612716 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.612826 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.612864 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.614968 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.615076 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.617548 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.617628 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.617753 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.620036 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.622010 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.622106 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.622398 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.622479 140147660288000 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:21.622587 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.622625 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.622656 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.622719 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.625034 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.630481 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.630740 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.633492 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.646500 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.646556 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.646590 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.646621 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.646685 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.647240 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.647314 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.647670 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.648353 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.650851 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.651476 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.651551 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.651590 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.651647 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.651777 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.651904 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.651950 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.653883 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.653975 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.656432 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.656510 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.656615 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.658832 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.660784 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.660878 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.661163 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.661243 140147660288000 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:21.661351 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.661388 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.661419 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.661480 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.663740 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.669201 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.669455 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.672199 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.684726 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.684782 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.684817 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.684845 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.684905 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.685453 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.685527 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.685893 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.686581 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.689039 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.689654 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.689730 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.689763 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.689826 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.689956 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.690063 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.690101 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.692001 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.692093 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.694497 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.694576 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.694685 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.696864 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.698771 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.698866 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.699153 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.699234 140147660288000 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:21.699341 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.699379 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.699410 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.699471 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.701719 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.707100 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.707358 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.710029 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.722615 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.722670 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.722704 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.722734 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.722808 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.723385 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.723461 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.723827 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.724530 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.727014 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.727633 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.727711 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.727746 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.727814 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.727970 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.728080 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.728119 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.730319 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.730414 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.732876 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.732954 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.733059 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.735258 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.737098 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.737191 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.737474 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.737553 140147660288000 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:21.737667 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.737706 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.737736 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.737798 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.740079 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.745426 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.745700 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.748292 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.760720 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.760775 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.760809 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.760838 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.760898 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.761452 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.761527 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.761893 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.762585 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.765127 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.765754 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.765830 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.765864 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.765921 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.766052 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.766159 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.766196 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.768054 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.768146 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.770526 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.770605 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.770713 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.772941 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.774775 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.774870 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.775153 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.775233 140147660288000 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:21.775340 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.775378 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.775408 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.775468 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.777684 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.783046 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.783305 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.785979 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.798361 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.798415 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.798449 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.798479 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.798538 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.799085 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.799160 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.799517 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.800207 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.802680 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.803295 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.803370 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.803404 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.803461 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.803587 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.803701 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.803740 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.805631 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.805732 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.808102 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.808179 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.808285 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.810494 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.812329 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.812421 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.812706 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.812787 140147660288000 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:21.812895 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.812933 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.812963 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.813025 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.815235 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.820783 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.821037 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.823634 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.836498 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.836553 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.836588 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.836616 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.836676 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.837220 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.837294 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.837658 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.838345 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.840813 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.841468 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.841543 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.841578 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.841634 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.841773 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.841880 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.841923 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.843765 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.843858 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.846251 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.846331 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.846435 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.848678 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.850587 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.850682 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.850971 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.851052 140147660288000 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:21.851159 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.851197 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.851226 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.851287 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.853489 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.858886 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.859163 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.861817 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.874418 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.874473 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.874507 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.874537 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.874597 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.875223 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.875298 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.875656 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.876367 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.878883 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.879519 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.879595 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.879629 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.879688 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.879814 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.879919 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.879962 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.881840 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.881932 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.884380 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.884458 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.884563 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.886793 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.888664 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.888758 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.889045 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.889126 140147660288000 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:21.889235 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.889273 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.889303 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.889364 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.891636 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.897153 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.897408 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.900065 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.912683 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.912739 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.912773 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.912803 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.912864 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.913414 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.913489 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.913852 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.914535 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.917092 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.917769 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.917847 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.917881 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.917937 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.918064 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.918169 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.918207 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.920098 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.920192 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.922569 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.922648 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.922753 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.924999 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.926970 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.927066 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.927353 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.927434 140147660288000 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:21.927542 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.927581 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.927612 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.927673 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.929948 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.935378 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.935635 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.938627 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.951332 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.951387 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.951421 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.951450 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.951511 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.952133 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.952208 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.952567 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.953261 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.955782 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.956422 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.956498 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.956531 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.956591 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.956718 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.956823 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.956861 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.958710 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.958807 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.961265 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.961344 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.961447 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:21.963681 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.965565 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.965667 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.965956 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.966037 140147660288000 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:21.966144 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:21.966183 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:21.966213 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:21.966274 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.968529 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:21.974061 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.974315 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:21.976983 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:21.989555 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:21.989609 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:21.989650 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:21.989681 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.989741 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.990289 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.990365 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.990720 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.991429 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.993901 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.994554 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.994630 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:21.994664 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:21.994720 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.994864 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:21.994988 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:21.995026 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:21.996929 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.997026 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:21.999440 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:21.999518 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:21.999624 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.001801 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.003696 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.003790 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.004075 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.004155 140147660288000 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:22.004262 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.004301 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.004331 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.004393 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.006610 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.011934 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.012188 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.014777 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.027176 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.027230 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.027263 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.027292 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.027353 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.027906 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.027980 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.028335 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.029018 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.031538 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.032146 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.032222 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.032256 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.032313 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.032441 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.032546 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.032583 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.034458 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.034552 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.036940 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.037018 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.037123 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.039703 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.041554 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.041656 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.041945 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.042029 140147660288000 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:22.044869 140147660288000 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:22.100178 140147660288000 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.100268 140147660288000 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:22.100321 140147660288000 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:22.100425 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.100462 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.100490 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.100551 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.102876 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.108250 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.108506 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.111077 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.123255 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.123313 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.123347 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.123377 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.123436 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.123987 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.124063 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.124413 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.125082 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.127572 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.128178 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.128253 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.128287 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.128343 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.128467 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.128586 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.128625 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.130449 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.130542 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.132893 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.132971 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.133078 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.135299 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.137115 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.137209 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.137490 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.137570 140147660288000 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:22.137685 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.137725 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.137755 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.137817 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.140017 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.145303 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.145554 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.148179 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.160234 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.160290 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.160325 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.160355 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.160415 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.160970 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.161046 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.161395 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.162080 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.164617 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.165223 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.165299 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.165333 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.165390 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.165517 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.165623 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.165684 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.167511 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.167604 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.169970 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.170048 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.170154 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.172380 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.174221 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.174315 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.174599 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.174679 140147660288000 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:22.174785 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.174823 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.174853 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.174913 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.177121 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.182431 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.182685 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.185314 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.197530 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.197586 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.197620 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.197658 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.197721 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.198271 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.198347 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.198700 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.199361 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.202423 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.203037 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.203113 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.203147 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.203204 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.203327 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.203432 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.203469 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.205281 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.205373 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.207725 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.207804 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.207910 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.210143 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.211973 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.212067 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.212351 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.212430 140147660288000 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:22.212535 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.212572 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.212602 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.212661 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.214851 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.220132 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.220385 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.223016 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.235173 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.235227 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.235266 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.235304 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.235366 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.235927 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.236000 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.236355 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.237030 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.239519 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.240124 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.240198 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.240231 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.240287 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.240409 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.240514 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.240552 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.242400 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.242491 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.244843 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.244919 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.245027 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.247287 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.249117 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.249208 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.249493 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.249571 140147660288000 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:22.249683 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.249722 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.249750 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.249809 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.252015 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.257305 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.257553 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.260197 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.272486 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.272540 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.272573 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.272602 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.272660 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.273215 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.273289 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.273648 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.274335 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.276898 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.277505 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.277579 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.277612 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.277674 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.277798 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.277902 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.277939 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.279822 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.279936 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.282300 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.282377 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.282481 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.284756 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.286607 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.286701 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.286984 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.287063 140147660288000 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:22.287168 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.287204 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.287232 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.287291 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.289501 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.294886 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.295137 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.297797 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.310320 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.310374 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.310406 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.310435 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.310493 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.311040 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.311114 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.311466 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.312134 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.315056 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.315670 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.315745 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.315778 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.315832 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.315954 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.316059 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.316096 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.317937 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.318033 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.320390 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.320466 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.320569 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.322824 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.324662 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.324754 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.325034 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.325112 140147660288000 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:22.325215 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.325251 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.325279 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.325338 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.327560 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.332889 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.333139 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.335814 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.348063 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.348116 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.348149 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.348178 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.348237 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.348787 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.348860 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.349215 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.349910 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.352441 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.353057 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.353131 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.353164 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.353219 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.353343 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.353447 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.353483 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.355402 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.355493 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.357872 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.357949 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.358054 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.360300 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.362147 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.362241 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.362523 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.362602 140147660288000 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:22.362707 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.362743 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.362772 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.362832 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.365036 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.370424 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.370681 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.373327 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.385652 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.385706 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.385739 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.385766 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.385825 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.386370 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.386444 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.386795 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.387469 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.389998 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.390624 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.390698 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.390731 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.390788 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.390913 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.391016 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.391052 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.392887 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.392977 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.395333 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.395416 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.395523 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.397785 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.399619 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.399712 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.399993 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.400072 140147660288000 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:22.400176 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.400213 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.400241 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.400300 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.402503 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.407826 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.408082 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.410757 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.422964 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.423017 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.423049 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.423077 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.423135 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.423686 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.423762 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.424112 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.424793 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.427689 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.428304 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.428379 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.428411 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.428466 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.428590 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.428694 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.428730 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.430570 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.430662 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.433009 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.433092 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.433197 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.435471 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.437305 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.437397 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.437686 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.437766 140147660288000 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:22.437871 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.437908 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.437937 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.437998 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.440197 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.445526 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.445788 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.448450 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.460721 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.460775 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.460808 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.460836 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.460896 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.461447 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.461523 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.461881 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.462554 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.465064 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.465686 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.465765 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.465798 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.465854 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.465978 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.466087 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.466125 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.468405 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.468497 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.470835 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.470917 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.471029 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.473249 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.475076 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.475168 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.475449 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.475528 140147660288000 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:22.475635 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.475671 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.475700 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.475759 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.478052 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.483372 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.483627 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.486317 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.498561 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.498615 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.498649 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.498677 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.498739 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.499301 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.499375 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.499723 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.500403 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.502932 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.503547 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.503622 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.503655 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.503710 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.503837 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.503941 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.503978 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.505812 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.505902 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.508247 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.508322 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.508425 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.510672 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.512502 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.512593 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.512875 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.512953 140147660288000 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:22.513057 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:22.513093 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:22.513121 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:22.513180 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.515399 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:22.520691 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.520944 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:22.523597 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:22.535809 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:22.535862 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:22.535895 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:22.535923 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.535983 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.536530 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.536605 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.536951 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.537628 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.540486 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.541095 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.541169 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:22.541202 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:22.541256 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.541378 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:22.541488 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:22.541525 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.543359 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.543449 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.545808 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.545885 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:22.545988 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:22.548215 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:22.550061 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.550154 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:22.550435 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.550520 140147660288000 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:22.553300 140147660288000 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:26.990596 140147660288000 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:32:27.527340 140147660288000 training_loop.py:409] No working directory specified.
I0123 12:32:27.527473 140147660288000 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:32:27.528279 140147660288000 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:32:31.018200 140147660288000 training_loop.py:447] Only restoring trainable parameters.
I0123 12:32:31.018915 140147660288000 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:32:31.018992 140147660288000 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.019042 140147660288000 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.019086 140147660288000 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.019128 140147660288000 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.019169 140147660288000 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.019209 140147660288000 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.019249 140147660288000 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.019288 140147660288000 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.019326 140147660288000 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.019366 140147660288000 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.019404 140147660288000 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.019441 140147660288000 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.019478 140147660288000 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.019514 140147660288000 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.019551 140147660288000 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.019587 140147660288000 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.019623 140147660288000 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.019659 140147660288000 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.019695 140147660288000 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.019757 140147660288000 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.019795 140147660288000 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.019831 140147660288000 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.019868 140147660288000 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.019903 140147660288000 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.019940 140147660288000 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.019976 140147660288000 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020014 140147660288000 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020050 140147660288000 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.020085 140147660288000 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.020121 140147660288000 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020157 140147660288000 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.020192 140147660288000 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.020228 140147660288000 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.020263 140147660288000 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020298 140147660288000 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.020334 140147660288000 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020370 140147660288000 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020405 140147660288000 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.020440 140147660288000 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.020476 140147660288000 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020512 140147660288000 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.020549 140147660288000 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.020586 140147660288000 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.020622 140147660288000 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020658 140147660288000 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.020699 140147660288000 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020736 140147660288000 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020772 140147660288000 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.020808 140147660288000 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.020844 140147660288000 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.020880 140147660288000 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.020916 140147660288000 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.020951 140147660288000 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.020987 140147660288000 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021024 140147660288000 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.021059 140147660288000 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021094 140147660288000 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021130 140147660288000 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.021165 140147660288000 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.021201 140147660288000 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021236 140147660288000 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.021271 140147660288000 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.021307 140147660288000 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.021341 140147660288000 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021376 140147660288000 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.021412 140147660288000 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021448 140147660288000 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021484 140147660288000 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.021518 140147660288000 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.021554 140147660288000 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021589 140147660288000 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.021625 140147660288000 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.021691 140147660288000 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.021730 140147660288000 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021767 140147660288000 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.021803 140147660288000 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021839 140147660288000 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021874 140147660288000 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.021909 140147660288000 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.021945 140147660288000 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.021981 140147660288000 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.022016 140147660288000 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.022051 140147660288000 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.022086 140147660288000 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022122 140147660288000 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.022157 140147660288000 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022192 140147660288000 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022228 140147660288000 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.022262 140147660288000 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.022297 140147660288000 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022331 140147660288000 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.022367 140147660288000 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.022402 140147660288000 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.022436 140147660288000 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022471 140147660288000 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.022506 140147660288000 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022542 140147660288000 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022578 140147660288000 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.022613 140147660288000 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.022654 140147660288000 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022691 140147660288000 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.022728 140147660288000 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.022763 140147660288000 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.022799 140147660288000 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022835 140147660288000 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.022870 140147660288000 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022904 140147660288000 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.022940 140147660288000 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.022975 140147660288000 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.023010 140147660288000 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.023045 140147660288000 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.023081 140147660288000 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:31.023116 140147660288000 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:31.023152 140147660288000 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.023187 140147660288000 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.023222 140147660288000 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.023257 140147660288000 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.023292 140147660288000 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:31.023326 140147660288000 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:31.023361 140147660288000 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:31.023396 140147660288000 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:31.023424 140147660288000 training_loop.py:725] Total parameters: 152072288
I0123 12:32:31.023630 140147660288000 training_loop.py:739] Total state size: 0
I0123 12:32:31.046491 140147660288000 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:32:31.046750 140147660288000 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:32:31.047112 140147660288000 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:32:31.047458 140147660288000 training_loop.py:89] registering functions: dict_keys([])
I0123 12:32:31.068383 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h ? coll j e k
I0123 12:32:31.668210 140147660288000 ddar.py:60] Depth 1/1000 time = 0.5494015216827393
I0123 12:32:34.647195 140147660288000 ddar.py:60] Depth 2/1000 time = 2.97882342338562
I0123 12:32:37.035885 140147660288000 ddar.py:60] Depth 3/1000 time = 2.3884994983673096
I0123 12:32:40.037570 140147660288000 ddar.py:60] Depth 4/1000 time = 3.0014700889587402
I0123 12:32:43.219652 140147660288000 ddar.py:60] Depth 5/1000 time = 3.181779623031616
I0123 12:32:46.330015 140147660288000 ddar.py:60] Depth 6/1000 time = 3.1100404262542725
I0123 12:32:49.568795 140147660288000 ddar.py:60] Depth 7/1000 time = 3.2383532524108887
I0123 12:32:49.595712 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:32:49.595795 140147660288000 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 12:32:49.595830 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00
I0123 12:32:49.595858 140147660288000 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00
I0123 12:32:49.747666 140147660288000 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.747860 140147660288000 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:32:49.747956 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748029 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748099 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748166 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748234 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748300 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748366 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748431 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748497 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748562 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748628 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748692 140147660288000 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:32:49.748731 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:49.748774 140147660288000 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:49.748878 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:49.748915 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:49.748943 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:49.750791 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.753208 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:49.758877 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.759141 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:49.761698 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:49.765512 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:49.765567 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:49.765600 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:49.765632 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.765701 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.766317 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.766391 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.766749 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.767497 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.769971 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.770585 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.770658 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:49.770691 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:49.770747 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.770870 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:49.771194 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:49.771236 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.773600 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.773699 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.776117 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.776194 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:49.776607 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:49.778899 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.780796 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.780888 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.781176 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.781254 140147660288000 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:49.781358 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:49.781395 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:49.781424 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:49.783256 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.785528 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:49.790976 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.791232 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:49.793811 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:49.797388 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:49.797440 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:49.797474 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:49.797502 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.797561 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.798114 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.798188 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.798533 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.799274 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.801695 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.802357 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.802432 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:49.802465 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:49.802520 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.802643 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:49.802949 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:49.802990 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.804861 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.804950 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.807358 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.807435 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:49.807844 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:49.810126 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.811992 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.812085 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.812370 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.812450 140147660288000 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:49.812555 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:49.812592 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:49.812621 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:49.814388 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.816679 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:49.822379 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.822640 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:49.825199 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:49.828766 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:49.828819 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:49.828852 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:49.828882 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.828943 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.829533 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.829607 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.829969 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.830711 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.833143 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.833758 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.833833 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:49.833867 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:49.833922 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.834047 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:49.834358 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:49.834401 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.836340 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.836431 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.838876 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.838955 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:49.839374 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:49.841617 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.843554 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.843646 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.843932 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.844011 140147660288000 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:49.844115 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:49.844152 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:49.844182 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:49.846036 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.848333 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:49.853893 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.854148 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:49.856707 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:49.860350 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:49.860404 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:49.860437 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:49.860466 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.860527 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.861073 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.861147 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.861498 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.862261 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.864712 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.865319 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.865394 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:49.865427 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:49.865482 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.865606 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:49.865976 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:49.866019 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.867929 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.868020 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.870465 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.870544 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:49.870960 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:49.873176 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.875135 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.875229 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.875516 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.875596 140147660288000 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:49.875701 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:49.875738 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:49.875767 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:49.877539 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.879848 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:49.885816 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.886069 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:49.888604 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:49.892197 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:49.892253 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:49.892286 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:49.892315 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.892424 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.892976 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.893051 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.893397 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.894163 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.896559 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.897161 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.897236 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:49.897269 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:49.897325 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.897450 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:49.897773 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:49.897816 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.899741 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.899833 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.902274 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.902354 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:49.902794 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:49.905015 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.906908 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.907002 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.907287 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.907367 140147660288000 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:49.907471 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:49.907509 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:49.907538 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:49.909372 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.911714 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:49.917190 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.917440 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:49.920033 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:49.923863 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:49.923917 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:49.923957 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:49.923988 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.924048 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.924595 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.924669 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.925019 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.925783 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.928206 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.928857 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.928933 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:49.928965 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:49.929020 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.929172 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:49.929493 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:49.929534 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.931423 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.931514 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.933963 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.934041 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:49.934462 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:49.936754 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.938653 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.938745 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.939031 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.939111 140147660288000 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:49.939216 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:49.939253 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:49.939281 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:49.941046 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.943379 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:49.948970 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.949220 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:49.951761 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:49.955318 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:49.955372 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:49.955405 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:49.955439 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.955549 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.956094 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.956166 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.956510 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.957252 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.959665 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.960270 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.960345 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:49.960377 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:49.960431 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.960554 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:49.960860 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:49.960901 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.962806 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.962897 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.965324 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.965402 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:49.965834 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:49.968038 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.969910 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.970002 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.970285 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.970363 140147660288000 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:49.970466 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:49.970503 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:49.970532 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:49.972360 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.974649 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:49.980131 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.980384 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:49.982964 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:49.986545 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:49.986599 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:49.986632 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:49.986660 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.986725 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.987277 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.987352 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.987705 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.988453 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.990877 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.991882 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.991960 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:49.991994 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:49.992049 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.992172 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:49.992475 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:49.992516 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:49.994395 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.994485 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:49.996865 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:49.996942 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:49.997357 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:49.999644 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.001526 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.001620 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.001921 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.002002 140147660288000 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:50.002107 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.002144 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.002173 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.003951 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.006263 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.011852 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.012104 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.014663 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:50.018269 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.018324 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.018357 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.018385 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.018450 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.019065 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.019140 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.019495 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.020261 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.022760 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.023568 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.023644 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.023678 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.023733 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.023862 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.024176 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.024218 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.026119 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.026210 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.028835 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.028913 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.029330 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.031568 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.033446 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.033539 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.033832 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.033913 140147660288000 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:50.034018 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.034055 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.034084 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.035830 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.038200 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.043749 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.044005 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.046564 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:50.050191 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.050244 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.050277 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.050306 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.050420 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.050982 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.051057 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.051409 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.052168 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.054627 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.055242 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.055318 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.055351 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.055409 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.055535 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.055847 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.055887 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.057793 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.057889 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.060384 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.060463 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.060876 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.063128 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.065007 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.065099 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.065383 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.065462 140147660288000 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:50.065567 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.065604 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.065634 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.067410 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.069781 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.075331 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.075582 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.078113 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:50.081677 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.081732 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.081765 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.081793 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.081904 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.082454 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.082534 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.082890 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.083647 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.086080 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.086682 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.086756 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.086789 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.086844 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.086968 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.087275 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.087317 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.089197 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.089287 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.091758 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.091835 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.092253 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.094499 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.096372 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.096464 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.096749 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.096826 140147660288000 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:50.096930 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.096966 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.096995 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.098774 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.101136 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.106653 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.106906 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.109443 140147660288000 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:50.113045 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.113099 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.113131 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.113160 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.113655 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.114232 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.114312 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.114675 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.115436 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.117877 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.118486 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.118561 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.118594 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.118649 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.118774 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.119083 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.119124 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.121006 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.121096 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.123578 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.123656 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.124072 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.126298 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.128470 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.128563 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.128847 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.129091 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129156 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129209 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129261 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129311 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129361 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129410 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129459 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129507 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129556 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129605 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129667 140147660288000 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:32:50.129704 140147660288000 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:50.132812 140147660288000 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:50.176945 140147660288000 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.177035 140147660288000 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:50.177088 140147660288000 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:50.177189 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.177224 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.177253 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.177310 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.179968 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.185304 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.185556 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.188117 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.200690 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.200746 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.200779 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.200807 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.200868 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.201427 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.201503 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.201871 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.202550 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.205083 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.205705 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.205787 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.205820 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.205876 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.206003 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.206109 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.206147 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.207965 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.208057 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.210432 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.210510 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.210614 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.212831 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.214661 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.214755 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.215044 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.215130 140147660288000 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:50.215236 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.215274 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.215302 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.215362 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.217561 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.222882 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.223134 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.225791 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.238338 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.238393 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.238426 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.238455 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.238516 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.239067 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.239141 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.239489 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.240211 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.242679 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.243283 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.243358 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.243390 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.243446 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.243571 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.243675 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.243713 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.245537 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.245628 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.248020 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.248098 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.248202 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.250435 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.252280 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.252373 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.252660 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.252745 140147660288000 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:50.252851 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.252889 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.252918 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.252977 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.255188 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.260514 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.260768 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.263430 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.275702 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.275758 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.275791 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.275819 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.275881 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.276425 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.276499 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.276848 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.277577 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.280049 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.280667 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.280742 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.280775 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.280831 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.280957 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.281063 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.281099 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.282922 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.283013 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.285397 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.285475 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.285582 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.288255 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.290119 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.290212 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.290499 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.290578 140147660288000 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:50.290688 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.290725 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.290755 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.290813 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.293030 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.298351 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.298606 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.301258 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.313459 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.313513 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.313546 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.313575 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.313634 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.314196 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.314271 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.314620 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.315347 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.317797 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.318410 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.318485 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.318518 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.318573 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.318698 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.318801 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.318838 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.320663 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.320754 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.323158 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.323236 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.323340 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.325566 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.327404 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.327497 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.327786 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.327867 140147660288000 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:50.327972 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.328018 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.328050 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.328109 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.330335 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.335856 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.336116 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.338773 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.350927 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.350981 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.351014 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.351043 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.351102 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.351643 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.351718 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.352072 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.352805 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.355264 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.355877 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.355951 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.355984 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.356040 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.356165 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.356269 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.356306 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.358157 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.358249 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.360621 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.360699 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.360805 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.363059 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.364892 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.364983 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.365268 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.365349 140147660288000 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:50.365452 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.365490 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.365527 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.365589 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.367820 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.373159 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.373410 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.376060 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.388309 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.388363 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.388397 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.388426 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.388487 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.389032 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.389106 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.389457 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.390196 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.392646 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.393255 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.393330 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.393363 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.393418 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.393544 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.393654 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.393693 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.395506 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.395596 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.397959 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.398036 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.398141 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.400791 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.402631 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.402724 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.403012 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.403091 140147660288000 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:50.403195 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.403231 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.403266 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.403327 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.405531 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.410925 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.411178 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.413825 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.425914 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.425969 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.426001 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.426030 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.426088 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.426626 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.426700 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.427045 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.427770 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.430222 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.430826 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.430901 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.430934 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.430989 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.431116 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.431218 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.431254 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.433065 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.433154 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.435580 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.435658 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.435761 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.437960 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.439773 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.439866 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.440160 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.440239 140147660288000 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:50.440343 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.440380 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.440409 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.440475 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.442693 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.448029 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.448291 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.450940 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.463066 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.463120 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.463153 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.463181 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.463241 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.463791 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.463865 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.464216 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.464947 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.467396 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.468002 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.468077 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.468110 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.468164 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.468292 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.468398 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.468436 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.470264 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.470355 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.472717 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.472794 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.472900 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.475136 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.476961 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.477052 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.477336 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.477416 140147660288000 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:50.477520 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.477557 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.477586 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.477653 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.479866 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.485192 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.485447 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.488076 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.500323 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.500375 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.500409 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.500438 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.500499 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.501044 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.501118 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.501470 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.502163 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.504687 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.505294 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.505369 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.505403 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.505457 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.505586 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.505699 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.505743 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.507570 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.507661 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.510043 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.510121 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.510226 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.512873 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.514706 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.514800 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.515086 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.515166 140147660288000 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:50.515270 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.515308 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.515336 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.515395 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.517598 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.522916 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.523174 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.525828 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.538065 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.538120 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.538154 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.538184 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.538244 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.538787 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.538862 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.539220 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.539896 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.542395 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.542995 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.543070 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.543103 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.543159 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.543283 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.543387 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.543424 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.545255 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.545345 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.547722 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.547800 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.547904 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.550135 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.551954 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.552046 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.552331 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.552410 140147660288000 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:50.552515 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.552552 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.552582 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.552641 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.554839 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.560132 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.560385 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.562992 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.575115 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.575168 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.575201 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.575230 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.575289 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.575833 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.575907 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.576262 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.576944 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.579461 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.580072 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.580148 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.580183 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.580238 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.580363 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.580468 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.580505 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.582338 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.582429 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.584781 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.584857 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.584962 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.587171 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.588969 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.589061 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.589344 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.589424 140147660288000 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:50.589528 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.589565 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.589594 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.589659 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.591844 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.597146 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.597400 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.600044 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.612188 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.612242 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.612275 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.612305 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.612365 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.612909 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.612983 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.613333 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.614020 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.616516 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.617119 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.617194 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.617229 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.617285 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.617412 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.617517 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.617554 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.619374 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.619465 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.621827 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.621905 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.622009 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.624604 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.626428 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.626519 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.626803 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.626886 140147660288000 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:50.629667 140147660288000 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:50.678979 140147660288000 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.679065 140147660288000 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:50.679116 140147660288000 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:50.679217 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.679253 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.679287 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.679347 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.681594 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.686960 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.687211 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.689732 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.702104 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.702158 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.702191 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.702220 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.702280 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.702827 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.702902 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.703250 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.703910 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.706306 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.706906 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.706981 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.707014 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.707068 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.707193 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.707299 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.707336 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.709200 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.709290 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.711642 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.711720 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.711825 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.713954 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.715746 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.715839 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.716128 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.716207 140147660288000 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:50.716311 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.716348 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.716384 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.716446 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.718648 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.723999 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.724255 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.726790 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.738774 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.738827 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.738860 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.738889 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.738948 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.739491 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.739565 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.739911 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.740574 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.742979 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.743582 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.743658 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.743691 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.743747 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.743870 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.743974 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.744010 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.745895 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.745987 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.748336 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.748413 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.748516 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.750652 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.752455 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.752546 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.752831 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.752911 140147660288000 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:50.753015 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.753052 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.753081 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.753153 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.755346 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.760774 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.761026 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.763559 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.775533 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.775587 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.775620 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.775650 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.775711 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.776251 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.776326 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.776674 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.777331 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.779717 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.780313 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.780387 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.780419 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.780475 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.780598 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.780701 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.780738 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.782624 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.782717 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.785049 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.785126 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.785231 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.787373 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.789177 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.789269 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.789553 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.789633 140147660288000 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:50.789745 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.789783 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.789812 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.789872 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.792075 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.797461 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.797721 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.800267 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.812866 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.812920 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.812952 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.812980 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.813040 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.813582 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.813662 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.814012 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.814674 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.817058 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.817663 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.817738 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.817770 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.817825 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.817949 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.818053 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.818090 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.819951 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.820041 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.822410 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.822487 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.822591 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.824740 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.826546 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.826638 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.826922 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.827002 140147660288000 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:50.827105 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.827142 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.827170 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.827229 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.829419 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.834819 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.835076 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.837647 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.849775 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.849828 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.849862 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.849890 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.849950 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.850492 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.850567 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.850916 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.851581 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.854000 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.854603 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.854677 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.854710 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.854766 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.854889 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.854995 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.855032 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.856926 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.857017 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.859618 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.859696 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.859801 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.861974 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.863785 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.863877 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.864161 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.864241 140147660288000 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:50.864345 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.864382 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.864410 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.864470 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.866670 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.872016 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.872267 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.874816 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.886988 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.887048 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.887081 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.887109 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.887170 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.887741 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.887815 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.888170 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.888843 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.891294 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.891894 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.891969 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.892002 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.892057 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.892181 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.892285 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.892322 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.894222 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.894313 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.896645 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.896722 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.896827 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.898980 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.900805 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.900898 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.901181 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.901260 140147660288000 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:50.901363 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.901399 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.901428 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.901487 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.903663 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.909005 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.909267 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.911821 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.924349 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.924402 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.924435 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.924463 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.924523 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.925067 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.925141 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.925491 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.926168 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.928583 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.929181 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.929255 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.929288 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.929342 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.929466 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.929569 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.929606 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.931506 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.931597 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.933945 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.934023 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.934128 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.936275 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.938115 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.938208 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.938495 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.938573 140147660288000 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:50.938677 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.938714 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.938743 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.938802 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.940996 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.946411 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.946673 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.949242 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.961732 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.961786 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.961818 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.961846 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.961905 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.962452 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.962526 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.962877 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.963551 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.966007 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.966619 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.966692 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:50.966726 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:50.966781 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.966904 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:50.967009 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:50.967047 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.968942 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.969030 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.971409 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.971486 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:50.971590 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:50.973759 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:50.975586 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.975678 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:50.975959 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.976036 140147660288000 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:50.976140 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:50.976176 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:50.976205 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:50.976264 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.978454 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:50.983829 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.984088 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:50.986646 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:50.998750 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:50.998805 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:50.998838 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:50.998866 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.998924 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.999468 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.999541 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:50.999888 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.000551 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.002974 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.003574 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.003648 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:51.003681 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:51.003736 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.003860 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:51.003966 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:51.004004 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:51.005888 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.005978 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:51.008328 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.008404 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:51.008510 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:51.010659 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:51.012459 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.012550 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:51.012835 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.012912 140147660288000 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:51.013017 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:51.013053 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:51.013083 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:51.013143 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.015311 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:51.020614 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.020863 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:51.023389 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:51.035826 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:51.035924 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:51.035960 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:51.035990 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.036050 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.036594 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.036666 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.037019 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.037696 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.040105 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.040706 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.040779 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:51.040811 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:51.040865 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.040987 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:51.041092 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:51.041129 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:51.043007 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.043097 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:51.045431 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.045506 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:51.045610 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:51.047761 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:51.049551 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.049647 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:51.049938 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.050016 140147660288000 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:51.050119 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:51.050156 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:51.050185 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:51.050244 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.052400 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:51.057724 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.057983 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:51.060503 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:51.072677 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:51.072732 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:51.072766 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:51.072794 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.072853 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.073393 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.073466 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.073822 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.074490 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.076888 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.077493 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.077566 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:51.077599 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:51.077662 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.077789 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:51.077894 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:51.077932 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:51.079817 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.079907 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:51.082262 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.082339 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:51.082445 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:51.084590 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:51.086410 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.086502 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:51.086786 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.086865 140147660288000 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:51.086969 140147660288000 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:51.087006 140147660288000 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:51.087035 140147660288000 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:51.087095 140147660288000 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.089263 140147660288000 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:51.094605 140147660288000 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.094856 140147660288000 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:51.097388 140147660288000 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:51.109629 140147660288000 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:51.109696 140147660288000 attention.py:418] Single window, no scan.
I0123 12:32:51.109730 140147660288000 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:51.109757 140147660288000 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.109815 140147660288000 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.110360 140147660288000 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.110434 140147660288000 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.110781 140147660288000 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.111453 140147660288000 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.113860 140147660288000 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.114464 140147660288000 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.114539 140147660288000 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:51.114572 140147660288000 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:51.114627 140147660288000 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.114750 140147660288000 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:51.114856 140147660288000 nn_components.py:325] mlp: activation = None
I0123 12:32:51.114892 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:51.116776 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.116865 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:51.119206 140147660288000 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.119282 140147660288000 transformer_base.py:443] tbase: final FFN
I0123 12:32:51.119386 140147660288000 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:51.121518 140147660288000 nn_components.py:329] mlp: final activation = None
I0123 12:32:51.123326 140147660288000 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.123418 140147660288000 nn_components.py:261] mlp: residual
I0123 12:32:51.123701 140147660288000 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:51.123783 140147660288000 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:51.126561 140147660288000 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:03.075800 140147660288000 alphageometry.py:566] LM output (score=-0.785902): "l : C h i l 15 D h l i l 16 ;"
I0123 12:33:03.076076 140147660288000 alphageometry.py:567] Translation: "l = on_line l h i, on_bline l i h"

I0123 12:33:03.076125 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l h i, on_bline l i h ? coll j e k"
I0123 12:33:03.076267 140147660288000 graph.py:498] 
I0123 12:33:03.076319 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l h i, on_bline l i h ? coll j e k
I0123 12:33:04.154837 140147660288000 ddar.py:60] Depth 1/1000 time = 1.0186612606048584
I0123 12:33:07.831221 140147660288000 ddar.py:60] Depth 2/1000 time = 3.6761717796325684
I0123 12:33:11.049363 140147660288000 ddar.py:60] Depth 3/1000 time = 3.2179322242736816
I0123 12:33:14.942486 140147660288000 ddar.py:60] Depth 4/1000 time = 3.892888307571411
I0123 12:33:18.173772 140147660288000 ddar.py:60] Depth 5/1000 time = 3.2310643196105957
I0123 12:33:21.940759 140147660288000 ddar.py:60] Depth 6/1000 time = 3.7667694091796875
I0123 12:33:26.567384 140147660288000 ddar.py:60] Depth 7/1000 time = 4.626353979110718
I0123 12:33:31.249109 140147660288000 ddar.py:60] Depth 8/1000 time = 4.6813952922821045
I0123 12:33:36.130677 140147660288000 ddar.py:60] Depth 9/1000 time = 4.881308555603027
I0123 12:33:40.980572 140147660288000 ddar.py:60] Depth 10/1000 time = 4.849260568618774
I0123 12:33:46.137505 140147660288000 ddar.py:60] Depth 11/1000 time = 5.096375942230225
I0123 12:33:46.142642 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:33:46.142825 140147660288000 alphageometry.py:566] LM output (score=-1.135278): "l : C b c l 15 D b l c l 16 ;"
I0123 12:33:46.142865 140147660288000 alphageometry.py:567] Translation: "l = on_line l b c, on_bline l c b"

I0123 12:33:46.142920 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b c, on_bline l c b ? coll j e k"
I0123 12:33:46.143085 140147660288000 graph.py:498] 
I0123 12:33:46.143137 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b c, on_bline l c b ? coll j e k
I0123 12:33:47.396800 140147660288000 ddar.py:60] Depth 1/1000 time = 1.193246841430664
I0123 12:33:51.260788 140147660288000 ddar.py:60] Depth 2/1000 time = 3.863779067993164
I0123 12:33:54.804025 140147660288000 ddar.py:60] Depth 3/1000 time = 3.5430171489715576
I0123 12:33:59.057923 140147660288000 ddar.py:60] Depth 4/1000 time = 4.253680229187012
I0123 12:34:03.808930 140147660288000 ddar.py:60] Depth 5/1000 time = 4.750762939453125
I0123 12:34:07.557565 140147660288000 ddar.py:60] Depth 6/1000 time = 3.7483301162719727
I0123 12:34:11.527305 140147660288000 ddar.py:60] Depth 7/1000 time = 3.969165802001953
I0123 12:34:11.572086 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:34:11.572216 140147660288000 alphageometry.py:566] LM output (score=-1.193239): "l : C a i l 15 D a l i l 16 ;"
I0123 12:34:11.572253 140147660288000 alphageometry.py:567] Translation: "l = on_line l a i, on_bline l i a"

I0123 12:34:11.572288 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a i, on_bline l i a ? coll j e k"
I0123 12:34:11.572457 140147660288000 graph.py:498] 
I0123 12:34:11.572508 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a i, on_bline l i a ? coll j e k
I0123 12:34:12.340585 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7080965042114258
I0123 12:34:15.770036 140147660288000 ddar.py:60] Depth 2/1000 time = 3.4290850162506104
I0123 12:34:19.077604 140147660288000 ddar.py:60] Depth 3/1000 time = 3.307337522506714
I0123 12:34:23.120180 140147660288000 ddar.py:60] Depth 4/1000 time = 4.042311906814575
I0123 12:34:27.442470 140147660288000 ddar.py:60] Depth 5/1000 time = 4.322014570236206
I0123 12:34:31.861449 140147660288000 ddar.py:60] Depth 6/1000 time = 4.418719530105591
I0123 12:34:36.313143 140147660288000 ddar.py:60] Depth 7/1000 time = 4.451106786727905
I0123 12:34:36.355733 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:34:36.355851 140147660288000 alphageometry.py:566] LM output (score=-1.449960): "l : C a c l 15 D a l c l 16 ;"
I0123 12:34:36.355890 140147660288000 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2635, in add_clause
    raise PointTooCloseError()
graph.PointTooCloseError
"

I0123 12:34:36.355923 140147660288000 alphageometry.py:566] LM output (score=-1.495577): "l : C b g l 15 D b l g l 16 ;"
I0123 12:34:36.355948 140147660288000 alphageometry.py:567] Translation: "l = on_line l b g, on_bline l g b"

I0123 12:34:36.355977 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b g, on_bline l g b ? coll j e k"
I0123 12:34:36.356124 140147660288000 graph.py:498] 
I0123 12:34:36.356175 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b g, on_bline l g b ? coll j e k
I0123 12:34:37.406370 140147660288000 ddar.py:60] Depth 1/1000 time = 0.9901068210601807
I0123 12:34:41.118727 140147660288000 ddar.py:60] Depth 2/1000 time = 3.7121307849884033
I0123 12:34:44.497316 140147660288000 ddar.py:60] Depth 3/1000 time = 3.378347396850586
I0123 12:34:48.323254 140147660288000 ddar.py:60] Depth 4/1000 time = 3.8257081508636475
I0123 12:34:52.421177 140147660288000 ddar.py:60] Depth 5/1000 time = 4.097701787948608
I0123 12:34:56.259106 140147660288000 ddar.py:60] Depth 6/1000 time = 3.8376951217651367
I0123 12:35:00.958561 140147660288000 ddar.py:60] Depth 7/1000 time = 4.699243783950806
I0123 12:35:06.058177 140147660288000 ddar.py:60] Depth 8/1000 time = 5.099393844604492
I0123 12:35:10.798410 140147660288000 ddar.py:60] Depth 9/1000 time = 4.7399632930755615
I0123 12:35:15.775047 140147660288000 ddar.py:60] Depth 10/1000 time = 4.976073980331421
I0123 12:35:20.778921 140147660288000 ddar.py:60] Depth 11/1000 time = 4.942720651626587
I0123 12:35:20.785066 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:35:20.785200 140147660288000 alphageometry.py:566] LM output (score=-1.530939): "l : C g i l 15 D g l i l 16 ;"
I0123 12:35:20.785237 140147660288000 alphageometry.py:567] Translation: "l = on_line l g i, on_bline l i g"

I0123 12:35:20.785279 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l g i, on_bline l i g ? coll j e k"
I0123 12:35:20.785429 140147660288000 graph.py:498] 
I0123 12:35:20.785481 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l g i, on_bline l i g ? coll j e k
I0123 12:35:21.575199 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7297418117523193
I0123 12:35:25.441870 140147660288000 ddar.py:60] Depth 2/1000 time = 3.866447687149048
I0123 12:35:29.117568 140147660288000 ddar.py:60] Depth 3/1000 time = 3.6753604412078857
I0123 12:35:33.285200 140147660288000 ddar.py:60] Depth 4/1000 time = 4.167403936386108
I0123 12:35:37.103574 140147660288000 ddar.py:60] Depth 5/1000 time = 3.8181331157684326
I0123 12:35:41.177013 140147660288000 ddar.py:60] Depth 6/1000 time = 4.073199033737183
I0123 12:35:45.706760 140147660288000 ddar.py:60] Depth 7/1000 time = 4.529449224472046
I0123 12:35:49.774823 140147660288000 ddar.py:60] Depth 8/1000 time = 4.067378759384155
I0123 12:35:54.164392 140147660288000 ddar.py:60] Depth 9/1000 time = 4.336939573287964
I0123 12:35:54.164631 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:35:54.164762 140147660288000 alphageometry.py:566] LM output (score=-1.575214): "l : C b d l 15 D b l d l 16 ;"
I0123 12:35:54.164798 140147660288000 alphageometry.py:567] Translation: "l = on_line l b d, on_bline l d b"

I0123 12:35:54.164843 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b d, on_bline l d b ? coll j e k"
I0123 12:35:54.164996 140147660288000 graph.py:498] 
I0123 12:35:54.165047 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b d, on_bline l d b ? coll j e k
I0123 12:35:55.151921 140147660288000 ddar.py:60] Depth 1/1000 time = 0.9273314476013184
I0123 12:35:58.550453 140147660288000 ddar.py:60] Depth 2/1000 time = 3.3983306884765625
I0123 12:36:01.956252 140147660288000 ddar.py:60] Depth 3/1000 time = 3.4055933952331543
I0123 12:36:05.465525 140147660288000 ddar.py:60] Depth 4/1000 time = 3.509045124053955
I0123 12:36:09.605369 140147660288000 ddar.py:60] Depth 5/1000 time = 4.139541149139404
I0123 12:36:13.652996 140147660288000 ddar.py:60] Depth 6/1000 time = 4.047207832336426
I0123 12:36:17.896560 140147660288000 ddar.py:60] Depth 7/1000 time = 4.242983341217041
I0123 12:36:17.932703 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:36:17.932837 140147660288000 alphageometry.py:566] LM output (score=-1.669458): "l : C b f l 15 D b l f l 16 ;"
I0123 12:36:17.932873 140147660288000 alphageometry.py:567] Translation: "l = on_line l b f, on_bline l f b"

I0123 12:36:17.932915 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b f, on_bline l f b ? coll j e k"
I0123 12:36:17.933073 140147660288000 graph.py:498] 
I0123 12:36:17.933124 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b f, on_bline l f b ? coll j e k
I0123 12:36:18.721177 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7266106605529785
I0123 12:36:22.422460 140147660288000 ddar.py:60] Depth 2/1000 time = 3.701106309890747
I0123 12:36:25.431607 140147660288000 ddar.py:60] Depth 3/1000 time = 3.0089235305786133
I0123 12:36:29.176283 140147660288000 ddar.py:60] Depth 4/1000 time = 3.7444071769714355
I0123 12:36:33.134155 140147660288000 ddar.py:60] Depth 5/1000 time = 3.957528829574585
I0123 12:36:37.400060 140147660288000 ddar.py:60] Depth 6/1000 time = 4.265616178512573
I0123 12:36:41.692738 140147660288000 ddar.py:60] Depth 7/1000 time = 4.292097568511963
I0123 12:36:45.881229 140147660288000 ddar.py:60] Depth 8/1000 time = 4.147510051727295
I0123 12:36:45.881507 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:36:45.881620 140147660288000 alphageometry.py:566] LM output (score=-1.761005): "l : C b h l 15 D b l h l 16 ;"
I0123 12:36:45.881690 140147660288000 alphageometry.py:567] Translation: "l = on_line l b h, on_bline l h b"

I0123 12:36:45.881738 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b h, on_bline l h b ? coll j e k"
I0123 12:36:45.881893 140147660288000 graph.py:498] 
I0123 12:36:45.881945 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b h, on_bline l h b ? coll j e k
I0123 12:36:46.921577 140147660288000 ddar.py:60] Depth 1/1000 time = 0.9791133403778076
I0123 12:36:50.584787 140147660288000 ddar.py:60] Depth 2/1000 time = 3.662929058074951
I0123 12:36:54.159094 140147660288000 ddar.py:60] Depth 3/1000 time = 3.5739634037017822
I0123 12:36:58.294569 140147660288000 ddar.py:60] Depth 4/1000 time = 4.135218858718872
I0123 12:37:02.613524 140147660288000 ddar.py:60] Depth 5/1000 time = 4.318731784820557
I0123 12:37:07.882904 140147660288000 ddar.py:60] Depth 6/1000 time = 5.269141435623169
I0123 12:37:13.473435 140147660288000 ddar.py:60] Depth 7/1000 time = 5.590182542800903
I0123 12:37:19.019421 140147660288000 ddar.py:60] Depth 8/1000 time = 5.545546531677246
I0123 12:37:24.543279 140147660288000 ddar.py:60] Depth 9/1000 time = 5.5232861042022705
I0123 12:37:30.153846 140147660288000 ddar.py:60] Depth 10/1000 time = 5.557392358779907
I0123 12:37:30.154114 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:37:30.154245 140147660288000 alphageometry.py:566] LM output (score=-1.826113): "l : C a c l 15 T a c d l 16 ;"
I0123 12:37:30.154282 140147660288000 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2635, in add_clause
    raise PointTooCloseError()
graph.PointTooCloseError
"

I0123 12:37:30.154322 140147660288000 alphageometry.py:566] LM output (score=-1.903323): "l : C c i l 15 D c l i l 16 ;"
I0123 12:37:30.154349 140147660288000 alphageometry.py:567] Translation: "l = on_line l c i, on_bline l i c"

I0123 12:37:30.154383 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l c i, on_bline l i c ? coll j e k"
I0123 12:37:30.154540 140147660288000 graph.py:498] 
I0123 12:37:30.154591 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l c i, on_bline l i c ? coll j e k
I0123 12:37:30.999782 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7838289737701416
I0123 12:37:35.358363 140147660288000 ddar.py:60] Depth 2/1000 time = 4.358315944671631
I0123 12:37:39.155956 140147660288000 ddar.py:60] Depth 3/1000 time = 3.797227382659912
I0123 12:37:43.520201 140147660288000 ddar.py:60] Depth 4/1000 time = 4.364022731781006
I0123 12:37:48.095921 140147660288000 ddar.py:60] Depth 5/1000 time = 4.575480699539185
I0123 12:37:52.201678 140147660288000 ddar.py:60] Depth 6/1000 time = 4.105477809906006
I0123 12:37:56.086963 140147660288000 ddar.py:60] Depth 7/1000 time = 3.884675979614258
I0123 12:37:56.128149 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:37:56.128314 140147660288000 alphageometry.py:566] LM output (score=-1.907002): "l : C c g l 15 D c l g l 16 ;"
I0123 12:37:56.128351 140147660288000 alphageometry.py:567] Translation: "l = on_line l c g, on_bline l g c"

I0123 12:37:56.128429 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l c g, on_bline l g c ? coll j e k"
I0123 12:37:56.128598 140147660288000 graph.py:498] 
I0123 12:37:56.128648 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l c g, on_bline l g c ? coll j e k
I0123 12:37:56.970645 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7817444801330566
I0123 12:38:00.984419 140147660288000 ddar.py:60] Depth 2/1000 time = 4.01354718208313
I0123 12:38:04.757424 140147660288000 ddar.py:60] Depth 3/1000 time = 3.7727837562561035
I0123 12:38:09.140356 140147660288000 ddar.py:60] Depth 4/1000 time = 4.382713794708252
I0123 12:38:13.733041 140147660288000 ddar.py:60] Depth 5/1000 time = 4.592450141906738
I0123 12:38:18.459098 140147660288000 ddar.py:60] Depth 6/1000 time = 4.7257301807403564
I0123 12:38:23.485892 140147660288000 ddar.py:60] Depth 7/1000 time = 5.026049375534058
I0123 12:38:23.525863 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:38:23.525994 140147660288000 alphageometry.py:566] LM output (score=-1.955943): "l : C a c l 15 D a c a l 16 ;"
I0123 12:38:23.526031 140147660288000 alphageometry.py:567] Translation: "l = on_line l a c, on_circle l a c"

I0123 12:38:23.526073 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a c, on_circle l a c ? coll j e k"
I0123 12:38:23.526232 140147660288000 graph.py:498] 
I0123 12:38:23.526283 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a c, on_circle l a c ? coll j e k
I0123 12:38:24.386698 140147660288000 ddar.py:60] Depth 1/1000 time = 0.8003945350646973
I0123 12:38:28.514096 140147660288000 ddar.py:60] Depth 2/1000 time = 4.127206087112427
I0123 12:38:32.169592 140147660288000 ddar.py:60] Depth 3/1000 time = 3.655273675918579
I0123 12:38:36.297022 140147660288000 ddar.py:60] Depth 4/1000 time = 4.127192497253418
I0123 12:38:39.808075 140147660288000 ddar.py:60] Depth 5/1000 time = 3.5107998847961426
I0123 12:38:43.073336 140147660288000 ddar.py:60] Depth 6/1000 time = 3.264920711517334
I0123 12:38:46.574673 140147660288000 ddar.py:60] Depth 7/1000 time = 3.5007970333099365
I0123 12:38:50.087544 140147660288000 ddar.py:60] Depth 8/1000 time = 3.5120275020599365
I0123 12:38:50.120159 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:38:50.120291 140147660288000 alphageometry.py:566] LM output (score=-1.969684): "l : C a c l 15 D a c c l 16 ;"
I0123 12:38:50.120328 140147660288000 alphageometry.py:567] Translation: "l = on_line l a c, on_circle l c a"

I0123 12:38:50.120365 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a c, on_circle l c a ? coll j e k"
I0123 12:38:50.120535 140147660288000 graph.py:498] 
I0123 12:38:50.120586 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a c, on_circle l c a ? coll j e k
I0123 12:38:51.256532 140147660288000 ddar.py:60] Depth 1/1000 time = 1.0741808414459229
I0123 12:38:55.437747 140147660288000 ddar.py:60] Depth 2/1000 time = 4.18100118637085
I0123 12:38:58.850825 140147660288000 ddar.py:60] Depth 3/1000 time = 3.4128506183624268
I0123 12:39:02.983978 140147660288000 ddar.py:60] Depth 4/1000 time = 4.132911682128906
I0123 12:39:07.335499 140147660288000 ddar.py:60] Depth 5/1000 time = 4.351248502731323
I0123 12:39:10.907035 140147660288000 ddar.py:60] Depth 6/1000 time = 3.571272134780884
I0123 12:39:14.179250 140147660288000 ddar.py:60] Depth 7/1000 time = 3.2716944217681885
I0123 12:39:17.706761 140147660288000 ddar.py:60] Depth 8/1000 time = 3.526668071746826
I0123 12:39:17.740047 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:39:17.740181 140147660288000 alphageometry.py:566] LM output (score=-1.986011): "l : C a h l 15 D a l h l 16 ;"
I0123 12:39:17.740218 140147660288000 alphageometry.py:567] Translation: "l = on_line l a h, on_bline l h a"

I0123 12:39:17.740253 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a h, on_bline l h a ? coll j e k"
I0123 12:39:17.740405 140147660288000 graph.py:498] 
I0123 12:39:17.740457 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a h, on_bline l h a ? coll j e k
I0123 12:39:18.799565 140147660288000 ddar.py:60] Depth 1/1000 time = 0.9989275932312012
I0123 12:39:22.447555 140147660288000 ddar.py:60] Depth 2/1000 time = 3.6477179527282715
I0123 12:39:26.126779 140147660288000 ddar.py:60] Depth 3/1000 time = 3.6788766384124756
I0123 12:39:30.118324 140147660288000 ddar.py:60] Depth 4/1000 time = 3.991323471069336
I0123 12:39:34.688494 140147660288000 ddar.py:60] Depth 5/1000 time = 4.569934129714966
I0123 12:39:39.073211 140147660288000 ddar.py:60] Depth 6/1000 time = 4.384463787078857
I0123 12:39:43.727539 140147660288000 ddar.py:60] Depth 7/1000 time = 4.653641939163208
I0123 12:39:43.771665 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:39:43.771837 140147660288000 alphageometry.py:566] LM output (score=-2.002628): "l : C a g l 15 D a l g l 16 ;"
I0123 12:39:43.771875 140147660288000 alphageometry.py:567] Translation: "l = on_line l a g, on_bline l g a"

I0123 12:39:43.771926 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a g, on_bline l g a ? coll j e k"
I0123 12:39:43.772094 140147660288000 graph.py:498] 
I0123 12:39:43.772146 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a g, on_bline l g a ? coll j e k
I0123 12:39:44.538595 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7066376209259033
I0123 12:39:48.385272 140147660288000 ddar.py:60] Depth 2/1000 time = 3.846463441848755
I0123 12:39:51.549778 140147660288000 ddar.py:60] Depth 3/1000 time = 3.1642863750457764
I0123 12:39:55.451352 140147660288000 ddar.py:60] Depth 4/1000 time = 3.901350736618042
I0123 12:39:59.046005 140147660288000 ddar.py:60] Depth 5/1000 time = 3.594425678253174
I0123 12:40:02.769303 140147660288000 ddar.py:60] Depth 6/1000 time = 3.7230286598205566
I0123 12:40:06.230695 140147660288000 ddar.py:60] Depth 7/1000 time = 3.460779905319214
I0123 12:40:06.275852 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:40:06.276029 140147660288000 alphageometry.py:566] LM output (score=-2.101787): "l : C a c l 15 T a c b l 16 ;"
I0123 12:40:06.276068 140147660288000 alphageometry.py:567] Translation: "l = on_line l a c, on_tline l b a c"

I0123 12:40:06.276114 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a c, on_tline l b a c ? coll j e k"
I0123 12:40:06.276271 140147660288000 graph.py:498] 
I0123 12:40:06.276322 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a c, on_tline l b a c ? coll j e k
I0123 12:40:07.438460 140147660288000 ddar.py:60] Depth 1/1000 time = 1.1131837368011475
I0123 12:40:11.594814 140147660288000 ddar.py:60] Depth 2/1000 time = 4.15613055229187
I0123 12:40:15.363561 140147660288000 ddar.py:60] Depth 3/1000 time = 3.768524646759033
I0123 12:40:19.251827 140147660288000 ddar.py:60] Depth 4/1000 time = 3.8880386352539062
I0123 12:40:22.789892 140147660288000 ddar.py:60] Depth 5/1000 time = 3.537837505340576
I0123 12:40:26.442626 140147660288000 ddar.py:60] Depth 6/1000 time = 3.6524219512939453
I0123 12:40:30.099238 140147660288000 ddar.py:60] Depth 7/1000 time = 3.655918598175049
I0123 12:40:30.130386 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:40:30.130524 140147660288000 alphageometry.py:566] LM output (score=-2.134666): "l : C a b l 15 D a l b l 16 ;"
I0123 12:40:30.130562 140147660288000 alphageometry.py:567] Translation: "l = on_line l a b, on_bline l b a"

I0123 12:40:30.130603 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a b, on_bline l b a ? coll j e k"
I0123 12:40:30.130760 140147660288000 graph.py:498] 
I0123 12:40:30.130819 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a b, on_bline l b a ? coll j e k
I0123 12:40:31.211989 140147660288000 ddar.py:60] Depth 1/1000 time = 1.0206375122070312
I0123 12:40:35.115395 140147660288000 ddar.py:60] Depth 2/1000 time = 3.9031877517700195
I0123 12:40:38.409242 140147660288000 ddar.py:60] Depth 3/1000 time = 3.293621063232422
I0123 12:40:42.732665 140147660288000 ddar.py:60] Depth 4/1000 time = 4.323199272155762
I0123 12:40:47.018302 140147660288000 ddar.py:60] Depth 5/1000 time = 4.285362482070923
I0123 12:40:51.725675 140147660288000 ddar.py:60] Depth 6/1000 time = 4.7069292068481445
I0123 12:40:56.464231 140147660288000 ddar.py:60] Depth 7/1000 time = 4.737890005111694
I0123 12:40:56.505927 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:40:56.506074 140147660288000 alphageometry.py:566] LM output (score=-2.184323): "l : C d i l 15 D d l i l 16 ;"
I0123 12:40:56.506112 140147660288000 alphageometry.py:567] Translation: "l = on_line l d i, on_bline l i d"

I0123 12:40:56.506159 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l d i, on_bline l i d ? coll j e k"
I0123 12:40:56.506318 140147660288000 graph.py:498] 
I0123 12:40:56.506367 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l d i, on_bline l i d ? coll j e k
I0123 12:40:57.297672 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7291984558105469
I0123 12:41:00.841174 140147660288000 ddar.py:60] Depth 2/1000 time = 3.5433430671691895
I0123 12:41:03.886992 140147660288000 ddar.py:60] Depth 3/1000 time = 3.0455894470214844
I0123 12:41:07.464972 140147660288000 ddar.py:60] Depth 4/1000 time = 3.577745199203491
I0123 12:41:11.828595 140147660288000 ddar.py:60] Depth 5/1000 time = 4.363339900970459
I0123 12:41:15.085436 140147660288000 ddar.py:60] Depth 6/1000 time = 3.256535053253174
I0123 12:41:18.324136 140147660288000 ddar.py:60] Depth 7/1000 time = 3.238112449645996
I0123 12:41:22.091621 140147660288000 ddar.py:60] Depth 8/1000 time = 3.7268543243408203
I0123 12:41:22.091873 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:22.092007 140147660288000 alphageometry.py:566] LM output (score=-2.277595): "l : C b e l 15 D b l e l 16 ;"
I0123 12:41:22.092044 140147660288000 alphageometry.py:567] Translation: "l = on_line l b e, on_bline l e b"

I0123 12:41:22.092089 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b e, on_bline l e b ? coll j e k"
I0123 12:41:22.092440 140147660288000 graph.py:498] 
I0123 12:41:22.092490 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l b e, on_bline l e b ? coll j e k
I0123 12:41:22.865101 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7114660739898682
I0123 12:41:26.356054 140147660288000 ddar.py:60] Depth 2/1000 time = 3.490787982940674
I0123 12:41:29.503591 140147660288000 ddar.py:60] Depth 3/1000 time = 3.147306203842163
I0123 12:41:33.352174 140147660288000 ddar.py:60] Depth 4/1000 time = 3.8482539653778076
I0123 12:41:37.084275 140147660288000 ddar.py:60] Depth 5/1000 time = 3.731839179992676
I0123 12:41:40.050098 140147660288000 ddar.py:60] Depth 6/1000 time = 2.965582847595215
I0123 12:41:43.018007 140147660288000 ddar.py:60] Depth 7/1000 time = 2.9673373699188232
I0123 12:41:43.049280 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:43.049401 140147660288000 alphageometry.py:566] LM output (score=-2.380574): "l : C a f l 15 D a f f l 16 ;"
I0123 12:41:43.049438 140147660288000 alphageometry.py:567] Translation: "l = on_line l a f, on_circle l f a"

I0123 12:41:43.049474 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a f, on_circle l f a ? coll j e k"
I0123 12:41:43.049635 140147660288000 graph.py:498] 
I0123 12:41:43.049695 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a f, on_circle l f a ? coll j e k
I0123 12:41:43.820382 140147660288000 ddar.py:60] Depth 1/1000 time = 0.710075855255127
I0123 12:41:47.941510 140147660288000 ddar.py:60] Depth 2/1000 time = 4.120932340621948
I0123 12:41:52.432322 140147660288000 ddar.py:60] Depth 3/1000 time = 4.49057412147522
I0123 12:41:57.628490 140147660288000 ddar.py:60] Depth 4/1000 time = 5.195921897888184
I0123 12:42:03.814688 140147660288000 ddar.py:60] Depth 5/1000 time = 6.1859776973724365
I0123 12:42:08.999231 140147660288000 ddar.py:60] Depth 6/1000 time = 5.1842827796936035
I0123 12:42:14.157011 140147660288000 ddar.py:60] Depth 7/1000 time = 5.157181024551392
I0123 12:42:14.202473 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:14.202627 140147660288000 alphageometry.py:566] LM output (score=-2.455510): "l : C c d l 15 D c l d l 16 ;"
I0123 12:42:14.202667 140147660288000 alphageometry.py:567] Translation: "l = on_line l c d, on_bline l d c"

I0123 12:42:14.202719 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l c d, on_bline l d c ? coll j e k"
I0123 12:42:14.202893 140147660288000 graph.py:498] 
I0123 12:42:14.202946 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l c d, on_bline l d c ? coll j e k
I0123 12:42:15.340945 140147660288000 ddar.py:60] Depth 1/1000 time = 1.0771336555480957
I0123 12:42:18.910245 140147660288000 ddar.py:60] Depth 2/1000 time = 3.5690817832946777
I0123 12:42:22.153495 140147660288000 ddar.py:60] Depth 3/1000 time = 3.243027687072754
I0123 12:42:25.807106 140147660288000 ddar.py:60] Depth 4/1000 time = 3.653362989425659
I0123 12:42:29.402816 140147660288000 ddar.py:60] Depth 5/1000 time = 3.595484972000122
I0123 12:42:32.774407 140147660288000 ddar.py:60] Depth 6/1000 time = 3.371325731277466
I0123 12:42:36.553536 140147660288000 ddar.py:60] Depth 7/1000 time = 3.778602361679077
I0123 12:42:36.586400 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:36.586533 140147660288000 alphageometry.py:566] LM output (score=-2.487749): "l : C a c l 15 D a e e l 16 ;"
I0123 12:42:36.586570 140147660288000 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2635, in add_clause
    raise PointTooCloseError()
graph.PointTooCloseError
"

I0123 12:42:36.586608 140147660288000 alphageometry.py:566] LM output (score=-2.672210): "l : C a e l 15 D a l e l 16 ;"
I0123 12:42:36.586635 140147660288000 alphageometry.py:567] Translation: "l = on_line l a e, on_bline l e a"

I0123 12:42:36.586668 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a e, on_bline l e a ? coll j e k"
I0123 12:42:36.586825 140147660288000 graph.py:498] 
I0123 12:42:36.586874 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a e, on_bline l e a ? coll j e k
I0123 12:42:37.451181 140147660288000 ddar.py:60] Depth 1/1000 time = 0.8021690845489502
I0123 12:42:41.693898 140147660288000 ddar.py:60] Depth 2/1000 time = 4.242521286010742
I0123 12:42:45.116327 140147660288000 ddar.py:60] Depth 3/1000 time = 3.4222066402435303
I0123 12:42:49.363740 140147660288000 ddar.py:60] Depth 4/1000 time = 4.247190952301025
I0123 12:42:53.460564 140147660288000 ddar.py:60] Depth 5/1000 time = 4.096593618392944
I0123 12:42:57.149196 140147660288000 ddar.py:60] Depth 6/1000 time = 3.68831205368042
I0123 12:43:00.511960 140147660288000 ddar.py:60] Depth 7/1000 time = 3.3620636463165283
I0123 12:43:04.189422 140147660288000 ddar.py:60] Depth 8/1000 time = 3.6765973567962646
I0123 12:43:04.224469 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:43:04.224623 140147660288000 alphageometry.py:566] LM output (score=-2.677477): "l : C d g l 15 D d l g l 16 ;"
I0123 12:43:04.224687 140147660288000 alphageometry.py:567] Translation: "l = on_line l d g, on_bline l g d"

I0123 12:43:04.224725 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l d g, on_bline l g d ? coll j e k"
I0123 12:43:04.224884 140147660288000 graph.py:498] 
I0123 12:43:04.224936 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l d g, on_bline l g d ? coll j e k
I0123 12:43:05.018632 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7313895225524902
I0123 12:43:08.959007 140147660288000 ddar.py:60] Depth 2/1000 time = 3.940199613571167
I0123 12:43:12.092652 140147660288000 ddar.py:60] Depth 3/1000 time = 3.133427619934082
I0123 12:43:15.748407 140147660288000 ddar.py:60] Depth 4/1000 time = 3.655531167984009
I0123 12:43:18.959025 140147660288000 ddar.py:60] Depth 5/1000 time = 3.2103402614593506
I0123 12:43:22.280226 140147660288000 ddar.py:60] Depth 6/1000 time = 3.320808172225952
I0123 12:43:25.558991 140147660288000 ddar.py:60] Depth 7/1000 time = 3.2782022953033447
I0123 12:43:29.337805 140147660288000 ddar.py:60] Depth 8/1000 time = 3.7379486560821533
I0123 12:43:29.338051 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:43:29.338186 140147660288000 alphageometry.py:566] LM output (score=-2.729208): "l : C e g l 15 D e l g l 16 ;"
I0123 12:43:29.338225 140147660288000 alphageometry.py:567] Translation: "l = on_line l e g, on_bline l g e"

I0123 12:43:29.338268 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l e g, on_bline l g e ? coll j e k"
I0123 12:43:29.338431 140147660288000 graph.py:498] 
I0123 12:43:29.338481 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l e g, on_bline l g e ? coll j e k
I0123 12:43:30.113939 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7141575813293457
I0123 12:43:33.656675 140147660288000 ddar.py:60] Depth 2/1000 time = 3.542555093765259
I0123 12:43:36.784293 140147660288000 ddar.py:60] Depth 3/1000 time = 3.1273903846740723
I0123 12:43:40.361334 140147660288000 ddar.py:60] Depth 4/1000 time = 3.576773166656494
I0123 12:43:44.447729 140147660288000 ddar.py:60] Depth 5/1000 time = 4.086037874221802
I0123 12:43:47.467467 140147660288000 ddar.py:60] Depth 6/1000 time = 3.0194711685180664
I0123 12:43:50.512968 140147660288000 ddar.py:60] Depth 7/1000 time = 3.0449485778808594
I0123 12:43:50.545847 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:43:50.545982 140147660288000 alphageometry.py:566] LM output (score=-2.731870): "l : C e i l 15 D e l i l 16 ;"
I0123 12:43:50.546021 140147660288000 alphageometry.py:567] Translation: "l = on_line l e i, on_bline l i e"

I0123 12:43:50.546056 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l e i, on_bline l i e ? coll j e k"
I0123 12:43:50.546217 140147660288000 graph.py:498] 
I0123 12:43:50.546267 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l e i, on_bline l i e ? coll j e k
I0123 12:43:51.318235 140147660288000 ddar.py:60] Depth 1/1000 time = 0.711068868637085
I0123 12:43:54.854623 140147660288000 ddar.py:60] Depth 2/1000 time = 3.5362179279327393
I0123 12:43:57.967787 140147660288000 ddar.py:60] Depth 3/1000 time = 3.1129424571990967
I0123 12:44:01.508867 140147660288000 ddar.py:60] Depth 4/1000 time = 3.540806770324707
I0123 12:44:05.598070 140147660288000 ddar.py:60] Depth 5/1000 time = 4.08884596824646
I0123 12:44:09.432720 140147660288000 ddar.py:60] Depth 6/1000 time = 3.83439564704895
I0123 12:44:13.287176 140147660288000 ddar.py:60] Depth 7/1000 time = 3.8539087772369385
I0123 12:44:13.319267 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:44:13.319388 140147660288000 alphageometry.py:566] LM output (score=-2.737225): "l : C c h l 15 D c l h l 16 ;"
I0123 12:44:13.319424 140147660288000 alphageometry.py:567] Translation: "l = on_line l c h, on_bline l h c"

I0123 12:44:13.319460 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l c h, on_bline l h c ? coll j e k"
I0123 12:44:13.319626 140147660288000 graph.py:498] 
I0123 12:44:13.319677 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l c h, on_bline l h c ? coll j e k
I0123 12:44:14.165904 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7852864265441895
I0123 12:44:18.353878 140147660288000 ddar.py:60] Depth 2/1000 time = 4.187763690948486
I0123 12:44:21.874959 140147660288000 ddar.py:60] Depth 3/1000 time = 3.5208098888397217
I0123 12:44:26.541548 140147660288000 ddar.py:60] Depth 4/1000 time = 4.6662492752075195
I0123 12:44:30.188694 140147660288000 ddar.py:60] Depth 5/1000 time = 3.6468727588653564
I0123 12:44:34.242948 140147660288000 ddar.py:60] Depth 6/1000 time = 4.054011106491089
I0123 12:44:38.324912 140147660288000 ddar.py:60] Depth 7/1000 time = 4.081370830535889
I0123 12:44:38.369914 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:44:38.370033 140147660288000 alphageometry.py:566] LM output (score=-2.775656): "l : C a c l 15 C b d l 16 ;"
I0123 12:44:38.370071 140147660288000 alphageometry.py:567] Translation: "l = on_line l a c, on_line l b d"

I0123 12:44:38.370107 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a c, on_line l b d ? coll j e k"
I0123 12:44:38.370258 140147660288000 graph.py:498] 
I0123 12:44:38.370309 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a c, on_line l b d ? coll j e k
I0123 12:44:39.593581 140147660288000 ddar.py:60] Depth 1/1000 time = 1.1723127365112305
I0123 12:44:43.550402 140147660288000 ddar.py:60] Depth 2/1000 time = 3.9565422534942627
I0123 12:44:47.469662 140147660288000 ddar.py:60] Depth 3/1000 time = 3.918893575668335
I0123 12:44:51.741489 140147660288000 ddar.py:60] Depth 4/1000 time = 4.2715744972229
I0123 12:44:55.906529 140147660288000 ddar.py:60] Depth 5/1000 time = 4.1648108959198
I0123 12:45:00.549202 140147660288000 ddar.py:60] Depth 6/1000 time = 4.642428159713745
I0123 12:45:04.811551 140147660288000 ddar.py:60] Depth 7/1000 time = 4.261835098266602
I0123 12:45:04.840332 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:45:04.840511 140147660288000 alphageometry.py:566] LM output (score=-2.814451): "l : C a e l 15 D a e a l 16 ;"
I0123 12:45:04.840549 140147660288000 alphageometry.py:567] Translation: "l = on_line l a e, on_circle l a e"

I0123 12:45:04.840598 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a e, on_circle l a e ? coll j e k"
I0123 12:45:04.840764 140147660288000 graph.py:498] 
I0123 12:45:04.840816 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l a e, on_circle l a e ? coll j e k
I0123 12:45:06.092605 140147660288000 ddar.py:60] Depth 1/1000 time = 1.1906788349151611
I0123 12:45:10.461929 140147660288000 ddar.py:60] Depth 2/1000 time = 4.369086027145386
I0123 12:45:14.195904 140147660288000 ddar.py:60] Depth 3/1000 time = 3.7337467670440674
I0123 12:45:18.749191 140147660288000 ddar.py:60] Depth 4/1000 time = 4.553070545196533
I0123 12:45:22.190792 140147660288000 ddar.py:60] Depth 5/1000 time = 3.441377878189087
I0123 12:45:25.757813 140147660288000 ddar.py:60] Depth 6/1000 time = 3.566770076751709
I0123 12:45:29.726270 140147660288000 ddar.py:60] Depth 7/1000 time = 3.9679055213928223
I0123 12:45:33.302781 140147660288000 ddar.py:60] Depth 8/1000 time = 3.5758047103881836
I0123 12:45:33.335094 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:45:33.335245 140147660288000 alphageometry.py:566] LM output (score=-2.861984): "l : C i k l 15 D i l k l 16 ;"
I0123 12:45:33.335283 140147660288000 alphageometry.py:567] Translation: "l = on_line l i k, on_bline l k i"

I0123 12:45:33.335319 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l i k, on_bline l k i ? coll j e k"
I0123 12:45:33.335475 140147660288000 graph.py:498] 
I0123 12:45:33.335526 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l i k, on_bline l k i ? coll j e k
I0123 12:45:34.475365 140147660288000 ddar.py:60] Depth 1/1000 time = 1.078827142715454
I0123 12:45:38.021533 140147660288000 ddar.py:60] Depth 2/1000 time = 3.5459542274475098
I0123 12:45:41.131186 140147660288000 ddar.py:60] Depth 3/1000 time = 3.1094303131103516
I0123 12:45:44.699851 140147660288000 ddar.py:60] Depth 4/1000 time = 3.5684428215026855
I0123 12:45:48.483242 140147660288000 ddar.py:60] Depth 5/1000 time = 3.783118724822998
I0123 12:45:52.403539 140147660288000 ddar.py:60] Depth 6/1000 time = 3.9198994636535645
I0123 12:45:56.300252 140147660288000 ddar.py:60] Depth 7/1000 time = 3.896150827407837
I0123 12:45:56.333728 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:45:56.333877 140147660288000 alphageometry.py:566] LM output (score=-2.866464): "l : C e h l 15 D e l h l 16 ;"
I0123 12:45:56.333919 140147660288000 alphageometry.py:567] Translation: "l = on_line l e h, on_bline l h e"

I0123 12:45:56.333966 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l e h, on_bline l h e ? coll j e k"
I0123 12:45:56.334124 140147660288000 graph.py:498] 
I0123 12:45:56.334176 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l e h, on_bline l h e ? coll j e k
I0123 12:45:57.104445 140147660288000 ddar.py:60] Depth 1/1000 time = 0.7087831497192383
I0123 12:46:00.679096 140147660288000 ddar.py:60] Depth 2/1000 time = 3.5744669437408447
I0123 12:46:03.844057 140147660288000 ddar.py:60] Depth 3/1000 time = 3.1647393703460693
I0123 12:46:07.400772 140147660288000 ddar.py:60] Depth 4/1000 time = 3.556488037109375
I0123 12:46:11.170078 140147660288000 ddar.py:60] Depth 5/1000 time = 3.769022226333618
I0123 12:46:15.077129 140147660288000 ddar.py:60] Depth 6/1000 time = 3.9066522121429443
I0123 12:46:18.951231 140147660288000 ddar.py:60] Depth 7/1000 time = 3.8735668659210205
I0123 12:46:18.982566 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:46:18.982715 140147660288000 alphageometry.py:540] Depth 1. There are 29 nodes to expand:
I0123 12:46:18.982754 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C h i l 15 D h l i l 16 ; x00
I0123 12:46:18.982788 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C b c l 15 D b l c l 16 ; x00
I0123 12:46:18.982813 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a i l 15 D a l i l 16 ; x00
I0123 12:46:18.982837 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C b g l 15 D b l g l 16 ; x00
I0123 12:46:18.982861 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C g i l 15 D g l i l 16 ; x00
I0123 12:46:18.982884 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C b d l 15 D b l d l 16 ; x00
I0123 12:46:18.982907 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C b f l 15 D b l f l 16 ; x00
I0123 12:46:18.982929 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C b h l 15 D b l h l 16 ; x00
I0123 12:46:18.982953 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C c i l 15 D c l i l 16 ; x00
I0123 12:46:18.983000 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C c g l 15 D c l g l 16 ; x00
I0123 12:46:18.983025 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a c l 15 D a c a l 16 ; x00
I0123 12:46:18.983048 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a c l 15 D a c c l 16 ; x00
I0123 12:46:18.983070 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a h l 15 D a l h l 16 ; x00
I0123 12:46:18.983092 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a g l 15 D a l g l 16 ; x00
I0123 12:46:18.983115 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a c l 15 T a c b l 16 ; x00
I0123 12:46:18.983138 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a b l 15 D a l b l 16 ; x00
I0123 12:46:18.983160 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C d i l 15 D d l i l 16 ; x00
I0123 12:46:18.983182 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C b e l 15 D b l e l 16 ; x00
I0123 12:46:18.983204 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a f l 15 D a f f l 16 ; x00
I0123 12:46:18.983226 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C c d l 15 D c l d l 16 ; x00
I0123 12:46:18.983253 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a e l 15 D a l e l 16 ; x00
I0123 12:46:18.983277 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C d g l 15 D d l g l 16 ; x00
I0123 12:46:18.983300 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C e g l 15 D e l g l 16 ; x00
I0123 12:46:18.983322 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C e i l 15 D e l i l 16 ; x00
I0123 12:46:18.983345 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C c h l 15 D c l h l 16 ; x00
I0123 12:46:18.983368 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a c l 15 C b d l 16 ; x00
I0123 12:46:18.983391 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C a e l 15 D a e a l 16 ; x00
I0123 12:46:18.983413 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C i k l 15 D i l k l 16 ; x00
I0123 12:46:18.983434 140147660288000 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C e h l 15 D e l h l 16 ; x00
I0123 12:46:18.983459 140147660288000 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 T a c d e 03 ; f : C d e f 04 D d e e f 05 ; g : C b c g 06 D c f f g 07 ; h : D c d d h 08 ; i : C c h i 09 D c f f i 10 ; j : C b i j 11 D b j i j 12 ; k : C g h k 13 D g k h k 14 ? C j e k {F1} x00 l : C h i l 15 D h l i l 16 ; x00
I0123 12:46:25.943757 140147660288000 alphageometry.py:566] LM output (score=-0.295898): "m : C b h m 17 D b m h m 18 ;"
I0123 12:46:25.943956 140147660288000 alphageometry.py:567] Translation: "m = on_line m b h, on_bline m h b"

I0123 12:46:25.943998 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l h i, on_bline l i h; m = on_line m b h, on_bline m h b ? coll j e k"
I0123 12:46:25.944168 140147660288000 graph.py:498] 
I0123 12:46:25.944219 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l h i, on_bline l i h; m = on_line m b h, on_bline m h b ? coll j e k
I0123 12:46:27.437669 140147660288000 ddar.py:60] Depth 1/1000 time = 1.4189739227294922
I0123 12:46:32.812070 140147660288000 ddar.py:60] Depth 2/1000 time = 5.374181747436523
I0123 12:46:41.363954 140147660288000 ddar.py:60] Depth 3/1000 time = 8.551594734191895
I0123 12:46:50.543621 140147660288000 ddar.py:60] Depth 4/1000 time = 9.179291725158691
I0123 12:46:58.810400 140147660288000 ddar.py:60] Depth 5/1000 time = 8.266550779342651
I0123 12:47:10.109267 140147660288000 ddar.py:60] Depth 6/1000 time = 11.298641681671143
I0123 12:47:24.315717 140147660288000 ddar.py:60] Depth 7/1000 time = 14.206199169158936
I0123 12:47:38.386280 140147660288000 ddar.py:60] Depth 8/1000 time = 14.070275068283081
I0123 12:47:53.097378 140147660288000 ddar.py:60] Depth 9/1000 time = 14.710789918899536
I0123 12:48:07.466630 140147660288000 ddar.py:60] Depth 10/1000 time = 14.368340253829956
I0123 12:48:07.562646 140147660288000 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:48:07.562743 140147660288000 alphageometry.py:566] LM output (score=-0.807826): "m : C b g m 17 D b m g m 18 ;"
I0123 12:48:07.562780 140147660288000 alphageometry.py:567] Translation: "m = on_line m b g, on_bline m g b"

I0123 12:48:07.562830 140147660288000 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l h i, on_bline l i h; m = on_line m b g, on_bline m g b ? coll j e k"
I0123 12:48:07.563009 140147660288000 graph.py:498] 
I0123 12:48:07.563063 140147660288000 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e d c a; f = mirror f d e; g = on_circle g f c, on_line g b c; h = on_circle h d c; i = on_circle i f c, on_line i h c; j = midpoint j b i; k = midpoint k g h; l = on_line l h i, on_bline l i h; m = on_line m b g, on_bline m g b ? coll j e k
I0123 12:48:09.165376 140147660288000 ddar.py:60] Depth 1/1000 time = 1.5249791145324707
I0123 12:48:14.112561 140147660288000 ddar.py:60] Depth 2/1000 time = 4.9469709396362305
I0123 12:48:18.637243 140147660288000 ddar.py:60] Depth 3/1000 time = 4.5244505405426025
I0123 12:48:25.202373 140147660288000 ddar.py:60] Depth 4/1000 time = 6.5648956298828125
I0123 12:48:31.629955 140147660288000 ddar.py:60] Depth 5/1000 time = 6.427316665649414
I0123 12:48:40.482598 140147660288000 ddar.py:60] Depth 6/1000 time = 8.852363586425781
I0123 12:48:52.778156 140147660288000 ddar.py:60] Depth 7/1000 time = 12.295196056365967
I0123 12:48:52.830270 140147660288000 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K : Points
DA = DB [00]
DB = DC [01]
A,C,E are collinear [02]
DE ⟂ AC [03]
∠(DE-AC) = ∠(DE-AC) [04]
D,F,E are collinear [05]
ED = EF [06]
G,B,C are collinear [07]
FG = FC [08]
DH = DC [09]
H,I,C are collinear [10]
FI = FC [11]
BH:BI = BH:BI [12]
GI:GB = GI:GB [13]
B,I,J are collinear [14]
JB = JI [15]
G,H,K are collinear [16]
KG = KH [17]

 * Auxiliary Constructions:
L M : Points
L,I,H are collinear [18]
LI = LH [19]
B,M,G are collinear [20]
MG = MB [21]

 * Proof steps:
001. D,F,E are collinear [05] & ED = EF [06] ⇒  E is midpoint of DF [22]
002. A,C,E are collinear [02] & D,F,E are collinear [05] & AC ⟂ DE [03] ⇒  AE ⟂ DF [23]
003. E is midpoint of DF [22] & AE ⟂ DF [23] ⇒  AD = AF [24]
004. DA = DB [00] & DB = DC [01] ⇒  DA = DC [25]
005. DA = DB [00] & DB = DC [01] ⇒  D is the circumcenter of \Delta BAC [26]
006. A,C,E are collinear [02] & D,F,E are collinear [05] & AC ⟂ DE [03] ⇒  CE ⟂ DF [27]
007. E is midpoint of DF [22] & CE ⟂ DF [27] ⇒  CD = CF [28]
008. AD = AF [24] & CD = AD [25] & CD = CF [28] & FG = FC [08] ⇒  FA = FG [29]
009. FG = FC [08] & GF = AF [29] ⇒  F is the circumcenter of \Delta GCA [30]
010. DC = DA [25] ⇒  ∠DCA = ∠CAD [31]
011. D,F,E are collinear [05] & A,C,E are collinear [02] & DE ⟂ AC [03] ⇒  ∠FEA = ∠AED [32]
012. ED = EF [06] & ∠FEA = ∠AED [32] (SAS)⇒  ∠FAE = ∠EAD [33]
013. A,C,E are collinear [02] & ∠DCA = ∠CAD [31] & ∠FAE = ∠EAD [33] ⇒  ∠FAE = ∠DCE [34]
014. D,F,E are collinear [05] & A,C,E are collinear [02] & ∠(DE-AC) = ∠(DE-AC) [04] ⇒  ∠FEA = ∠DEC [35]
015. ∠FAE = ∠DCE [34] & ∠FEA = ∠DEC [35] (Similar Triangles)⇒  AF:CD = AE:CE [36]
016. A,C,E are collinear [02] & AF:CD = AE:CE [36] & AD = AF [24] & CD = AD [25] ⇒  E is midpoint of AC [37]
017. F is the circumcenter of \Delta GCA [30] & E is midpoint of AC [37] ⇒  ∠CGA = ∠CFE [38]
018. D,F,E are collinear [05] & A,C,E are collinear [02] & DE ⟂ AC [03] ⇒  ∠FEC = ∠CED [39]
019. ED = EF [06] & ∠FEC = ∠CED [39] (SAS)⇒  ∠FCE = ∠ECD [40]
020. ∠FCE = ∠ECD [40] & A,C,E are collinear [02] & ∠ACD = ∠DAC [31] ⇒  ∠DAC = ∠FCA [41]
021. ∠DAC = ∠FCA [41] ⇒  DA ∥ FC [42]
022. D is the circumcenter of \Delta BAC [26] & E is midpoint of AC [37] ⇒  ∠ABC = ∠ADE [43]
023. G,B,C are collinear [07] & ∠CGA = ∠CFE [38] & D,F,E are collinear [05] & AD ∥ CF [42] & ∠ABC = ∠ADE [43] ⇒  ∠ABG = ∠BGA [44]
024. ∠ABG = ∠BGA [44] ⇒  AB = AG [45]
025. MG = MB [21] & AB = AG [45] ⇒  GB ⟂ MA [46]
026. B,M,G are collinear [20] & G,B,C are collinear [07] & GB ⟂ MA [46] ⇒  CM ⟂ MA [47]
027. CM ⟂ MA [47] & E is midpoint of AC [37] ⇒  CE = ME [48]
028. FI = FC [11] & AD = AF [24] & CD = AD [25] & CD = CF [28] ⇒  F is the circumcenter of \Delta ICA [49]
029. F is the circumcenter of \Delta ICA [49] & E is midpoint of AC [37] ⇒  ∠CIA = ∠CFE [50]
030. DH = DC [09] & DB = DC [01] & DA = DB [00] ⇒  B,A,H,C are concyclic [51]
031. DH = DC [09] & DB = DC [01] & DA = DB [00] ⇒  D is the circumcenter of \Delta HAC [52]
032. D is the circumcenter of \Delta HAC [52] & E is midpoint of AC [37] ⇒  ∠AHC = ∠ADE [53]
033. H,I,C are collinear [10] & ∠CIA = ∠CFE [50] & D,F,E are collinear [05] & AD ∥ CF [42] & ∠AHC = ∠ADE [53] ⇒  ∠AHI = ∠HIA [54]
034. ∠AHI = ∠HIA [54] ⇒  AH = AI [55]
035. LI = LH [19] & AH = AI [55] ⇒  IH ⟂ LA [56]
036. L,I,H are collinear [18] & H,I,C are collinear [10] & IH ⟂ LA [56] ⇒  CL ⟂ LA [57]
037. CL ⟂ LA [57] & E is midpoint of AC [37] ⇒  CE = LE [58]
038. CE = ME [48] & CE = LE [58] ⇒  EL = EM [59]
039. M,G,B are collinear [20] & MG = MB [21] ⇒  M is midpoint of GB [60]
040. G,H,K are collinear [16] & KG = KH [17] ⇒  K is midpoint of GH [61]
041. M is midpoint of GB [60] & K is midpoint of GH [61] ⇒  MK ∥ BH [62]
042. KM ∥ BH [62] & G,H,K are collinear [16] & B,M,G are collinear [20] ⇒  GH:GK = HB:KM [63]
043. B,I,J are collinear [14] & JB = JI [15] ⇒  J is midpoint of BI [64]
044. J is midpoint of BI [64] & K is midpoint of GH [61] ⇒  JB:BI = KG:GH [65]
045. L,H,I are collinear [18] & LI = LH [19] ⇒  L is midpoint of HI [66]
046. L is midpoint of HI [66] & J is midpoint of BI [64] ⇒  LJ ∥ HB [67]
047. JL ∥ BH [67] & B,I,J are collinear [14] & I,L,H are collinear [18] ⇒  IJ:BI = LJ:BH [68]
048. GH:GK = HB:KM [63] & KG = KH [17] & JB:BI = KG:GH [65] & JB = JI [15] & IJ:BI = LJ:BH [68] ⇒  LJ:BH = MK:BH [69]
049. LJ:BH = MK:BH [69] & BH:BI = BH:BI [12] ⇒  LJ = MK [70]
050. L is midpoint of HI [66] & K is midpoint of GH [61] ⇒  LK ∥ IG [71]
051. KL ∥ GI [71] & G,H,K are collinear [16] & I,L,H are collinear [18] ⇒  GH:HK = GI:LK [72]
052. KL ∥ GI [71] & G,H,K are collinear [16] & I,L,H are collinear [18] ⇒  LH:HI = LK:GI [73]
053. GH:HK = GI:LK [72] & GH:GK = HB:KM [63] & KG = KH [17] ⇒  BH:MK = GI:LK [74]
054. FI = FC [11] & FG = FC [08] ⇒  FG = FI [75]
055. FI = FC [11] & FG = FI [75] & FG = FA [29] ⇒  G,A,I,C are concyclic [76]
056. G,A,I,C are concyclic [76] ⇒  ∠GIA = ∠GCA [77]
057. G,A,I,C are concyclic [76] ⇒  ∠GAI = ∠GCI [78]
058. B,A,H,C are concyclic [51] ⇒  ∠BHA = ∠BCA [79]
059. B,A,H,C are concyclic [51] ⇒  ∠HAB = ∠HCB [80]
060. ∠GIA = ∠GCA [77] & G,B,C are collinear [07] & ∠BHA = ∠BCA [79] ⇒  ∠BHA = ∠GIA [81]
061. ∠GAI = ∠GCI [78] & G,B,C are collinear [07] & H,I,C are collinear [10] & ∠HAB = ∠HCB [80] ⇒  ∠HAB = ∠IAG [82]
062. ∠BHA = ∠GIA [81] & ∠HAB = ∠IAG [82] (Similar Triangles)⇒  BH:BA = GI:GA [83]
063. BH:BA = GI:GA [83] & AB = AG [45] ⇒  BH:BA = GI:BA [84]
064. BH:MK = GI:LK [74] & BH:BA = GI:BA [84] ⇒  MK = LK [85]
065. M is midpoint of GB [60] & L is midpoint of HI [66] ⇒  MG:GB = LH:HI [86]
066. M is midpoint of GB [60] & J is midpoint of BI [64] ⇒  MJ ∥ GI [87]
067. JM ∥ GI [87] & B,M,G are collinear [20] & B,I,J are collinear [14] ⇒  MB:GB = MJ:GI [88]
068. LH:HI = LK:GI [73] & MG:GB = LH:HI [86] & MG = MB [21] & MB:GB = MJ:GI [88] ⇒  MJ:GI = LK:GI [89]
069. MJ:GI = LK:GI [89] & GI:GB = GI:GB [13] ⇒  MJ = LK [90]
070. LJ = MK [70] & MK = LK [85] & MJ = LK [90] ⇒  JM = JL [91]
071. EL = EM [59] & JM = JL [91] ⇒  LM ⟂ EJ [92]
072. MK = LK [85] & MJ = LK [90] ⇒  MJ = MK [93]
073. LJ = MK [70] & MK = LK [85] ⇒  LK = LJ [94]
074. MJ = MK [93] & LK = LJ [94] ⇒  JK ⟂ LM [95]
075. LM ⟂ EJ [92] & JK ⟂ LM [95] ⇒  J,K,E are collinear
==========================

I0123 12:48:52.830376 140147660288000 alphageometry.py:582] Solved.
