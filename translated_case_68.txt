I0123 16:32:12.347368 140576590110720 inference_utils.py:69] Parsing gin configuration.
I0123 16:32:12.347471 140576590110720 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 16:32:12.347680 140576590110720 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 16:32:12.347714 140576590110720 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 16:32:12.347744 140576590110720 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 16:32:12.347772 140576590110720 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 16:32:12.347799 140576590110720 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 16:32:12.347824 140576590110720 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 16:32:12.347850 140576590110720 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 16:32:12.347875 140576590110720 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 16:32:12.347901 140576590110720 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 16:32:12.347925 140576590110720 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 16:32:12.347970 140576590110720 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 16:32:12.348109 140576590110720 resource_reader.py:55] Path not found: base_htrans.gin
I0123 16:32:12.348327 140576590110720 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 16:32:12.348425 140576590110720 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 16:32:12.354898 140576590110720 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 16:32:12.355015 140576590110720 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 16:32:12.355338 140576590110720 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 16:32:12.355443 140576590110720 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 16:32:12.355722 140576590110720 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 16:32:12.355822 140576590110720 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 16:32:12.356229 140576590110720 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 16:32:12.356329 140576590110720 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 16:32:12.359922 140576590110720 training_loop.py:334] ==== Training loop: initializing model ====
I0123 16:32:12.459782 140576590110720 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 16:32:12.460509 140576590110720 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 16:32:12.467067 140576590110720 training_loop.py:335] Process 0 of 1
I0123 16:32:12.467123 140576590110720 training_loop.py:336] Local device count = 1
I0123 16:32:12.467165 140576590110720 training_loop.py:337] Number of replicas = 1
I0123 16:32:12.467197 140576590110720 training_loop.py:339] Using random number seed 42
I0123 16:32:12.951362 140576590110720 training_loop.py:359] Initializing the model.
I0123 16:32:13.381012 140576590110720 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.381273 140576590110720 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 16:32:13.381380 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.381461 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.381540 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.381624 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.381712 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.381786 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.381859 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.381929 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.381999 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.382069 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.382139 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.382209 140576590110720 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:13.382249 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.382297 140576590110720 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:32:13.382417 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.382459 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.382492 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.384598 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.390045 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.401070 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.401353 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.405805 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.416639 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.416698 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.416738 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.416771 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.416833 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.418038 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.418118 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.418853 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.421373 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.427284 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.429048 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.429130 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.429167 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.429231 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.429363 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.429718 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.429767 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.431735 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.431837 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.435459 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.435602 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.436138 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.446619 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.455672 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.455772 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.456080 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.456166 140576590110720 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:32:13.456280 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.456321 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.456355 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.458269 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.460837 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.466795 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.467067 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.469795 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.473704 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.473760 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.473797 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.473828 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.473891 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.474494 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.474573 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.474944 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.475726 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.478262 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.478900 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.478981 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.479017 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.479078 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.479207 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.479533 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.479578 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.481555 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.481656 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.484210 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.484294 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.484736 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.487119 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.489065 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.489161 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.489466 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.489547 140576590110720 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:32:13.489665 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.489706 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.489739 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.491687 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.494119 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.500206 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.500478 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.503230 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.507125 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.507182 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.507219 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.507251 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.507314 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.507883 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.507959 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.508336 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.509117 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.511700 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.512385 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.512465 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.512501 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.512564 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.512695 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.513028 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.513073 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.515046 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.515142 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.517723 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.517809 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.518314 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.520719 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.522738 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.522834 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.523135 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.523217 140576590110720 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:32:13.523329 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.523368 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.523400 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.525358 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.527819 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.533600 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.533878 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.536577 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.540429 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.540488 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.540524 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.540556 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.540617 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.541190 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.541266 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.541630 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.542439 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.545029 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.545667 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.545748 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.545784 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.545845 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.545978 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.546309 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.546353 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.548303 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.548397 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.551043 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.551130 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.551573 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.553893 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.555847 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.555942 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.556246 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.556328 140576590110720 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:32:13.556440 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.556480 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.556512 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.558472 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.560906 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.566953 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.567218 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.569983 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.573815 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.573872 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.573910 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.573944 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.574007 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.574602 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.574683 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.575052 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.575837 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.578789 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.579426 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.579508 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.579545 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.579609 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.579750 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.580082 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.580127 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.582088 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.582184 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.584796 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.584877 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.585318 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.587654 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.589659 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.589755 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.590063 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.590145 140576590110720 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:32:13.590258 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.590298 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.590329 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.592212 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.594652 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.600440 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.600704 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.603457 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.607291 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.607346 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.607383 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.607415 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.607479 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.608101 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.608179 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.608545 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.609340 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.611884 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.612507 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.612586 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.612622 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.612689 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.612825 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.613154 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.613198 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.615135 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.615232 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.617866 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.617947 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.618391 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.620778 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.622766 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.622865 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.623170 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.623252 140576590110720 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:32:13.623366 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.623406 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.623439 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.625330 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.627862 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.633652 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.633924 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.636644 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.640514 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.640571 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.640609 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.640642 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.640707 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.641276 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.641353 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.641734 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.642532 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.645080 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.645716 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.645798 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.645835 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.645898 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.646032 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.646356 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.646400 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.648401 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.648500 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.651066 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.651150 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.651596 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.654317 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.656269 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.656371 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.656670 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.656754 140576590110720 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:32:13.656867 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.656908 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.656940 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.798191 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.801273 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.807244 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.807542 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.810322 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.814292 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.814351 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.814389 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.814422 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.814490 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.815121 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.815198 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.815571 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.816362 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.819001 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.819673 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.819752 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.819789 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.819852 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.819982 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.820329 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.820375 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.822346 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.822444 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.825081 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.825160 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.825606 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.827996 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.829964 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.830074 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.830381 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.830466 140576590110720 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:32:13.830579 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.830620 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.830653 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.832643 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.835075 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.840933 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.841202 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.843977 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.847872 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.847930 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.847968 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.848001 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.848064 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.848647 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.848725 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.849095 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.849896 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.852523 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.853164 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.853243 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.853279 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.853341 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.853475 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.853816 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.853863 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.855784 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.855882 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.858518 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.858602 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.859074 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.861512 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.863634 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.863739 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.864043 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.864133 140576590110720 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:32:13.864247 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.864288 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.864320 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.866229 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.868775 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.874842 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.875122 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.878252 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.882142 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.882198 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.882236 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.882268 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.882330 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.882951 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.883029 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.883394 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.884189 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.886814 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.887476 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.887557 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.887595 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.887657 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.887790 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.888126 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.888170 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.890114 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.890210 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.892870 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.892951 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.893394 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.895825 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.897777 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.897876 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.898176 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.898268 140576590110720 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:32:13.898386 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.898428 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.898462 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.900378 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.902915 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.908695 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.908960 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.911704 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.915581 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.915639 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.915678 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.915712 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.915777 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.916360 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.916436 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.916805 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.917590 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.920189 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.920817 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.920895 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.920930 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.920992 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.921125 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.921458 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.921503 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.923575 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.923674 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.926634 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.926717 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.927176 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.929577 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.931582 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.931683 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.931987 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.932070 140576590110720 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:32:13.932190 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:13.932232 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:13.932266 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:13.934234 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.936656 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:13.942422 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.942692 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:13.945408 140576590110720 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:13.949300 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:13.949356 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:13.949393 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:13.949426 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.949489 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.950073 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.950152 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.950520 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.951307 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.953846 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.954842 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.954922 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:13.954959 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:13.955022 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.955154 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:13.955487 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:13.955531 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.957475 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.957569 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.960140 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.960221 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:13.960712 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:13.963022 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:13.964965 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.965061 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:13.965358 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:13.965646 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.965734 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.965804 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.965866 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.965926 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.965982 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.966039 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.966094 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.966148 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.966201 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.966254 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.966308 140576590110720 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:13.966346 140576590110720 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:32:13.969960 140576590110720 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:14.018852 140576590110720 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.018938 140576590110720 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:32:14.018995 140576590110720 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:32:14.019101 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.019141 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.019173 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.019238 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.021726 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.027338 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.027605 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.030313 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.047235 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.047292 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.047329 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.047362 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.047424 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.048572 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.048652 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.049381 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.051434 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.056311 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.057654 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.057744 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.057784 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.057847 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.057986 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.058100 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.058140 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.060086 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.060183 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.062704 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.062787 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.062900 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.065182 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.067212 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.067312 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.067616 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.067699 140576590110720 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:32:14.067812 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.067853 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.067886 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.067954 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.070290 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.076052 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.076316 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.079085 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.092484 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.092542 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.092581 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.092613 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.092676 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.093251 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.093329 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.097558 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.098476 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.101148 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.101845 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.101928 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.101971 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.102052 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.102190 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.102310 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.102353 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.104454 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.104550 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.107124 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.107205 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.107319 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.109632 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.111646 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.111743 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.112039 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.112125 140576590110720 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:32:14.112239 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.112280 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.112313 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.112380 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.114737 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.120378 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.120644 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.123475 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.136611 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.136668 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.136705 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.136737 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.136801 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.137364 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.137442 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.137820 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.138531 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.141090 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.141736 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.141815 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.141851 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.141921 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.142055 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.142166 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.142206 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.144203 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.144299 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.146825 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.146905 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.147017 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.149318 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.151320 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.151420 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.151720 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.151804 140576590110720 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:32:14.151917 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.151957 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.151989 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.152056 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.154370 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.159979 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.160248 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.163006 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.176457 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.176513 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.176550 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.176582 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.176645 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.177219 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.177297 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.177668 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.178377 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.180925 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.181559 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.181637 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.181679 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.181742 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.181882 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.181996 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.182036 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.184031 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.184127 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.186614 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.186695 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.186809 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.189073 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.190990 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.191087 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.191383 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.191465 140576590110720 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:32:14.191576 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.191617 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.191649 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.191716 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.194372 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.200010 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.200286 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.202983 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.215977 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.216034 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.216071 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.216104 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.216166 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.216735 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.216812 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.217179 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.217890 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.220526 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.221166 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.221244 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.221281 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.221343 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.221482 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.221593 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.221632 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.223578 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.223671 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.226172 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.226252 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.226362 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.228688 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.230631 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.230728 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.231025 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.231107 140576590110720 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:32:14.231219 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.231258 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.231289 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.231353 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.233661 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.239267 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.239531 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.242292 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.255217 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.255273 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.255311 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.255343 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.255405 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.255974 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.256051 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.256413 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.257135 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.259687 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.260316 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.260393 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.260429 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.260488 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.260622 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.260742 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.260781 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.262781 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.262877 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.265348 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.265428 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.265537 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.267802 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.269706 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.269803 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.270099 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.270182 140576590110720 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:32:14.270293 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.270333 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.270365 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.270431 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.272732 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.278606 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.278871 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.281560 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.294498 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.294556 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.294593 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.294625 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.294687 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.295262 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.295339 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.295709 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.296405 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.298947 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.299948 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.300029 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.300065 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.300127 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.300262 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.300376 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.300420 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.302383 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.302478 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.304913 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.304992 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.305102 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.307362 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.309334 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.309431 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.309736 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.309820 140576590110720 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:32:14.309933 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.309973 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.310005 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.310070 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.312380 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.318010 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.318289 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.321041 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.334032 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.334088 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.334125 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.334156 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.334220 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.334837 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.334914 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.335277 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.335981 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.338528 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.339163 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.339241 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.339277 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.339338 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.339472 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.339584 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.339629 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.341552 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.341654 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.344207 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.344288 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.344404 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.346693 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.348634 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.348731 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.349025 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.349108 140576590110720 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:32:14.349219 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.349259 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.349291 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.349355 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.351679 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.357359 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.357628 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.360331 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.373338 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.373394 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.373431 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.373464 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.373525 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.374117 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.374196 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.374564 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.375468 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.378200 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.378889 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.378968 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.379005 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.379067 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.379202 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.379313 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.379352 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.381268 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.381364 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.383821 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.383903 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.384014 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.386260 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.388254 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.388351 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.388648 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.388731 140576590110720 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:32:14.388842 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.388884 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.388916 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.388982 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.391285 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.396900 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.397198 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.399949 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.413115 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.413171 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.413207 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.413239 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.413301 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.413923 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.414001 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.414366 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.415068 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.417611 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.418261 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.418339 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.418375 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.418435 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.418569 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.418682 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.418722 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.420641 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.420743 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.423289 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.423371 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.423482 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.425783 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.427690 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.427788 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.428083 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.428166 140576590110720 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:32:14.428277 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.428316 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.428347 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.428414 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.430716 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.436373 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.436639 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.439327 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.452192 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.452249 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.452286 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.452318 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.452384 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.452950 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.453028 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.453394 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.454101 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.456643 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.457316 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.457394 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.457431 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.457491 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.457625 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.457743 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.457783 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.459703 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.459803 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.462279 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.462359 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.462469 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.464713 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.466678 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.466775 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.467068 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.467151 140576590110720 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:32:14.467263 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.467303 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.467335 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.467399 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.469689 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.475324 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.475601 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.478609 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.491662 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.491719 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.491756 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.491789 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.491857 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.492426 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.492503 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.492863 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.493607 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.496142 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.496774 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.496852 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.496889 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.496951 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.497080 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.497192 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.497231 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.499148 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.499245 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.501718 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.501798 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.501910 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.504239 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.506158 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.506255 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.506550 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.506642 140576590110720 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:32:14.509598 140576590110720 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:14.566089 140576590110720 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.566177 140576590110720 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:32:14.566234 140576590110720 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:32:14.566341 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.566380 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.566412 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.566478 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.569192 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.574669 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.574936 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.577538 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.590299 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.590356 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.590393 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.590426 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.590489 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.591060 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.591139 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.591506 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.592188 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.594746 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.595370 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.595447 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.595483 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.595545 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.595675 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.595794 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.595834 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.597728 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.597825 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.600276 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.600357 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.600470 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.602781 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.604654 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.604751 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.605045 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.605129 140576590110720 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:32:14.605241 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.605281 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.605313 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.605381 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.607673 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.613235 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.613500 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.616236 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.628774 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.628830 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.628867 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.628901 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.628965 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.629527 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.629605 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.629976 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.630657 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.633199 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.633832 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.633912 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.633948 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.634009 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.634137 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.634248 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.634293 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.636162 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.636257 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.638701 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.638781 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.638893 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.641184 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.643087 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.643185 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.643479 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.643562 140576590110720 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:32:14.643672 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.643713 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.643746 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.643812 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.646082 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.651522 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.651784 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.654490 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.666991 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.667048 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.667084 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.667117 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.667181 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.667746 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.667823 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.668186 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.668877 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.671429 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.672053 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.672133 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.672168 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.672231 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.672361 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.672472 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.672512 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.674407 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.674501 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.676938 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.677018 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.677129 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.679890 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.681779 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.681877 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.682170 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.682253 140576590110720 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:32:14.682362 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.682402 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.682435 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.682500 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.684911 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.690402 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.690671 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.693398 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.706223 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.706279 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.706318 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.706365 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.706432 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.706998 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.707075 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.707442 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.708145 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.710749 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.711375 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.711451 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.711486 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.711549 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.711677 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.711786 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.711827 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.713743 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.713837 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.716285 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.716363 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.716472 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.718828 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.720739 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.720834 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.721124 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.721204 140576590110720 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:32:14.721313 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.721351 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.721382 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.721447 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.723730 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.729219 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.729481 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.732213 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.744914 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.744969 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.745003 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.745034 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.745095 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.745665 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.745741 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.746103 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.746785 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.749344 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.749983 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.750060 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.750094 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.750154 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.750281 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.750389 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.750426 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.752340 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.752440 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.754909 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.754987 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.755096 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.757437 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.759340 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.759441 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.759742 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.759825 140576590110720 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:32:14.759934 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.759974 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.760006 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.760070 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.762374 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.767914 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.768177 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.770928 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.783640 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.783694 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.783729 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.783759 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.783821 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.784380 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.784455 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.784818 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.785517 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.788111 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.788733 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.788810 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.788845 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.788904 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.789032 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.789141 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.789179 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.791109 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.791209 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.793631 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.793714 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.793823 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.796653 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.798547 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.798643 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.798935 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.799016 140576590110720 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:32:14.799127 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.799165 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.799196 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.799260 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.801543 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.807079 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.807342 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.810069 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.822759 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.822813 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.822848 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.822879 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.822939 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.823511 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.823589 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.823953 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.824648 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.827232 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.827866 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.827943 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.827977 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.828038 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.828166 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.828274 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.828310 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.830216 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.830309 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.832732 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.832810 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.832918 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.835239 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.837110 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.837203 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.837491 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.837569 140576590110720 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:32:14.837687 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.837727 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.837758 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.837822 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.840183 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.845724 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.845985 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.848699 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.861622 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.861683 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.861720 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.861755 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.861819 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.862411 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.862489 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.862866 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.863584 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.866192 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.866853 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.866933 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.866968 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.867032 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.867165 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.867277 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.867316 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.869240 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.869332 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.871838 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.871924 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.872035 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.874365 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.876312 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.876408 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.876699 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.876779 140576590110720 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:32:14.876886 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.876924 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.876954 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.877020 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.879353 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.884878 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.885140 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.887981 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.900977 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.901037 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.901072 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.901103 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.901163 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.901741 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.901816 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.902174 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.902892 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.905542 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.906175 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.906255 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.906290 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.906351 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.906484 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.906596 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.906635 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.908594 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.908686 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.911158 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.911246 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.911360 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.914093 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.916083 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.916177 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.916469 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.916549 140576590110720 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:32:14.916657 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.916695 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.916726 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.916789 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.919133 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.924787 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.925050 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.927819 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.940861 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.940916 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.940951 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.940982 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.941048 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.941629 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.941713 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.942082 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.942803 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.945434 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.946071 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.946148 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.946183 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.946243 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.946376 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.946490 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.946530 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.949012 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.949106 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.951591 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.951673 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.951795 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.954096 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.956031 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.956125 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.956415 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.956496 140576590110720 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:32:14.956604 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.956642 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.956672 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.956735 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.959032 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:14.964627 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.964889 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:14.967668 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:14.980524 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:14.980578 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:14.980613 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:14.980644 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.980705 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.981272 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.981346 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.981712 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.982409 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.984981 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.985608 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.985693 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:14.985728 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:14.985787 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.985918 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:14.986029 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:14.986067 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.987962 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.988053 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.990481 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.990560 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:14.990669 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:14.992993 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:14.994869 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.994964 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:14.995251 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.995330 140576590110720 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:32:14.995437 140576590110720 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:14.995474 140576590110720 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:14.995505 140576590110720 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:14.995567 140576590110720 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:14.997834 140576590110720 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:15.003329 140576590110720 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.003591 140576590110720 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:15.006339 140576590110720 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:15.019281 140576590110720 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:15.019335 140576590110720 attention.py:418] Single window, no scan.
I0123 16:32:15.019370 140576590110720 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:15.019400 140576590110720 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.019461 140576590110720 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.020036 140576590110720 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.020112 140576590110720 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.020472 140576590110720 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.021181 140576590110720 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.023782 140576590110720 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.024414 140576590110720 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.024490 140576590110720 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:15.024525 140576590110720 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:15.024584 140576590110720 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.024715 140576590110720 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:15.024826 140576590110720 nn_components.py:325] mlp: activation = None
I0123 16:32:15.024863 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:15.026777 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.026870 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:15.029320 140576590110720 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.029398 140576590110720 transformer_base.py:443] tbase: final FFN
I0123 16:32:15.029508 140576590110720 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:15.032242 140576590110720 nn_components.py:329] mlp: final activation = None
I0123 16:32:15.034151 140576590110720 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.034246 140576590110720 nn_components.py:261] mlp: residual
I0123 16:32:15.034538 140576590110720 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:15.034623 140576590110720 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:32:15.037496 140576590110720 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:19.487685 140576590110720 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 16:32:19.997554 140576590110720 training_loop.py:409] No working directory specified.
I0123 16:32:19.997683 140576590110720 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 16:32:19.998449 140576590110720 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 16:32:23.076845 140576590110720 training_loop.py:447] Only restoring trainable parameters.
I0123 16:32:23.077474 140576590110720 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 16:32:23.077533 140576590110720 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.077580 140576590110720 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.077623 140576590110720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.077677 140576590110720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.077719 140576590110720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.077758 140576590110720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.077796 140576590110720 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.077834 140576590110720 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.077871 140576590110720 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.077908 140576590110720 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.077944 140576590110720 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.077981 140576590110720 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.078017 140576590110720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.078054 140576590110720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078090 140576590110720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.078126 140576590110720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078162 140576590110720 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078198 140576590110720 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.078236 140576590110720 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.078284 140576590110720 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078324 140576590110720 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.078362 140576590110720 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.078399 140576590110720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.078437 140576590110720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078474 140576590110720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.078513 140576590110720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078550 140576590110720 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078587 140576590110720 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.078624 140576590110720 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.078661 140576590110720 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078698 140576590110720 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.078736 140576590110720 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.078774 140576590110720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.078810 140576590110720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078846 140576590110720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.078883 140576590110720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078920 140576590110720 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.078957 140576590110720 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.078993 140576590110720 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.079030 140576590110720 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079068 140576590110720 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.079106 140576590110720 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.079143 140576590110720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.079180 140576590110720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079217 140576590110720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.079261 140576590110720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079300 140576590110720 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079337 140576590110720 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.079375 140576590110720 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.079412 140576590110720 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079450 140576590110720 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.079487 140576590110720 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.079523 140576590110720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.079560 140576590110720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079596 140576590110720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.079634 140576590110720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079670 140576590110720 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079707 140576590110720 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.079744 140576590110720 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.079782 140576590110720 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079820 140576590110720 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.079857 140576590110720 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.079894 140576590110720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.079931 140576590110720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.079967 140576590110720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.080002 140576590110720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080038 140576590110720 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080073 140576590110720 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.080108 140576590110720 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.080143 140576590110720 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080178 140576590110720 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.080213 140576590110720 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.080252 140576590110720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.080289 140576590110720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080325 140576590110720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.080360 140576590110720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080395 140576590110720 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080430 140576590110720 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.080465 140576590110720 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.080500 140576590110720 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080536 140576590110720 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.080572 140576590110720 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.080607 140576590110720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.080642 140576590110720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080677 140576590110720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.080713 140576590110720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080748 140576590110720 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080783 140576590110720 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.080818 140576590110720 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.080854 140576590110720 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.080890 140576590110720 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.080925 140576590110720 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.080960 140576590110720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.080996 140576590110720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081032 140576590110720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.081067 140576590110720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081102 140576590110720 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081138 140576590110720 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.081172 140576590110720 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.081213 140576590110720 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081251 140576590110720 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.081286 140576590110720 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.081322 140576590110720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.081358 140576590110720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081394 140576590110720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.081430 140576590110720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081466 140576590110720 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081501 140576590110720 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.081537 140576590110720 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.081572 140576590110720 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081608 140576590110720 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.081650 140576590110720 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:23.081689 140576590110720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:23.081726 140576590110720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081762 140576590110720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.081799 140576590110720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081835 140576590110720 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081870 140576590110720 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:23.081906 140576590110720 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:23.081941 140576590110720 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:23.081977 140576590110720 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:23.082005 140576590110720 training_loop.py:725] Total parameters: 152072288
I0123 16:32:23.082210 140576590110720 training_loop.py:739] Total state size: 0
I0123 16:32:23.103960 140576590110720 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 16:32:23.104192 140576590110720 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 16:32:23.104737 140576590110720 training_loop.py:652] Compiling mode beam_search with jit.
I0123 16:32:23.105050 140576590110720 training_loop.py:89] registering functions: dict_keys([])
I0123 16:32:23.120739 140576590110720 graph.py:499] a b c = triangle a b c; d = midpoint d c a; e = mirror e b d; f = foot f b c a; g = foot g e c a; h = midpoint h c b; i = midpoint i a e ? para f h g i
I0123 16:32:23.425763 140576590110720 ddar.py:60] Depth 1/1000 time = 0.27366089820861816
I0123 16:32:25.194062 140576590110720 ddar.py:60] Depth 2/1000 time = 1.7681312561035156
I0123 16:32:28.325279 140576590110720 ddar.py:60] Depth 3/1000 time = 3.131035089492798
I0123 16:32:28.327787 140576590110720 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I : Points
C,D,A are collinear [00]
DC = DA [01]
B,E,D are collinear [02]
DB = DE [03]
C,F,A are collinear [04]
BF  AC [05]
C,G,A are collinear [06]
CA  GE [07]
B,C,H are collinear [08]
HC = HB [09]
I,E,A are collinear [10]
IA = IE [11]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. C,F,A are collinear [04] & BF  AC [05]   BF  FC [12]
002. B,C,H are collinear [08] & HC = HB [09]   H is midpoint of CB [13]
003. BF  FC [12] & H is midpoint of CB [13]   BH = FH [14]
004. BH = FH [14]   HBF = BFH [15]
005. C,D,A are collinear [00] & DC = DA [01]   D is midpoint of CA [16]
006. B,E,D are collinear [02] & DB = DE [03]   D is midpoint of BE [17]
007. D is midpoint of CA [16] & D is midpoint of BE [17]   CB  AE [18]
008. C,G,A are collinear [06] & EG  AC [07]   EG  GA [19]
009. I,E,A are collinear [10] & IA = IE [11]   I is midpoint of AE [20]
010. EG  GA [19] & I is midpoint of AE [20]   EI = GI [21]
011. EI = GI [21]   IGE = GEI [22]
012. HBF = BFH [15] & B,C,H are collinear [08] & BC  AE [18] & IGE = GEI [22] & I,E,A are collinear [10] & CA  GE [07] & BF  AC [05]   (GI-BF) = HFB [23]
013. (GI-BF) = HFB [23]   GI  FH
==========================

