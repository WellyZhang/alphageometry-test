I0123 15:58:22.693602 139669263372288 inference_utils.py:69] Parsing gin configuration.
I0123 15:58:22.693738 139669263372288 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 15:58:22.693953 139669263372288 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 15:58:22.693988 139669263372288 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 15:58:22.694016 139669263372288 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 15:58:22.694044 139669263372288 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 15:58:22.694070 139669263372288 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 15:58:22.694096 139669263372288 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 15:58:22.694123 139669263372288 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 15:58:22.694148 139669263372288 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 15:58:22.694172 139669263372288 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 15:58:22.694198 139669263372288 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 15:58:22.694246 139669263372288 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 15:58:22.694391 139669263372288 resource_reader.py:55] Path not found: base_htrans.gin
I0123 15:58:22.694640 139669263372288 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 15:58:22.694746 139669263372288 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 15:58:22.701231 139669263372288 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 15:58:22.701363 139669263372288 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 15:58:22.701702 139669263372288 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 15:58:22.701809 139669263372288 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 15:58:22.702094 139669263372288 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 15:58:22.702195 139669263372288 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 15:58:22.702608 139669263372288 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 15:58:22.702709 139669263372288 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 15:58:22.706452 139669263372288 training_loop.py:334] ==== Training loop: initializing model ====
I0123 15:58:22.808910 139669263372288 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 15:58:22.809720 139669263372288 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 15:58:22.816441 139669263372288 training_loop.py:335] Process 0 of 1
I0123 15:58:22.816495 139669263372288 training_loop.py:336] Local device count = 1
I0123 15:58:22.816535 139669263372288 training_loop.py:337] Number of replicas = 1
I0123 15:58:22.816568 139669263372288 training_loop.py:339] Using random number seed 42
I0123 15:58:23.312200 139669263372288 training_loop.py:359] Initializing the model.
I0123 15:58:23.698621 139669263372288 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.698966 139669263372288 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 15:58:23.699074 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699151 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699226 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699307 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699378 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699447 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699516 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699583 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699649 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699716 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699784 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699852 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:58:23.699890 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:23.699935 139669263372288 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:58:23.700052 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:23.700091 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:23.700123 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:23.702161 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.707461 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:23.718004 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.718287 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:23.722607 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:23.733650 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:23.733708 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:23.733746 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:23.733778 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.733841 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.735044 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.735124 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.735835 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.738338 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.744471 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.745805 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.745891 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:23.745927 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:23.745989 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.746120 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:23.746459 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:23.746506 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.748411 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.748512 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.751388 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.751474 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:23.751972 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:23.762105 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.770908 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.771007 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.771300 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.771382 139669263372288 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:58:23.771494 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:23.771534 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:23.771566 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:23.773430 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.775910 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:23.781467 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.781744 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:23.784359 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:23.788191 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:23.788248 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:23.788285 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:23.788317 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.788379 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.788945 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.789021 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.789378 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.790153 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.792614 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.793235 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.793312 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:23.793347 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:23.793405 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.793533 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:23.793867 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:23.793915 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.795842 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.795934 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.798410 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.798490 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:23.798922 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:23.801244 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.803142 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.803237 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.803530 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.803610 139669263372288 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:58:23.803720 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:23.803759 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:23.803789 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:23.806056 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.808412 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:23.813994 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.814255 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:23.816899 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:23.820727 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:23.820782 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:23.820817 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:23.820848 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.820910 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.821472 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.821547 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.821909 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.822668 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.825131 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.825815 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.825893 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:23.825928 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:23.825986 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.826111 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:23.826440 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:23.826483 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.828368 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.828459 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.830942 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.831028 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:23.831511 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:23.833790 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.835688 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.835781 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.836072 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.836152 139669263372288 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:58:23.836261 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:23.836300 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:23.836331 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:23.838234 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.840584 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:23.846143 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.846409 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:23.849007 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:23.852805 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:23.852861 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:23.852896 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:23.852927 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.852993 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.853555 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.853635 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.854003 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.854772 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.857281 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.857902 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.857983 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:23.858019 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:23.858079 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.858207 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:23.858533 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:23.858576 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.860447 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.860543 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.863200 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.863286 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:23.863720 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:23.865994 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.867945 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.868044 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.868336 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.868417 139669263372288 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:58:23.868527 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:23.868567 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:23.868598 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:23.870556 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.872967 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:23.878690 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.878965 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:23.882001 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:23.885829 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:23.885885 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:23.885923 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:23.885955 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.886026 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.886615 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.886693 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.887059 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.887835 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.890481 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.891134 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.891211 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:23.891247 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:23.891305 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.891437 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:23.891767 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:23.891811 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.893713 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.893808 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.896412 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.896490 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:23.896924 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:23.899258 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.901214 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.901308 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.901599 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.901691 139669263372288 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:58:23.901806 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:23.901846 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:23.901878 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:23.903784 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.906191 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:23.911882 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.912138 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:23.914860 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:23.918671 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:23.918728 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:23.918766 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:23.918799 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.918869 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.919493 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.919569 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.919926 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.920706 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.923238 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.923858 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.923935 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:23.923970 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:23.924029 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.924155 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:23.924485 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:23.924527 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.926429 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.926528 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.929099 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.929177 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:23.929618 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:23.931994 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.933935 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.934037 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.934331 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.934420 139669263372288 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:58:23.934533 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:23.934574 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:23.934606 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:23.936485 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.938969 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:23.944647 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.944911 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:23.947603 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:23.951495 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:23.951551 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:23.951588 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:23.951619 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.951681 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.952248 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.952327 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.952689 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.953461 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.956053 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.956679 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.956756 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:23.956792 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:23.956850 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.956977 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:23.957299 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:23.957342 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:23.959697 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.959791 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:23.962311 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:23.962395 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:23.962852 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.105093 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.107434 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.107604 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.107922 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.108013 139669263372288 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:58:24.108131 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.108171 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.108204 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.110468 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.113090 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.118952 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.119240 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.121963 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:24.125937 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.125995 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.126032 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.126064 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.126127 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.126747 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.126823 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.127179 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.127958 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.130579 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.131229 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.131308 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.131343 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.131402 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.131533 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.131864 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.131908 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.133831 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.133928 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.136451 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.136530 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.137019 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.139340 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.141268 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.141374 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.141675 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.141760 139669263372288 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:58:24.141871 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.141910 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.141941 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.143857 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.146230 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.151850 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.152107 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.154787 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:24.158590 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.158645 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.158681 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.158713 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.158776 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.159349 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.159425 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.159777 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.160546 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.163092 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.163716 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.163792 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.163827 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.163884 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.164011 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.164330 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.164372 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.166255 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.166348 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.168874 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.168954 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.169389 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.171670 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.173552 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.173654 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.173942 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.174030 139669263372288 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:58:24.174143 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.174181 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.174212 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.176120 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.178490 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.184432 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.184700 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.187394 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:24.191145 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.191202 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.191238 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.191270 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.191331 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.191898 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.191974 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.192327 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.193137 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.195599 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.196219 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.196295 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.196330 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.196387 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.196515 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.196834 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.196877 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.198772 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.198866 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.201381 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.201463 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.201911 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.204158 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.206119 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.206216 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.206505 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.206593 139669263372288 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:58:24.206705 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.206744 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.206775 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.208639 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.211098 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.216657 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.216923 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.219596 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:24.223398 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.223454 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.223490 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.223520 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.223622 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.224177 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.224253 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.224605 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.225391 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.227889 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.228510 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.228585 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.228620 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.228676 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.228806 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.229128 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.229171 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.231134 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.231235 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.233983 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.234063 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.234498 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.236817 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.238700 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.238795 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.239083 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.239164 139669263372288 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:58:24.239282 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.239321 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.239352 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.241203 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.243636 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.249214 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.249477 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.252113 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:58:24.256240 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.256297 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.256333 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.256364 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.256425 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.256986 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.257062 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.257415 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.258199 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.260684 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.261305 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.261383 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.261417 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.261475 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.261609 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.261936 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.261980 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.263939 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.264037 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.266551 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.266630 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.267072 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.269372 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.271267 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.271363 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.271652 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.271929 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.271999 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272065 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272123 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272179 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272233 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272287 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272340 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272393 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272445 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272498 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272550 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:58:24.272587 139669263372288 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:58:24.276111 139669263372288 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:58:24.323654 139669263372288 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.323740 139669263372288 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:58:24.323793 139669263372288 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:58:24.323897 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.323936 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.323965 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.324028 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.326465 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.331920 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.332180 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.334816 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.351404 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.351461 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.351498 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.351530 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.351593 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.352726 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.352805 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.353499 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.355504 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.360233 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.361554 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.361647 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.361686 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.361745 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.361875 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.361985 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.362024 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.363935 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.364028 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.366456 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.366537 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.366646 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.368897 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.370850 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.370947 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.371238 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.371320 139669263372288 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:58:24.371431 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.371470 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.371501 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.371565 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.373810 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.379236 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.379497 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.382180 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.395453 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.395511 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.395546 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.395577 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.395642 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.396213 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.396290 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.396653 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.397339 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.399825 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.400441 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.400517 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.400558 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.400617 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.400748 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.400857 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.400894 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.402836 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.402931 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.405333 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.405412 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.405520 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.407740 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.409693 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.409795 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.410083 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.410167 139669263372288 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:58:24.410278 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.410317 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.410347 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.410410 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.412648 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.418122 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.418383 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.421073 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.433824 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.433883 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.433917 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.433947 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.434008 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.434572 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.434648 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.434997 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.435678 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.438130 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.438755 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.438832 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.438866 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.438929 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.439058 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.439167 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.439204 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.441110 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.441205 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.443601 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.443686 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.443795 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.445990 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.447897 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.447994 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.448274 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.448354 139669263372288 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:58:24.448462 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.448501 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.448531 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.448593 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.450803 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.456198 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.456456 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.459094 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.471900 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.471956 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.471992 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.472023 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.472086 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.472649 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.472726 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.473080 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.473769 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.476237 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.476857 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.476933 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.476968 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.477025 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.477161 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.477270 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.477309 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.479541 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.479636 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.482041 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.482120 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.482229 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.484430 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.486298 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.486394 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.486679 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.486760 139669263372288 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:58:24.486868 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.486906 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.486936 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.486998 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.489287 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.494739 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.495006 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.497614 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.510629 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.510684 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.510719 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.510749 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.510810 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.511360 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.511436 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.511792 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.512487 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.515135 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.515760 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.515837 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.515872 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.515929 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.516069 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.516178 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.516216 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.518107 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.518203 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.520734 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.520814 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.520923 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.523205 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.525065 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.525161 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.525445 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.525527 139669263372288 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:58:24.525637 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.525686 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.525718 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.525781 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.528025 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.533465 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.533731 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.536410 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.549207 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.549264 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.549299 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.549330 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.549393 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.549960 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.550038 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.550393 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.551080 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.553521 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.554148 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.554228 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.554264 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.554325 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.554452 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.554566 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.554605 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.556518 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.556611 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.559002 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.559082 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.559191 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.561416 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.563262 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.563358 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.563642 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.563724 139669263372288 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:58:24.563833 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.563873 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.563903 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.563966 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.566202 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.571695 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.571956 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.574562 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.587645 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.587701 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.587736 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.587767 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.587832 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.588391 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.588466 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.588816 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.589489 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.591938 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.592605 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.592683 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.592718 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.592776 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.592908 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.593017 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.593061 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.594940 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.595034 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.597414 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.597493 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.597599 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.599812 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.601711 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.601806 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.602089 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.602171 139669263372288 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:58:24.602279 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.602319 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.602350 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.602412 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.604603 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.609947 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.610217 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.612863 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.625657 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.625713 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.625748 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.625778 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.625841 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.626448 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.626525 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.626883 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.627570 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.630051 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.630679 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.630756 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.630791 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.630853 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.630982 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.631090 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.631136 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.633006 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.633099 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.635551 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.635632 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.635740 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.637963 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.639791 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.639886 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.640170 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.640252 139669263372288 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:58:24.640362 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.640400 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.640430 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.640492 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.642719 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.648172 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.648433 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.651034 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.663775 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.663830 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.663866 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.663897 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.663959 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.664525 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.664601 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.664955 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.665627 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.668091 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.668758 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.668836 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.668871 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.668928 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.669054 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.669162 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.669201 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.671085 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.671180 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.673544 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.673624 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.673741 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.675945 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.677864 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.677960 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.678243 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.678324 139669263372288 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:58:24.678433 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.678472 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.678503 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.678565 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.680789 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.686185 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.686440 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.689393 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.702099 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.702156 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.702192 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.702222 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.702283 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.702891 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.702967 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.703318 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.704009 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.706465 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.707091 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.707168 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.707203 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.707259 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.707387 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.707499 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.707538 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.709391 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.709491 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.711946 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.712025 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.712138 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.714344 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.716182 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.716276 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.716556 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.716636 139669263372288 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:58:24.716745 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.716784 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.716814 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.716876 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.719114 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.724563 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.724822 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.727455 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.740185 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.740241 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.740277 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.740308 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.740372 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.740931 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.741008 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.741362 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.742061 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.744498 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.745166 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.745244 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.745279 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.745336 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.745465 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.745573 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.745610 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.747481 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.747580 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.750001 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.750084 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.750192 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.752387 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.754292 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.754389 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.754675 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.754757 139669263372288 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:58:24.754867 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.754906 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.754937 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.754998 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.757219 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.762620 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.762881 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.765485 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.778227 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.778283 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.778319 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.778349 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.778410 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.778975 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.779054 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.779404 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.780084 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.782619 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.783240 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.783316 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.783351 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.783408 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.783538 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.783646 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.783686 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.785538 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.785631 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.788035 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.788113 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.788221 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.790815 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.792661 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.792756 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.793039 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.793126 139669263372288 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:58:24.795974 139669263372288 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:58:24.851502 139669263372288 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.851591 139669263372288 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:58:24.851646 139669263372288 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:58:24.851751 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.851789 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.851819 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.851881 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.854211 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.859520 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.859783 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.862418 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.875160 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.875216 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.875252 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.875284 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.875347 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.875912 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.875990 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.876348 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.877019 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.879554 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.880178 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.880256 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.880291 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.880350 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.880479 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.880599 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.880637 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.882490 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.882588 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.884972 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.885052 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.885162 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.887455 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.889302 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.889397 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.889690 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.889774 139669263372288 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:58:24.889884 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.889923 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.889955 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.890019 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.892261 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.897653 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.897913 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.900581 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.913275 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.913331 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.913365 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.913395 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.913457 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.914032 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.914112 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.914481 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.915181 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.917686 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.918318 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.918399 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.918435 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.918496 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.918628 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.918742 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.918789 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.920627 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.920721 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.923145 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.923225 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.923335 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.925567 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.927457 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.927554 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.927836 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.927918 139669263372288 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:58:24.928025 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.928064 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.928094 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.928157 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.930411 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.935829 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.936087 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.938771 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.951371 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.951428 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.951463 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.951494 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.951556 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.952110 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.952187 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.952540 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.953207 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.956163 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.956784 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.956861 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.956897 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.956956 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.957086 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.957195 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.957235 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.959131 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.959226 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.961591 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.961677 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.961789 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:24.964062 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.965901 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.965997 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.966283 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.966364 139669263372288 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:58:24.966472 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:24.966512 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:24.966542 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:24.966604 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.968804 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:24.974127 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.974386 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:24.977027 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:24.989611 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:24.989672 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:24.989709 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:24.989756 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.989821 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.990383 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.990458 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.990813 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.991502 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.994027 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.994649 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.994724 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:24.994757 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:24.994814 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.994941 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:24.995048 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:24.995087 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:24.996924 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.997015 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:24.999375 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:24.999454 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:24.999562 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:25.001852 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.003686 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.003780 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.004060 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.004142 139669263372288 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:58:25.004248 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:25.004286 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:25.004316 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:25.004377 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.006593 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:25.011921 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.012178 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:25.014846 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:25.027472 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:25.027526 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:25.027560 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:25.027590 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.027650 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.028202 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.028276 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.028631 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.029316 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.031822 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.032435 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.032511 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:25.032544 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:25.032600 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.032727 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:25.032834 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:25.032871 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.034747 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.034846 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.037218 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.037296 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:25.037404 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:25.039662 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.041501 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.041594 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.041887 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.041969 139669263372288 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:58:25.042077 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:25.042114 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:25.042144 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:25.042204 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.044419 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:25.049801 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.050060 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:25.052712 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:25.065342 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:25.065396 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:25.065430 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:25.065461 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.065522 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.066092 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.066169 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.066518 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.067197 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.070148 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.070772 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.070849 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:25.070883 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:25.070939 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.071065 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:25.071177 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:25.071214 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.073058 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.073156 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.075541 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.075620 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:25.075729 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:25.078022 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.079887 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.079980 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.080262 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.080343 139669263372288 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:58:25.080452 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:25.080490 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:25.080521 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:25.080582 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.082808 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:25.088207 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.088462 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:25.091141 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:25.103800 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:25.103854 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:25.103891 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:25.103922 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.103983 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.104546 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.104619 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.104970 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.105657 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.108186 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.108815 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.108891 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:25.108926 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:25.108983 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.109109 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:25.109216 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:25.109254 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.111109 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.111201 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.113572 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.113656 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:25.113766 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:25.116030 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.117884 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.117979 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.118262 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.118341 139669263372288 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:58:25.118448 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:25.118485 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:25.118515 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:25.118574 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.120769 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:25.126172 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.126431 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:25.129111 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:25.141765 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:25.141819 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:25.141852 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:25.141882 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.141942 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.142506 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.142580 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.142932 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.143615 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.146135 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.146757 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.146833 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:25.146868 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:25.146926 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.147054 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:25.147164 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:25.147201 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.149051 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.149143 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.151529 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.151613 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:25.151721 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:25.153996 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.155830 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.155923 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.156202 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.156282 139669263372288 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:58:25.156389 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:25.156426 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:25.156455 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:25.156515 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.158727 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:25.164054 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.164312 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:25.166968 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:25.179548 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:25.179604 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:25.179639 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:25.179668 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.179728 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.180288 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.180361 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.180721 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.181403 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.184304 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.184919 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.184996 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:25.185029 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:25.185085 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.185212 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:25.185319 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:25.185356 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.187204 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.187299 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.189662 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.189748 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:25.189857 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:25.192117 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.193969 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.194064 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.194345 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.194427 139669263372288 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:58:25.194533 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:25.194571 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:25.194600 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:25.194661 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.196879 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:25.202267 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.202523 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:25.205176 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:25.217756 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:25.217811 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:25.217844 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:25.217873 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.217934 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.218493 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.218567 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.218918 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.219612 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.222142 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.222763 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.222838 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:25.222872 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:25.222927 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.223053 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:25.223159 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:25.223196 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.225363 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.225456 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.227851 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.227930 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:25.228045 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:25.230294 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.232106 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.232200 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.232483 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.232563 139669263372288 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:58:25.232671 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:25.232707 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:25.232737 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:25.232798 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.235028 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:25.240397 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.240654 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:25.243315 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:25.255903 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:25.255958 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:25.255992 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:25.256022 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.256083 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.256634 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.256709 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.257060 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.257751 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.260252 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.260868 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.260943 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:25.260976 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:25.261032 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.261157 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:25.261262 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:25.261298 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.263145 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.263238 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.265583 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.265669 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:25.265775 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:25.268036 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.269870 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.269963 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.270240 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.270319 139669263372288 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:58:25.270424 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:58:25.270462 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:58:25.270491 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:58:25.270552 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.272753 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:58:25.278099 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.278359 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:58:25.281016 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:58:25.293605 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:58:25.293666 139669263372288 attention.py:418] Single window, no scan.
I0123 15:58:25.293701 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 15:58:25.293730 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.293791 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.294358 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.294432 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.294787 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.295470 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.298366 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.298997 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.299076 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 15:58:25.299110 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 15:58:25.299166 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.299294 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:58:25.299407 139669263372288 nn_components.py:325] mlp: activation = None
I0123 15:58:25.299444 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.301293 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.301383 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.303764 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.303847 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 15:58:25.303953 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:58:25.306248 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 15:58:25.308084 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.308177 139669263372288 nn_components.py:261] mlp: residual
I0123 15:58:25.308457 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:25.308542 139669263372288 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:58:25.311374 139669263372288 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:58:29.755697 139669263372288 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 15:58:30.268808 139669263372288 training_loop.py:409] No working directory specified.
I0123 15:58:30.268951 139669263372288 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 15:58:30.269862 139669263372288 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 15:58:33.212137 139669263372288 training_loop.py:447] Only restoring trainable parameters.
I0123 15:58:33.212901 139669263372288 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 15:58:33.212986 139669263372288 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.213039 139669263372288 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.213084 139669263372288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.213127 139669263372288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.213169 139669263372288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.213209 139669263372288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.213248 139669263372288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.213287 139669263372288 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.213325 139669263372288 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.213362 139669263372288 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.213400 139669263372288 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.213437 139669263372288 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.213474 139669263372288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.213512 139669263372288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.213550 139669263372288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.213591 139669263372288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.213630 139669263372288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.213691 139669263372288 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.213728 139669263372288 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.213793 139669263372288 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.213832 139669263372288 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.213869 139669263372288 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.213905 139669263372288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.213942 139669263372288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.213978 139669263372288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.214014 139669263372288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214049 139669263372288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214084 139669263372288 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.214120 139669263372288 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.214155 139669263372288 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214192 139669263372288 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.214227 139669263372288 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.214263 139669263372288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.214298 139669263372288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214334 139669263372288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.214370 139669263372288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214405 139669263372288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214442 139669263372288 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.214477 139669263372288 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.214511 139669263372288 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214546 139669263372288 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.214581 139669263372288 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.214617 139669263372288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.214653 139669263372288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214688 139669263372288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.214730 139669263372288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214767 139669263372288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214802 139669263372288 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.214838 139669263372288 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.214873 139669263372288 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.214908 139669263372288 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.214944 139669263372288 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.214980 139669263372288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.215015 139669263372288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215051 139669263372288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.215085 139669263372288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215121 139669263372288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215157 139669263372288 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.215192 139669263372288 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.215227 139669263372288 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215262 139669263372288 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.215298 139669263372288 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.215333 139669263372288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.215368 139669263372288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215403 139669263372288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.215438 139669263372288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215473 139669263372288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215509 139669263372288 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.215544 139669263372288 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.215579 139669263372288 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215615 139669263372288 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.215650 139669263372288 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.215692 139669263372288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.215729 139669263372288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215765 139669263372288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.215802 139669263372288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215838 139669263372288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215873 139669263372288 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.215908 139669263372288 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.215943 139669263372288 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.215978 139669263372288 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.216013 139669263372288 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.216049 139669263372288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.216084 139669263372288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216119 139669263372288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.216154 139669263372288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216188 139669263372288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216223 139669263372288 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.216257 139669263372288 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.216292 139669263372288 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216327 139669263372288 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.216362 139669263372288 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.216397 139669263372288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.216432 139669263372288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216467 139669263372288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.216503 139669263372288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216537 139669263372288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216572 139669263372288 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.216608 139669263372288 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.216648 139669263372288 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216686 139669263372288 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.216722 139669263372288 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.216757 139669263372288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.216792 139669263372288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216827 139669263372288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.216862 139669263372288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216897 139669263372288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.216932 139669263372288 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.216966 139669263372288 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.217000 139669263372288 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.217035 139669263372288 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.217070 139669263372288 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:58:33.217106 139669263372288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:58:33.217141 139669263372288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.217176 139669263372288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.217210 139669263372288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.217244 139669263372288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.217278 139669263372288 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:58:33.217312 139669263372288 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:58:33.217345 139669263372288 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:58:33.217379 139669263372288 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:58:33.217407 139669263372288 training_loop.py:725] Total parameters: 152072288
I0123 15:58:33.217610 139669263372288 training_loop.py:739] Total state size: 0
I0123 15:58:33.239829 139669263372288 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 15:58:33.240086 139669263372288 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 15:58:33.240609 139669263372288 training_loop.py:652] Compiling mode beam_search with jit.
I0123 15:58:33.240930 139669263372288 training_loop.py:89] registering functions: dict_keys([])
I0123 15:58:33.258556 139669263372288 graph.py:499] a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e ? coll r s q
I0123 15:58:35.187583 139669263372288 ddar.py:60] Depth 1/1000 time = 1.8545997142791748
I0123 15:58:49.391741 139669263372288 ddar.py:60] Depth 2/1000 time = 14.20386004447937
I0123 15:59:33.023111 139669263372288 ddar.py:60] Depth 3/1000 time = 43.630908727645874
I0123 16:00:20.878654 139669263372288 ddar.py:60] Depth 4/1000 time = 47.85500168800354
I0123 16:01:08.974022 139669263372288 ddar.py:60] Depth 5/1000 time = 48.09478974342346
I0123 16:01:56.335921 139669263372288 ddar.py:60] Depth 6/1000 time = 47.36052083969116
I0123 16:02:44.363239 139669263372288 ddar.py:60] Depth 7/1000 time = 47.899203300476074
I0123 16:03:35.691665 139669263372288 ddar.py:60] Depth 8/1000 time = 51.32799768447876
I0123 16:04:26.778483 139669263372288 ddar.py:60] Depth 9/1000 time = 51.08623147010803
I0123 16:05:17.979797 139669263372288 ddar.py:60] Depth 10/1000 time = 51.20076775550842
I0123 16:06:13.860476 139669263372288 ddar.py:60] Depth 11/1000 time = 55.51338315010071
I0123 16:07:16.850516 139669263372288 ddar.py:60] Depth 12/1000 time = 62.989474058151245
I0123 16:08:19.985720 139669263372288 ddar.py:60] Depth 13/1000 time = 63.134538412094116
I0123 16:09:24.079672 139669263372288 ddar.py:60] Depth 14/1000 time = 64.09336304664612
I0123 16:10:29.684500 139669263372288 ddar.py:60] Depth 15/1000 time = 65.51902723312378
I0123 16:11:36.899688 139669263372288 ddar.py:60] Depth 16/1000 time = 67.17261719703674
I0123 16:12:46.373677 139669263372288 ddar.py:60] Depth 17/1000 time = 69.38583207130432
I0123 16:12:46.374551 139669263372288 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:12:46.374673 139669263372288 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 16:12:46.374713 139669263372288 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a b d 00 D a d b d 01 ; e : C a c e 02 D a e c e 03 ; f : C b c f 04 D b f c f 05 ; g : ^ a b a g a g a c 06 ^ c a c g c g c b 07 ; h : C a c h 08 T a c g h 09 ; i : C b c i 10 T b c g i 11 ; j : C a b j 12 T a b g j 13 ; k : C d g k 14 T d g j k 15 ; l : C j k l 16 D j k k l 17 ; m : C e g m 18 T e g h m 19 ; n : C h m n 20 D h m m n 21 ; o : C f g o 22 T f g i o 23 ; p : C i o p 24 D i o o p 25 ; q : C d l q 26 C e f q 27 ; r : C d f r 28 C e n r 29 ; s : C d e s 30 C f p s 31 ? C r s q {F1} x00
I0123 16:12:46.374745 139669263372288 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : C a b d 00 D a d b d 01 ; e : C a c e 02 D a e c e 03 ; f : C b c f 04 D b f c f 05 ; g : ^ a b a g a g a c 06 ^ c a c g c g c b 07 ; h : C a c h 08 T a c g h 09 ; i : C b c i 10 T b c g i 11 ; j : C a b j 12 T a b g j 13 ; k : C d g k 14 T d g j k 15 ; l : C j k l 16 D j k k l 17 ; m : C e g m 18 T e g h m 19 ; n : C h m n 20 D h m m n 21 ; o : C f g o 22 T f g i o 23 ; p : C i o p 24 D i o o p 25 ; q : C d l q 26 C e f q 27 ; r : C d f r 28 C e n r 29 ; s : C d e s 30 C f p s 31 ? C r s q {F1} x00
I0123 16:12:46.534379 139669263372288 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.534634 139669263372288 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 16:12:46.534742 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.534819 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.534893 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.534960 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.535046 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.535115 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.535182 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.535250 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.535315 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.535382 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.535448 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.535516 139669263372288 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:12:46.535560 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.535606 139669263372288 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:12:46.535719 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.535760 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.535792 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.537746 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.540232 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.545880 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.546157 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.548777 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.552722 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.552788 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.552827 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.552861 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.552926 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.553568 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.553657 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.554010 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.554776 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.557330 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.557979 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.558062 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.558099 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.558160 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.558287 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.558613 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.558662 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.560536 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.560644 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.563036 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.563119 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.563543 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.565956 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.567865 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.567965 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.568252 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.568334 139669263372288 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:12:46.568442 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.568481 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.568512 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.570278 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.572508 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.578467 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.578728 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.581206 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.584848 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.584908 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.584946 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.584977 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.585039 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.585655 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.585736 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.586084 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.586834 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.589195 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.589816 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.589898 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.589934 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.589992 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.590117 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.590434 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.590480 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.592391 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.592487 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.594866 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.594951 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.595371 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.597563 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.599440 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.599539 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.599821 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.599903 139669263372288 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:12:46.600012 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.600050 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.600082 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.601927 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.604157 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.609570 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.609837 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.612300 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.615929 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.615989 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.616027 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.616060 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.616123 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.616675 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.616754 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.617099 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.617859 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.620228 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.620838 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.620919 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.620954 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.621012 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.621140 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.621500 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.621547 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.623419 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.623516 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.625889 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.625981 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.626405 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.628608 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.630561 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.630661 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.630944 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.631028 139669263372288 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:12:46.631137 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.631175 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.631207 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.632956 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.635206 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.640727 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.640988 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.643507 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.647098 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.647159 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.647195 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.647226 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.647289 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.647885 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.647965 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.648315 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.649078 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.651483 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.652089 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.652172 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.652208 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.652266 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.652394 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.652709 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.652755 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.654685 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.654783 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.657142 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.657227 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.657667 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.659869 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.661753 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.661852 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.662133 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.662219 139669263372288 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:12:46.662327 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.662365 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.662397 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.664219 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.666461 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.671870 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.672126 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.674617 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.678215 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.678276 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.678311 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.678343 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.678405 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.678955 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.679036 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.679382 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.680133 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.682532 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.683143 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.683224 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.683260 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.683316 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.683442 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.684139 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.684188 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.686065 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.686162 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.688532 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.688616 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.689037 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.691249 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.693178 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.693277 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.693558 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.693650 139669263372288 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:12:46.693760 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.693799 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.693832 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.695580 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.697832 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.703355 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.703614 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.706127 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.709729 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.709789 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.709825 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.709855 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.709917 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.710513 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.710593 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.710937 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.711679 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.714081 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.714691 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.714772 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.714807 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.714863 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.715001 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.715313 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.715360 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.717265 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.717362 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.719732 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.719815 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.720235 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.722465 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.724354 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.724452 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.724736 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.724822 139669263372288 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:12:46.724932 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.724971 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.725003 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.726853 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.729106 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.734579 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.734838 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.737314 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.740947 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.741007 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.741043 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.741075 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.741137 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.741698 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.741780 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.742129 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.742897 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.745282 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.745910 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.745993 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.746028 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.746085 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.746212 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.746578 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.746626 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.748478 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.748574 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.750940 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.751024 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.751447 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.753647 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.755579 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.755688 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.755975 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.756060 139669263372288 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:12:46.756169 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.756207 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.756240 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.758021 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.760266 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.765779 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.766036 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.768544 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.772164 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.772226 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.772264 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.772295 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.772357 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.772958 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.773038 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.773391 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.774153 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.776570 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.777183 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.777266 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.777302 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.777360 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.777488 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.777816 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.777864 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.779807 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.779904 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.782311 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.782396 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.782817 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.785032 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.786924 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.787033 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.787317 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.787403 139669263372288 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:12:46.787511 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.787550 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.787581 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.789742 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.792014 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.797474 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.797746 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.800256 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.803883 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.803944 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.803981 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.804011 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.804074 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.804622 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.804702 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.805046 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.805793 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.808184 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.808799 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.808879 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.808915 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.808971 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.809098 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.809411 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.809458 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.811403 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.811499 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.813896 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.813980 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.814408 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.816616 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.818496 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.818597 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.818883 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.818981 139669263372288 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:12:46.819092 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.819131 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.819164 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.820998 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.823261 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.828709 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.828967 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.831461 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.835094 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.835156 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.835193 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.835226 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.835288 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.835839 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.835922 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.836272 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.837024 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.839400 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.840013 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.840092 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.840127 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.840184 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.840310 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.840624 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.840670 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.842609 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.842707 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.845074 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.845159 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.845577 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.847803 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.849665 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.849765 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.850050 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.850144 139669263372288 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:12:46.850255 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.850294 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.850327 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.852171 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.854416 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.859824 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.860082 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.862568 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.866199 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.866258 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.866294 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.866324 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.866387 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.866939 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.867019 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.867367 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.868102 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.870495 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.871114 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.871198 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.871233 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.871290 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.871416 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.871731 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.871778 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.873712 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.873811 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.876186 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.876270 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.876690 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.878907 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.880768 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.880867 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.881150 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.881236 139669263372288 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:12:46.881354 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.881397 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.881427 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.883276 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.885527 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.890974 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.891232 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.893749 139669263372288 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:12:46.897372 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.897432 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.897468 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.897499 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.897562 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.898123 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.898205 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.898556 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.899300 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.901673 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.902286 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.902367 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.902402 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.902458 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.902583 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.902895 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.902941 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.904864 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.904960 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.907362 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.907444 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.907869 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.910080 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.911920 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.912017 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.912298 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.912546 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.912619 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.912686 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.912742 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.912796 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.912849 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.912899 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.912951 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.913002 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.913053 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.913106 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.913157 139669263372288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:12:46.913192 139669263372288 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:12:46.916035 139669263372288 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:12:46.960257 139669263372288 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.960346 139669263372288 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:12:46.960400 139669263372288 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:12:46.960504 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.960543 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.960573 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.960633 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.962945 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:46.968169 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.968429 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:46.970937 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:46.983911 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:46.983972 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:46.984008 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:46.984039 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.984102 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.984667 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.984747 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.985103 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.985789 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.988253 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.988863 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.988953 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:46.988990 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:46.989048 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.989176 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:46.989285 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:46.989325 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.991134 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.991232 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.993549 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.993633 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:46.993750 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:46.995928 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:46.997726 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.997826 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:46.998108 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:46.998194 139669263372288 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:12:46.998303 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:46.998341 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:46.998373 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:46.998434 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.000595 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.005827 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.006083 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.008658 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.020851 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.020911 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.020948 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.020980 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.021042 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.021590 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.021683 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.022033 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.022753 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.025116 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.025733 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.025823 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.025859 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.025918 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.026050 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.026160 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.026200 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.027993 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.028089 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.030420 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.030504 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.030610 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.032799 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.034635 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.034735 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.035021 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.035107 139669263372288 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:12:47.035219 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.035259 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.035292 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.035354 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.037526 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.042825 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.043089 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.045688 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.057847 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.057910 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.057946 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.057978 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.058040 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.058593 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.058674 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.059022 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.059755 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.062155 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.062770 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.062852 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.062898 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.062959 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.063092 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.063201 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.063242 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.065042 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.065140 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.067483 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.067568 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.067677 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.069877 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.071678 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.071777 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.072059 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.072145 139669263372288 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:12:47.072253 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.072292 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.072325 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.072387 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.074544 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.079793 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.080056 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.082668 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.095271 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.095331 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.095367 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.095399 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.095461 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.096012 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.096092 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.096440 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.097156 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.099553 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.100165 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.100249 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.100284 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.100353 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.100488 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.100597 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.100638 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.102457 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.102555 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.104889 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.104973 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.105082 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.107298 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.109096 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.109193 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.109475 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.109559 139669263372288 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:12:47.109676 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.109716 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.109748 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.109809 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.111962 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.117187 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.117450 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.120042 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.132207 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.132267 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.132304 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.132335 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.132398 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.132942 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.133022 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.133368 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.134090 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.136466 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.137077 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.137157 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.137193 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.137250 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.137386 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.137497 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.137537 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.139358 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.139457 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.141783 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.141867 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.141976 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.144206 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.146066 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.146168 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.146451 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.146538 139669263372288 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:12:47.146647 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.146686 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.146718 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.146780 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.148938 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.154221 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.154486 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.157084 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.169219 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.169280 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.169315 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.169347 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.169409 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.169966 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.170048 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.170400 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.171126 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.173513 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.174133 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.174216 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.174252 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.174310 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.174446 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.174556 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.174597 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.176374 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.176473 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.178815 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.178899 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.179008 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.181213 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.183021 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.183119 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.183406 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.183491 139669263372288 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:12:47.183599 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.183637 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.183669 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.183730 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.185891 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.191089 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.191349 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.193916 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.206573 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.206634 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.206670 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.206701 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.206763 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.207312 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.207392 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.207741 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.208470 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.210841 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.211453 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.211534 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.211570 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.211629 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.211756 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.211874 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.211915 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.213712 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.213810 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.216125 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.216209 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.216317 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.218524 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.220339 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.220437 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.220724 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.220809 139669263372288 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:12:47.220918 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.220957 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.220991 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.221053 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.223225 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.228435 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.228699 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.231306 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.243507 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.243568 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.243605 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.243637 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.243701 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.244254 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.244335 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.244683 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.245408 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.247812 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.248426 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.248507 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.248543 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.248601 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.248731 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.248841 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.248890 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.250699 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.250796 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.253116 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.253200 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.253309 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.255505 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.257300 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.257400 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.257693 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.257780 139669263372288 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:12:47.257892 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.257932 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.257965 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.258027 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.260207 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.265449 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.265722 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.268324 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.280593 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.280654 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.280690 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.280722 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.280785 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.281334 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.281414 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.281771 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.282443 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.284895 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.285511 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.285591 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.285627 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.285693 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.285821 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.285929 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.285979 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.287782 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.287878 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.290225 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.290310 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.290420 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.292669 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.294494 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.294593 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.294883 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.294970 139669263372288 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:12:47.295082 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.295121 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.295154 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.295215 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.297406 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.302696 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.302955 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.305571 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.318272 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.318333 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.318370 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.318402 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.318465 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.319020 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.319101 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.319454 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.320127 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.322607 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.323226 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.323309 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.323345 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.323401 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.323528 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.323637 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.323677 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.325484 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.325583 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.327928 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.328012 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.328122 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.330323 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.332128 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.332225 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.332507 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.332592 139669263372288 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:12:47.332702 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.332740 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.332773 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.332834 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.335005 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.340253 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.340513 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.343095 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.355267 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.355328 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.355364 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.355394 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.355456 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.356005 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.356085 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.356434 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.357099 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.359553 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.360162 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.360244 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.360280 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.360337 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.360466 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.360575 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.360616 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.362416 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.362524 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.364855 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.364938 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.365047 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.367247 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.369040 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.369139 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.369423 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.369509 139669263372288 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:12:47.369618 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.369665 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.369699 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.369761 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.371919 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.377120 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.377380 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.379980 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.392113 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.392173 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.392209 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.392241 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.392302 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.392849 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.392930 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.393282 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.393960 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.396411 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.397022 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.397103 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.397138 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.397195 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.397323 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.397432 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.397472 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.399270 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.399378 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.401708 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.401792 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.401902 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.404105 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.405899 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.405997 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.406279 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.406369 139669263372288 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:12:47.409135 139669263372288 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:12:47.459427 139669263372288 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.459517 139669263372288 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:12:47.459572 139669263372288 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:12:47.459677 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.459715 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.459747 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.459808 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.462032 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.467370 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.467628 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.470143 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.482413 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.482474 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.482511 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.482542 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.482605 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.483153 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.483233 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.483582 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.484255 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.486634 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.487239 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.487321 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.487356 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.487414 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.487552 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.487662 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.487702 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.489937 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.490035 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.492376 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.492459 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.492568 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.494710 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.496510 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.496610 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.496896 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.496981 139669263372288 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:12:47.497089 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.497128 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.497160 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.497221 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.499393 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.504703 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.504959 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.507463 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.519726 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.519787 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.519827 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.519860 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.519923 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.520473 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.520554 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.520905 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.521577 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.523989 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.524600 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.524682 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.524718 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.524777 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.524904 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.525021 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.525062 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.526937 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.527034 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.529365 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.529449 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.529559 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.531690 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.533481 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.533580 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.533871 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.533959 139669263372288 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:12:47.534068 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.534107 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.534138 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.534198 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.536357 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.541731 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.541997 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.544533 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.556766 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.556826 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.556864 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.556896 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.556959 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.557507 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.557588 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.557944 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.558609 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.561002 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.561618 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.561709 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.561746 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.561804 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.561930 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.562040 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.562089 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.563974 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.564072 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.566432 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.566517 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.566626 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.568791 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.570600 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.570700 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.570983 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.571069 139669263372288 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:12:47.571176 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.571215 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.571248 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.571309 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.573486 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.578829 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.579090 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.581617 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.593833 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.593893 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.593930 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.593963 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.594026 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.594573 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.594656 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.595006 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.595674 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.598068 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.598675 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.598755 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.598792 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.598849 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.598977 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.599086 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.599127 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.601489 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.601587 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.603947 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.604031 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.604142 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.606286 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.608091 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.608190 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.608472 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.608557 139669263372288 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:12:47.608666 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.608704 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.608737 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.608799 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.610974 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.616287 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.616547 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.619076 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.631385 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.631445 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.631482 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.631513 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.631575 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.632126 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.632206 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.632555 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.633229 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.635643 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.636251 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.636332 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.636367 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.636425 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.636555 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.636663 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.636703 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.638601 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.638700 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.641046 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.641128 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.641237 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.643382 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.645212 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.645312 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.645598 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.645691 139669263372288 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:12:47.645799 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.645838 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.645871 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.645933 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.648114 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.653471 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.653745 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.656264 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.668607 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.668667 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.668703 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.668734 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.668797 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.669343 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.669423 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.669782 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.670454 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.672842 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.673449 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.673529 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.673564 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.673622 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.673756 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.673865 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.673905 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.675756 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.675861 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.678223 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.678308 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.678417 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.680569 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.682401 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.682501 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.682786 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.682873 139669263372288 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:12:47.682981 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.683019 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.683052 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.683114 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.685277 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.690613 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.690873 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.693392 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.705724 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.705785 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.705821 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.705851 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.705914 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.706467 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.706547 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.706894 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.707567 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.709979 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.710586 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.710668 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.710704 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.710762 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.710887 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.710995 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.711035 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.713338 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.713445 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.715794 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.715879 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.715988 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.718143 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.719953 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.720053 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.720335 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.720421 139669263372288 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:12:47.720529 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.720567 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.720599 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.720661 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.722827 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.728167 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.728428 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.730971 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.743305 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.743366 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.743402 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.743433 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.743495 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.744058 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.744140 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.744496 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.745175 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.747596 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.748204 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.748284 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.748319 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.748376 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.748502 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.748610 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.748650 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.750531 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.750626 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.752940 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.753021 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.753128 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.755297 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.757089 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.757187 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.757467 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.757550 139669263372288 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:12:47.757664 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.757705 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.757737 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.757797 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.759942 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.765216 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.765473 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.768003 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.780272 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.780331 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.780367 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.780397 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.780457 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.780997 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.781075 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.781423 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.782099 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.784502 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.785120 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.785200 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.785236 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.785293 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.785419 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.785529 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.785570 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.787443 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.787540 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.789871 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.789963 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.790074 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.792228 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.794052 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.794149 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.794434 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.794520 139669263372288 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:12:47.794629 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.794670 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.794703 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.794764 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.796926 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.802250 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.802511 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.805025 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.817346 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.817406 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.817441 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.817473 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.817533 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.818089 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.818170 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.818519 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.819197 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.821596 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.822214 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.822294 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.822329 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.822387 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.822513 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.822622 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.822663 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.824953 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.825050 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.827396 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.827486 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.827598 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.829747 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.831537 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.831635 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.831918 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.832002 139669263372288 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:12:47.832111 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.832150 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.832183 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.832244 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.834403 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.839745 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.840008 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.842563 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.854808 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.854868 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.854905 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.854936 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.854995 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.855543 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.855621 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.855972 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.856636 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.859008 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.859616 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.859696 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.859732 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.859789 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.859916 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.860025 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.860066 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.861938 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.862036 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.864342 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.864425 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.864543 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.866684 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.868468 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.868566 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.868850 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.868934 139669263372288 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:12:47.869042 139669263372288 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:12:47.869081 139669263372288 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:12:47.869115 139669263372288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:12:47.869176 139669263372288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.871329 139669263372288 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:12:47.876641 139669263372288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.876902 139669263372288 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:12:47.879434 139669263372288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:12:47.891758 139669263372288 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:12:47.891819 139669263372288 attention.py:418] Single window, no scan.
I0123 16:12:47.891855 139669263372288 transformer_layer.py:389] tlayer: self-attention.
I0123 16:12:47.891885 139669263372288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.891947 139669263372288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.892493 139669263372288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.892573 139669263372288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.892924 139669263372288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.893598 139669263372288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.896003 139669263372288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.896608 139669263372288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.896688 139669263372288 transformer_layer.py:468] tlayer: End windows.
I0123 16:12:47.896723 139669263372288 transformer_layer.py:472] tlayer: final FFN.
I0123 16:12:47.896780 139669263372288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.896903 139669263372288 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:12:47.897011 139669263372288 nn_components.py:325] mlp: activation = None
I0123 16:12:47.897052 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.898932 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.899030 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.901342 139669263372288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.901425 139669263372288 transformer_base.py:443] tbase: final FFN
I0123 16:12:47.901533 139669263372288 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:12:47.903699 139669263372288 nn_components.py:329] mlp: final activation = None
I0123 16:12:47.905496 139669263372288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.905593 139669263372288 nn_components.py:261] mlp: residual
I0123 16:12:47.905885 139669263372288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:12:47.905975 139669263372288 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:12:47.908718 139669263372288 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:13:04.011420 139669263372288 alphageometry.py:566] LM output (score=-2.033044): "t : D g h g t 32 T b c g t 33 ;"
I0123 16:13:04.011651 139669263372288 alphageometry.py:567] Translation: "t = on_circle t g h, on_tline t g b c"

I0123 16:13:04.011697 139669263372288 alphageometry.py:576] Solving: "a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_circle t g h, on_tline t g b c ? coll r s q"
I0123 16:13:04.011907 139669263372288 graph.py:498] 
I0123 16:13:04.011969 139669263372288 graph.py:499] a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_circle t g h, on_tline t g b c ? coll r s q
I0123 16:13:06.118053 139669263372288 ddar.py:60] Depth 1/1000 time = 2.026825189590454
I0123 16:13:30.002515 139669263372288 ddar.py:60] Depth 2/1000 time = 23.884258270263672
I0123 16:15:02.287396 139669263372288 ddar.py:60] Depth 3/1000 time = 92.2844660282135
I0123 16:16:32.718361 139669263372288 ddar.py:60] Depth 4/1000 time = 90.43028950691223
I0123 16:18:03.248703 139669263372288 ddar.py:60] Depth 5/1000 time = 90.52976250648499
I0123 16:19:33.895000 139669263372288 ddar.py:60] Depth 6/1000 time = 90.64473223686218
I0123 16:21:05.574698 139669263372288 ddar.py:60] Depth 7/1000 time = 91.49585151672363
I0123 16:22:53.651218 139669263372288 ddar.py:60] Depth 8/1000 time = 108.0759539604187
I0123 16:24:42.160689 139669263372288 ddar.py:60] Depth 9/1000 time = 108.50888776779175
I0123 16:26:31.727343 139669263372288 ddar.py:60] Depth 10/1000 time = 109.56614780426025
I0123 16:28:34.913615 139669263372288 ddar.py:60] Depth 11/1000 time = 122.38169717788696
I0123 16:30:37.688359 139669263372288 ddar.py:60] Depth 12/1000 time = 122.71166586875916
I0123 16:32:45.342598 139669263372288 ddar.py:60] Depth 13/1000 time = 127.48991203308105
I0123 16:32:45.343249 139669263372288 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:32:45.344204 139669263372288 alphageometry.py:566] LM output (score=-2.208521): "t : T j k l t 32 ;"
I0123 16:32:45.344264 139669263372288 alphageometry.py:567] Translation: "t = on_tline t l j k"

I0123 16:32:45.344330 139669263372288 alphageometry.py:576] Solving: "a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_tline t l j k ? coll r s q"
I0123 16:32:45.344579 139669263372288 graph.py:498] 
I0123 16:32:45.344649 139669263372288 graph.py:499] a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_tline t l j k ? coll r s q
I0123 16:32:47.180397 139669263372288 ddar.py:60] Depth 1/1000 time = 1.760249137878418
I0123 16:33:01.679142 139669263372288 ddar.py:60] Depth 2/1000 time = 14.498474597930908
I0123 16:33:37.838316 139669263372288 ddar.py:60] Depth 3/1000 time = 36.15866827964783
I0123 16:34:26.215566 139669263372288 ddar.py:60] Depth 4/1000 time = 48.376688957214355
I0123 16:35:14.352787 139669263372288 ddar.py:60] Depth 5/1000 time = 48.13658881187439
I0123 16:36:02.563456 139669263372288 ddar.py:60] Depth 6/1000 time = 48.208983182907104
I0123 16:36:53.836280 139669263372288 ddar.py:60] Depth 7/1000 time = 50.848209381103516
I0123 16:37:54.851215 139669263372288 ddar.py:60] Depth 8/1000 time = 61.014376401901245
I0123 16:38:57.072953 139669263372288 ddar.py:60] Depth 9/1000 time = 62.221158504486084
I0123 16:40:00.078171 139669263372288 ddar.py:60] Depth 10/1000 time = 63.00480389595032
I0123 16:41:06.727174 139669263372288 ddar.py:60] Depth 11/1000 time = 66.48911571502686
I0123 16:42:17.473630 139669263372288 ddar.py:60] Depth 12/1000 time = 70.61141228675842
I0123 16:42:17.474005 139669263372288 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:42:17.474349 139669263372288 alphageometry.py:566] LM output (score=-2.363193): "t : P j k l t 32 ;"
I0123 16:42:17.474392 139669263372288 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2570, in add_clause
    raise DepCheckFailError(
graph.DepCheckFailError: ncoll l j k
"

I0123 16:42:17.474434 139669263372288 alphageometry.py:566] LM output (score=-2.422172): "t : T j k j t 32 ;"
I0123 16:42:17.474462 139669263372288 alphageometry.py:567] Translation: "t = on_tline t j j k"

I0123 16:42:17.474501 139669263372288 alphageometry.py:576] Solving: "a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_tline t j j k ? coll r s q"
I0123 16:42:17.474720 139669263372288 graph.py:498] 
I0123 16:42:17.474786 139669263372288 graph.py:499] a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_tline t j j k ? coll r s q
I0123 16:42:19.364777 139669263372288 ddar.py:60] Depth 1/1000 time = 1.8113346099853516
I0123 16:42:34.276105 139669263372288 ddar.py:60] Depth 2/1000 time = 14.911105632781982
I0123 16:43:10.035938 139669263372288 ddar.py:60] Depth 3/1000 time = 35.75948524475098
I0123 16:43:58.608793 139669263372288 ddar.py:60] Depth 4/1000 time = 48.572333097457886
I0123 16:44:47.980310 139669263372288 ddar.py:60] Depth 5/1000 time = 49.3710503578186
I0123 16:45:37.269964 139669263372288 ddar.py:60] Depth 6/1000 time = 49.28760647773743
I0123 16:46:29.609114 139669263372288 ddar.py:60] Depth 7/1000 time = 51.90441131591797
I0123 16:47:30.691157 139669263372288 ddar.py:60] Depth 8/1000 time = 61.08151340484619
I0123 16:48:32.597316 139669263372288 ddar.py:60] Depth 9/1000 time = 61.905731439590454
I0123 16:49:35.598858 139669263372288 ddar.py:60] Depth 10/1000 time = 63.001067876815796
I0123 16:50:42.416410 139669263372288 ddar.py:60] Depth 11/1000 time = 66.66594123840332
I0123 16:51:52.610372 139669263372288 ddar.py:60] Depth 12/1000 time = 70.05914068222046
I0123 16:51:52.611151 139669263372288 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:51:52.611340 139669263372288 alphageometry.py:566] LM output (score=-2.489022): "t : T c g i t 32 ;"
I0123 16:51:52.611381 139669263372288 alphageometry.py:567] Translation: "t = on_tline t i c g"

I0123 16:51:52.611449 139669263372288 alphageometry.py:576] Solving: "a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_tline t i c g ? coll r s q"
I0123 16:51:52.611709 139669263372288 graph.py:498] 
I0123 16:51:52.611775 139669263372288 graph.py:499] a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_tline t i c g ? coll r s q
I0123 16:51:54.446240 139669263372288 ddar.py:60] Depth 1/1000 time = 1.759023904800415
I0123 16:52:10.038251 139669263372288 ddar.py:60] Depth 2/1000 time = 15.591813564300537
I0123 16:52:59.885764 139669263372288 ddar.py:60] Depth 3/1000 time = 49.84716296195984
I0123 16:53:50.958160 139669263372288 ddar.py:60] Depth 4/1000 time = 51.07180309295654
I0123 16:54:42.002937 139669263372288 ddar.py:60] Depth 5/1000 time = 51.04420852661133
I0123 16:55:32.144855 139669263372288 ddar.py:60] Depth 6/1000 time = 50.140685081481934
I0123 16:56:23.732385 139669263372288 ddar.py:60] Depth 7/1000 time = 51.43514156341553
I0123 16:57:17.916267 139669263372288 ddar.py:60] Depth 8/1000 time = 54.183496952056885
I0123 16:58:12.394217 139669263372288 ddar.py:60] Depth 9/1000 time = 54.477548599243164
I0123 16:59:06.522290 139669263372288 ddar.py:60] Depth 10/1000 time = 54.127601623535156
I0123 17:00:05.315334 139669263372288 ddar.py:60] Depth 11/1000 time = 58.42433452606201
I0123 17:01:11.628020 139669263372288 ddar.py:60] Depth 12/1000 time = 66.31204605102539
I0123 17:02:18.212815 139669263372288 ddar.py:60] Depth 13/1000 time = 66.58417463302612
I0123 17:03:25.181025 139669263372288 ddar.py:60] Depth 14/1000 time = 66.96763229370117
I0123 17:04:34.618224 139669263372288 ddar.py:60] Depth 15/1000 time = 69.35253047943115
I0123 17:05:44.826672 139669263372288 ddar.py:60] Depth 16/1000 time = 70.16697764396667
I0123 17:06:57.839610 139669263372288 ddar.py:60] Depth 17/1000 time = 72.92463946342468
I0123 17:06:57.840427 139669263372288 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:06:57.840609 139669263372288 alphageometry.py:566] LM output (score=-2.543621): "t : T h i i t 32 ;"
I0123 17:06:57.840647 139669263372288 alphageometry.py:567] Translation: "t = on_tline t i h i"

I0123 17:06:57.840711 139669263372288 alphageometry.py:576] Solving: "a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_tline t i h i ? coll r s q"
I0123 17:06:57.840956 139669263372288 graph.py:498] 
I0123 17:06:57.841020 139669263372288 graph.py:499] a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_tline t i h i ? coll r s q
I0123 17:07:00.107707 139669263372288 ddar.py:60] Depth 1/1000 time = 2.1909019947052
I0123 17:07:17.313312 139669263372288 ddar.py:60] Depth 2/1000 time = 17.205382108688354
I0123 17:07:50.990449 139669263372288 ddar.py:60] Depth 3/1000 time = 33.676783323287964
I0123 17:08:41.655334 139669263372288 ddar.py:60] Depth 4/1000 time = 50.66428756713867
I0123 17:09:32.156210 139669263372288 ddar.py:60] Depth 5/1000 time = 50.500229597091675
I0123 17:10:22.291090 139669263372288 ddar.py:60] Depth 6/1000 time = 50.133350133895874
I0123 17:11:14.413058 139669263372288 ddar.py:60] Depth 7/1000 time = 51.753556966781616
I0123 17:12:14.476060 139669263372288 ddar.py:60] Depth 8/1000 time = 60.06240439414978
I0123 17:13:15.657857 139669263372288 ddar.py:60] Depth 9/1000 time = 61.181171894073486
I0123 17:14:18.342286 139669263372288 ddar.py:60] Depth 10/1000 time = 62.68378758430481
I0123 17:15:27.063061 139669263372288 ddar.py:60] Depth 11/1000 time = 68.50726366043091
I0123 17:16:38.459510 139669263372288 ddar.py:60] Depth 12/1000 time = 71.27066683769226
I0123 17:16:38.460191 139669263372288 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:16:38.460376 139669263372288 alphageometry.py:566] LM output (score=-2.543911): "t : D d q d t 32 ;"
I0123 17:16:38.460415 139669263372288 alphageometry.py:567] Translation: "t = on_circle t d q"

I0123 17:16:38.460478 139669263372288 alphageometry.py:576] Solving: "a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_circle t d q ? coll r s q"
I0123 17:16:38.460720 139669263372288 graph.py:498] 
I0123 17:16:38.460787 139669263372288 graph.py:499] a b c = triangle a b c; d = midpoint d b a; e = midpoint e a c; f = midpoint f c b; g = incenter g c b a; h = foot h g c a; i = foot i g c b; j = foot j g b a; k = foot k j d g; l = mirror l j k; m = foot m h e g; n = mirror n h m; o = foot o i f g; p = mirror p i o; q = on_line q d l, on_line q e f; r = on_line r e n, on_line r d f; s = on_line s f p, on_line s d e; t = on_circle t d q ? coll r s q
I0123 17:16:40.906744 139669263372288 ddar.py:60] Depth 1/1000 time = 2.3678455352783203
I0123 17:16:58.431401 139669263372288 ddar.py:60] Depth 2/1000 time = 17.52434206008911
I0123 17:17:41.849577 139669263372288 ddar.py:60] Depth 3/1000 time = 43.417667865753174
I0123 17:18:33.031886 139669263372288 ddar.py:60] Depth 4/1000 time = 51.18172264099121
I0123 17:19:23.977726 139669263372288 ddar.py:60] Depth 5/1000 time = 50.94516348838806
I0123 17:20:15.374378 139669263372288 ddar.py:60] Depth 6/1000 time = 51.395190715789795
I0123 17:21:06.466802 139669263372288 ddar.py:60] Depth 7/1000 time = 50.947813749313354
I0123 17:22:01.478080 139669263372288 ddar.py:60] Depth 8/1000 time = 55.010842084884644
I0123 17:22:55.899623 139669263372288 ddar.py:60] Depth 9/1000 time = 54.420918464660645
I0123 17:23:50.613434 139669263372288 ddar.py:60] Depth 10/1000 time = 54.713210344314575
I0123 17:24:49.921718 139669263372288 ddar.py:60] Depth 11/1000 time = 58.95120096206665
I0123 17:25:56.378272 139669263372288 ddar.py:60] Depth 12/1000 time = 66.45603346824646
I0123 17:27:04.258661 139669263372288 ddar.py:60] Depth 13/1000 time = 67.87990736961365
I0123 17:28:11.868988 139669263372288 ddar.py:60] Depth 14/1000 time = 67.60970091819763
I0123 17:29:21.979039 139669263372288 ddar.py:60] Depth 15/1000 time = 70.02136301994324
I0123 17:29:22.021284 139669263372288 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:29:22.021342 139669263372288 alphageometry.py:585] Timeout.
