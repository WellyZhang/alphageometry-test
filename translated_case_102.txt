I0123 14:50:01.905371 139947808051200 inference_utils.py:69] Parsing gin configuration.
I0123 14:50:01.905470 139947808051200 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 14:50:01.905688 139947808051200 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 14:50:01.905721 139947808051200 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 14:50:01.905750 139947808051200 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 14:50:01.905777 139947808051200 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 14:50:01.905804 139947808051200 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 14:50:01.905830 139947808051200 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 14:50:01.905855 139947808051200 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 14:50:01.905881 139947808051200 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 14:50:01.905907 139947808051200 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 14:50:01.905934 139947808051200 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 14:50:01.905982 139947808051200 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 14:50:01.906116 139947808051200 resource_reader.py:55] Path not found: base_htrans.gin
I0123 14:50:01.906325 139947808051200 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 14:50:01.906431 139947808051200 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 14:50:01.912995 139947808051200 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 14:50:01.913124 139947808051200 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 14:50:01.913463 139947808051200 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 14:50:01.913572 139947808051200 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 14:50:01.913872 139947808051200 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 14:50:01.913977 139947808051200 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 14:50:01.914400 139947808051200 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 14:50:01.914504 139947808051200 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 14:50:01.918272 139947808051200 training_loop.py:334] ==== Training loop: initializing model ====
I0123 14:50:02.030864 139947808051200 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 14:50:02.031591 139947808051200 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 14:50:02.038822 139947808051200 training_loop.py:335] Process 0 of 1
I0123 14:50:02.038879 139947808051200 training_loop.py:336] Local device count = 1
I0123 14:50:02.038919 139947808051200 training_loop.py:337] Number of replicas = 1
I0123 14:50:02.038951 139947808051200 training_loop.py:339] Using random number seed 42
I0123 14:50:02.505452 139947808051200 training_loop.py:359] Initializing the model.
I0123 14:50:02.878958 139947808051200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.879230 139947808051200 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 14:50:02.879332 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.879467 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.879551 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.879634 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.879707 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.879777 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.879845 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.879911 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.879978 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.880045 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.880111 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.880177 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:50:02.880216 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:02.880259 139947808051200 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:50:02.880373 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:02.880411 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:02.880441 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:02.882410 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.887913 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:02.898657 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.898939 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:02.903249 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:02.913781 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:02.913838 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:02.913875 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:02.913907 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.913971 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.915153 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.915231 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.915943 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.918387 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.924581 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.925839 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.925925 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:02.925961 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:02.926022 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.926153 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:02.926488 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:02.926533 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:02.928436 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.928536 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:02.931490 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.931575 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:02.932015 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:02.942063 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:02.951025 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.951123 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:02.951421 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.951502 139947808051200 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:50:02.951615 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:02.951653 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:02.951683 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:02.953624 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.956018 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:02.961613 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.961886 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:02.964496 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:02.968287 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:02.968343 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:02.968378 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:02.968408 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.968470 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.969038 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.969115 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.969479 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.970251 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.972704 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.973369 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.973448 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:02.973483 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:02.973540 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.973674 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:02.974001 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:02.974044 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:02.975927 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.976020 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:02.978498 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.978578 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:02.979066 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:02.981293 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:02.983163 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.983257 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:02.983552 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.983632 139947808051200 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:50:02.983741 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:02.983780 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:02.983809 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:02.985744 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.988097 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:02.994018 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:02.994279 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:02.996896 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.000709 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.000765 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.000801 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.000832 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.000894 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.001449 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.001524 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.001889 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.002655 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.005213 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.005849 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.005927 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.005963 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.006021 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.006153 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.006479 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.006523 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.008407 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.008501 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.011661 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.011806 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.012265 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.014588 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.016480 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.016577 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.016871 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.016954 139947808051200 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:50:03.017064 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.017103 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.017134 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.019068 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.021446 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.027009 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.027278 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.029979 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.033775 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.033834 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.033869 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.033900 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.033962 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.034536 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.034613 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.034971 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.035736 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.038257 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.038878 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.038958 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.038992 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.039051 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.039178 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.039505 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.039548 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.041406 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.041497 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.044124 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.044211 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.044650 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.047078 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.049012 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.049109 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.049404 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.049485 139947808051200 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:50:03.049596 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.049635 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.049673 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.051503 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.053860 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.059368 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.059626 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.062274 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.065996 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.066051 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.066086 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.066116 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.066184 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.067094 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.067171 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.067532 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.068303 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.070786 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.071398 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.071478 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.071512 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.071569 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.071704 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.072024 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.072066 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.073939 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.074031 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.076559 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.076638 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.077080 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.079366 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.081238 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.081333 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.081631 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.081718 139947808051200 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:50:03.081828 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.081867 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.081898 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.083741 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.086185 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.091794 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.092046 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.094700 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.098468 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.098523 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.098564 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.098595 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.098656 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.099210 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.099287 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.099642 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.100414 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.102883 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.103508 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.103585 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.103619 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.103681 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.103808 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.104129 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.104171 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.106106 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.106199 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.108685 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.108763 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.109200 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.111499 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.113384 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.113480 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.113778 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.113871 139947808051200 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:50:03.113981 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.114020 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.114052 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.115939 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.118306 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.123837 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.124095 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.126738 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.130497 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.130553 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.130589 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.130619 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.130681 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.131242 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.131320 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.131672 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.132436 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.134894 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.135568 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.135644 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.135678 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.135735 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.135861 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.136177 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.136219 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.138103 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.138196 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.140638 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.140715 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.141501 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.143752 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.145769 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.145871 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.146163 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.146244 139947808051200 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:50:03.146353 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.146392 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.146423 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.285379 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.288238 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.294047 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.294350 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.297104 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.300971 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.301029 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.301066 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.301098 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.301161 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.301782 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.301859 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.302220 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.303005 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.305568 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.306207 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.306286 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.306321 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.306380 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.306509 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.306838 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.306881 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.308757 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.308851 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.311435 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.311515 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.311954 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.314275 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.316219 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.316323 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.316620 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.316703 139947808051200 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:50:03.316814 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.316853 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.316884 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.318754 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.321178 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.326693 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.326952 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.329607 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.333323 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.333377 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.333417 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.333449 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.333510 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.334121 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.334198 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.334555 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.335323 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.337780 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.338392 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.338467 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.338501 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.338560 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.338693 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.339010 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.339053 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.340913 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.341008 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.343547 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.343626 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.344052 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.346369 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.348255 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.348352 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.348642 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.348728 139947808051200 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:50:03.348838 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.348877 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.348907 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.350740 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.353152 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.359357 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.359619 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.362239 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.366011 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.366067 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.366102 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.366133 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.366200 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.366762 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.366838 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.367187 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.367951 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.370426 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.371044 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.371120 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.371153 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.371210 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.371337 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.371654 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.371695 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.373621 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.373723 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.376223 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.376302 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.376739 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.379040 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.380921 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.381019 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.381306 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.381392 139947808051200 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:50:03.381503 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.381542 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.381574 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.383490 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.385869 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.391376 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.391643 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.394252 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.398008 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.398064 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.398100 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.398132 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.398198 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.398762 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.398838 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.399189 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.399956 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.402431 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.403093 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.403171 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.403204 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.403263 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.403395 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.403718 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.403761 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.405666 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.405764 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.408455 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.408535 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.409023 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.411282 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.413208 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.413301 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.413594 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.413682 139947808051200 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:50:03.413800 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.413840 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.413872 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.415793 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.418144 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.423692 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.423952 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.426589 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:50:03.430376 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.430430 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.430465 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.430496 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.430560 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.431124 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.431201 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.431557 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.432328 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.435205 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.435828 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.435906 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.435941 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.436000 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.436136 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.436460 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.436502 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.438391 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.438484 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.441009 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.441092 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.441526 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.443836 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.445708 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.445804 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.446100 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.446382 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446451 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446516 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446574 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446627 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446679 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446731 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446781 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446832 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446883 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446934 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.446985 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:50:03.447022 139947808051200 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:50:03.450499 139947808051200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:50:03.497948 139947808051200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.498034 139947808051200 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:50:03.498089 139947808051200 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:50:03.498194 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.498233 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.498264 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.498326 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.500741 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.506177 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.506436 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.509051 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.525624 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.525688 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.525723 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.525753 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.525814 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.526925 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.527004 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.527697 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.529666 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.534389 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.535689 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.535777 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.535813 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.535872 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.536006 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.536117 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.536156 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.538033 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.538128 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.540593 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.540673 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.540782 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.543036 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.545034 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.545129 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.545419 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.545500 139947808051200 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:50:03.545608 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.545652 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.545686 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.545750 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.548000 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.553426 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.553690 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.556327 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.569382 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.569438 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.569473 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.569503 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.569566 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.570132 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.570209 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.570574 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.571262 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.573773 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.574390 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.574467 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.574507 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.574565 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.574696 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.574804 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.574842 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.576749 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.576842 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.579241 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.579321 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.579431 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.581631 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.583532 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.583628 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.583915 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.583997 139947808051200 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:50:03.584107 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.584146 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.584177 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.584239 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.586475 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.591840 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.592097 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.594753 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.607479 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.607535 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.607570 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.607601 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.607664 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.608219 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.608295 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.608657 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.609346 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.611819 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.612446 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.612523 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.612559 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.612624 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.612752 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.612860 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.612897 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.614826 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.614920 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.617322 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.617402 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.617509 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.619702 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.621605 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.621706 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.621993 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.622074 139947808051200 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:50:03.622182 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.622221 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.622252 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.622315 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.624529 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.629902 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.630158 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.632812 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.645581 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.645638 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.645682 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.645713 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.645774 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.646327 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.646402 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.646758 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.647443 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.649919 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.650538 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.650618 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.650653 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.650711 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.650847 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.650957 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.650996 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.652894 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.652987 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.655377 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.655456 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.655563 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.657770 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.659590 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.659684 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.659967 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.660047 139947808051200 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:50:03.660156 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.660195 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.660226 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.660289 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.662858 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.668274 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.668536 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.674844 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.687913 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.687973 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.688009 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.688042 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.688119 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.688730 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.688812 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.689176 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.689885 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.692456 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.693093 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.693171 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.693206 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.693267 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.693406 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.693522 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.693562 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.695512 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.695606 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.698064 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.698143 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.698258 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.700566 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.702477 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.702572 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.702860 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.702941 139947808051200 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:50:03.703050 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.703092 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.703124 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.703188 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.705446 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.710888 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.711145 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.713848 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.726651 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.726706 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.726741 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.726771 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.726836 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.727401 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.727477 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.727833 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.728534 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.731042 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.731670 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.731747 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.731781 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.731838 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.731969 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.732085 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.732125 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.734071 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.734165 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.736549 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.736628 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.736735 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.739055 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.740920 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.741014 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.741297 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.741377 139947808051200 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:50:03.741486 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.741524 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.741554 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.741616 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.743875 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.749383 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.749638 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.752249 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.764994 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.765049 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.765083 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.765113 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.765179 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.765750 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.765826 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.766184 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.766874 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.769349 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.770339 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.770418 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.770452 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.770510 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.770643 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.770754 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.770796 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.772675 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.772769 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.775159 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.775241 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.775348 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.777606 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.779512 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.779606 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.779889 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.779970 139947808051200 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:50:03.780078 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.780117 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.780147 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.780209 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.782444 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.787838 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.788108 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.790793 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.803620 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.803676 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.803711 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.803741 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.803803 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.804412 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.804487 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.804838 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.805516 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.807986 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.808609 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.808686 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.808720 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.808779 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.808908 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.809017 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.809060 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.810932 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.811027 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.813472 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.813552 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.813667 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.815869 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.817729 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.817826 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.818113 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.818195 139947808051200 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:50:03.818305 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.818343 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.818374 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.818437 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.820659 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.826141 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.826402 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.829017 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.841747 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.841803 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.841839 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.841871 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.841934 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.842502 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.842579 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.842940 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.843636 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.846172 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.846849 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.846927 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.846962 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.847021 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.847155 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.847266 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.847306 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.849188 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.849282 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.851686 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.851770 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.851877 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.854101 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.856015 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.856109 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.856393 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.856474 139947808051200 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:50:03.856583 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.856621 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.856653 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.856716 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.858946 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.864493 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.864751 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.867528 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.880737 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.880795 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.880831 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.880861 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.880927 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.881533 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.881608 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.881976 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.882768 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.885357 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.885997 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.886076 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.886111 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.886170 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.886303 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.886415 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.886453 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.888319 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.888420 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.890950 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.891031 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.891139 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.893354 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.895205 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.895300 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.895585 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.895665 139947808051200 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:50:03.895772 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.895810 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.895841 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.895903 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.898135 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.903576 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.903829 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.906449 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.920038 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.920096 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.920131 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.920161 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.920226 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.920799 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.920877 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.921245 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.921975 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.924589 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.925277 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.925356 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.925391 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.925450 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.925585 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.925709 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.925752 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.927667 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.927770 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.930267 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.930352 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.930467 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.932770 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.934777 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.934876 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.935172 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.935256 139947808051200 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:50:03.935369 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:03.935409 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:03.935440 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:03.935503 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.937729 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:03.943109 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.943372 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:03.946039 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:03.958662 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:03.958719 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:03.958754 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:03.958784 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.958845 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.959401 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.959476 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.959824 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.960552 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.963062 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.963716 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.963793 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:03.963827 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:03.963884 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.964013 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:03.964121 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:03.964159 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.966046 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.966144 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.968572 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.968652 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:03.968759 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:03.971026 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:03.972848 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.972942 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:03.973225 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:03.973314 139947808051200 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:50:03.976175 139947808051200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:50:04.031390 139947808051200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.031475 139947808051200 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:50:04.031528 139947808051200 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:50:04.031633 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.031671 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.031702 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.031764 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.034424 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.039735 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.039994 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.042544 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.055017 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.055074 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.055109 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.055139 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.055201 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.055754 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.055830 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.056183 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.056852 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.059342 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.059950 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.060026 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.060060 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.060119 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.060250 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.060367 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.060406 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.062234 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.062328 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.064682 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.064761 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.064869 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.067107 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.068932 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.069027 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.069312 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.069392 139947808051200 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:50:04.069497 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.069536 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.069566 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.069628 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.071837 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.077165 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.077424 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.080044 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.092315 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.092371 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.092406 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.092437 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.092500 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.093054 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.093130 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.093479 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.094166 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.096650 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.097259 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.097335 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.097370 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.097428 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.097554 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.097669 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.097715 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.099551 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.099646 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.102026 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.102106 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.102213 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.104439 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.106262 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.106357 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.106643 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.106724 139947808051200 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:50:04.106831 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.106870 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.106900 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.106962 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.109150 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.114446 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.114703 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.117328 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.129659 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.129715 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.129751 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.129783 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.129847 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.130400 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.130476 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.130828 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.131500 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.134000 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.134614 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.134691 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.134727 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.134786 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.134915 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.135024 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.135063 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.136890 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.136984 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.139344 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.139424 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.139531 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.142216 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.144048 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.144142 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.144428 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.144510 139947808051200 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:50:04.144618 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.144658 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.144688 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.144749 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.147003 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.152335 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.152591 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.155239 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.167701 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.167757 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.167799 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.167840 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.167903 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.168468 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.168542 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.168898 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.169578 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.172099 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.172713 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.172789 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.172822 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.172879 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.173006 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.173111 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.173150 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.175004 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.175096 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.177464 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.177541 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.177651 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.179904 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.181738 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.181833 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.182115 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.182195 139947808051200 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:50:04.182300 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.182337 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.182366 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.182427 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.184623 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.189919 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.190173 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.192814 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.205319 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.205373 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.205407 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.205436 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.205497 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.206063 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.206140 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.206497 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.207172 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.209702 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.210324 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.210398 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.210431 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.210487 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.210621 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.210727 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.210763 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.212603 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.212701 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.215187 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.215265 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.215374 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.217611 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.219456 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.219549 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.219833 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.219913 139947808051200 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:50:04.220018 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.220056 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.220086 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.220147 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.222367 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.227722 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.227976 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.230651 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.243132 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.243186 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.243220 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.243249 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.243309 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.243867 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.243941 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.244297 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.244976 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.247485 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.248094 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.248167 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.248200 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.248255 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.248381 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.248486 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.248522 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.250352 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.250449 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.252813 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.252890 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.252995 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.255721 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.257550 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.257649 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.257936 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.258015 139947808051200 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:50:04.258121 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.258158 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.258187 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.258247 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.260445 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.265814 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.266066 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.268708 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.281165 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.281218 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.281252 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.281281 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.281342 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.281912 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.281987 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.282344 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.283024 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.285528 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.286159 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.286234 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.286267 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.286322 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.286447 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.286552 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.286590 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.288435 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.288526 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.290918 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.291000 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.291105 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.293363 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.295211 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.295304 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.295588 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.295669 139947808051200 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:50:04.295777 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.295814 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.295843 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.295905 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.298112 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.303456 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.303714 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.306377 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.318887 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.318940 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.318974 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.319003 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.319063 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.319616 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.319690 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.320038 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.320706 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.323199 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.323817 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.323891 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.323924 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.323981 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.324106 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.324210 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.324246 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.326085 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.326176 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.328503 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.328587 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.328694 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.330969 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.332813 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.332906 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.333186 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.333266 139947808051200 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:50:04.333372 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.333409 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.333438 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.333499 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.335712 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.341043 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.341297 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.343938 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.356393 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.356447 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.356481 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.356509 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.356569 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.357121 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.357196 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.357548 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.358234 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.360751 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.361363 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.361437 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.361470 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.361526 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.361655 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.361762 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.361798 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.363633 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.363723 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.366065 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.366147 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.366254 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.368887 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.370712 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.370806 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.371088 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.371167 139947808051200 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:50:04.371273 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.371309 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.371337 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.371397 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.373603 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.378930 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.379181 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.381826 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.394390 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.394444 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.394478 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.394507 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.394568 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.395126 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.395200 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.395548 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.396227 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.398733 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.399352 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.399427 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.399459 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.399513 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.399641 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.399746 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.399782 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.402092 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.402186 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.404513 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.404590 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.404698 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.406920 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.408723 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.408815 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.409096 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.409174 139947808051200 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:50:04.409279 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.409316 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.409345 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.409405 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.411589 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.416859 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.417115 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.419750 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.432159 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.432213 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.432246 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.432275 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.432336 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.432895 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.432969 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.433324 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.434009 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.436525 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.437144 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.437220 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.437254 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.437310 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.437440 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.437546 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.437583 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.439418 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.439510 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.441881 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.441960 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.442070 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.444322 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.446152 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.446245 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.446527 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.446606 139947808051200 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:50:04.446711 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:50:04.446748 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:50:04.446778 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:50:04.446836 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.449023 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:50:04.454330 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.454586 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:50:04.457215 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:50:04.469690 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:50:04.469743 139947808051200 attention.py:418] Single window, no scan.
I0123 14:50:04.469777 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:50:04.469806 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.469867 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.470425 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.470499 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.470853 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.471532 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.474046 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.474665 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.474740 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:50:04.474773 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:50:04.474828 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.474952 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:50:04.475062 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:50:04.475099 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.476937 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.477028 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.479385 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.479463 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:50:04.479572 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:50:04.482197 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:50:04.484044 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.484136 139947808051200 nn_components.py:261] mlp: residual
I0123 14:50:04.484419 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:04.484503 139947808051200 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:50:04.487324 139947808051200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:50:08.894791 139947808051200 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 14:50:09.406814 139947808051200 training_loop.py:409] No working directory specified.
I0123 14:50:09.406940 139947808051200 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 14:50:09.407691 139947808051200 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 14:50:12.324671 139947808051200 training_loop.py:447] Only restoring trainable parameters.
I0123 14:50:12.325375 139947808051200 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 14:50:12.325432 139947808051200 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.325477 139947808051200 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.325518 139947808051200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.325558 139947808051200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.325599 139947808051200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.325639 139947808051200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.325686 139947808051200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.325726 139947808051200 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.325764 139947808051200 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.325802 139947808051200 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.325840 139947808051200 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.325877 139947808051200 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.325914 139947808051200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.325951 139947808051200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.325987 139947808051200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.326024 139947808051200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326061 139947808051200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326099 139947808051200 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.326136 139947808051200 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.326184 139947808051200 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326222 139947808051200 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.326258 139947808051200 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.326294 139947808051200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.326329 139947808051200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326365 139947808051200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.326400 139947808051200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326435 139947808051200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326470 139947808051200 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.326506 139947808051200 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.326541 139947808051200 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326577 139947808051200 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.326612 139947808051200 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.326647 139947808051200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.326683 139947808051200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326718 139947808051200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.326753 139947808051200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326788 139947808051200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326824 139947808051200 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.326859 139947808051200 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.326894 139947808051200 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.326931 139947808051200 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.326967 139947808051200 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.327002 139947808051200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.327038 139947808051200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327072 139947808051200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.327112 139947808051200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327149 139947808051200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327184 139947808051200 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.327219 139947808051200 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.327255 139947808051200 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327291 139947808051200 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.327326 139947808051200 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.327361 139947808051200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.327396 139947808051200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327430 139947808051200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.327466 139947808051200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327501 139947808051200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327536 139947808051200 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.327572 139947808051200 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.327607 139947808051200 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327642 139947808051200 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.327677 139947808051200 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.327712 139947808051200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.327747 139947808051200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327781 139947808051200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.327816 139947808051200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327851 139947808051200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327886 139947808051200 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.327920 139947808051200 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.327955 139947808051200 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.327991 139947808051200 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.328026 139947808051200 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.328066 139947808051200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.328103 139947808051200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328138 139947808051200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.328173 139947808051200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328208 139947808051200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328243 139947808051200 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.328277 139947808051200 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.328312 139947808051200 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328347 139947808051200 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.328382 139947808051200 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.328417 139947808051200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.328451 139947808051200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328485 139947808051200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.328521 139947808051200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328555 139947808051200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328590 139947808051200 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.328626 139947808051200 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.328661 139947808051200 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328697 139947808051200 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.328732 139947808051200 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.328768 139947808051200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.328803 139947808051200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328839 139947808051200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.328875 139947808051200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328909 139947808051200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.328945 139947808051200 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.328980 139947808051200 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.329021 139947808051200 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.329058 139947808051200 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.329093 139947808051200 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.329130 139947808051200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.329167 139947808051200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.329203 139947808051200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.329239 139947808051200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.329274 139947808051200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.329309 139947808051200 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.329344 139947808051200 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.329380 139947808051200 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.329414 139947808051200 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.329449 139947808051200 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:50:12.329484 139947808051200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:50:12.329518 139947808051200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.329553 139947808051200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.329590 139947808051200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.329625 139947808051200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.329675 139947808051200 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:50:12.329712 139947808051200 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:50:12.329749 139947808051200 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:50:12.329783 139947808051200 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:50:12.329811 139947808051200 training_loop.py:725] Total parameters: 152072288
I0123 14:50:12.330021 139947808051200 training_loop.py:739] Total state size: 0
I0123 14:50:12.351369 139947808051200 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 14:50:12.351632 139947808051200 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 14:50:12.351992 139947808051200 training_loop.py:652] Compiling mode beam_search with jit.
I0123 14:50:12.352310 139947808051200 training_loop.py:89] registering functions: dict_keys([])
I0123 14:50:12.368817 139947808051200 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i ? coll m d n
I0123 14:50:14.519715 139947808051200 ddar.py:60] Depth 1/1000 time = 2.0956428050994873
I0123 14:50:18.288856 139947808051200 ddar.py:60] Depth 2/1000 time = 3.7689690589904785
I0123 14:50:25.219011 139947808051200 ddar.py:60] Depth 3/1000 time = 6.929947853088379
I0123 14:50:35.153905 139947808051200 ddar.py:60] Depth 4/1000 time = 9.934577226638794
I0123 14:50:50.240921 139947808051200 ddar.py:60] Depth 5/1000 time = 15.086652755737305
I0123 14:51:06.698675 139947808051200 ddar.py:60] Depth 6/1000 time = 16.45737886428833
I0123 14:51:23.936649 139947808051200 ddar.py:60] Depth 7/1000 time = 17.237707376480103
I0123 14:51:47.285226 139947808051200 ddar.py:60] Depth 8/1000 time = 23.348283052444458
I0123 14:52:10.613469 139947808051200 ddar.py:60] Depth 9/1000 time = 23.32792830467224
I0123 14:52:33.176320 139947808051200 ddar.py:60] Depth 10/1000 time = 22.56173062324524
I0123 14:52:56.870022 139947808051200 ddar.py:60] Depth 11/1000 time = 23.569982767105103
I0123 14:53:20.856184 139947808051200 ddar.py:60] Depth 12/1000 time = 23.95072913169861
I0123 14:53:44.285518 139947808051200 ddar.py:60] Depth 13/1000 time = 23.349892616271973
I0123 14:53:44.286022 139947808051200 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:53:44.286129 139947808051200 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 14:53:44.286166 139947808051200 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b d e 02 T b d c e 03 ; f : C c e f 04 D c e e f 05 ; g : C b f g 06 D b g f g 07 ; h : C d g h 08 T b d b h 09 ; i : C a f i 10 D a i f i 11 ; j : C d i j 12 T a d a j 13 ; k : C a b k 14 T a b h k 15 ; l : C a b l 16 T a b j l 17 ; m : C k l m 18 D k m l m 19 ; n : C g k n 20 C i l n 21 ? C m d n {F1} x00
I0123 14:53:44.286199 139947808051200 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b d e 02 T b d c e 03 ; f : C c e f 04 D c e e f 05 ; g : C b f g 06 D b g f g 07 ; h : C d g h 08 T b d b h 09 ; i : C a f i 10 D a i f i 11 ; j : C d i j 12 T a d a j 13 ; k : C a b k 14 T a b h k 15 ; l : C a b l 16 T a b j l 17 ; m : C k l m 18 D k m l m 19 ; n : C g k n 20 C i l n 21 ? C m d n {F1} x00
I0123 14:53:44.431489 139947808051200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.431671 139947808051200 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 14:53:44.431771 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.431845 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.431914 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.431981 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.432046 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.432113 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.432179 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.432244 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.432309 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.432374 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.432440 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.432517 139947808051200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:53:44.432560 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.432608 139947808051200 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:53:44.432717 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.432755 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.432785 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.434731 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.437204 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.442856 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.443124 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.445779 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.449631 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.449694 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.449728 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.449759 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.449821 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.450499 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.450573 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.450934 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.451705 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.454186 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.454800 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.454875 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.454907 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.454967 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.455093 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.455410 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.455453 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.457396 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.457487 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.459907 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.459985 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.460408 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.462732 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.464648 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.464741 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.465027 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.465111 139947808051200 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:53:44.465218 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.465255 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.465285 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.467538 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.469847 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.475324 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.475577 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.478134 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.481716 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.481769 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.481801 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.481830 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.481891 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.482440 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.482514 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.482862 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.483607 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.486015 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.486670 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.486746 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.486779 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.486835 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.486961 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.487270 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.487310 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.489202 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.489293 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.491708 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.491786 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.492201 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.494482 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.496356 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.496448 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.496736 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.496821 139947808051200 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:53:44.496929 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.496967 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.496996 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.498780 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.501049 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.506589 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.506841 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.509360 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.512983 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.513036 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.513070 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.513100 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.513160 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.513764 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.513838 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.514187 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.514935 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.517338 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.517951 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.518027 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.518060 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.518116 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.518243 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.518551 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.518591 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.520515 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.520605 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.523097 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.523175 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.523592 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.525796 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.527829 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.527921 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.528206 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.528285 139947808051200 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:53:44.528397 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.528435 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.528463 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.530302 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.532567 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.538044 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.538296 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.540807 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.544472 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.544526 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.544560 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.544589 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.544650 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.545200 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.545275 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.545626 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.546389 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.548768 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.549369 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.549443 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.549476 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.549531 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.549664 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.550031 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.550071 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.551932 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.552024 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.554413 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.554491 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.554907 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.557111 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.559062 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.559156 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.559444 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.559523 139947808051200 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:53:44.559628 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.559672 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.559702 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.561478 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.563823 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.569470 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.569732 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.572245 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.575832 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.575886 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.575919 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.575948 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.576009 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.576931 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.577007 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.577359 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.578115 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.580512 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.581112 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.581186 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.581219 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.581274 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.581398 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.581717 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.581759 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.583686 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.583777 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.586195 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.586275 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.586699 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.588927 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.590819 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.590914 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.591201 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.591280 139947808051200 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:53:44.591385 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.591422 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.591456 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.593303 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.595577 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.601092 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.601342 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.603859 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.607646 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.607699 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.607732 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.607761 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.607820 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.608362 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.608436 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.608786 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.609671 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.612061 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.612666 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.612740 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.612773 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.612828 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.612970 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.613327 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.613368 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.615250 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.615341 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.617751 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.617827 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.618247 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.620440 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.622371 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.622463 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.622748 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.622827 139947808051200 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:53:44.622931 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.622968 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.622997 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.624758 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.627074 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.632641 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.632894 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.635402 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.638974 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.639028 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.639061 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.639090 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.639151 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.639754 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.639828 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.640181 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.640926 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.643342 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.643955 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.644030 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.644063 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.644119 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.644245 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.644559 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.644599 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.646538 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.646630 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.649040 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.649117 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.649532 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.651760 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.653664 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.653757 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.654040 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.654119 139947808051200 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:53:44.654224 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.654260 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.654289 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.656124 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.658403 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.663911 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.664160 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.666735 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.670321 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.670373 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.670406 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.670436 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.670497 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.671048 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.671123 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.671472 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.672214 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.674612 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.675275 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.675350 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.675383 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.675438 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.675564 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.675876 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.675915 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.677789 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.677881 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.680280 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.680358 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.680777 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.683396 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.685289 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.685381 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.685676 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.685757 139947808051200 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:53:44.685863 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.685900 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.685929 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.687682 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.689950 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.695463 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.695711 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.698223 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.701813 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.701866 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.701899 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.701927 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.701987 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.702589 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.702664 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.703015 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.703763 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.706181 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.706789 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.706865 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.706897 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.706951 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.707076 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.707381 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.707421 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.709467 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.709558 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.712055 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.712133 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.712553 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.714982 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.716861 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.716953 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.717240 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.717320 139947808051200 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:53:44.717430 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.717466 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.717495 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.719261 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.721619 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.727126 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.727385 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.729931 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.733515 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.733568 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.733601 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.733630 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.733700 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.734303 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.734378 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.734728 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.735484 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.737915 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.738523 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.738597 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.738629 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.738685 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.738810 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.739118 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.739158 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.741013 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.741103 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.743570 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.743649 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.744067 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.746278 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.748155 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.748247 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.748534 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.748612 139947808051200 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:53:44.748717 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.748754 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.748782 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.750542 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.752909 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.758378 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.758637 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.761151 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.764726 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.764779 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.764811 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.764840 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.764901 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.765495 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.765568 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.765926 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.766672 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.769074 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.769686 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.769762 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.769794 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.769850 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.769975 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.770284 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.770324 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.772187 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.772276 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.774742 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.774821 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.775239 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.777438 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.779324 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.779417 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.779705 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.779783 139947808051200 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:53:44.779889 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.779927 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.779956 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.781708 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.784079 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.789577 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.789842 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.792382 139947808051200 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:53:44.795974 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.796028 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.796061 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.796090 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.796150 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.796758 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.796832 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.797182 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.797934 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.800334 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.800939 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.801014 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.801047 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.801102 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.801228 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.801537 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.801576 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.803429 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.803520 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.806388 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.806467 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.806889 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.809100 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.810986 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.811080 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.811367 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.811608 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.811674 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.811729 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.811781 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.811832 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.811883 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.811933 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.811982 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.812041 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.812093 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.812142 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.812191 139947808051200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:53:44.812226 139947808051200 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:53:44.815226 139947808051200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:53:44.859304 139947808051200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.859390 139947808051200 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:53:44.859443 139947808051200 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:53:44.859548 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.859585 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.859615 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.859678 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.862049 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.867460 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.867723 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.870284 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:44.883668 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.883722 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.883754 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.883783 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.883844 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.884402 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.884477 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.884833 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.885668 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.888245 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.888856 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.888930 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.888962 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.889018 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.889145 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.889250 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.889286 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.891149 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.891251 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.893618 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.893703 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.893809 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.896021 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.897845 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.897937 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.898226 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.898306 139947808051200 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:53:44.898411 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.898448 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.898477 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.898536 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.900733 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.906042 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.906298 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.908918 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:44.921125 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.921179 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.921212 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.921241 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.921302 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.921854 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.921928 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.922292 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.923041 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.925470 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.926091 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.926166 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.926199 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.926254 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.926381 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.926487 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.926523 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.928337 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.928427 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.930829 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.930911 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.931020 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.933255 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.935089 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.935182 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.935470 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.935555 139947808051200 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:53:44.935665 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.935702 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.935730 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.935790 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.938020 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.943320 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.943576 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.946192 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:44.958523 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.958576 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.958609 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.958638 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.958699 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.959251 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.959326 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.959677 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.960389 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.962801 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.963404 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.963479 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:44.963512 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:44.963567 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.963693 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:44.963798 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:44.963834 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.965630 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.965726 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.968093 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.968175 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:44.968283 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:44.970504 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:44.972321 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.972412 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:44.972697 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.972776 139947808051200 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:53:44.972882 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:44.972919 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:44.972947 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:44.973006 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.975205 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:44.980587 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.980853 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:44.983925 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:44.996396 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:44.996449 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:44.996483 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:44.996512 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.996572 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.997122 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.997197 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.997544 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:44.998273 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.000680 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.001289 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.001365 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.001399 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.001456 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.001584 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.001697 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.001735 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.003568 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.003660 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.006042 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.006126 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.006235 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.008460 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.010287 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.010379 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.010664 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.010743 139947808051200 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:53:45.010848 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.010885 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.010913 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.010971 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.013145 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.018425 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.018679 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.021287 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.033557 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.033611 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.033652 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.033684 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.033746 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.034292 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.034366 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.034716 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.035436 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.037872 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.038483 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.038558 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.038590 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.038646 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.038773 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.038879 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.038915 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.040720 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.040811 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.043181 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.043265 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.043374 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.045595 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.047413 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.047506 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.047791 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.047870 139947808051200 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:53:45.047977 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.048014 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.048043 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.048102 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.050289 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.055548 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.055801 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.058419 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.070763 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.070816 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.070849 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.070877 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.070936 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.071478 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.071550 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.071896 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.072616 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.075031 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.075633 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.075707 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.075740 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.075796 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.075922 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.076026 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.076062 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.077877 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.077969 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.080318 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.080394 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.080506 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.082697 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.084500 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.084592 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.084877 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.084956 139947808051200 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:53:45.085059 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.085096 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.085124 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.085181 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.087348 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.092615 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.092866 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.095870 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.108064 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.108117 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.108151 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.108181 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.108243 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.108796 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.108871 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.109225 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.109955 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.112377 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.112982 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.113057 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.113090 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.113145 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.113271 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.113377 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.113413 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.115226 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.115317 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.117697 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.117775 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.117881 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.120095 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.121926 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.122020 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.122305 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.122384 139947808051200 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:53:45.122489 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.122526 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.122554 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.122613 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.124810 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.130075 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.130330 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.132923 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.145146 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.145199 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.145232 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.145260 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.145320 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.145876 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.145951 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.146302 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.147017 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.149423 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.150037 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.150113 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.150145 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.150201 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.150326 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.150432 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.150468 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.152270 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.152360 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.154703 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.154781 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.154888 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.157086 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.158914 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.159006 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.159291 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.159370 139947808051200 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:53:45.159475 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.159511 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.159540 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.159599 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.161955 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.167256 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.167511 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.170133 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.182427 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.182480 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.182513 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.182541 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.182601 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.183153 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.183227 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.183579 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.184249 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.186726 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.187328 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.187402 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.187434 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.187489 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.187616 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.187721 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.187757 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.189564 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.189661 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.192161 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.192239 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.192344 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.194576 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.196394 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.196494 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.196783 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.196862 139947808051200 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:53:45.196968 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.197005 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.197034 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.197093 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.199289 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.204602 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.204859 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.207897 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.220171 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.220226 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.220259 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.220288 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.220349 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.220896 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.220970 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.221324 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.222005 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.224466 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.225078 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.225154 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.225186 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.225242 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.225369 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.225477 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.225513 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.227333 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.227424 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.229787 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.229866 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.229972 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.232193 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.234014 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.234113 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.234405 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.234485 139947808051200 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:53:45.234592 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.234630 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.234660 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.234720 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.236914 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.242266 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.242523 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.245314 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.257651 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.257704 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.257738 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.257766 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.257828 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.258374 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.258449 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.258800 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.259473 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.261987 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.262591 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.262665 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.262697 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.262752 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.262877 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.262984 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.263020 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.264823 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.264914 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.267283 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.267361 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.267465 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.269667 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.271470 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.271560 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.271851 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.271929 139947808051200 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:53:45.272034 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.272071 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.272100 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.272161 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.274334 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.279564 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.279819 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.282431 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.294710 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.294764 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.294796 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.294825 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.294885 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.295432 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.295505 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.295854 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.296526 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.298998 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.299599 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.299674 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.299706 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.299762 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.299887 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.299992 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.300027 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.301839 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.301931 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.304275 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.304352 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.304456 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.306664 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.308466 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.308559 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.308845 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.308935 139947808051200 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:53:45.311725 139947808051200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:53:45.361917 139947808051200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.362002 139947808051200 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:53:45.362054 139947808051200 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:53:45.362156 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.362193 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.362221 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.362280 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.364540 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.369908 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.370166 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.372709 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.385451 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.385504 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.385537 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.385566 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.385626 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.386195 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.386273 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.386636 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.387324 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.389754 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.390358 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.390434 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.390467 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.390523 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.390648 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.390754 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.390790 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.392652 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.392743 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.395107 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.395186 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.395292 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.397458 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.399279 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.399373 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.399663 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.399741 139947808051200 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:53:45.399846 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.399882 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.399911 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.399971 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.402175 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.407577 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.407836 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.410392 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.422665 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.422719 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.422752 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.422780 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.422841 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.423384 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.423459 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.423809 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.424475 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.426877 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.427481 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.427556 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.427589 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.427647 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.427774 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.427879 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.427916 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.429795 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.429888 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.432249 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.432325 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.432432 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.434610 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.436425 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.436517 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.436808 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.436888 139947808051200 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:53:45.436994 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.437031 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.437059 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.437118 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.439316 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.444693 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.444948 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.447747 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.460584 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.460637 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.460670 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.460699 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.460759 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.461311 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.461385 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.461746 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.462427 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.464848 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.465460 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.465535 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.465569 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.465625 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.465762 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.465868 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.465904 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.467774 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.467866 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.470238 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.470317 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.470425 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.472571 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.474383 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.474475 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.474760 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.474839 139947808051200 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:53:45.474944 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.474981 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.475010 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.475069 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.477230 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.482600 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.482858 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.485381 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.498165 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.498219 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.498251 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.498281 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.498341 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.498888 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.498961 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.499308 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.499971 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.502388 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.502997 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.503072 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.503105 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.503162 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.503289 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.503395 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.503432 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.505299 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.505390 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.507765 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.507843 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.507950 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.510094 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.511892 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.511991 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.512279 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.512360 139947808051200 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:53:45.512464 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.512500 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.512529 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.512589 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.514779 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.520144 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.520399 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.522978 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.535256 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.535309 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.535343 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.535372 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.535432 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.535979 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.536053 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.536404 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.537069 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.539483 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.540086 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.540161 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.540194 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.540250 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.540374 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.540478 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.540514 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.542388 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.542480 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.544815 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.544892 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.544999 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.547157 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.548965 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.549063 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.549349 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.549428 139947808051200 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:53:45.549534 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.549571 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.549599 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.549666 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.551980 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.557488 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.557755 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.560291 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.572571 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.572626 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.572660 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.572689 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.572751 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.573301 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.573375 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.573732 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.574395 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.576803 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.577409 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.577484 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.577517 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.577575 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.577709 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.577816 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.577852 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.579737 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.579829 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.582182 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.582260 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.582366 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.584491 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.586304 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.586397 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.586691 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.586770 139947808051200 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:53:45.586878 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.586914 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.586944 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.587004 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.589168 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.594577 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.594832 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.597355 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.610106 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.610160 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.610193 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.610221 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.610282 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.610827 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.610901 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.611250 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.611916 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.614329 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.614929 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.615003 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.615035 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.615090 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.615214 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.615319 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.615355 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.617216 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.617306 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.619665 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.619741 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.619848 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.621987 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.623787 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.623879 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.624166 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.624251 139947808051200 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:53:45.624357 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.624394 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.624422 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.624482 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.626678 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.632026 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.632280 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.634799 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.647041 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.647095 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.647128 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.647157 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.647218 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.647768 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.647843 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.648198 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.648870 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.651262 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.651860 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.652079 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.652112 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.652167 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.652291 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.652397 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.652433 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.654311 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.654402 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.656875 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.656951 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.657059 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.659216 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.661011 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.661101 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.661386 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.661469 139947808051200 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:53:45.661576 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.661613 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.661651 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.661715 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.663909 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.669264 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.669519 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.672063 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.684314 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.684367 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.684399 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.684427 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.684486 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.685028 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.685100 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.685443 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.686110 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.688485 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.689090 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.689164 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.689197 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.689253 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.689379 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.689484 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.689521 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.691406 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.691497 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.693858 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.693936 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.694041 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.696165 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.697958 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.698050 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.698334 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.698412 139947808051200 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:53:45.698521 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.698557 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.698586 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.698646 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.700831 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.706175 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.706429 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.708942 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.721621 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.721679 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.721713 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.721742 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.721800 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.722341 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.722414 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.722757 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.723415 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.725792 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.726389 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.726461 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.726493 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.726547 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.726670 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.726774 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.726811 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.728699 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.728787 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.731160 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.731237 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.731342 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.733501 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.735310 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.735401 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.735687 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.735764 139947808051200 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:53:45.735868 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.735910 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.735939 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.735999 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.738206 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.743632 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.743888 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.746469 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.759013 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.759068 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.759101 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.759129 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.759189 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.759742 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.759815 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.760166 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.760834 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.763261 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.763865 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.763939 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.763971 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.764028 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.764152 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.764258 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.764294 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.766172 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.766263 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.768761 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.768837 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.768942 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.771087 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.772878 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.772969 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.773252 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.773330 139947808051200 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:53:45.773434 139947808051200 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:53:45.773470 139947808051200 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:53:45.773504 139947808051200 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:53:45.773564 139947808051200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.775736 139947808051200 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:53:45.781077 139947808051200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.781332 139947808051200 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:53:45.783872 139947808051200 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:53:45.796177 139947808051200 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:53:45.796230 139947808051200 attention.py:418] Single window, no scan.
I0123 14:53:45.796263 139947808051200 transformer_layer.py:389] tlayer: self-attention.
I0123 14:53:45.796293 139947808051200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.796353 139947808051200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.796900 139947808051200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.796973 139947808051200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.797326 139947808051200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.798001 139947808051200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.800425 139947808051200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.801032 139947808051200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.801105 139947808051200 transformer_layer.py:468] tlayer: End windows.
I0123 14:53:45.801139 139947808051200 transformer_layer.py:472] tlayer: final FFN.
I0123 14:53:45.801196 139947808051200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.801322 139947808051200 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:53:45.801427 139947808051200 nn_components.py:325] mlp: activation = None
I0123 14:53:45.801463 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.803347 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.803438 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.805812 139947808051200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.805888 139947808051200 transformer_base.py:443] tbase: final FFN
I0123 14:53:45.805994 139947808051200 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:53:45.808139 139947808051200 nn_components.py:329] mlp: final activation = None
I0123 14:53:45.809955 139947808051200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.810047 139947808051200 nn_components.py:261] mlp: residual
I0123 14:53:45.810332 139947808051200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:45.810414 139947808051200 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:53:45.813190 139947808051200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:53:58.569909 139947808051200 alphageometry.py:566] LM output (score=-0.728906): "o : C b c o 22 D b o c o 23 ;"
I0123 14:53:58.570074 139947808051200 alphageometry.py:567] Translation: "o = on_line o b c, on_bline o c b"

I0123 14:53:58.570121 139947808051200 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o b c, on_bline o c b ? coll m d n"
I0123 14:53:58.570357 139947808051200 graph.py:498] 
I0123 14:53:58.570415 139947808051200 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o b c, on_bline o c b ? coll m d n
I0123 14:54:00.809533 139947808051200 ddar.py:60] Depth 1/1000 time = 2.1760058403015137
I0123 14:54:06.859730 139947808051200 ddar.py:60] Depth 2/1000 time = 6.050026893615723
I0123 14:54:18.698211 139947808051200 ddar.py:60] Depth 3/1000 time = 11.838305711746216
I0123 14:54:36.043705 139947808051200 ddar.py:60] Depth 4/1000 time = 17.345289945602417
I0123 14:54:59.824329 139947808051200 ddar.py:60] Depth 5/1000 time = 23.78037118911743
I0123 14:55:25.973721 139947808051200 ddar.py:60] Depth 6/1000 time = 26.14913249015808
I0123 14:55:52.794043 139947808051200 ddar.py:60] Depth 7/1000 time = 26.820045471191406
I0123 14:56:27.519077 139947808051200 ddar.py:60] Depth 8/1000 time = 34.7246310710907
I0123 14:57:02.078274 139947808051200 ddar.py:60] Depth 9/1000 time = 34.558732748031616
I0123 14:57:35.776560 139947808051200 ddar.py:60] Depth 10/1000 time = 33.696975231170654
I0123 14:58:11.273093 139947808051200 ddar.py:60] Depth 11/1000 time = 35.28741431236267
I0123 14:58:46.633836 139947808051200 ddar.py:60] Depth 12/1000 time = 35.321826696395874
I0123 14:59:21.698408 139947808051200 ddar.py:60] Depth 13/1000 time = 34.96761727333069
I0123 14:59:21.699064 139947808051200 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:59:21.699200 139947808051200 alphageometry.py:566] LM output (score=-0.817999): "o : C g l o 22 D g o l o 23 ;"
I0123 14:59:21.699237 139947808051200 alphageometry.py:567] Translation: "o = on_line o g l, on_bline o l g"

I0123 14:59:21.699288 139947808051200 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o g l, on_bline o l g ? coll m d n"
I0123 14:59:21.699499 139947808051200 graph.py:498] 
I0123 14:59:21.699565 139947808051200 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o g l, on_bline o l g ? coll m d n
I0123 14:59:24.153161 139947808051200 ddar.py:60] Depth 1/1000 time = 2.3929483890533447
I0123 14:59:29.107038 139947808051200 ddar.py:60] Depth 2/1000 time = 4.953711032867432
I0123 14:59:36.761552 139947808051200 ddar.py:60] Depth 3/1000 time = 7.654333114624023
I0123 14:59:48.050828 139947808051200 ddar.py:60] Depth 4/1000 time = 11.289024829864502
I0123 15:00:04.860483 139947808051200 ddar.py:60] Depth 5/1000 time = 16.809314250946045
I0123 15:00:24.186464 139947808051200 ddar.py:60] Depth 6/1000 time = 19.325544834136963
I0123 15:00:43.162635 139947808051200 ddar.py:60] Depth 7/1000 time = 18.97575330734253
I0123 15:01:09.313214 139947808051200 ddar.py:60] Depth 8/1000 time = 26.150225400924683
I0123 15:01:35.740584 139947808051200 ddar.py:60] Depth 9/1000 time = 26.42665410041809
I0123 15:02:01.729465 139947808051200 ddar.py:60] Depth 10/1000 time = 25.98723578453064
I0123 15:02:27.724173 139947808051200 ddar.py:60] Depth 11/1000 time = 25.8520405292511
I0123 15:02:54.512986 139947808051200 ddar.py:60] Depth 12/1000 time = 26.7428081035614
I0123 15:03:21.289394 139947808051200 ddar.py:60] Depth 13/1000 time = 26.689329862594604
I0123 15:03:21.289987 139947808051200 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:03:21.290129 139947808051200 alphageometry.py:566] LM output (score=-1.343205): "o : C f k o 22 D f o k o 23 ;"
I0123 15:03:21.290165 139947808051200 alphageometry.py:567] Translation: "o = on_line o f k, on_bline o k f"

I0123 15:03:21.290221 139947808051200 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o f k, on_bline o k f ? coll m d n"
I0123 15:03:21.290426 139947808051200 graph.py:498] 
I0123 15:03:21.290486 139947808051200 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o f k, on_bline o k f ? coll m d n
I0123 15:03:23.899943 139947808051200 ddar.py:60] Depth 1/1000 time = 2.548607349395752
I0123 15:03:29.800477 139947808051200 ddar.py:60] Depth 2/1000 time = 5.900363922119141
I0123 15:03:39.234552 139947808051200 ddar.py:60] Depth 3/1000 time = 9.433849334716797
I0123 15:03:52.973783 139947808051200 ddar.py:60] Depth 4/1000 time = 13.73897385597229
I0123 15:04:12.390466 139947808051200 ddar.py:60] Depth 5/1000 time = 19.416410446166992
I0123 15:04:33.417330 139947808051200 ddar.py:60] Depth 6/1000 time = 21.026565551757812
I0123 15:04:55.289777 139947808051200 ddar.py:60] Depth 7/1000 time = 21.87215280532837
I0123 15:05:23.387693 139947808051200 ddar.py:60] Depth 8/1000 time = 28.09755849838257
I0123 15:05:51.812908 139947808051200 ddar.py:60] Depth 9/1000 time = 28.424789905548096
I0123 15:06:19.958049 139947808051200 ddar.py:60] Depth 10/1000 time = 28.144835948944092
I0123 15:06:49.927769 139947808051200 ddar.py:60] Depth 11/1000 time = 29.969332456588745
I0123 15:07:19.213338 139947808051200 ddar.py:60] Depth 12/1000 time = 29.28506064414978
I0123 15:07:48.242871 139947808051200 ddar.py:60] Depth 13/1000 time = 29.029066562652588
I0123 15:08:17.853235 139947808051200 ddar.py:60] Depth 14/1000 time = 29.60900115966797
I0123 15:08:48.507228 139947808051200 ddar.py:60] Depth 15/1000 time = 30.46926498413086
I0123 15:09:19.752008 139947808051200 ddar.py:60] Depth 16/1000 time = 31.191349267959595
I0123 15:09:49.634704 139947808051200 ddar.py:60] Depth 17/1000 time = 29.781631231307983
I0123 15:10:19.804128 139947808051200 ddar.py:60] Depth 18/1000 time = 30.168978691101074
I0123 15:10:50.694025 139947808051200 ddar.py:60] Depth 19/1000 time = 30.88950824737549
I0123 15:11:21.772110 139947808051200 ddar.py:60] Depth 20/1000 time = 31.077582836151123
I0123 15:11:53.203184 139947808051200 ddar.py:60] Depth 21/1000 time = 31.430594444274902
I0123 15:11:53.239604 139947808051200 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:11:53.239716 139947808051200 alphageometry.py:566] LM output (score=-1.563483): "o : C g i o 22 D g i i o 23 ;"
I0123 15:11:53.239753 139947808051200 alphageometry.py:567] Translation: "o = on_line o g i, on_circle o i g"

I0123 15:11:53.239799 139947808051200 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o g i, on_circle o i g ? coll m d n"
I0123 15:11:53.239984 139947808051200 graph.py:498] 
I0123 15:11:53.240047 139947808051200 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o g i, on_circle o i g ? coll m d n
I0123 15:11:55.976064 139947808051200 ddar.py:60] Depth 1/1000 time = 2.6728644371032715
I0123 15:12:02.107067 139947808051200 ddar.py:60] Depth 2/1000 time = 6.130830764770508
I0123 15:12:12.461171 139947808051200 ddar.py:60] Depth 3/1000 time = 10.353863716125488
I0123 15:12:25.527861 139947808051200 ddar.py:60] Depth 4/1000 time = 13.066385984420776
I0123 15:12:45.176591 139947808051200 ddar.py:60] Depth 5/1000 time = 19.648466110229492
I0123 15:13:06.473986 139947808051200 ddar.py:60] Depth 6/1000 time = 21.297115325927734
I0123 15:13:28.085322 139947808051200 ddar.py:60] Depth 7/1000 time = 21.61102032661438
I0123 15:13:57.659021 139947808051200 ddar.py:60] Depth 8/1000 time = 29.573318243026733
I0123 15:14:26.559183 139947808051200 ddar.py:60] Depth 9/1000 time = 28.899643421173096
I0123 15:14:54.500824 139947808051200 ddar.py:60] Depth 10/1000 time = 27.940229892730713
I0123 15:15:23.810055 139947808051200 ddar.py:60] Depth 11/1000 time = 29.167462825775146
I0123 15:15:53.518556 139947808051200 ddar.py:60] Depth 12/1000 time = 29.669307231903076
I0123 15:16:23.269838 139947808051200 ddar.py:60] Depth 13/1000 time = 29.67423415184021
I0123 15:16:23.270268 139947808051200 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:16:23.270375 139947808051200 alphageometry.py:566] LM output (score=-1.773628): "o : C a c o 22 D a o c o 23 ;"
I0123 15:16:23.270413 139947808051200 alphageometry.py:567] Translation: "o = on_line o a c, on_bline o c a"

I0123 15:16:23.270457 139947808051200 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o a c, on_bline o c a ? coll m d n"
I0123 15:16:23.270641 139947808051200 graph.py:498] 
I0123 15:16:23.270706 139947808051200 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o a c, on_bline o c a ? coll m d n
I0123 15:16:24.812761 139947808051200 ddar.py:60] Depth 1/1000 time = 1.4816772937774658
I0123 15:16:30.968806 139947808051200 ddar.py:60] Depth 2/1000 time = 6.155843734741211
I0123 15:16:42.852554 139947808051200 ddar.py:60] Depth 3/1000 time = 11.883521795272827
I0123 15:16:58.922880 139947808051200 ddar.py:60] Depth 4/1000 time = 16.070050716400146
I0123 15:17:20.767269 139947808051200 ddar.py:60] Depth 5/1000 time = 21.84401297569275
I0123 15:17:44.940275 139947808051200 ddar.py:60] Depth 6/1000 time = 24.17256236076355
I0123 15:18:09.713078 139947808051200 ddar.py:60] Depth 7/1000 time = 24.772469758987427
I0123 15:18:41.898749 139947808051200 ddar.py:60] Depth 8/1000 time = 32.185298204422
I0123 15:19:14.053129 139947808051200 ddar.py:60] Depth 9/1000 time = 32.15385103225708
I0123 15:19:47.370792 139947808051200 ddar.py:60] Depth 10/1000 time = 33.31605625152588
I0123 15:20:20.327205 139947808051200 ddar.py:60] Depth 11/1000 time = 32.7661235332489
I0123 15:20:52.247716 139947808051200 ddar.py:60] Depth 12/1000 time = 31.882015466690063
I0123 15:21:25.627027 139947808051200 ddar.py:60] Depth 13/1000 time = 33.287841796875
I0123 15:21:25.627504 139947808051200 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:21:25.627627 139947808051200 alphageometry.py:566] LM output (score=-1.851184): "o : C i k o 22 D i o k o 23 ;"
I0123 15:21:25.627665 139947808051200 alphageometry.py:567] Translation: "o = on_line o i k, on_bline o k i"

I0123 15:21:25.627714 139947808051200 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o i k, on_bline o k i ? coll m d n"
I0123 15:21:25.627908 139947808051200 graph.py:498] 
I0123 15:21:25.627968 139947808051200 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o i k, on_bline o k i ? coll m d n
I0123 15:21:27.837281 139947808051200 ddar.py:60] Depth 1/1000 time = 2.149414300918579
I0123 15:21:33.069088 139947808051200 ddar.py:60] Depth 2/1000 time = 5.231617450714111
I0123 15:21:41.072826 139947808051200 ddar.py:60] Depth 3/1000 time = 8.003555297851562
I0123 15:21:52.043502 139947808051200 ddar.py:60] Depth 4/1000 time = 10.970399141311646
I0123 15:22:08.729144 139947808051200 ddar.py:60] Depth 5/1000 time = 16.685291290283203
I0123 15:22:28.257645 139947808051200 ddar.py:60] Depth 6/1000 time = 19.528228759765625
I0123 15:22:47.312496 139947808051200 ddar.py:60] Depth 7/1000 time = 19.054550409317017
I0123 15:23:14.162235 139947808051200 ddar.py:60] Depth 8/1000 time = 26.849425554275513
I0123 15:23:40.740209 139947808051200 ddar.py:60] Depth 9/1000 time = 26.577604055404663
I0123 15:24:06.667066 139947808051200 ddar.py:60] Depth 10/1000 time = 25.925390481948853
I0123 15:24:33.611503 139947808051200 ddar.py:60] Depth 11/1000 time = 26.799644708633423
I0123 15:25:01.121785 139947808051200 ddar.py:60] Depth 12/1000 time = 27.467080116271973
I0123 15:25:28.250128 139947808051200 ddar.py:60] Depth 13/1000 time = 27.042905807495117
I0123 15:25:28.250432 139947808051200 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:25:28.250525 139947808051200 alphageometry.py:566] LM output (score=-1.920783): "o : C g i o 22 D g i g o 23 ;"
I0123 15:25:28.250562 139947808051200 alphageometry.py:567] Translation: "o = on_line o g i, on_circle o g i"

I0123 15:25:28.250605 139947808051200 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o g i, on_circle o g i ? coll m d n"
I0123 15:25:28.250784 139947808051200 graph.py:498] 
I0123 15:25:28.250848 139947808051200 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o g i, on_circle o g i ? coll m d n
I0123 15:25:29.814754 139947808051200 ddar.py:60] Depth 1/1000 time = 1.502692699432373
I0123 15:25:36.027889 139947808051200 ddar.py:60] Depth 2/1000 time = 6.212916612625122
I0123 15:25:46.281410 139947808051200 ddar.py:60] Depth 3/1000 time = 10.253334760665894
I0123 15:25:59.413390 139947808051200 ddar.py:60] Depth 4/1000 time = 13.131762504577637
I0123 15:26:18.701247 139947808051200 ddar.py:60] Depth 5/1000 time = 19.28758430480957
I0123 15:26:40.069284 139947808051200 ddar.py:60] Depth 6/1000 time = 21.36772322654724
I0123 15:27:00.897142 139947808051200 ddar.py:60] Depth 7/1000 time = 20.827513217926025
I0123 15:27:29.213948 139947808051200 ddar.py:60] Depth 8/1000 time = 28.316394567489624
I0123 15:27:57.010649 139947808051200 ddar.py:60] Depth 9/1000 time = 27.79633665084839
I0123 15:28:25.295915 139947808051200 ddar.py:60] Depth 10/1000 time = 28.283892154693604
I0123 15:28:54.005766 139947808051200 ddar.py:60] Depth 11/1000 time = 28.568871021270752
I0123 15:29:23.300428 139947808051200 ddar.py:60] Depth 12/1000 time = 29.256386041641235
I0123 15:29:52.442336 139947808051200 ddar.py:60] Depth 13/1000 time = 29.06694531440735
I0123 15:29:52.442789 139947808051200 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:29:52.442923 139947808051200 alphageometry.py:566] LM output (score=-2.004032): "o : C g d o 22 D g d d o 23 ;"
I0123 15:29:52.442960 139947808051200 alphageometry.py:567] Translation: "o = on_line o g d, on_circle o d g"

I0123 15:29:52.443017 139947808051200 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o g d, on_circle o d g ? coll m d n"
I0123 15:29:52.443217 139947808051200 graph.py:498] 
I0123 15:29:52.443277 139947808051200 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b d; f = mirror f c e; g = midpoint g f b; h = lc_tangent h b d, on_line h g d; i = midpoint i f a; j = lc_tangent j a d, on_line j i d; k = foot k h b a; l = foot l j b a; m = midpoint m k l; n = on_line n k g, on_line n l i; o = on_line o g d, on_circle o d g ? coll m d n
I0123 15:29:55.714609 139947808051200 ddar.py:60] Depth 1/1000 time = 3.205333709716797
I0123 15:30:00.742686 139947808051200 ddar.py:60] Depth 2/1000 time = 5.027914524078369
I0123 15:30:10.033720 139947808051200 ddar.py:60] Depth 3/1000 time = 9.290788650512695
I0123 15:30:23.932397 139947808051200 ddar.py:60] Depth 4/1000 time = 13.898356914520264
I0123 15:30:43.239480 139947808051200 ddar.py:60] Depth 5/1000 time = 19.30680751800537
I0123 15:31:05.463577 139947808051200 ddar.py:60] Depth 6/1000 time = 22.223787307739258
I0123 15:31:28.840174 139947808051200 ddar.py:60] Depth 7/1000 time = 23.37630343437195
I0123 15:32:00.215839 139947808051200 ddar.py:60] Depth 8/1000 time = 31.375381469726562
I0123 15:32:00.269514 139947808051200 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L M N : Points
DA = DB [00]
DB = DC [01]
B,D,E are collinear [02]
CE ⟂ BD [03]
F,C,E are collinear [04]
EC = EF [05]
F,G,B are collinear [06]
GF = GB [07]
G,D,H are collinear [08]
BD ⟂ BH [09]
F,I,A are collinear [10]
IF = IA [11]
D,J,I are collinear [12]
AJ ⟂ AD [13]
B,K,A are collinear [14]
HK ⟂ AB [15]
B,L,A are collinear [16]
BA ⟂ LJ [17]
K,L,M are collinear [18]
MK = ML [19]
N,L,I are collinear [20]
N,G,K are collinear [21]

 * Auxiliary Constructions:
O : Points
G,D,O are collinear [22]
DO = DG [23]

 * Proof steps:
001. F,G,B are collinear [06] & GF = GB [07] ⇒  G is midpoint of FB [24]
002. M,K,L are collinear [18] & MK = ML [19] ⇒  M is midpoint of KL [25]
003. G is midpoint of FB [24] & M is midpoint of KL [25] ⇒  GF:MK = FB:KL [26]
004. F,C,E are collinear [04] & EC = EF [05] ⇒  E is midpoint of FC [27]
005. B,D,E are collinear [02] & F,C,E are collinear [04] & BD ⟂ CE [03] ⇒  BE ⟂ FC [28]
006. E is midpoint of FC [27] & BE ⟂ FC [28] ⇒  BF = BC [29]
007. GF:MK = FB:KL [26] & GF = GB [07] & MK = ML [19] & BF = BC [29] ⇒  GB:LM = BC:KL [30]
008. F,I,A are collinear [10] & IF = IA [11] ⇒  I is midpoint of FA [31]
009. G is midpoint of FB [24] & I is midpoint of FA [31] ⇒  GI ∥ BA [32]
010. GI ∥ AB [32] & F,G,B are collinear [06] & F,I,A are collinear [10] ⇒  FB:BA = FG:GI [33]
011. GI ∥ AB [32] & F,G,B are collinear [06] & F,I,A are collinear [10] ⇒  FA:AB = FI:IG [34]
012. FB:BA = FG:GI [33] & BF = BC [29] & GF = GB [07] ⇒  BC:BA = GB:GI [35]
013. GB:LM = BC:KL [30] & BC:BA = GB:GI [35] ⇒  LM:KL = GI:BA [36]
014. G,D,O are collinear [22] & DO = DG [23] ⇒  D is midpoint of GO [37]
015. D is midpoint of GO [37] & I is midpoint of FA [31] ⇒  DG:IF = GO:FA [38]
016. GO:FA = GD:FI [38] & FI:GI = FA:BA [34] ⇒  GI:GD = BA:GO [39]
017. D,B,E are collinear [02] & F,C,E are collinear [04] & BD ⟂ CE [03] ⇒  DE ⟂ FC [40]
018. E is midpoint of FC [27] & DE ⟂ FC [40] ⇒  DF = DC [41]
019. DF = DC [41] & DA = DB [00] & DB = DC [01] ⇒  DA = DF [42]
020. DA = DF [42] & IF = IA [11] ⇒  AF ⟂ DI [43]
021. D,J,I are collinear [12] & F,A,I are collinear [10] & AF ⟂ DI [43] & AJ ⟂ AD [13] ⇒  ∠JAD = ∠(DJ-FI) [44]
022. DA = DB [00] & DF = DC [41] & DB = DC [01] ⇒  D is the circumcenter of \Delta BAF [45]
023. D is the circumcenter of \Delta BAF [45] & G is midpoint of FB [24] ⇒  ∠ABD = ∠(AF-DG) [46]
024. DA = DB [00] ⇒  ∠ABD = ∠DAB [47]
025. B,L,A are collinear [16] & B,K,A are collinear [14] & F,A,I are collinear [10] & ∠ABD = ∠(AF-DG) [46] & ∠ABD = ∠DAB [47] ⇒  ∠(DA-KL) = ∠(FI-GD) [48]
026. ∠JAD = ∠(DJ-FI) [44] & ∠(DA-KL) = ∠(FI-GD) [48] ⇒  ∠(JA-KL) = ∠JDG [49]
027. J,D,I are collinear [12] & F,A,I are collinear [10] & B,L,A are collinear [16] & HK ⟂ AB [15] & AF ⟂ DI [43] & BA ⟂ LJ [17] ⇒  ∠JIA = ∠JLA [50]
028. ∠JIA = ∠JLA [50] ⇒  I,J,L,A are concyclic [51]
029. I,J,L,A are concyclic [51] ⇒  ∠IJA = ∠ILA [52]
030. N,L,I are collinear [20] & B,L,A are collinear [16] & B,K,A are collinear [14] & ∠(JA-KL) = ∠JDG [49] & D,J,I are collinear [12] & ∠IJA = ∠ILA [52] & AB ∥ GI [32] ⇒  ∠NLK = ∠DGI [53]
031. B,L,A are collinear [16] & B,K,A are collinear [14] & F,A,I are collinear [10] & D,J,I are collinear [12] & HK ⟂ AB [15] & AF ⟂ DI [43] & BA ⟂ LJ [17] ⇒  ∠JLK = ∠(FI-DJ) [54]
032. F,C,E are collinear [04] & BD ⟂ CE [03] ⇒  BD ⟂ FC [55]
033. B,L,A are collinear [16] & B,K,A are collinear [14] & JL ⟂ AB [17] ⇒  JL ⟂ KL [56]
034. BD ⟂ FC [55] & JL ⟂ KL [56] ⇒  ∠(BD-KL) = ∠(FC-JL) [57]
035. F,C,E are collinear [04] & F,A,I are collinear [10] & ∠ABD = ∠(AF-DG) [46] & ∠(BD-KL) = ∠(FC-JL) [57] & B,L,A are collinear [16] & B,K,A are collinear [14] & BA ⟂ LJ [17] & HK ⟂ AB [15] ⇒  ∠(JL-FC) = ∠(FI-GD) [58]
036. ∠JLK = ∠(FI-DJ) [54] & ∠(JL-FC) = ∠(FI-GD) [58] ⇒  ∠(KL-FC) = ∠JDG [59]
037. DF = DC [41] & DB = DC [01] ⇒  DB = DF [60]
038. DB = DF [60] & GF = GB [07] ⇒  BF ⟂ DG [61]
039. B,K,A are collinear [14] & F,G,B are collinear [06] & G,D,H are collinear [08] & BF ⟂ DG [61] & HK ⟂ AB [15] ⇒  ∠BKH = ∠BGH [62]
040. ∠BKH = ∠BGH [62] ⇒  G,B,K,H are concyclic [63]
041. G,B,K,H are concyclic [63] ⇒  ∠GKB = ∠GHB [64]
042. G,B,K,H are concyclic [63] ⇒  ∠GBH = ∠GKH [65]
043. N,G,K are collinear [21] & B,L,A are collinear [16] & B,K,A are collinear [14] & ∠(KL-FC) = ∠JDG [59] & F,C,E are collinear [04] & D,J,I are collinear [12] & ∠GKB = ∠GHB [64] & G,D,H are collinear [08] & BD ⟂ BH [09] & CE ⟂ BD [03] & AB ∥ GI [32] ⇒  ∠NKL = ∠DIG [66]
044. ∠NLK = ∠DGI [53] & ∠NKL = ∠DIG [66] (Similar Triangles)⇒  KL:NL = GI:GD [67]
045. GI:GD = BA:GO [39] & KL:NL = GI:GD [67] ⇒  KL:NL = BA:GO [68]
046. LM:KL = GI:BA [36] & KL:NL = BA:GO [68] ⇒  LM:NL = GI:GO [69]
047. K,L,M are collinear [18] & B,L,A are collinear [16] & B,K,A are collinear [14] & N,L,I are collinear [20] & G,D,O are collinear [22] & ∠(JA-KL) = ∠JDG [49] & D,J,I are collinear [12] & ∠IJA = ∠ILA [52] & AB ∥ GI [32] ⇒  ∠MLN = ∠IGO [70]
048. LM:NL = GI:GO [69] & ∠MLN = ∠IGO [70] (Similar Triangles)⇒  ∠LMN = ∠GIO [71]
049. N,L,I are collinear [20] & ∠(JA-KL) = ∠JDG [49] & B,L,A are collinear [16] & B,K,A are collinear [14] & D,J,I are collinear [12] & ∠IJA = ∠ILA [52] & AB ∥ GI [32] ⇒  ∠DGI = ∠NIG [72]
050. N,G,K are collinear [21] & ∠(KL-FC) = ∠JDG [59] & B,L,A are collinear [16] & B,K,A are collinear [14] & F,C,E are collinear [04] & D,J,I are collinear [12] & ∠GKB = ∠GHB [64] & G,D,H are collinear [08] & BD ⟂ BH [09] & CE ⟂ BD [03] & AB ∥ GI [32] ⇒  ∠DIG = ∠NGI [73]
051. ∠DGI = ∠NIG [72] & ∠DIG = ∠NGI [73] (Similar Triangles)⇒  ID = GN [74]
052. F,C,E are collinear [04] & B,L,A are collinear [16] & B,K,A are collinear [14] & ∠(BD-KL) = ∠(FC-JL) [57] ⇒  ∠(BD-FC) = ∠KLJ [75]
053. DB = DF [60] ⇒  ∠DBF = ∠BFD [76]
054. D is the circumcenter of \Delta BAF [45] & I is midpoint of FA [31] ⇒  ∠BFD = ∠(AB-DI) [77]
055. B,L,A are collinear [16] & B,K,A are collinear [14] & D,J,I are collinear [12] & F,G,B are collinear [06] & ∠DBF = ∠BFD [76] & ∠BFD = ∠(AB-DI) [77] ⇒  ∠(KL-DJ) = ∠(BD-FG) [78]
056. ∠(BD-FC) = ∠KLJ [75] & ∠(KL-DJ) = ∠(BD-FG) [78] ⇒  ∠LJD = ∠CFG [79]
057. N,G,K are collinear [21] & F,G,B are collinear [06] & D,J,I are collinear [12] & ∠LJD = ∠CFG [79] & F,C,E are collinear [04] & BA ⟂ LJ [17] & HK ⟂ AB [15] & ∠GBH = ∠GKH [65] & BD ⟂ BH [09] & CE ⟂ BD [03] ⇒  ∠NGF = ∠(DJ-FG) [80]
058. ∠NGF = ∠(DJ-FG) [80] ⇒  NG ∥ DJ [81]
059. N,G,K are collinear [21] & G,D,O are collinear [22] & NG ∥ DJ [81] & D,J,I are collinear [12] ⇒  ∠DGN = ∠ODI [82]
060. DO = DG [23] & ID = GN [74] & ∠DGN = ∠ODI [82] (SAS)⇒  ∠GDN = ∠DOI [83]
061. ∠LMN = ∠GIO [71] & K,L,M are collinear [18] & B,L,A are collinear [16] & B,K,A are collinear [14] & GI ∥ AB [32] & ∠GDN = ∠DOI [83] & G,D,O are collinear [22] ⇒  ND ∥ NM [84]
062. ND ∥ NM [84] ⇒  N,D,M are collinear
==========================

I0123 15:32:00.269727 139947808051200 alphageometry.py:582] Solved.
