I0123 19:06:19.565384 140706200571904 inference_utils.py:69] Parsing gin configuration.
I0123 19:06:19.565479 140706200571904 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 19:06:19.565682 140706200571904 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 19:06:19.565717 140706200571904 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 19:06:19.565746 140706200571904 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 19:06:19.565775 140706200571904 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 19:06:19.565802 140706200571904 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 19:06:19.565829 140706200571904 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 19:06:19.565855 140706200571904 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 19:06:19.565882 140706200571904 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 19:06:19.565910 140706200571904 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 19:06:19.565936 140706200571904 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 19:06:19.565983 140706200571904 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 19:06:19.566112 140706200571904 resource_reader.py:55] Path not found: base_htrans.gin
I0123 19:06:19.566310 140706200571904 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 19:06:19.566405 140706200571904 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 19:06:19.572630 140706200571904 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 19:06:19.572748 140706200571904 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 19:06:19.573076 140706200571904 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 19:06:19.573179 140706200571904 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 19:06:19.573459 140706200571904 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 19:06:19.573559 140706200571904 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 19:06:19.573980 140706200571904 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 19:06:19.574083 140706200571904 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 19:06:19.577756 140706200571904 training_loop.py:334] ==== Training loop: initializing model ====
I0123 19:06:19.677208 140706200571904 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 19:06:19.677933 140706200571904 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 19:06:19.684692 140706200571904 training_loop.py:335] Process 0 of 1
I0123 19:06:19.684745 140706200571904 training_loop.py:336] Local device count = 1
I0123 19:06:19.684783 140706200571904 training_loop.py:337] Number of replicas = 1
I0123 19:06:19.684813 140706200571904 training_loop.py:339] Using random number seed 42
I0123 19:06:20.160911 140706200571904 training_loop.py:359] Initializing the model.
I0123 19:06:20.532588 140706200571904 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.532829 140706200571904 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 19:06:20.532932 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533009 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533084 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533166 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533238 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533307 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533375 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533442 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533508 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533579 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533656 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533727 140706200571904 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:06:20.533765 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.533810 140706200571904 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:06:20.533922 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.533961 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.533991 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:20.535992 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.541201 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:20.551626 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.551902 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:20.556118 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:20.566650 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:20.566709 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.566749 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:20.566784 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.566851 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.568028 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.568105 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.568804 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.571237 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.576840 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.578547 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.578631 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:20.578666 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:20.578728 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.578857 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:20.579205 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:20.579252 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.581115 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.581215 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.584025 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.584104 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:20.584601 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:20.594480 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.603102 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.603202 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.603493 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.603575 140706200571904 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:06:20.603687 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.603726 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.603758 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:20.605596 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.608025 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:20.613440 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.613708 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:20.616249 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:20.619973 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:20.620029 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.620065 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:20.620095 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.620155 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.620708 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.620785 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.621132 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.621883 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.624263 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.624874 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.624949 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:20.624982 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:20.625039 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.625163 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:20.625480 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:20.625524 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.627466 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.627560 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.629988 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.630069 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:20.630493 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:20.632734 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.634576 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.634675 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.634954 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.635033 140706200571904 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:06:20.635141 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.635179 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.635210 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:20.637077 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.639376 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:20.645166 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.645425 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:20.647981 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:20.651750 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:20.651805 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.651841 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:20.651872 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.651934 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.652492 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.652567 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.652916 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.653669 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.656100 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.656758 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.656833 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:20.656869 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:20.656926 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.657051 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:20.657369 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:20.657411 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.659278 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.659370 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.661822 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.661905 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:20.662386 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:20.664591 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.666475 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.666570 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.666854 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.666933 140706200571904 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:06:20.667041 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.667079 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.667108 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:20.668987 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.671307 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:20.677558 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.677887 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:20.680470 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:20.684255 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:20.684313 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.684348 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:20.684378 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.684445 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.685010 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.685084 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.685437 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.686208 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.688691 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.689314 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.689390 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:20.689425 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:20.689485 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.689614 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:20.689943 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:20.689985 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.691852 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.691952 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.694456 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.694539 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:20.694966 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:20.697180 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.699045 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.699142 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.699424 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.699505 140706200571904 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:06:20.699613 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.699651 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.699682 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:20.701588 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.703914 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:20.709429 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.709698 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:20.712314 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:20.715950 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:20.716004 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.716039 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:20.716071 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.716131 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.716702 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.716776 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.717121 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.717875 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.720649 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.721265 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.721343 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:20.721377 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:20.721435 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.721570 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:20.721895 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:20.721938 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.723785 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.723876 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.726365 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.726447 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:20.726873 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:20.729100 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.731018 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.731115 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.731393 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.731473 140706200571904 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:06:20.731581 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.731619 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.731650 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:20.733473 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.735968 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:20.741469 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.741734 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:20.744342 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:20.748007 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:20.748062 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.748098 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:20.748128 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.748188 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.748781 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.748859 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.749212 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.749985 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.752382 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.752996 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.753071 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:20.753106 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:20.753165 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.753293 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:20.753613 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:20.753662 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.755516 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.755607 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.758107 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.758184 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:20.758610 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:20.760882 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.762766 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.762861 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.763148 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.763227 140706200571904 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:06:20.763335 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.763373 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.763404 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:20.765230 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.767617 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:20.773070 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.773330 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:20.775875 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:20.779599 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:20.779654 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.779690 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:20.779721 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.779783 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.780344 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.780419 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.780765 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.781522 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.783924 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.784539 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.784614 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:20.784648 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:20.784705 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.784831 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:20.785150 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:20.785192 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.787131 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.787226 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.789674 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.789753 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:20.790179 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:20.792753 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.794605 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.794705 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.794989 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.795069 140706200571904 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:06:20.795176 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.795214 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.795245 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:20.934770 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.937833 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:20.943724 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.944025 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:20.946723 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:20.950698 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:20.950756 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.950794 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:20.950827 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.950893 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.951500 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.951577 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.951947 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.952740 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.955310 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.955947 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.956024 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:20.956061 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:20.956124 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.956254 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:20.956594 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:20.956639 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.958580 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.958675 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.961244 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.961328 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:20.961778 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:20.964107 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.965988 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.966096 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.966388 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.966470 140706200571904 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:06:20.966580 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.966619 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.966649 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:20.968596 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.970953 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:20.976546 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.976814 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:20.979475 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:20.983214 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:20.983269 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:20.983306 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:20.983337 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.983399 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.983957 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.984036 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.984401 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.985157 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.987667 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.988290 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.988367 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:20.988402 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:20.988460 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.988586 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:20.988914 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:20.988957 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.990835 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.990929 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.993442 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.993519 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:20.993957 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:20.996199 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:20.998170 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.998266 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:20.998552 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:20.998639 140706200571904 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:06:20.998749 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:20.998788 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:20.998819 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.000643 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.003090 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.008756 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.009024 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.012005 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:21.015686 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.015741 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.015777 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.015808 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.015869 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.016461 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.016536 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.016886 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.017663 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.020267 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.020878 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.020954 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.020988 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.021045 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.021171 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.021493 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.021536 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.023416 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.023510 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.026021 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.026100 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.026535 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.028828 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.030707 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.030802 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.031090 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.031177 140706200571904 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:06:21.031288 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.031327 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.031357 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.033182 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.035625 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.041177 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.041440 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.044029 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:21.047797 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.047852 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.047888 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.047919 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.047981 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.048540 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.048615 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.048960 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.049723 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.052148 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.052769 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.052844 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.052879 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.052937 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.053063 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.053390 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.053433 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.055373 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.055467 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.058201 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.058280 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.058903 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.061362 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.063255 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.063351 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.063639 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.063720 140706200571904 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:06:21.063838 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.063878 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.063909 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.065903 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.068253 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.073818 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.074085 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.076666 140706200571904 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:06:21.080431 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.080487 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.080522 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.080554 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.080615 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.081168 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.081241 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.081592 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.082353 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.084779 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.085774 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.085854 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.085890 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.085950 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.086079 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.086396 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.086439 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.088320 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.088412 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.090872 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.090952 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.091426 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.093648 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.095528 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.095623 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.095907 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.096179 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096247 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096313 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096368 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096420 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096473 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096524 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096578 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096631 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096682 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096732 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096783 140706200571904 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:06:21.096820 140706200571904 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:06:21.100294 140706200571904 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:06:21.147636 140706200571904 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.147722 140706200571904 decoder_stack.py:333] dstack: autoregressive generator.
I0123 19:06:21.147776 140706200571904 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:06:21.147880 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.147917 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.147948 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.148011 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.150447 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.155880 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.156144 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.158766 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.175532 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.175588 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.175625 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.175657 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.175719 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.176844 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.176922 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.177623 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.179630 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.184313 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.185623 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.185715 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.185752 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.185816 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.185948 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.186060 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.186099 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.187986 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.188080 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.190492 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.190572 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.190682 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.192889 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.194823 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.194920 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.195203 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.195284 140706200571904 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:06:21.195393 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.195431 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.195462 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.195527 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.197741 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.203100 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.203363 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.206010 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.219017 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.219073 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.219109 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.219141 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.219203 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.219756 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.219830 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.220182 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.220854 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.223291 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.223905 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.223980 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.224020 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.224079 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.224205 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.224312 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.224350 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.226241 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.226335 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.228706 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.228782 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.228890 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.231078 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.232960 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.233053 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.233332 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.233413 140706200571904 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:06:21.233522 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.233560 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.233591 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.233660 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.235882 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.241247 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.241508 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.244129 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.256800 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.256856 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.256893 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.256923 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.256984 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.257545 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.257621 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.257992 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.258674 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.261108 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.261734 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.261811 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.261844 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.261908 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.262035 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.262142 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.262180 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.264092 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.264184 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.266579 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.266658 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.266765 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.268955 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.270858 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.270953 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.271234 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.271314 140706200571904 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:06:21.271422 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.271461 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.271491 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.271553 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.273770 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.279114 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.279382 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.282009 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.295060 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.295118 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.295154 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.295185 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.295247 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.295804 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.295879 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.296228 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.296901 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.299338 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.299962 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.300037 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.300072 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.300131 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.300265 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.300374 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.300413 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.302334 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.302428 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.304800 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.304881 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.304991 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.307164 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.309001 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.309094 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.309370 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.309451 140706200571904 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:06:21.309559 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.309597 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.309627 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.309696 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.312259 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.317645 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.317913 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.320485 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.333100 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.333155 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.333190 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.333220 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.333285 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.338348 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.338466 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.338928 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.339666 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.342300 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.342972 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.343051 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.343087 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.343158 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.343296 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.343412 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.343451 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.345439 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.345534 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.347989 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.348067 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.348179 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.350483 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.352349 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.352444 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.352724 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.352808 140706200571904 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:06:21.352921 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.352962 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.352995 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.353063 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.355309 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.360732 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.360996 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.363707 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.376648 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.376703 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.376740 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.376771 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.376833 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.377391 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.377466 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.377820 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.378530 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.380983 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.381595 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.381676 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.381711 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.381769 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.381899 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.382017 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.382056 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.383978 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.384071 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.386476 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.386555 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.386663 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.388877 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.390921 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.391017 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.391299 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.391379 140706200571904 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:06:21.391486 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.391524 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.391555 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.391617 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.393833 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.399307 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.399565 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.402155 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.415001 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.415057 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.415095 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.415125 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.415187 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.415742 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.415818 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.416163 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.416844 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.419283 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.420268 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.420347 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.420382 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.420441 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.420572 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.420681 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.420725 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.422615 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.422709 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.425086 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.425165 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.425273 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.427468 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.429379 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.429473 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.429760 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.429842 140706200571904 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:06:21.429950 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.429989 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.430021 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.430083 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.432293 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.437698 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.437969 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.440594 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.453351 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.453405 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.453441 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.453471 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.453531 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.454138 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.454215 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.454565 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.455254 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.457702 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.458329 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.458405 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.458439 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.458497 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.458626 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.458736 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.458779 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.460639 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.460732 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.463150 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.463232 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.463344 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.465521 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.467365 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.467460 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.467741 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.467822 140706200571904 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:06:21.467930 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.467968 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.468000 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.468062 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.470277 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.475727 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.475987 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.478579 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.491564 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.491620 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.491655 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.491686 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.491746 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.492308 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.492382 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.492737 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.493418 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.495889 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.496556 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.496633 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.496667 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.496727 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.496857 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.496965 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.497004 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.498862 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.498955 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.501321 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.501399 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.501511 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.503707 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.505625 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.505728 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.506015 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.506096 140706200571904 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:06:21.506205 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.506244 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.506276 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.506339 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.508543 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.513894 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.514153 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.516790 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.529722 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.529778 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.529813 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.529844 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.529906 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.530512 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.530587 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.530936 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.531626 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.534053 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.534669 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.534743 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.534777 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.534835 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.534963 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.535072 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.535111 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.536974 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.537072 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.539494 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.539573 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.539680 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.541901 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.543730 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.543824 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.544104 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.544184 140706200571904 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:06:21.544292 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.544330 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.544360 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.544422 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.546637 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.552066 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.552325 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.554928 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.567627 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.567682 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.567717 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.567747 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.567808 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.568363 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.568439 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.568786 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.569469 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.571896 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.572562 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.572638 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.572673 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.572730 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.572860 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.572970 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.573008 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.574883 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.574983 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.577371 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.577448 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.577557 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.579762 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.581681 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.581776 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.582057 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.582139 140706200571904 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:06:21.582251 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.582290 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.582321 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.582383 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.584585 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.589982 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.590248 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.593065 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.605681 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.605737 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.605773 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.605805 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.605867 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.606426 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.606501 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.606852 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.607580 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.610058 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.610678 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.610754 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.610788 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.610846 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.610976 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.611087 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.611126 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.612971 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.613064 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.615456 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.615535 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.615643 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.617906 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.619863 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.619963 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.620246 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.620335 140706200571904 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:06:21.623174 140706200571904 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:06:21.678225 140706200571904 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.678309 140706200571904 decoder_stack.py:333] dstack: autoregressive generator.
I0123 19:06:21.678363 140706200571904 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:06:21.678468 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.678506 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.678537 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.678598 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.681215 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.686536 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.686800 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.689340 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.701885 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.701940 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.701976 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.702007 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.702067 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.702615 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.702689 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.703032 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.703691 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.706124 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.706725 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.706800 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.706835 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.706893 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.707019 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.707132 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.707172 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.708983 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.709076 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.711405 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.711484 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.711592 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.713791 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.715593 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.715688 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.715968 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.716050 140706200571904 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:06:21.716157 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.716196 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.716227 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.716289 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.718489 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.723771 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.724030 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.726640 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.738795 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.738850 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.738885 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.738916 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.738978 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.739522 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.739597 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.739941 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.740603 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.743046 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.743657 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.743732 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.743767 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.743824 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.743951 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.744058 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.744102 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.745950 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.746044 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.748397 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.748476 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.748586 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.750827 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.752640 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.752734 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.753011 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.753091 140706200571904 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:06:21.753199 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.753237 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.753268 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.753329 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.755510 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.760761 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.761017 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.763608 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.775719 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.775774 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.775810 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.775841 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.775902 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.776450 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.776524 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.776869 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.777527 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.779951 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.780558 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.780634 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.780668 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.780725 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.780851 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.780957 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.780995 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.782795 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.782890 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.785187 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.785265 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.785372 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.788022 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.789831 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.789927 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.790208 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.790289 140706200571904 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:06:21.790396 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.790433 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.790462 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.790523 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.792686 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.797953 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.798211 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.800888 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.813103 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.813157 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.813194 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.813241 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.813305 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.813858 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.813931 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.814354 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.815020 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.817456 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.818063 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.818138 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.818172 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.818229 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.818354 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.818461 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.818504 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.820345 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.820436 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.822775 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.822852 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.822957 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.825197 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.827023 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.827116 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.827396 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.827476 140706200571904 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:06:21.827581 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.827619 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.827648 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.827709 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.829892 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.835179 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.835433 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.838059 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.850421 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.850474 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.850509 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.850538 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.850601 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.851150 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.851224 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.851566 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.852239 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.854676 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.855284 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.855357 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.855390 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.855447 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.855571 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.855677 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.855713 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.857555 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.857661 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.859998 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.860074 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.860180 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.862527 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.864377 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.864470 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.864746 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.864824 140706200571904 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:06:21.864929 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.864966 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.864995 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.865055 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.867303 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.872679 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.872941 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.875586 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.888189 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.888241 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.888275 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.888305 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.888364 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.888908 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.888981 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.889330 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.890017 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.892536 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.893140 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.893213 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.893246 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.893303 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.893426 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.893534 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.893572 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.895408 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.895507 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.897829 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.897905 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.898021 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.900800 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.902635 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.902730 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.903007 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.903087 140706200571904 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:06:21.903192 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.903230 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.903260 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.903322 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.905492 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.910916 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.911184 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.913829 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.926358 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.926415 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.926450 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.926481 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.926543 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.927125 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.927198 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.927545 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.928209 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.930694 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.931317 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.931392 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.931426 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.931481 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.931604 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.931710 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.931748 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.933591 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.933690 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.936077 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.936154 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.936261 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.938524 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.940353 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.940446 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.940721 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.940799 140706200571904 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:06:21.940904 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.940941 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.940971 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.941031 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.943226 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.948567 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.948822 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.951459 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:21.963940 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:21.963994 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:21.964028 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:21.964057 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.964117 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.964672 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.964745 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.965088 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.965764 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.968252 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.968873 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.968948 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:21.968981 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:21.969038 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.969167 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:21.969275 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:21.969312 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.971172 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.971264 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.973617 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.973706 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:21.973813 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:21.976069 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:21.977897 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.977991 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:21.978269 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.978348 140706200571904 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:06:21.978454 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:21.978491 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:21.978520 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:21.978580 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.980754 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:21.986082 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:21.986339 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:21.988968 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:22.001466 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:22.001519 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:22.001553 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:22.001583 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.001650 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.002209 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.002282 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.002625 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.003297 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.005793 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.006404 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.006479 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:22.006513 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:22.006571 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.006695 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:22.006802 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:22.006840 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:22.008668 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.008759 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:22.011097 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.011179 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:22.011288 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:22.013926 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:22.015788 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.015881 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:22.016160 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.016241 140706200571904 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:06:22.016347 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:22.016385 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:22.016414 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:22.016475 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.018669 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:22.024015 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.024271 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:22.026924 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:22.039397 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:22.039452 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:22.039486 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:22.039516 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.039576 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.040125 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.040199 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.040550 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.041221 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.043694 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.044304 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.044381 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:22.044414 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:22.044470 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.044593 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:22.044700 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:22.044738 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:22.047105 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.047199 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:22.049516 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.049593 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:22.049710 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:22.051912 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:22.053714 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.053808 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:22.054085 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.054165 140706200571904 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:06:22.054270 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:22.054308 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:22.054337 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:22.054398 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.056580 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:22.061885 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.062140 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:22.064733 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:22.077148 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:22.077202 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:22.077236 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:22.077266 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.077327 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.077889 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.077964 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.078318 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.078988 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.081459 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.082077 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.082152 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:22.082185 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:22.082242 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.082365 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:22.082473 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:22.082510 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:22.084346 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.084438 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:22.086795 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.086876 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:22.086983 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:22.089234 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:22.091059 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.091153 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:22.091432 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.091513 140706200571904 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:06:22.091617 140706200571904 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:06:22.091654 140706200571904 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:06:22.091683 140706200571904 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:06:22.091743 140706200571904 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.093922 140706200571904 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:06:22.099244 140706200571904 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.099500 140706200571904 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:06:22.102113 140706200571904 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:06:22.114500 140706200571904 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:06:22.114554 140706200571904 attention.py:418] Single window, no scan.
I0123 19:06:22.114588 140706200571904 transformer_layer.py:389] tlayer: self-attention.
I0123 19:06:22.114616 140706200571904 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.114676 140706200571904 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.115221 140706200571904 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.115294 140706200571904 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.115644 140706200571904 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.116324 140706200571904 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.118780 140706200571904 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.119394 140706200571904 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.119468 140706200571904 transformer_layer.py:468] tlayer: End windows.
I0123 19:06:22.119501 140706200571904 transformer_layer.py:472] tlayer: final FFN.
I0123 19:06:22.119556 140706200571904 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.119681 140706200571904 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:06:22.119789 140706200571904 nn_components.py:325] mlp: activation = None
I0123 19:06:22.119827 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:22.121654 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.121746 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:22.124079 140706200571904 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.124156 140706200571904 transformer_base.py:443] tbase: final FFN
I0123 19:06:22.124262 140706200571904 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:06:22.126869 140706200571904 nn_components.py:329] mlp: final activation = None
I0123 19:06:22.128706 140706200571904 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.128800 140706200571904 nn_components.py:261] mlp: residual
I0123 19:06:22.129079 140706200571904 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:22.129163 140706200571904 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:06:22.131926 140706200571904 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:06:26.547886 140706200571904 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 19:06:27.060760 140706200571904 training_loop.py:409] No working directory specified.
I0123 19:06:27.060893 140706200571904 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 19:06:27.061685 140706200571904 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 19:06:30.257230 140706200571904 training_loop.py:447] Only restoring trainable parameters.
I0123 19:06:30.257830 140706200571904 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 19:06:30.257906 140706200571904 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.257954 140706200571904 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.257997 140706200571904 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.258037 140706200571904 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258075 140706200571904 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.258113 140706200571904 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258150 140706200571904 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258187 140706200571904 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.258224 140706200571904 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.258260 140706200571904 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258297 140706200571904 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.258333 140706200571904 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.258368 140706200571904 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.258404 140706200571904 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258441 140706200571904 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.258477 140706200571904 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258513 140706200571904 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258549 140706200571904 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.258584 140706200571904 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.258633 140706200571904 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258672 140706200571904 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.258709 140706200571904 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.258745 140706200571904 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.258780 140706200571904 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258816 140706200571904 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.258852 140706200571904 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258887 140706200571904 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.258922 140706200571904 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.258958 140706200571904 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.258992 140706200571904 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259027 140706200571904 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.259063 140706200571904 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.259098 140706200571904 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.259133 140706200571904 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259167 140706200571904 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.259202 140706200571904 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259238 140706200571904 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259273 140706200571904 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.259308 140706200571904 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.259343 140706200571904 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259377 140706200571904 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.259412 140706200571904 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.259447 140706200571904 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.259483 140706200571904 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259519 140706200571904 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.259558 140706200571904 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259596 140706200571904 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259632 140706200571904 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.259667 140706200571904 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.259702 140706200571904 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259736 140706200571904 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.259770 140706200571904 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.259804 140706200571904 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.259840 140706200571904 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259875 140706200571904 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.259910 140706200571904 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259945 140706200571904 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.259979 140706200571904 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.260013 140706200571904 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.260047 140706200571904 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260081 140706200571904 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.260115 140706200571904 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.260149 140706200571904 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.260183 140706200571904 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260218 140706200571904 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.260253 140706200571904 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260288 140706200571904 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260322 140706200571904 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.260356 140706200571904 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.260391 140706200571904 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260426 140706200571904 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.260460 140706200571904 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.260498 140706200571904 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.260534 140706200571904 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260568 140706200571904 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.260603 140706200571904 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260638 140706200571904 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260673 140706200571904 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.260707 140706200571904 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.260741 140706200571904 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260777 140706200571904 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.260812 140706200571904 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.260847 140706200571904 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.260881 140706200571904 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260915 140706200571904 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.260950 140706200571904 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.260984 140706200571904 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261018 140706200571904 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.261053 140706200571904 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.261087 140706200571904 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261122 140706200571904 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.261156 140706200571904 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.261190 140706200571904 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.261225 140706200571904 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261260 140706200571904 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.261295 140706200571904 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261329 140706200571904 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261363 140706200571904 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.261397 140706200571904 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.261436 140706200571904 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261474 140706200571904 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.261509 140706200571904 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.261545 140706200571904 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.261580 140706200571904 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261615 140706200571904 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.261655 140706200571904 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261694 140706200571904 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261729 140706200571904 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.261765 140706200571904 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.261800 140706200571904 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261835 140706200571904 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.261869 140706200571904 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:06:30.261904 140706200571904 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:06:30.261938 140706200571904 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.261972 140706200571904 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.262006 140706200571904 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.262040 140706200571904 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.262074 140706200571904 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:06:30.262108 140706200571904 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:06:30.262142 140706200571904 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:06:30.262177 140706200571904 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:06:30.262204 140706200571904 training_loop.py:725] Total parameters: 152072288
I0123 19:06:30.262408 140706200571904 training_loop.py:739] Total state size: 0
I0123 19:06:30.286233 140706200571904 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 19:06:30.286480 140706200571904 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 19:06:30.286895 140706200571904 training_loop.py:652] Compiling mode beam_search with jit.
I0123 19:06:30.287230 140706200571904 training_loop.py:89] registering functions: dict_keys([])
I0123 19:06:30.302958 140706200571904 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = foot f e b c; g = foot g e b a; h = foot h e c a ? coll h g f
I0123 19:06:30.494639 140706200571904 ddar.py:60] Depth 1/1000 time = 0.17636966705322266
I0123 19:06:30.846400 140706200571904 ddar.py:60] Depth 2/1000 time = 0.35167694091796875
I0123 19:06:31.380380 140706200571904 ddar.py:60] Depth 3/1000 time = 0.5338926315307617
I0123 19:06:32.031934 140706200571904 ddar.py:60] Depth 4/1000 time = 0.6514391899108887
I0123 19:06:32.034100 140706200571904 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H : Points
DC = DB [00]
DB = DA [01]
DE = DA [02]
F,C,B are collinear [03]
EF  BC [04]
G,A,B are collinear [05]
EG  AB [06]
C,A,H are collinear [07]
EH  AC [08]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. A,C,H are collinear [07] & F,C,B are collinear [03] & EF  BC [04] & EH  AC [08]   EHC = EFC [09]
002. EHC = EFC [09]   F,C,H,E are concyclic [10]
003. F,C,H,E are concyclic [10]   FHC = FEC [11]
004. DC = DB [00] & DB = DA [01] & DE = DA [02]   C,E,A,B are concyclic [12]
005. C,E,A,B are concyclic [12]   ACE = ABE [13]
006. G,A,B are collinear [05] & F,C,B are collinear [03] & EF  BC [04] & EG  AB [06]   BGE = BFE [14]
007. BGE = BFE [14]   F,E,G,B are concyclic [15]
008. F,E,G,B are concyclic [15]   FEB = FGB [16]
009. FHC = FEC [11] & C,A,H are collinear [07] & ACE = ABE [13] & FEB = FGB [16] & G,A,B are collinear [05]   GFE = HFE [17]
010. GFE = HFE [17]   FG  FH [18]
011. FG  FH [18]   F,G,H are collinear
==========================

