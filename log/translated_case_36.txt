I0123 12:59:27.420434 140684118781952 inference_utils.py:69] Parsing gin configuration.
I0123 12:59:27.420532 140684118781952 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:59:27.420729 140684118781952 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:59:27.420763 140684118781952 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:59:27.420792 140684118781952 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:59:27.420821 140684118781952 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:59:27.420848 140684118781952 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:59:27.420875 140684118781952 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:59:27.420901 140684118781952 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:59:27.420927 140684118781952 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:59:27.420953 140684118781952 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:59:27.420979 140684118781952 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:59:27.421027 140684118781952 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:59:27.421164 140684118781952 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:59:27.421369 140684118781952 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:59:27.421469 140684118781952 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:59:27.427810 140684118781952 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:59:27.427928 140684118781952 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:59:27.428256 140684118781952 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:59:27.428361 140684118781952 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:59:27.428642 140684118781952 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:59:27.428742 140684118781952 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:59:27.429160 140684118781952 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:59:27.429260 140684118781952 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:59:27.432960 140684118781952 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:59:27.525863 140684118781952 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:59:27.526597 140684118781952 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:59:27.533356 140684118781952 training_loop.py:335] Process 0 of 1
I0123 12:59:27.533411 140684118781952 training_loop.py:336] Local device count = 1
I0123 12:59:27.533451 140684118781952 training_loop.py:337] Number of replicas = 1
I0123 12:59:27.533485 140684118781952 training_loop.py:339] Using random number seed 42
I0123 12:59:28.006898 140684118781952 training_loop.py:359] Initializing the model.
I0123 12:59:28.426510 140684118781952 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.426788 140684118781952 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:59:28.426894 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.426975 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427050 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427131 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427203 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427273 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427343 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427413 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427482 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427551 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427621 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427690 140684118781952 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:59:28.427731 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.427776 140684118781952 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:59:28.427890 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.427930 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.427960 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.430000 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.435339 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.446084 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.446368 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.450754 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.461430 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.461487 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.461525 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.461557 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.461621 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.462822 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.462901 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.463608 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.466104 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.471903 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.473653 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.473736 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.473772 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.473832 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.473963 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.474306 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.474353 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.476296 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.476402 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.479322 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.479404 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.479911 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.490936 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.499899 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.500009 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.500308 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.500393 140684118781952 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:59:28.500505 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.500545 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.500576 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.502474 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.504967 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.510603 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.510871 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.513632 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.517534 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.517591 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.517627 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.517667 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.517729 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.518318 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.518395 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.518757 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.519532 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.522051 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.522681 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.522759 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.522795 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.522854 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.522981 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.523312 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.523356 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.525321 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.525420 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.527968 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.528052 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.528494 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.530829 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.532741 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.532836 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.533130 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.533210 140684118781952 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:59:28.533319 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.533358 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.533388 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.535317 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.537672 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.543679 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.543948 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.546630 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.550528 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.550584 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.550621 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.550652 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.550718 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.551290 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.551367 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.551727 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.552503 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.555030 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.555706 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.555784 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.555819 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.555876 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.556007 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.556332 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.556375 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.558300 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.558396 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.560901 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.560986 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.561476 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.563823 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.565757 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.565852 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.566154 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.566237 140684118781952 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:59:28.566349 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.566389 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.566420 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.568383 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.570805 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.576488 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.576754 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.579424 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.583254 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.583309 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.583345 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.583376 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.583438 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.583999 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.584075 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.584439 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.585213 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.587805 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.588433 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.588512 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.588548 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.588610 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.588744 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.589095 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.589139 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.591066 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.591166 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.593783 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.593873 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.594311 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.596585 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.598520 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.598622 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.598912 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.598998 140684118781952 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:59:28.599107 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.599146 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.599176 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.601093 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.603493 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.609108 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.609369 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.612083 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.615919 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.615975 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.616010 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.616039 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.616102 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.616671 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.616747 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.617105 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.617887 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.620743 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.621365 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.621445 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.621480 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.621540 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.621682 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.622010 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.622054 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.623950 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.624045 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.626618 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.626703 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.627149 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.629419 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.631378 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.631474 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.631757 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.631837 140684118781952 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:59:28.631945 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.631984 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.632015 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.633871 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.636253 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.641864 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.642124 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.644796 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.648556 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.648612 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.648647 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.648677 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.648739 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.649341 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.649417 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.649788 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.650571 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.653073 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.653695 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.653774 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.653808 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.653866 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.653994 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.654321 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.654363 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.656254 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.656348 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.658905 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.658986 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.659425 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.661732 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.663633 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.663728 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.664023 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.664104 140684118781952 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:59:28.664214 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.664253 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.664283 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.666155 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.668575 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.674175 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.674443 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.677073 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.680854 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.680909 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.680948 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.680979 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.681042 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.681600 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.681684 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.682043 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.682822 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.685290 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.685915 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.685992 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.686026 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.686083 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.686218 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.686539 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.686581 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.688519 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.688615 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.691135 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.691220 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.691659 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.694283 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.696185 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.696286 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.696581 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.696663 140684118781952 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:59:28.696774 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.696813 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.696844 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.837011 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.840152 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.846163 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.846467 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.849230 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.853294 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.853353 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.853392 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.853426 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.853493 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.854122 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.854200 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.854569 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.855357 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.857958 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.858599 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.858679 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.858716 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.858778 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.858907 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.859250 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.859293 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.861207 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.861301 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.863973 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.864056 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.864508 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.866892 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.868891 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.869001 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.869309 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.869396 140684118781952 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:59:28.869511 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.869552 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.869584 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.871596 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.874003 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.879751 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.880013 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.882743 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.886579 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.886634 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.886669 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.886699 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.886764 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.887328 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.887403 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.887764 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.888532 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.891447 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.892071 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.892149 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.892184 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.892241 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.892367 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.892689 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.892732 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.894634 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.894729 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.897314 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.897393 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.897837 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.900111 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.902081 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.902176 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.902469 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.902560 140684118781952 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:59:28.902672 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.902711 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.902742 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.904581 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.907024 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.912549 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.912815 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.915863 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.919644 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.919700 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.919736 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.919765 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.919826 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.920428 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.920505 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.920866 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.921637 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.924108 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.924728 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.924804 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.924839 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.924895 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.925020 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.925344 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.925387 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.927276 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.927371 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.929927 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.930008 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.930440 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.932747 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.934665 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.934761 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.935058 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.935147 140684118781952 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:59:28.935260 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.935300 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.935331 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.937171 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.939636 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.945180 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.945443 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.948090 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.951889 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.951945 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.951981 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.952012 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.952074 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.952645 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.952722 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.953080 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.953866 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.956331 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.956962 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.957039 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.957074 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.957133 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.957257 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.957580 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.957623 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.959575 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.959668 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.962447 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.962527 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.962962 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.965269 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.967158 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.967255 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.967547 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.967627 140684118781952 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:59:28.967741 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:28.967781 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:28.967810 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:28.969739 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.972113 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:28.977721 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.977974 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:28.980611 140684118781952 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:59:28.984388 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:28.984444 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:28.984479 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:28.984508 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.984571 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.985135 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.985212 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.985575 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.986364 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.988840 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.989980 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.990061 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:28.990097 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:28.990156 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.990286 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:28.990616 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:28.990659 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.992576 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.992671 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:28.995171 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:28.995252 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:28.995738 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:28.998023 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:28.999916 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:29.000011 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.000301 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:29.000582 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.000651 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.000717 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.000776 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.000832 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.000885 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.000938 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.000990 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.001042 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.001094 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.001146 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.001199 140684118781952 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:59:29.001237 140684118781952 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:59:29.004729 140684118781952 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:59:29.052369 140684118781952 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.052454 140684118781952 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:59:29.052506 140684118781952 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:59:29.052608 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.052645 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.052675 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.052740 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.055172 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.060647 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.060908 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.063581 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.080121 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.080179 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.080215 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.080246 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.080308 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.081439 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.081517 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.082231 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.084228 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.088964 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.090290 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.090379 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.090416 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.090475 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.090605 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.090716 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.090755 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.092650 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.092745 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.095179 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.095262 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.095371 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.097616 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.099571 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.099669 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.099961 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.100044 140684118781952 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:59:29.100152 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.100190 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.100221 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.100284 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.102550 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.108047 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.108308 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.111003 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.124121 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.124177 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.124212 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.124241 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.124303 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.124860 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.124934 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.125290 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.125989 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.128474 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.129088 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.129166 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.129205 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.129265 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.129399 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.129508 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.129547 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.131462 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.131557 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.133984 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.134064 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.134173 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.136412 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.138326 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.138422 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.138707 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.138788 140684118781952 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:59:29.138896 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.138934 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.138964 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.139026 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.141280 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.151447 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.151763 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.154599 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.167518 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.167576 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.167613 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.167642 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.167704 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.168291 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.168403 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.168774 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.169490 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.172042 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.172679 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.172756 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.172791 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.172854 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.172986 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.173101 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.173140 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.175109 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.175204 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.177697 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.177783 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.177894 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.180136 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.182090 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.182187 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.182474 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.182557 140684118781952 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:59:29.182666 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.182708 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.182739 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.182802 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.185065 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.190592 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.190852 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.193564 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.206320 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.206375 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.206410 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.206440 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.206500 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.207061 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.207138 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.207491 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.208175 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.210669 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.211281 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.211357 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.211391 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.211449 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.211586 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.211698 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.211736 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.213681 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.213776 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.216193 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.216274 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.216382 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.218611 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.220479 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.220575 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.220861 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.220943 140684118781952 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:59:29.221051 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.221090 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.221121 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.221183 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.223776 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.229240 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.229505 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.232122 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.244793 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.244849 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.244884 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.244915 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.244976 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.245533 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.245609 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.245978 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.246667 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.249221 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.249851 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.249929 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.249964 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.250022 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.250157 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.250268 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.250307 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.252340 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.252434 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.254865 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.254945 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.255054 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.257319 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.259213 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.259309 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.259593 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.259673 140684118781952 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:59:29.259782 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.259821 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.259852 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.259913 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.262184 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.267626 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.267880 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.270570 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.283353 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.283409 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.283444 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.283475 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.283536 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.284093 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.284170 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.284532 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.285243 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.287743 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.288362 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.288439 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.288473 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.288535 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.288664 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.288779 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.288818 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.290776 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.290872 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.293272 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.293352 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.293460 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.295701 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.297562 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.297663 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.297949 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.298031 140684118781952 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:59:29.298139 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.298179 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.298209 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.298270 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.300508 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.306010 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.306271 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.308850 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.321480 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.321538 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.321574 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.321605 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.321675 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.322237 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.322314 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.322672 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.323368 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.325862 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.326848 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.326926 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.326960 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.327018 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.327149 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.327257 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.327301 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.329199 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.329293 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.331704 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.331789 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.331899 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.334129 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.336062 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.336158 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.336444 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.336524 140684118781952 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:59:29.336632 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.336669 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.336699 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.336760 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.339020 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.344686 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.344959 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.347655 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.360379 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.360435 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.360471 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.360502 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.360564 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.361172 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.361249 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.361608 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.362304 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.364770 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.365399 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.365477 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.365512 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.365573 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.365710 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.365820 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.365865 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.367743 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.367839 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.370301 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.370382 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.370490 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.372705 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.374576 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.374673 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.374957 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.375039 140684118781952 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:59:29.375147 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.375185 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.375216 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.375277 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.377512 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.383031 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.383291 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.385926 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.398684 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.398740 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.398774 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.398804 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.398869 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.399424 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.399501 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.399851 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.400538 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.403020 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.403687 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.403764 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.403798 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.403856 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.403982 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.404089 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.404127 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.406016 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.406110 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.408512 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.408592 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.408700 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.410906 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.412824 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.412919 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.413207 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.413289 140684118781952 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:59:29.413397 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.413436 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.413467 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.413530 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.415801 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.421200 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.421456 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.424121 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.437175 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.437231 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.437265 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.437295 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.437357 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.437966 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.438043 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.438399 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.439087 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.441547 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.442172 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.442250 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.442285 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.442344 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.442474 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.442589 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.442628 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.444514 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.444613 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.447082 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.447163 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.447271 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.449490 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.451357 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.451453 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.451735 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.451817 140684118781952 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:59:29.451925 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.451970 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.452001 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.452062 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.454302 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.459824 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.460083 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.462721 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.475445 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.475500 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.475535 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.475566 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.475627 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.476182 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.476261 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.476621 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.477306 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.479997 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.480658 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.480735 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.480769 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.480825 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.480951 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.481058 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.481096 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.483172 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.483273 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.485711 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.485793 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.485900 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.488106 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.490036 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.490132 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.490417 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.490499 140684118781952 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:59:29.490608 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.490647 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.490678 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.490740 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.492981 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.498426 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.498683 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.501372 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.514173 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.514229 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.514264 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.514294 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.514357 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.514918 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.514994 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.515350 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.516087 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.518583 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.519197 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.519273 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.519308 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.519365 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.519495 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.519603 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.519641 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.521505 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.521598 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.524012 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.524092 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.524199 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.526485 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.528360 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.528456 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.528744 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.528835 140684118781952 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:59:29.531700 140684118781952 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:59:29.586959 140684118781952 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.587043 140684118781952 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:59:29.587096 140684118781952 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:59:29.587199 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.587237 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.587267 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.587328 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.590006 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.595398 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.595658 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.598270 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.610996 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.611052 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.611087 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.611117 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.611179 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.611738 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.611815 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.612168 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.612849 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.615346 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.615951 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.616029 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.616062 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.616119 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.616246 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.616362 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.616402 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.618241 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.618338 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.620723 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.620803 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.620909 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.623152 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.624994 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.625091 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.625375 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.625456 140684118781952 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:59:29.625562 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.625601 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.625631 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.625703 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.627925 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.633277 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.633535 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.636195 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.648485 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.648539 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.648574 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.648604 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.648665 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.649208 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.649284 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.649636 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.650318 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.652800 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.653412 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.653489 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.653523 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.653581 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.653710 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.653818 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.653862 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.655687 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.655780 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.658167 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.658247 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.658355 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.660591 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.662441 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.662537 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.662822 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.662903 140684118781952 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:59:29.663011 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.663050 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.663081 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.663141 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.665349 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.670730 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.670991 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.673659 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.685925 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.685981 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.686015 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.686045 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.686104 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.686660 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.686738 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.687088 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.687762 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.690260 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.690874 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.690951 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.690986 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.691045 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.691172 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.691281 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.691320 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.693154 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.693248 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.695631 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.695712 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.695820 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.698509 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.700346 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.700443 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.700728 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.700811 140684118781952 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:59:29.700920 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.700958 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.700988 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.701050 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.703288 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.708644 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.708899 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.711670 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.724052 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.724108 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.724145 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.724190 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.724254 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.724802 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.724876 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.725232 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.725921 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.728425 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.729030 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.729105 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.729138 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.729195 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.729319 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.729424 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.729464 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.731347 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.731441 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.733840 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.733920 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.734026 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.736302 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.738149 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.738244 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.738526 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.738605 140684118781952 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:59:29.738712 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.738749 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.738777 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.738837 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.741047 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.746387 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.746644 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.749309 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.761842 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.761896 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.761929 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.761957 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.762017 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.762572 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.762650 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.763001 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.763677 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.766208 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.766822 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.766901 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.766934 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.766989 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.767115 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.767223 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.767261 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.769120 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.769221 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.771601 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.771681 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.771787 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.774049 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.775879 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.775974 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.776260 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.776341 140684118781952 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:59:29.776446 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.776483 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.776511 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.776571 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.778807 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.784168 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.784424 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.787076 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.799538 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.799592 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.799625 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.799654 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.799716 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.800264 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.800338 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.800693 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.801379 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.803896 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.804513 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.804589 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.804622 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.804678 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.804801 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.804907 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.804944 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.806800 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.806898 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.809282 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.809360 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.809467 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.812168 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.814035 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.814130 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.814414 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.814493 140684118781952 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:59:29.814600 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.814637 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.814665 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.814724 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.817033 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.822424 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.822674 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.825344 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.837779 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.837832 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.837865 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.837894 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.837954 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.838516 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.838590 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.838940 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.839620 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.842149 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.842775 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.842850 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.842883 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.842940 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.843063 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.843169 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.843205 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.845082 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.845174 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.847598 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.847676 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.847781 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.850068 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.851938 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.852033 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.852315 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.852396 140684118781952 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:59:29.852509 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.852546 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.852575 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.852635 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.854861 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.860315 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.860577 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.863342 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.876050 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.876103 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.876137 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.876165 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.876226 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.876784 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.876860 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.877219 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.877919 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.880493 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.881241 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.881316 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.881350 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.881406 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.881530 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.881639 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.881684 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.883605 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.883699 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.886201 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.886289 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.886403 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.888733 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.890620 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.890718 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.891011 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.891095 140684118781952 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:59:29.891206 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.891246 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.891276 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.891339 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.893562 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.898990 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.899246 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.901944 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.914443 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.914497 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.914530 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.914559 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.914620 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.915185 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.915260 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.915608 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.916283 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.918855 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.919476 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.919552 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.919585 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.919642 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.919767 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.919877 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.919914 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.921779 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.921871 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.924298 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.924382 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.924489 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.927154 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.929017 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.929111 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.929394 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.929474 140684118781952 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:59:29.929580 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.929617 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.929651 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.929714 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.931959 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.937390 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.937656 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.940334 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.952893 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.952947 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.952980 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.953009 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.953070 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.953623 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.953703 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.954059 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.954746 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.957273 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.957900 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.957978 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.958011 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.958065 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.958189 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.958298 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.958336 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.960628 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.960721 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.963108 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.963187 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:29.963298 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:29.965526 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.967358 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.967453 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:29.967735 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.967815 140684118781952 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:59:29.967920 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:29.967957 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:29.967986 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:29.968045 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.970268 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:29.975625 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.975879 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:29.978546 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:29.991297 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:29.991351 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:29.991384 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:29.991411 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.991471 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.992035 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.992110 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.992460 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.993141 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.995673 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.996294 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.996370 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:29.996403 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:29.996459 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.996586 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:29.996698 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:29.996735 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:29.998604 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:29.998697 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:30.001084 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.001162 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:30.001267 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:30.003538 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:30.005379 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.005473 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:30.005762 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.005842 140684118781952 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:59:30.005947 140684118781952 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:59:30.005985 140684118781952 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:59:30.006013 140684118781952 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:59:30.006072 140684118781952 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.008299 140684118781952 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:59:30.013698 140684118781952 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.013952 140684118781952 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:59:30.016618 140684118781952 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:59:30.029072 140684118781952 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:59:30.029125 140684118781952 attention.py:418] Single window, no scan.
I0123 12:59:30.029159 140684118781952 transformer_layer.py:389] tlayer: self-attention.
I0123 12:59:30.029187 140684118781952 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.029247 140684118781952 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.029809 140684118781952 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.029886 140684118781952 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.030240 140684118781952 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.030934 140684118781952 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.033459 140684118781952 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.034100 140684118781952 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.034178 140684118781952 transformer_layer.py:468] tlayer: End windows.
I0123 12:59:30.034211 140684118781952 transformer_layer.py:472] tlayer: final FFN.
I0123 12:59:30.034267 140684118781952 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.034396 140684118781952 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:59:30.034505 140684118781952 nn_components.py:325] mlp: activation = None
I0123 12:59:30.034542 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:30.036416 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.036509 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:30.038910 140684118781952 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.038989 140684118781952 transformer_base.py:443] tbase: final FFN
I0123 12:59:30.039094 140684118781952 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:59:30.041743 140684118781952 nn_components.py:329] mlp: final activation = None
I0123 12:59:30.043615 140684118781952 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.043708 140684118781952 nn_components.py:261] mlp: residual
I0123 12:59:30.043992 140684118781952 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:30.044076 140684118781952 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:59:30.046893 140684118781952 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:59:34.479052 140684118781952 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:59:35.027536 140684118781952 training_loop.py:409] No working directory specified.
I0123 12:59:35.027647 140684118781952 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:59:35.028370 140684118781952 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:59:38.149794 140684118781952 training_loop.py:447] Only restoring trainable parameters.
I0123 12:59:38.150502 140684118781952 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:59:38.150560 140684118781952 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.150604 140684118781952 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.150646 140684118781952 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.150686 140684118781952 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.150725 140684118781952 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.150764 140684118781952 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.150802 140684118781952 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.150840 140684118781952 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.150877 140684118781952 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.150914 140684118781952 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.150951 140684118781952 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.150988 140684118781952 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.151025 140684118781952 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.151063 140684118781952 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151100 140684118781952 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.151137 140684118781952 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151173 140684118781952 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151209 140684118781952 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.151246 140684118781952 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.151296 140684118781952 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151336 140684118781952 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.151374 140684118781952 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.151411 140684118781952 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.151447 140684118781952 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151483 140684118781952 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.151520 140684118781952 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151557 140684118781952 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151592 140684118781952 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.151627 140684118781952 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.151663 140684118781952 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151700 140684118781952 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.151737 140684118781952 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.151774 140684118781952 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.151811 140684118781952 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151847 140684118781952 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.151883 140684118781952 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151918 140684118781952 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.151955 140684118781952 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.151991 140684118781952 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.152027 140684118781952 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152064 140684118781952 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.152101 140684118781952 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.152137 140684118781952 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.152173 140684118781952 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152209 140684118781952 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.152250 140684118781952 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152288 140684118781952 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152323 140684118781952 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.152359 140684118781952 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.152395 140684118781952 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152431 140684118781952 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.152467 140684118781952 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.152503 140684118781952 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.152539 140684118781952 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152575 140684118781952 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.152611 140684118781952 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152646 140684118781952 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152681 140684118781952 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.152716 140684118781952 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.152751 140684118781952 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152786 140684118781952 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.152822 140684118781952 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.152857 140684118781952 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.152891 140684118781952 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152927 140684118781952 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.152963 140684118781952 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.152998 140684118781952 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153033 140684118781952 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.153069 140684118781952 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.153104 140684118781952 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153140 140684118781952 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.153176 140684118781952 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.153216 140684118781952 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.153254 140684118781952 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153290 140684118781952 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.153325 140684118781952 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153362 140684118781952 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153397 140684118781952 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.153433 140684118781952 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.153469 140684118781952 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153505 140684118781952 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.153541 140684118781952 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.153578 140684118781952 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.153614 140684118781952 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153668 140684118781952 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.153709 140684118781952 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153748 140684118781952 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153784 140684118781952 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.153820 140684118781952 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.153856 140684118781952 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.153891 140684118781952 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.153926 140684118781952 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.153961 140684118781952 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.153997 140684118781952 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154032 140684118781952 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.154067 140684118781952 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154102 140684118781952 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154138 140684118781952 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.154173 140684118781952 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.154214 140684118781952 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154251 140684118781952 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.154286 140684118781952 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.154321 140684118781952 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.154356 140684118781952 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154392 140684118781952 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.154428 140684118781952 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154464 140684118781952 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154499 140684118781952 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.154534 140684118781952 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.154569 140684118781952 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154604 140684118781952 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.154640 140684118781952 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:59:38.154675 140684118781952 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:59:38.154711 140684118781952 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154745 140684118781952 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.154781 140684118781952 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154817 140684118781952 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154852 140684118781952 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:59:38.154888 140684118781952 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:59:38.154923 140684118781952 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:59:38.154958 140684118781952 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:59:38.154986 140684118781952 training_loop.py:725] Total parameters: 152072288
I0123 12:59:38.155203 140684118781952 training_loop.py:739] Total state size: 0
I0123 12:59:38.174963 140684118781952 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:59:38.175231 140684118781952 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:59:38.175498 140684118781952 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:59:38.175811 140684118781952 training_loop.py:89] registering functions: dict_keys([])
I0123 12:59:38.191768 140684118781952 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = foot e c a b; f = midpoint f a b; g = foot g a c b; h = on_line h c e, on_line h a g; i = midpoint i c h; j = foot j c d i; k = mirror k c j; l = midpoint l h k; m = foot m k l d; n = mirror n k m; o = circle o e n f ? coll l o n
I0123 12:59:40.865990 140684118781952 ddar.py:60] Depth 1/1000 time = 2.5931880474090576
I0123 12:59:51.003358 140684118781952 ddar.py:60] Depth 2/1000 time = 10.13713526725769
I0123 13:00:08.038917 140684118781952 ddar.py:60] Depth 3/1000 time = 17.035212755203247
I0123 13:00:26.211631 140684118781952 ddar.py:60] Depth 4/1000 time = 18.17236876487732
I0123 13:00:48.434297 140684118781952 ddar.py:60] Depth 5/1000 time = 22.222338914871216
I0123 13:01:16.535202 140684118781952 ddar.py:60] Depth 6/1000 time = 28.10047173500061
I0123 13:01:52.066695 140684118781952 ddar.py:60] Depth 7/1000 time = 35.531047344207764
I0123 13:02:26.840995 140684118781952 ddar.py:60] Depth 8/1000 time = 34.773895502090454
I0123 13:03:01.706101 140684118781952 ddar.py:60] Depth 9/1000 time = 34.864776372909546
I0123 13:03:36.694892 140684118781952 ddar.py:60] Depth 10/1000 time = 34.98841309547424
I0123 13:04:11.616579 140684118781952 ddar.py:60] Depth 11/1000 time = 34.9204466342926
I0123 13:04:46.034815 140684118781952 ddar.py:60] Depth 12/1000 time = 34.41453766822815
I0123 13:04:46.278713 140684118781952 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L M N O : Points
DB = DA [00]
DC = DA [01]
CE ⟂ AB [02]
B,E,A are collinear [03]
FB = FA [04]
B,F,A are collinear [05]
AG ⟂ BC [06]
B,C,G are collinear [07]
H,G,A are collinear [08]
H,C,E are collinear [09]
H,C,I are collinear [10]
IC = IH [11]
CJ ⟂ DI [12]
D,J,I are collinear [13]
JC = JK [14]
J,K,C are collinear [15]
L,K,H are collinear [16]
LH = LK [17]
D,L,M are collinear [18]
KM ⟂ DL [19]
MK = MN [20]
N,K,M are collinear [21]
ON = OF [22]
OE = ON [23]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. FB = FA [04] & DB = DA [00] ⇒  FB:FA = DB:DA [24]
002. FB = FA [04] & DB = DA [00] ⇒  BA ⟂ DF [25]
003. FB:FA = DB:DA [24] & B,F,A are collinear [05] ⇒  ∠BDF = ∠FDA [26]
004. DA = DB [00] & DC = DA [01] ⇒  DC = DB [27]
005. DA = DB [00] & DC = DA [01] ⇒  D is the circumcenter of \Delta CBA [28]
006. DC = DB [27] ⇒  ∠DCB = ∠CBD [29]
007. DC = DA [01] ⇒  ∠DCA = ∠CAD [30]
008. ON = OF [22] ⇒  ∠ONF = ∠NFO [31]
009. OE = ON [23] ⇒  ∠ONE = ∠NEO [32]
010. OE = ON [23] & ON = OF [22] ⇒  OF = OE [33]
011. OF = OE [33] ⇒  ∠OFE = ∠FEO [34]
012. ∠OFE = ∠FEO [34] & B,F,A are collinear [05] & B,E,A are collinear [03] ⇒  ∠(FO-AB) = ∠(AB-EO) [35]
013. J,K,C are collinear [15] & D,J,I are collinear [13] & CJ ⟂ DI [12] ⇒  ∠KJD = ∠DJC [36]
014. JC = JK [14] & ∠KJD = ∠DJC [36] (SAS)⇒  ∠JKD = ∠DCJ [37]
015. ∠JKD = ∠DCJ [37] & J,K,C are collinear [15] ⇒  ∠(CJ-DK) = ∠DCJ [38]
016. N,K,M are collinear [21] & D,L,M are collinear [18] & KM ⟂ DL [19] ⇒  ∠NMD = ∠DMK [39]
017. MK = MN [20] & ∠NMD = ∠DMK [39] (SAS)⇒  ∠MND = ∠DKM [40]
018. ∠MND = ∠DKM [40] & N,K,M are collinear [21] ⇒  ∠(KM-DN) = ∠DKM [41]
019. N,K,M are collinear [21] & D,L,M are collinear [18] & KM ⟂ DL [19] ⇒  ∠NML = ∠LMK [42]
020. MK = MN [20] & ∠NML = ∠LMK [42] (SAS)⇒  ∠MNL = ∠LKM [43]
021. I,H,C are collinear [10] & IC = IH [11] ⇒  I is midpoint of HC [44]
022. J,K,C are collinear [15] & JC = JK [14] ⇒  J is midpoint of CK [45]
023. I is midpoint of HC [44] & J is midpoint of CK [45] ⇒  IJ ∥ HK [46]
024. ∠MNL = ∠LKM [43] & N,K,M are collinear [21] & L,K,H are collinear [16] & IJ ∥ HK [46] & D,J,I are collinear [13] ⇒  ∠(KM-LN) = ∠(DI-KM) [47]
025. B,E,A are collinear [03] & B,C,G are collinear [07] & AG ⟂ BC [06] & CE ⟂ AB [02] ⇒  ∠AEC = ∠AGC [48]
026. ∠AEC = ∠AGC [48] ⇒  G,C,E,A are concyclic [49]
027. G,C,E,A are concyclic [49] ⇒  ∠GEC = ∠GAC [50]
028. ∠GEC = ∠GAC [50] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  ∠(EG-DF) = ∠GAC [51]
029. H,G,A are collinear [08] & B,C,G are collinear [07] & AG ⟂ BC [06] ⇒  HG ⟂ GC [52]
030. HG ⟂ GC [52] & I is midpoint of HC [44] ⇒  HI = GI [53]
031. HG ⟂ GC [52] & I is midpoint of HC [44] ⇒  CI = GI [54]
032. HI = GI [53] ⇒  ∠IGH = ∠GHI [55]
033. ∠IGH = ∠GHI [55] & H,G,A are collinear [08] & H,C,I are collinear [10] & H,C,E are collinear [09] ⇒  ∠IGA = ∠(AG-CE) [56]
034. B,C,G are collinear [07] & AG ⟂ BC [06] ⇒  AG ⟂ GB [57]
035. B,F,A are collinear [05] & FA = FB [04] ⇒  F is midpoint of BA [58]
036. AG ⟂ GB [57] & F is midpoint of BA [58] ⇒  AF = GF [59]
037. AG ⟂ GB [57] & F is midpoint of BA [58] ⇒  BF = GF [60]
038. AF = GF [59] ⇒  ∠FGA = ∠GAF [61]
039. H,G,A are collinear [08] & AG ⟂ BC [06] ⇒  HG ⟂ BC [62]
040. B,F,A are collinear [05] & H,C,E are collinear [09] & AB ⟂ CE [02] ⇒  FB ⟂ HC [63]
041. HG ⟂ BC [62] & FB ⟂ HC [63] ⇒  ∠(HG-FB) = ∠BCH [64]
042. CI = GI [54] ⇒  ∠IGC = ∠GCI [65]
043. H,G,A are collinear [08] & ∠FGA = ∠GAF [61] & B,F,A are collinear [05] & ∠(HG-FB) = ∠BCH [64] & H,C,E are collinear [09] & ∠IGC = ∠GCI [65] & B,C,G are collinear [07] & H,C,I are collinear [10] ⇒  ∠(BC-GI) = ∠HGF [66]
044. J,K,C are collinear [15] & D,J,I are collinear [13] & CJ ⟂ DI [12] ⇒  JK ⟂ DJ [67]
045. HG ⟂ BC [62] & JK ⟂ DJ [67] ⇒  ∠(HG-DJ) = ∠(BC-JK) [68]
046. L,K,H are collinear [16] & LH = LK [17] ⇒  L is midpoint of HK [69]
047. I is midpoint of HC [44] & L is midpoint of HK [69] ⇒  IL ∥ CK [70]
048. H,G,A are collinear [08] & H,K,L are collinear [16] & ∠(HG-DJ) = ∠(BC-JK) [68] & D,J,I are collinear [13] & J,K,C are collinear [15] & IL ∥ CK [70] & IJ ∥ HK [46] ⇒  ∠(BC-LI) = ∠(HG-LK) [71]
049. ∠(BC-GI) = ∠HGF [66] & ∠(BC-LI) = ∠(HG-LK) [71] ⇒  ∠(GF-LK) = ∠GIL [72]
050. B,C,G are collinear [07] & B,F,A are collinear [05] & AG ⟂ BC [06] & CE ⟂ AB [02] & BA ⟂ DF [25] ⇒  ∠AGC = ∠AFD [73]
051. D is the circumcenter of \Delta CBA [28] & F is midpoint of BA [58] ⇒  ∠ACB = ∠ADF [74]
052. B,C,G are collinear [07] & ∠ACB = ∠ADF [74] ⇒  ∠ACG = ∠ADF [75]
053. ∠AGC = ∠AFD [73] & ∠ACG = ∠ADF [75] (Similar Triangles)⇒  GA:GC = FA:FD [76]
054. B,C,G are collinear [07] & H,G,A are collinear [08] & AG ⟂ BC [06] ⇒  ∠BGA = ∠HGC [77]
055. H,C,E are collinear [09] & B,C,G are collinear [07] & ∠(HG-FB) = ∠BCH [64] & H,G,A are collinear [08] & B,F,A are collinear [05] ⇒  ∠HCG = ∠BAG [78]
056. ∠BGA = ∠HGC [77] & ∠HCG = ∠BAG [78] (Similar Triangles)⇒  HC:BA = CG:GA [79]
057. I is midpoint of HC [44] & F is midpoint of BA [58] ⇒  IH:FB = HC:BA [80]
058. D,J,I are collinear [13] & J,K,C are collinear [15] & DI ⟂ CJ [12] ⇒  IJ ⟂ KC [81]
059. J is midpoint of CK [45] & IJ ⟂ KC [81] ⇒  IK = IC [82]
060. J,K,C are collinear [15] & L,K,H are collinear [16] & IJ ∥ HK [46] & D,J,I are collinear [13] & IL ∥ CK [70] ⇒  ∠JKL = ∠ILH [83]
061. J is midpoint of CK [45] & L is midpoint of HK [69] ⇒  JL ∥ CH [84]
062. H,K,L are collinear [16] & H,C,I are collinear [10] & H,C,E are collinear [09] & JL ∥ CH [84] & IJ ∥ HK [46] & D,J,I are collinear [13] ⇒  ∠JLK = ∠IHL [85]
063. ∠JKL = ∠ILH [83] & ∠JLK = ∠IHL [85] (Similar Triangles)⇒  LK:HL = LJ:HI [86]
064. ∠JKL = ∠ILH [83] & ∠JLK = ∠IHL [85] (Similar Triangles)⇒  KL:LH = KJ:LI [87]
065. GA:GC = FA:FD [76] & AF = GF [59] & HC:BA = CG:GA [79] & IH:FB = HC:BA [80] & IK = IC [82] & IC = IH [11] & BF = GF [60] & LK:HL = LJ:HI [86] & LH = LK [17] ⇒  JL:GF = DF:GF [88]
066. KL:LH = KJ:LI [87] & LH = LK [17] ⇒  GF:LI = GF:LI [89]
067. JL:GF = DF:GF [88] & GF:LI = GF:LI [89] ⇒  JL = DF [90]
068. JL ∥ CH [84] & H,C,E are collinear [09] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  ∠JLD = ∠FDL [91]
069. JL = DF [90] & ∠JLD = ∠FDL [91] (SAS)⇒  ∠LJD = ∠DFL [92]
070. ∠(GF-LK) = ∠GIL [72] & L,K,H are collinear [16] & IJ ∥ HK [46] & D,J,I are collinear [13] & IL ∥ CK [70] & J,K,C are collinear [15] & ∠LJD = ∠DFL [92] & JL ∥ CH [84] & H,C,E are collinear [09] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  ∠FGI = ∠FLI [93]
071. ∠FGI = ∠FLI [93] ⇒  G,L,F,I are concyclic [94]
072. H,G,A are collinear [08] & B,F,A are collinear [05] & ∠(HG-FB) = ∠BCH [64] & H,C,E are collinear [09] & JL ∥ CH [84] ⇒  ∠(BC-JL) = ∠(HG-FB) [95]
073. ∠(BC-GI) = ∠HGF [66] & ∠(BC-JL) = ∠(HG-FB) [95] ⇒  ∠GFB = ∠(GI-JL) [96]
074. B,F,A are collinear [05] & B,E,A are collinear [03] & H,C,I are collinear [10] & H,C,E are collinear [09] & ∠GFB = ∠(GI-JL) [96] & JL ∥ CH [84] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  ∠FGI = ∠FEI [97]
075. ∠FGI = ∠FEI [97] ⇒  G,F,E,I are concyclic [98]
076. G,L,F,I are concyclic [94] & G,F,E,I are concyclic [98] ⇒  G,I,L,E are concyclic [99]
077. G,I,L,E are concyclic [99] ⇒  ∠GIL = ∠GEL [100]
078. ∠GIL = ∠GEL [100] & IL ∥ CK [70] & J,K,C are collinear [15] ⇒  ∠(GI-CJ) = ∠GEL [101]
079. FB ⟂ HC [63] & JK ⟂ DJ [67] ⇒  ∠(FB-JK) = ∠(HC-DJ) [102]
080. H,K,L are collinear [16] & B,F,A are collinear [05] & B,E,A are collinear [03] & H,C,E are collinear [09] & ∠(FB-JK) = ∠(HC-DJ) [102] & J,K,C are collinear [15] & D,J,I are collinear [13] & IL ∥ CK [70] & IJ ∥ HK [46] ⇒  ∠ILK = ∠FEH [103]
081. J,K,C are collinear [15] & D,J,I are collinear [13] & CJ ⟂ DI [12] ⇒  ∠KJI = ∠IJC [104]
082. JC = JK [14] & ∠KJI = ∠IJC [104] (SAS)⇒  ∠KIJ = ∠JIC [105]
083. JL = DF [90] & LK:HL = LJ:HI [86] & LH = LK [17] ⇒  HI = DF [106]
084. H,C,I are collinear [10] & H,C,E are collinear [09] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  ∠HIF = ∠DFI [107]
085. HI = DF [106] & ∠HIF = ∠DFI [107] (SAS)⇒  ∠IHF = ∠FDI [108]
086. HI = DF [106] & ∠HIF = ∠DFI [107] (SAS)⇒  FH = ID [109]
087. L,K,H are collinear [16] & H,C,E are collinear [09] & ∠KIJ = ∠JIC [105] & D,J,I are collinear [13] & H,C,I are collinear [10] & IJ ∥ HK [46] & ∠IHF = ∠FDI [108] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  ∠IKL = ∠FHE [110]
088. ∠ILK = ∠FEH [103] & ∠IKL = ∠FHE [110] (Similar Triangles)⇒  KI:HF = KL:HE [111]
089. KI:HF = KL:HE [111] & IK = IC [82] & IC = IH [11] & LK:HL = LJ:HI [86] & LH = LK [17] & FH = ID [109] & DF = JL [90] ⇒  HL:HE = DF:DI [112]
090. H,K,L are collinear [16] & IJ ∥ HK [46] & D,J,I are collinear [13] & ∠LJD = ∠DFL [92] & JL ∥ CH [84] & H,C,E are collinear [09] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  LF ∥ LK [113]
091. LF ∥ LK [113] ⇒  K,L,F are collinear [114]
092. K,L,F are collinear [114] & L,K,H are collinear [16] & H,C,E are collinear [09] & ∠IHF = ∠FDI [108] & H,C,I are collinear [10] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  ∠LHE = ∠IDF [115]
093. HL:HE = DF:DI [112] & ∠LHE = ∠IDF [115] (Similar Triangles)⇒  LH:LE = FD:FI [116]
094. JL = DF [90] & IK = IC [82] & IC = IH [11] & LK:HL = LJ:HI [86] & LH = LK [17] ⇒  KI = FD [117]
095. ∠KIJ = ∠JIC [105] & D,J,I are collinear [13] & H,C,I are collinear [10] & H,C,E are collinear [09] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  ∠KID = ∠IDF [118]
096. KI = FD [117] & ∠KID = ∠IDF [118] (SAS)⇒  DK = IF [119]
097. N,K,M are collinear [21] & MK = MN [20] ⇒  M is midpoint of KN [120]
098. D,L,M are collinear [18] & N,K,M are collinear [21] & DL ⟂ KM [19] ⇒  DM ⟂ NK [121]
099. M is midpoint of KN [120] & DM ⟂ NK [121] ⇒  DN = DK [122]
100. D,L,M are collinear [18] & N,K,M are collinear [21] & DL ⟂ KM [19] ⇒  LM ⟂ NK [123]
101. M is midpoint of KN [120] & LM ⟂ NK [123] ⇒  LN = LK [124]
102. LH:LE = FD:FI [116] & LH = LK [17] & JL = DF [90] & DK = IF [119] & DN = DK [122] & NL = LK [124] ⇒  DN:DF = LE:LN [125]
103. IK = IC [82] & CI = GI [54] ⇒  I is the circumcenter of \Delta GKC [126]
104. I is the circumcenter of \Delta GKC [126] & J is midpoint of CK [45] ⇒  ∠KGC = ∠KIJ [127]
105. B,C,G are collinear [07] & ∠KGC = ∠KIJ [127] & D,J,I are collinear [13] ⇒  ∠KGB = ∠KID [128]
106. D,J,I are collinear [13] & J,K,C are collinear [15] & DI ⟂ CJ [12] ⇒  DJ ⟂ KC [129]
107. J is midpoint of CK [45] & DJ ⟂ KC [129] ⇒  DK = DC [130]
108. DK = DC [130] & BD = CD [27] ⇒  D is the circumcenter of \Delta BKC [131]
109. D is the circumcenter of \Delta BKC [131] & J is midpoint of CK [45] ⇒  ∠KBC = ∠KDJ [132]
110. B,C,G are collinear [07] & ∠KBC = ∠KDJ [132] & D,J,I are collinear [13] ⇒  ∠KBG = ∠KDI [133]
111. ∠KGB = ∠KID [128] & ∠KBG = ∠KDI [133] (Similar Triangles)⇒  GK:GB = IK:ID [134]
112. GK:GB = IK:ID [134] & IK = IC [82] & IC = IH [11] & LK:HL = LJ:HI [86] & LH = LK [17] & KI:HF = KL:HE [111] & FH = ID [109] ⇒  HL:HE = GK:GB [135]
113. CI = GI [54] & IK = IC [82] & IC = IH [11] ⇒  K,H,C,G are concyclic [136]
114. K,H,C,G are concyclic [136] ⇒  ∠KHC = ∠KGC [137]
115. K,L,F are collinear [114] & L,K,H are collinear [16] & H,C,E are collinear [09] & B,C,G are collinear [07] & ∠KHC = ∠KGC [137] & IJ ∥ HK [46] & D,J,I are collinear [13] & BA ⟂ DF [25] & CE ⟂ AB [02] & ∠IHF = ∠FDI [108] & H,C,I are collinear [10] ⇒  ∠LHE = ∠KGB [138]
116. HL:HE = GK:GB [135] & ∠LHE = ∠KGB [138] (Similar Triangles)⇒  ∠LEH = ∠KBG [139]
117. ∠LEH = ∠KBG [139] & H,C,E are collinear [09] & B,C,G are collinear [07] & JL ∥ CH [84] ⇒  ∠(BC-JL) = ∠(KB-LE) [140]
118. DK = DC [130] & DN = DK [122] ⇒  D is the circumcenter of \Delta CKN [141]
119. DK = DC [130] & DN = DK [122] ⇒  DC = DN [142]
120. D is the circumcenter of \Delta CKN [141] & J is midpoint of CK [45] ⇒  ∠CNK = ∠CDJ [143]
121. DC = DN [142] ⇒  ∠DCN = ∠CND [144]
122. N,K,M are collinear [21] & ∠(KM-LN) = ∠(DI-KM) [47] & ∠CNK = ∠CDJ [143] & D,J,I are collinear [13] & ∠DCN = ∠CND [144] ⇒  ∠CND = ∠KNL [145]
123. DC = DA [01] & DK = DC [130] & DA = DB [00] & DN = DK [122] ⇒  N,K,C,B are concyclic [146]
124. C,N,B,K are concyclic [146] ⇒  ∠CNK = ∠CBK [147]
125. N,K,M are collinear [21] & ∠CNK = ∠CBK [147] ⇒  ∠NKB = ∠NCB [148]
126. ∠CND = ∠KNL [145] & ∠NKB = ∠NCB [148] ⇒  ∠(LN-BK) = ∠(DN-BC) [149]
127. ∠(BC-JL) = ∠(KB-LE) [140] & ∠(LN-BK) = ∠(DN-BC) [149] ⇒  ∠ELN = ∠(JL-DN) [150]
128. ∠ELN = ∠(JL-DN) [150] & JL ∥ CH [84] & H,C,E are collinear [09] & BA ⟂ DF [25] & CE ⟂ AB [02] ⇒  ∠ELN = ∠FDN [151]
129. DN:DF = LE:LN [125] & ∠ELN = ∠FDN [151] (Similar Triangles)⇒  ∠DNF = ∠NEL [152]
130. CE ⟂ AB [02] & AG ⟂ BC [06] & CJ ⟂ DI [12] & ∠BDF = ∠FDA [26] & ∠DCB = ∠CBD [29] & ∠DCA = ∠CAD [30] & ∠ONF = ∠NFO [31] & ∠ONE = ∠NEO [32] & ∠(FO-AB) = ∠(AB-EO) [35] & ∠(CJ-DK) = ∠DCJ [38] & ∠(KM-DN) = ∠DKM [41] & ∠(KM-LN) = ∠(DI-KM) [47] & ∠(EG-DF) = ∠GAC [51] & ∠IGA = ∠(AG-CE) [56] & ∠(GI-CJ) = ∠GEL [101] & ∠DNF = ∠NEL [152] (Angle chase)⇒  NO ∥ LN [153]
131. NO ∥ LN [153] ⇒  N,L,O are collinear
==========================

