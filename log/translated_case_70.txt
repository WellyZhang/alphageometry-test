I0123 16:04:41.038066 139995557732352 inference_utils.py:69] Parsing gin configuration.
I0123 16:04:41.038168 139995557732352 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 16:04:41.038374 139995557732352 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 16:04:41.038409 139995557732352 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 16:04:41.038439 139995557732352 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 16:04:41.038467 139995557732352 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 16:04:41.038493 139995557732352 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 16:04:41.038520 139995557732352 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 16:04:41.038547 139995557732352 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 16:04:41.038573 139995557732352 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 16:04:41.038599 139995557732352 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 16:04:41.038624 139995557732352 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 16:04:41.038671 139995557732352 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 16:04:41.038803 139995557732352 resource_reader.py:55] Path not found: base_htrans.gin
I0123 16:04:41.039005 139995557732352 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 16:04:41.039113 139995557732352 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 16:04:41.045447 139995557732352 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 16:04:41.045573 139995557732352 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 16:04:41.045908 139995557732352 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 16:04:41.046016 139995557732352 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 16:04:41.046299 139995557732352 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 16:04:41.046400 139995557732352 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 16:04:41.046802 139995557732352 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 16:04:41.046903 139995557732352 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 16:04:41.050578 139995557732352 training_loop.py:334] ==== Training loop: initializing model ====
I0123 16:04:41.151666 139995557732352 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 16:04:41.152383 139995557732352 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 16:04:41.159049 139995557732352 training_loop.py:335] Process 0 of 1
I0123 16:04:41.159104 139995557732352 training_loop.py:336] Local device count = 1
I0123 16:04:41.159144 139995557732352 training_loop.py:337] Number of replicas = 1
I0123 16:04:41.159176 139995557732352 training_loop.py:339] Using random number seed 42
I0123 16:04:41.638054 139995557732352 training_loop.py:359] Initializing the model.
I0123 16:04:42.053701 139995557732352 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.053967 139995557732352 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 16:04:42.054071 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054149 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054223 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054304 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054375 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054445 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054514 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054584 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054654 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054723 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054792 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054860 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:04:42.054899 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.054944 139995557732352 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:04:42.055055 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.055094 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.055125 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.057115 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.062416 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.072923 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.073194 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.077553 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.087986 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.088045 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.088083 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.088115 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.088178 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.089356 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.089433 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.090154 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.092579 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.098327 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.100030 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.100111 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.100146 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.100207 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.100337 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.100673 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.100719 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.102660 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.102761 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.105790 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.105874 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.106375 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.116470 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.125252 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.125350 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.125658 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.125741 139995557732352 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:04:42.125851 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.125890 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.125921 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.127775 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.130236 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.135787 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.136046 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.138682 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.143110 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.143218 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.143255 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.143285 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.143357 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.143968 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.144047 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.144423 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.145218 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.147695 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.148305 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.148381 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.148414 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.148474 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.148603 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.148950 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.148993 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.150954 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.151046 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.153533 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.153614 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.154056 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.156341 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.158220 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.158316 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.158601 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.158691 139995557732352 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:04:42.158802 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.158842 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.158873 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.160778 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.163132 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.169003 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.169256 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.171986 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.175835 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.175890 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.175926 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.175956 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.176017 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.176572 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.176647 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.177009 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.177775 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.180280 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.180937 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.181012 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.181046 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.181104 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.181231 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.181555 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.181598 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.183502 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.183598 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.186115 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.186204 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.186686 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.188936 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.190851 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.190945 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.191231 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.191310 139995557732352 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:04:42.191418 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.191456 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.191487 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.193366 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.195744 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.201355 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.201614 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.204257 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.208032 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.208086 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.208121 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.208151 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.208211 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.208763 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.208839 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.209198 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.209973 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.212481 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.213097 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.213172 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.213206 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.213264 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.213388 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.213715 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.213759 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.215838 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.215929 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.218457 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.218540 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.218974 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.221192 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.223082 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.223176 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.223467 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.223549 139995557732352 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:04:42.223657 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.223695 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.223725 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.225610 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.227967 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.233473 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.233737 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.236399 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.240104 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.240158 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.240193 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.240222 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.240283 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.240844 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.240921 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.241285 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.242072 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.244922 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.245546 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.245626 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.245673 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.245735 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.245871 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.246200 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.246243 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.248126 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.248218 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.250785 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.250868 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.251306 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.253550 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.255505 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.255604 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.255902 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.255984 139995557732352 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:04:42.256093 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.256132 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.256163 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.258008 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.260372 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.265995 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.266249 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.268945 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.272670 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.272724 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.272760 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.272793 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.272855 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.273462 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.273539 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.273913 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.274701 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.277226 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.277858 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.277935 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.277971 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.278030 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.278157 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.278479 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.278521 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.280395 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.280486 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.283008 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.283087 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.283514 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.285818 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.287734 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.287828 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.288127 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.288207 139995557732352 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:04:42.288317 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.288356 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.288387 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.290241 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.292677 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.298255 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.298512 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.301133 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.304865 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.304921 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.304955 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.304986 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.305047 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.305609 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.305693 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.306050 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.306815 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.309289 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.309916 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.309993 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.310028 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.310086 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.310212 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.310539 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.310583 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.312524 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.312617 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.315286 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.315365 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.315795 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.318432 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.320345 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.320445 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.320741 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.320822 139995557732352 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:04:42.320931 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.320970 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.321002 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.462622 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.465854 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.471798 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.472116 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.474834 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.478767 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.478826 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.478863 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.478896 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.478957 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.479575 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.479652 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.480028 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.480820 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.483412 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.484048 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.484127 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.484162 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.484222 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.484352 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.484700 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.484745 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.486662 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.486757 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.489333 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.489414 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.489862 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.492193 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.494110 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.494219 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.494514 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.494597 139995557732352 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:04:42.494709 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.494748 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.494779 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.496730 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.499127 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.504738 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.505000 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.507703 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.511460 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.511519 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.511557 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.511590 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.511652 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.512228 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.512305 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.512666 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.513444 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.516000 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.516621 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.516699 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.516735 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.516793 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.516921 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.517248 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.517292 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.519192 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.519285 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.521856 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.521936 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.522380 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.524631 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.526587 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.526684 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.526978 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.527065 139995557732352 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:04:42.527178 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.527218 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.527249 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.529268 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.531706 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.537256 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.537516 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.540541 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.544236 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.544292 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.544328 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.544359 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.544422 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.545021 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.545096 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.545459 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.546243 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.548770 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.549391 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.549469 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.549503 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.549561 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.549697 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.550031 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.550076 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.552037 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.552129 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.554687 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.554768 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.555216 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.557528 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.559447 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.559544 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.559837 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.559922 139995557732352 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:04:42.560035 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.560075 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.560106 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.561949 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.564403 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.570075 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.570342 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.572978 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.576738 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.576794 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.576830 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.576863 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.576925 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.577487 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.577563 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.577935 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.578714 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.581199 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.581834 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.581911 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.581945 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.582005 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.582130 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.582456 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.582500 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.584442 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.584534 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.587316 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.587396 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.587831 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.590276 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.592155 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.592249 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.592535 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.592615 139995557732352 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:04:42.592730 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.592769 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.592800 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.594678 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.597036 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.602629 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.602882 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.605489 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:04:42.609236 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.609301 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.609368 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.609412 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.609490 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.610130 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.610212 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.610593 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.611470 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.614045 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.615060 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.615142 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.615179 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.615246 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.615380 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.615732 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.615783 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.617667 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.617761 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.620254 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.620332 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.620813 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.623043 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.624942 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.625036 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.625326 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.625607 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.625706 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.625777 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.625835 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.625889 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.625942 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.625996 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.626049 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.626101 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.626153 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.626204 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.626257 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:04:42.626295 139995557732352 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:04:42.629803 139995557732352 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:04:42.676843 139995557732352 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.676927 139995557732352 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:04:42.676981 139995557732352 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:04:42.677083 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.677121 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.677150 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.677213 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.679617 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.685046 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.685296 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.687934 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:42.704593 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.704653 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.704689 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.704721 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.704784 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.705992 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.706073 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.706786 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.708749 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.713479 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.714803 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.714890 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.714927 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.714987 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.715121 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.715230 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.715270 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.717164 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.717257 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.719693 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.719772 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.719879 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.722092 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.724060 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.724154 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.724444 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.724525 139995557732352 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:04:42.724634 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.724673 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.724704 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.724769 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.727053 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.732507 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.732761 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.735444 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:42.748510 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.748565 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.748601 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.748632 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.748693 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.749246 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.749320 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.749679 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.750376 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.752840 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.753447 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.753521 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.753561 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.753618 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.753759 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.753870 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.753910 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.755821 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.755913 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.758317 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.758394 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.758498 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.760693 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.762639 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.762734 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.763024 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.763106 139995557732352 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:04:42.763214 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.763252 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.763283 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.763345 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.765588 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.770969 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.771224 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.773893 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:42.786358 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.786413 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.786448 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.786478 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.786540 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.787106 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.787182 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.787541 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.788227 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.790717 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.791479 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.791556 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.791590 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.791654 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.791784 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.791893 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.791932 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.794001 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.794095 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.796525 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.796607 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.796717 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.803761 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.805800 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.805911 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.806213 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.806299 139995557732352 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:04:42.806410 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.806452 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.806484 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.806551 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.808880 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.814383 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.814649 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.817402 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:42.830202 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.830260 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.830296 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.830327 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.830388 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.830970 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.831045 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.831410 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.832103 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.834617 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.835233 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.835309 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.835343 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.835400 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.835536 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.835646 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.835684 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.837603 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.837702 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.840106 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.840183 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.840293 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.842518 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.844399 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.844493 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.844781 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.844861 139995557732352 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:04:42.844970 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.845009 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.845039 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.845103 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.847701 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.853150 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.853405 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.856036 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:42.868604 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.868659 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.868694 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.868724 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.868794 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.869367 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.869442 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.869810 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.870501 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.873043 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.873679 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.873755 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.873790 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.873849 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.873982 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.874091 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.874130 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.876032 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.876123 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.878530 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.878609 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.878715 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.880980 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.882864 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.882959 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.883248 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.883328 139995557732352 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:04:42.883435 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.883475 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.883506 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.883569 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.885805 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.891185 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.891434 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.894242 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:42.906811 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.906868 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.906903 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.906932 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.906991 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.907545 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.907621 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.907979 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.908681 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.911170 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.911784 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.911859 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.911892 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.911949 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.912076 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.912189 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.912229 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.914153 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.914246 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.916770 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.916848 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.916967 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.919177 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.921051 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.921145 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.921428 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.921508 139995557732352 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:04:42.921614 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.921664 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.921705 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.921774 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.924014 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.929478 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.929741 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.932340 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:42.944843 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.944897 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.944933 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.944962 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.945022 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.945571 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.945650 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.946013 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.946698 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.949149 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.950146 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.950225 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.950261 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.950320 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.950448 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.950559 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.950602 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.952488 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.952580 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.955002 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.955082 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.955191 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.957390 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.959320 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.959415 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.959706 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.959789 139995557732352 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:04:42.959899 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.959939 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.959970 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.960033 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.962298 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:42.967761 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.968027 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:42.970746 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:42.983387 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:42.983442 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:42.983476 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:42.983506 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.983566 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.984174 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.984250 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.984608 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.985300 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.987775 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.988398 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.988476 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:42.988511 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:42.988569 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.988702 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:42.988811 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:42.988859 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.990718 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.990811 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.993251 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.993329 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:42.993435 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:42.995646 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:42.997519 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.997614 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:42.997909 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:42.997990 139995557732352 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:04:42.998098 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:42.998137 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:42.998168 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:42.998230 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.000446 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.005899 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.006153 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.008753 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.021364 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.021418 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.021453 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.021483 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.021544 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.022105 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.022182 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.022540 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.023228 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.025821 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.026489 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.026565 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.026601 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.026663 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.026791 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.026899 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.026942 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.028798 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.028891 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.031290 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.031369 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.031475 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.033668 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.035598 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.035692 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.035980 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.036060 139995557732352 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:04:43.036168 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.036208 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.036240 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.036301 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.038540 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.043938 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.044196 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.046894 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.059649 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.059708 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.059745 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.059776 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.059838 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.060439 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.060514 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.060869 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.061557 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.064047 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.064667 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.064745 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.064780 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.064839 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.064964 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.065073 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.065112 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.066990 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.067089 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.069545 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.069623 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.069739 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.071951 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.073822 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.073917 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.074207 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.074288 139995557732352 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:04:43.074397 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.074436 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.074467 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.074530 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.076834 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.082305 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.082558 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.085198 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.097717 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.097772 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.097808 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.097838 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.097898 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.098446 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.098520 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.098871 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.099560 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.102040 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.102692 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.102767 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.102802 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.102861 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.102991 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.103103 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.103142 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.105015 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.105112 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.107521 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.107600 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.107709 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.109896 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.111816 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.111910 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.112199 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.112279 139995557732352 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:04:43.112389 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.112429 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.112460 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.112523 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.114752 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.120170 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.120424 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.123271 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.135737 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.135793 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.135829 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.135860 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.135925 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.136478 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.136554 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.136908 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.137651 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.140119 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.140733 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.140808 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.140842 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.140899 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.141021 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.141127 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.141165 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.143021 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.143114 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.145507 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.145588 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.145707 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.147953 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.149809 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.149903 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.150187 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.150278 139995557732352 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:04:43.153134 139995557732352 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:04:43.208051 139995557732352 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.208137 139995557732352 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:04:43.208190 139995557732352 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:04:43.208293 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.208331 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.208361 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.208423 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.211056 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.216376 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.216629 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.219205 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.231306 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.231362 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.231397 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.231428 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.231488 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.232043 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.232118 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.232470 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.233134 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.235625 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.236236 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.236313 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.236348 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.236407 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.236535 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.236651 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.236692 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.238502 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.238595 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.240952 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.241032 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.241139 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.243359 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.245152 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.245247 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.245530 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.245610 139995557732352 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:04:43.245722 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.245762 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.245793 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.245855 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.248036 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.253308 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.253560 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.256167 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.268120 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.268176 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.268211 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.268242 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.268303 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.268850 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.268925 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.269274 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.269959 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.272439 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.273042 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.273118 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.273154 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.273212 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.273338 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.273444 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.273488 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.275309 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.275403 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.277907 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.277989 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.278111 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.280429 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.282255 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.282351 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.282636 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.282717 139995557732352 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:04:43.282824 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.282862 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.282893 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.282956 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.285144 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.290449 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.290699 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.293315 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.305334 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.305393 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.305429 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.305461 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.305524 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.306078 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.306154 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.306504 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.307169 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.309656 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.310269 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.310346 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.310380 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.310439 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.310565 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.310673 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.310712 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.312523 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.312615 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.314988 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.315066 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.315173 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.317832 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.319648 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.319744 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.320031 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.320112 139995557732352 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:04:43.320220 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.320266 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.320296 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.320357 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.322551 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.327838 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.328094 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.330757 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.342858 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.342913 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.342956 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.342995 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.343058 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.343609 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.343683 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.344036 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.344709 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.347213 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.347822 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.347898 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.347932 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.347990 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.348115 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.348222 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.348261 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.350115 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.350206 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.352580 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.352658 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.352764 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.355027 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.356865 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.356958 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.357242 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.357322 139995557732352 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:04:43.357427 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.357464 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.357493 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.357553 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.359762 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.365123 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.365374 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.368064 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.380390 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.380445 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.380479 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.380508 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.380568 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.381119 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.381191 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.381542 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.382229 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.384725 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.385338 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.385415 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.385449 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.385505 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.385629 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.385750 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.385787 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.387630 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.387726 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.390115 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.390193 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.390299 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.392550 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.394387 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.394481 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.394762 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.394842 139995557732352 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:04:43.394949 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.394986 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.395015 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.395075 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.397280 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.402619 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.402869 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.405523 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.417728 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.417782 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.417816 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.417845 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.417904 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.418450 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.418522 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.418878 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.419549 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.422072 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.422681 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.422755 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.422787 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.422843 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.422965 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.423069 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.423105 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.424936 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.425030 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.427395 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.427472 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.427582 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.430240 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.432263 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.432355 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.432639 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.432718 139995557732352 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:04:43.432825 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.432862 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.432892 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.432952 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.435170 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.440605 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.440857 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.443558 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.455862 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.455918 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.455953 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.455984 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.456046 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.456625 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.456702 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.457071 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.457781 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.460297 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.460917 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.460993 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.461027 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.461083 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.461208 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.461318 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.461355 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.463190 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.463282 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.465664 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.465740 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.465845 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.468072 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.469903 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.469997 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.470280 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.470360 139995557732352 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:04:43.470466 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.470504 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.470533 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.470594 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.472774 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.478142 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.478391 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.481020 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.493196 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.493250 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.493284 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.493313 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.493378 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.493940 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.494015 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.494368 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.495036 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.497515 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.498140 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.498216 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.498249 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.498307 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.498433 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.498544 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.498583 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.500409 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.500500 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.502881 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.502962 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.503070 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.505303 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.507122 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.507216 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.507499 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.507578 139995557732352 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:04:43.507683 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.507721 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.507750 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.507810 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.510002 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.515317 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.515569 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.518221 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.530449 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.530503 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.530537 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.530566 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.530626 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.531255 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.531330 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.531684 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.532372 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.534965 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.535580 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.535654 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.535687 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.535743 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.535866 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.535971 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.536007 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.537852 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.537942 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.540293 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.540374 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.540483 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.543120 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.544955 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.545048 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.545335 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.545415 139995557732352 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:04:43.545520 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.545558 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.545588 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.545655 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.547876 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.553254 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.553505 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.556183 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.568493 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.568546 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.568580 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.568608 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.568667 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.569226 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.569301 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.569659 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.570349 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.572869 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.573484 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.573561 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.573594 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.573657 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.573787 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.573894 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.573930 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.576203 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.576294 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.578808 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.578886 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.578998 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.581216 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.583039 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.583132 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.583418 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.583497 139995557732352 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:04:43.583601 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.583638 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.583668 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.583730 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.585964 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.591319 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.591570 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.594244 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.606549 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.606605 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.606639 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.606668 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.606729 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.607279 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.607352 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.607707 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.608383 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.610900 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.611506 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.611582 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.611629 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.611695 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.611824 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.611954 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.611994 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.613896 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.613988 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.616364 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.616441 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.616547 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.618837 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.620669 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.620762 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.621049 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.621131 139995557732352 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:04:43.621238 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:04:43.621276 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:04:43.621306 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:04:43.621366 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.623601 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:04:43.628974 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.629227 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:04:43.631894 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:04:43.644194 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:04:43.644248 139995557732352 attention.py:418] Single window, no scan.
I0123 16:04:43.644281 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:04:43.644309 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.644370 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.644919 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.644993 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.645352 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.646041 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.648551 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.649164 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.649239 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:04:43.649272 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:04:43.649328 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.649451 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:04:43.649557 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:04:43.649594 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.651448 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.651540 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.653923 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.654002 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:04:43.654107 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:04:43.656744 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:04:43.658592 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.658686 139995557732352 nn_components.py:261] mlp: residual
I0123 16:04:43.658973 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:43.659056 139995557732352 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:04:43.661836 139995557732352 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:04:48.098504 139995557732352 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 16:04:48.637218 139995557732352 training_loop.py:409] No working directory specified.
I0123 16:04:48.637334 139995557732352 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 16:04:48.638081 139995557732352 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 16:04:51.829485 139995557732352 training_loop.py:447] Only restoring trainable parameters.
I0123 16:04:51.830240 139995557732352 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 16:04:51.830300 139995557732352 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.830346 139995557732352 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.830389 139995557732352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.830429 139995557732352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.830469 139995557732352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.830508 139995557732352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.830546 139995557732352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.830584 139995557732352 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.830622 139995557732352 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.830660 139995557732352 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.830698 139995557732352 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.830736 139995557732352 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.830774 139995557732352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.830810 139995557732352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.830846 139995557732352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.830882 139995557732352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.830919 139995557732352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.830954 139995557732352 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.830989 139995557732352 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.831039 139995557732352 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831077 139995557732352 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.831115 139995557732352 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.831152 139995557732352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.831189 139995557732352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831225 139995557732352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.831261 139995557732352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831297 139995557732352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831333 139995557732352 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.831368 139995557732352 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.831403 139995557732352 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831439 139995557732352 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.831475 139995557732352 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.831514 139995557732352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.831550 139995557732352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831586 139995557732352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.831622 139995557732352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831658 139995557732352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831694 139995557732352 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.831730 139995557732352 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.831764 139995557732352 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831799 139995557732352 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.831834 139995557732352 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.831869 139995557732352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.831904 139995557732352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.831939 139995557732352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.831979 139995557732352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832016 139995557732352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832052 139995557732352 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.832088 139995557732352 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.832122 139995557732352 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832157 139995557732352 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.832191 139995557732352 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.832227 139995557732352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.832263 139995557732352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832298 139995557732352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.832334 139995557732352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832369 139995557732352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832404 139995557732352 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.832438 139995557732352 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.832473 139995557732352 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832516 139995557732352 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.832551 139995557732352 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.832585 139995557732352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.832620 139995557732352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832656 139995557732352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.832691 139995557732352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832726 139995557732352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832761 139995557732352 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.832795 139995557732352 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.832831 139995557732352 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.832865 139995557732352 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.832901 139995557732352 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.832942 139995557732352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.832979 139995557732352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833015 139995557732352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.833051 139995557732352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833087 139995557732352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833122 139995557732352 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.833156 139995557732352 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.833191 139995557732352 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833225 139995557732352 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.833259 139995557732352 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.833294 139995557732352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.833329 139995557732352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833364 139995557732352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.833400 139995557732352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833436 139995557732352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833471 139995557732352 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.833506 139995557732352 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.833541 139995557732352 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833576 139995557732352 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.833611 139995557732352 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.833656 139995557732352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.833695 139995557732352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833731 139995557732352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.833766 139995557732352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833801 139995557732352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833836 139995557732352 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.833870 139995557732352 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.833910 139995557732352 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.833947 139995557732352 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.833982 139995557732352 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.834017 139995557732352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.834053 139995557732352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.834088 139995557732352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.834124 139995557732352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.834159 139995557732352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.834194 139995557732352 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.834228 139995557732352 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.834263 139995557732352 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.834298 139995557732352 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.834333 139995557732352 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:04:51.834368 139995557732352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:04:51.834403 139995557732352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.834438 139995557732352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.834474 139995557732352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.834508 139995557732352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.834544 139995557732352 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:04:51.834579 139995557732352 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:04:51.834614 139995557732352 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:04:51.834649 139995557732352 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:04:51.834677 139995557732352 training_loop.py:725] Total parameters: 152072288
I0123 16:04:51.834910 139995557732352 training_loop.py:739] Total state size: 0
I0123 16:04:51.856899 139995557732352 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 16:04:51.857183 139995557732352 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 16:04:51.857739 139995557732352 training_loop.py:652] Compiling mode beam_search with jit.
I0123 16:04:51.858083 139995557732352 training_loop.py:89] registering functions: dict_keys([])
I0123 16:04:51.874989 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e ? perp n l l j
I0123 16:04:55.112783 139995557732352 ddar.py:60] Depth 1/1000 time = 3.1815876960754395
I0123 16:05:01.984418 139995557732352 ddar.py:60] Depth 2/1000 time = 6.871467351913452
I0123 16:05:12.110541 139995557732352 ddar.py:60] Depth 3/1000 time = 10.125938653945923
I0123 16:05:24.633953 139995557732352 ddar.py:60] Depth 4/1000 time = 12.523165225982666
I0123 16:05:37.790739 139995557732352 ddar.py:60] Depth 5/1000 time = 13.15643858909607
I0123 16:05:51.045122 139995557732352 ddar.py:60] Depth 6/1000 time = 13.254136800765991
I0123 16:06:04.310425 139995557732352 ddar.py:60] Depth 7/1000 time = 13.265033721923828
I0123 16:06:17.587841 139995557732352 ddar.py:60] Depth 8/1000 time = 13.276472330093384
I0123 16:06:31.677111 139995557732352 ddar.py:60] Depth 9/1000 time = 14.085766315460205
I0123 16:06:47.270423 139995557732352 ddar.py:60] Depth 10/1000 time = 15.59306025505066
I0123 16:07:02.578777 139995557732352 ddar.py:60] Depth 11/1000 time = 15.308089256286621
I0123 16:07:17.895360 139995557732352 ddar.py:60] Depth 12/1000 time = 15.316288232803345
I0123 16:07:33.587126 139995557732352 ddar.py:60] Depth 13/1000 time = 15.465062856674194
I0123 16:07:33.587555 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:07:33.587662 139995557732352 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 16:07:33.587702 139995557732352 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : D a d d e 02 D a e b e 03 ^ b a b e a e a b 04 ; f : C a b f 05 T a b c f 06 ; g : C a e g 07 C c f g 08 ; h : C c f h 09 D c d d h 10 ; i : C a h i 11 P a b g i 12 ; j : D a j g j 13 D a j i j 14 ; k : C d j k 15 T a k d j 16 ; l : C a k l 17 D a k k l 18 ; m : C a b m 19 ^ c a c m c m c b 20 ; n : C a e n 21 C c m n 22 ? T n l l j {F1} x00
I0123 16:07:33.587734 139995557732352 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D a d c d 01 ; e : D a d d e 02 D a e b e 03 ^ b a b e a e a b 04 ; f : C a b f 05 T a b c f 06 ; g : C a e g 07 C c f g 08 ; h : C c f h 09 D c d d h 10 ; i : C a h i 11 P a b g i 12 ; j : D a j g j 13 D a j i j 14 ; k : C d j k 15 T a k d j 16 ; l : C a k l 17 D a k k l 18 ; m : C a b m 19 ^ c a c m c m c b 20 ; n : C a e n 21 C c m n 22 ? T n l l j {F1} x00
I0123 16:07:33.734589 139995557732352 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.734769 139995557732352 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 16:07:33.734869 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.734947 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735020 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735091 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735161 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735231 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735302 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735371 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735440 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735509 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735590 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735661 139995557732352 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:07:33.735702 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:33.735749 139995557732352 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:07:33.735860 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:33.735901 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:33.735933 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:33.737817 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.740419 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:33.746208 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.746485 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:33.749113 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:33.753060 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:33.753119 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:33.753159 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:33.753196 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.753262 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.753890 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.753971 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.754344 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.755127 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.757715 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.758409 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.758491 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:33.758527 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:33.758589 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.758720 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:33.759056 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:33.759101 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.761022 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.761121 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.763635 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.763718 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:33.764155 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:33.766564 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.768548 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.768655 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.768957 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.769041 139995557732352 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:07:33.769152 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:33.769193 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:33.769225 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:33.771078 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.773442 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:33.779274 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.779543 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:33.782161 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:33.786030 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:33.786090 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:33.786127 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:33.786160 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.786224 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.786844 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.786924 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.787290 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.788081 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.790599 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.791230 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.791311 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:33.791346 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:33.791406 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.791537 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:33.791855 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:33.791899 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.793812 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.793906 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.796331 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.796411 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:33.796843 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:33.799203 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.801081 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.801176 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.801470 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.801554 139995557732352 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:07:33.801671 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:33.801712 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:33.801743 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:33.803587 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.805855 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:33.811409 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.811675 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:33.814197 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:33.817819 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:33.817875 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:33.817909 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:33.817940 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.818002 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.818554 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.818630 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.818981 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.819734 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.822190 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.822818 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.822896 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:33.822932 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:33.822988 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.823115 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:33.823793 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:33.823839 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.825726 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.825820 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.828201 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.828281 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:33.828700 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:33.830921 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.832869 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.832963 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.833256 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.833346 139995557732352 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:07:33.833455 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:33.833495 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:33.833525 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:33.835302 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.837581 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:33.843095 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.843350 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:33.845895 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:33.849436 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:33.849493 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:33.849530 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:33.849561 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.849624 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.850232 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.850310 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.850665 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.851418 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.853865 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.854472 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.854553 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:33.854588 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:33.854647 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.854774 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:33.855090 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:33.855136 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.857072 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.857167 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.859611 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.859697 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:33.860133 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:33.862460 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.864413 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.864510 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.864796 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.864879 139995557732352 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:07:33.864996 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:33.865037 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:33.865068 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:33.866979 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.869303 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:33.874946 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.875211 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:33.877875 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:33.881550 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:33.881606 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:33.881652 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:33.881685 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.881748 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.882328 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.882409 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.882782 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.883571 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.886064 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.886704 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.886785 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:33.886822 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:33.886883 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.887015 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:33.887395 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:33.887443 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.889374 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.889469 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.892004 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.892086 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:33.892516 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:33.894788 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.896770 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.896865 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.897157 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.897240 139995557732352 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:07:33.897349 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:33.897396 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:33.897429 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:33.899232 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.901535 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:33.907219 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.907497 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:33.910128 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:33.913768 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:33.913826 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:33.913872 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:33.913904 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.913969 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.914598 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.914681 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.915061 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.915836 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.918339 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.918974 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.919057 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:33.919093 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:33.919155 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.919318 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:33.919650 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:33.919694 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.921621 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.921725 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.924211 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.924291 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:33.924714 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:33.926976 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.928904 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.928999 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.929293 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.929379 139995557732352 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:07:33.929489 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:33.929530 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:33.929572 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:33.931808 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.934198 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:33.939816 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.940071 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:33.942717 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:33.946349 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:33.946408 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:33.946444 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:33.946476 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.946541 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.947109 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.947188 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.947551 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.948340 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.950830 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.951532 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.951612 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:33.951647 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:33.951704 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.951833 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:33.952151 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:33.952195 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.954095 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.954194 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.956680 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.956761 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:33.957187 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:33.959579 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.961483 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.961579 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.961879 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.961968 139995557732352 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:07:33.962087 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:33.962128 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:33.962161 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:33.964044 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.966358 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:33.972084 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.972339 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:33.974906 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:33.978519 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:33.978580 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:33.978617 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:33.978650 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.978715 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.979334 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.979415 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.979797 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.980540 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.983016 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.983646 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.983724 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:33.983758 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:33.983815 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.983938 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:33.984250 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:33.984293 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.986212 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.986312 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.988754 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.988834 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:33.989254 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:33.991519 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:33.993424 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.993519 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:33.993821 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.993904 139995557732352 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:07:33.994010 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:33.994051 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:33.994084 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:33.996000 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:33.998292 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.003905 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.004156 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.006710 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:34.010285 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.010344 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.010381 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.010413 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.010478 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.011042 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.011121 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.011484 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.012265 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.014687 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.015298 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.015377 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.015412 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.015470 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.015596 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.015961 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.016006 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.017899 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.017994 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.020478 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.020558 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.020977 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.023237 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.025196 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.025291 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.025579 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.025669 139995557732352 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:07:34.025779 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.025818 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.025849 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.027606 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.029896 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.035414 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.035666 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.038207 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:34.041749 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.041807 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.041842 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.041874 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.041938 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.042485 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.042563 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.042914 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.044023 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.046447 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.047053 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.047131 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.047165 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.047223 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.047350 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.047664 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.047707 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.049572 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.049673 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.052064 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.052144 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.052563 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.054828 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.056701 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.056797 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.057085 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.057168 139995557732352 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:07:34.057274 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.057313 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.057343 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.059104 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.061371 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.066941 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.067193 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.069729 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:34.073300 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.073356 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.073392 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.073423 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.073483 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.074037 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.074115 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.074466 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.075273 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.077703 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.078314 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.078393 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.078428 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.078485 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.078614 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.078933 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.078978 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.080851 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.080945 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.083358 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.083439 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.083859 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.086131 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.088001 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.088099 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.088395 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.088479 139995557732352 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:07:34.088587 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.088627 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.088657 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.090424 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.092700 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.098269 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.098534 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.101078 139995557732352 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:07:34.104639 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.104696 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.104732 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.104764 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.104825 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.105375 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.105453 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.105816 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.106621 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.109039 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.109656 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.109736 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.109771 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.109829 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.109958 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.110271 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.110316 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.112182 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.112275 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.114681 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.114763 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.115189 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.117452 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.119333 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.119430 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.119714 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.119961 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120032 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120089 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120145 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120200 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120252 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120306 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120370 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120426 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120480 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120532 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120585 139995557732352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:07:34.120621 139995557732352 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:07:34.123470 139995557732352 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:07:34.167300 139995557732352 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.167390 139995557732352 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:07:34.167446 139995557732352 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:07:34.167553 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.167591 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.167621 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.167682 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.170002 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.175271 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.175533 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.178078 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.190551 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.190610 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.190645 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.190677 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.190739 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.191291 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.191368 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.191722 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.192401 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.194921 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.195537 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.195616 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.195652 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.195711 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.195840 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.195948 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.195990 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.197829 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.197924 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.200301 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.200383 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.200492 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.202709 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.204514 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.204611 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.204900 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.204983 139995557732352 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:07:34.205090 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.205130 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.205160 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.205222 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.207416 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.212683 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.212939 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.215551 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.227967 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.228025 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.228061 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.228091 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.228153 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.228699 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.228776 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.229128 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.229859 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.232254 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.232857 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.232936 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.232970 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.233028 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.233154 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.233262 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.233302 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.235129 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.235233 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.237615 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.237704 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.237813 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.240013 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.241835 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.241932 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.242223 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.242305 139995557732352 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:07:34.242413 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.242455 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.242485 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.242549 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.244727 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.250000 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.250254 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.252850 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.264982 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.265039 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.265075 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.265105 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.265166 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.265719 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.265798 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.266156 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.266917 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.269405 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.270023 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.270102 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.270137 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.270195 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.270320 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.270427 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.270465 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.272438 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.272537 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.274965 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.275045 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.275154 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.277391 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.279267 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.279367 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.279661 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.279742 139995557732352 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:07:34.279849 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.279888 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.279918 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.279978 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.282166 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.287519 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.287775 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.290518 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.302729 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.302790 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.302828 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.302859 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.302921 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.303488 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.303568 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.303933 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.304687 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.307198 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.307814 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.307893 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.307929 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.307989 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.308119 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.308231 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.308272 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.310166 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.310265 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.312697 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.312777 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.312884 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.315133 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.316971 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.317070 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.317371 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.317456 139995557732352 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:07:34.317567 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.317609 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.317647 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.317715 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.319977 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.325412 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.325686 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.328373 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.340973 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.341030 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.341066 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.341096 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.341156 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.341714 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.341792 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.342151 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.342910 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.345399 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.346017 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.346099 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.346135 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.346194 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.346325 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.346435 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.346475 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.348371 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.348468 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.350893 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.350980 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.351093 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.353354 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.355237 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.355332 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.355618 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.355701 139995557732352 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:07:34.355809 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.355848 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.355879 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.355941 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.358164 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.363436 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.363695 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.366363 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.378868 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.378926 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.378962 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.378992 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.379053 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.379598 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.379675 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.380030 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.380764 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.383216 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.383819 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.383897 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.383932 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.383989 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.384114 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.384222 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.384259 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.386082 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.386177 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.388537 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.388623 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.388734 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.390958 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.392780 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.392874 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.393162 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.393244 139995557732352 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:07:34.393349 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.393389 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.393419 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.393480 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.395666 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.400926 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.401177 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.403791 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.415835 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.415893 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.415929 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.415960 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.416023 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.416618 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.416698 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.417060 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.417818 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.420301 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.420902 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.420980 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.421014 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.421072 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.421198 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.421306 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.421344 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.423164 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.423258 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.425580 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.425664 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.425782 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.427974 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.429778 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.429874 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.430164 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.430248 139995557732352 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:07:34.430356 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.430397 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.430428 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.430490 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.432691 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.437993 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.438276 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.440893 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.453357 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.453415 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.453451 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.453483 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.453544 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.454104 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.454183 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.454538 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.455265 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.457689 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.458300 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.458379 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.458413 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.458472 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.458598 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.458707 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.458746 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.460537 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.460629 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.462993 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.463074 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.463181 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.465407 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.467238 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.467335 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.467623 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.467706 139995557732352 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:07:34.467813 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.467852 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.467882 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.467942 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.470157 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.475674 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.475927 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.478532 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.490602 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.490660 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.490695 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.490725 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.490787 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.491340 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.491418 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.491775 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.492457 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.494964 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.495571 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.495649 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.495684 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.495740 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.495866 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.495975 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.496014 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.497831 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.497924 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.500254 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.500333 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.500440 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.502657 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.504483 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.504580 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.504868 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.504950 139995557732352 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:07:34.505058 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.505097 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.505127 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.505188 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.507409 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.512743 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.512999 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.515647 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.527664 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.527721 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.527757 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.527788 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.527849 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.528391 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.528468 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.528822 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.529504 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.532017 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.532622 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.532701 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.532736 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.532794 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.532921 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.533030 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.533071 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.534893 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.534988 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.537342 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.537423 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.537531 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.539770 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.541604 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.541707 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.541997 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.542081 139995557732352 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:07:34.542187 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.542225 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.542255 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.542316 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.544514 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.549984 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.550256 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.552937 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.565493 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.565552 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.565587 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.565617 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.565689 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.566248 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.566325 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.566681 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.567354 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.569837 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.570476 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.570554 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.570589 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.570646 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.570778 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.570893 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.570934 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.572828 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.572921 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.575474 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.575555 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.575665 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.577939 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.579772 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.579879 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.580175 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.580259 139995557732352 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:07:34.580368 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.580407 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.580437 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.580499 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.582740 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.588186 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.588443 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.591138 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.603368 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.603425 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.603461 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.603491 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.603553 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.604097 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.604174 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.604523 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.605202 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.607692 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.608304 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.608383 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.608417 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.608475 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.608602 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.608710 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.608749 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.610592 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.610692 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.613113 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.613193 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.613301 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.615599 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.617441 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.617545 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.617850 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.617939 139995557732352 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:07:34.620703 139995557732352 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:07:34.670145 139995557732352 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.670236 139995557732352 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:07:34.670291 139995557732352 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:07:34.670397 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.670437 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.670467 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.670529 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.672847 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.678174 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.678427 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.680983 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.693195 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.693252 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.693288 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.693320 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.693383 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.693944 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.694023 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.694375 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.695230 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.697882 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.698487 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.698564 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.698599 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.698659 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.698788 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.698899 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.698939 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.700752 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.700846 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.703196 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.703283 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.703394 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.705605 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.707479 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.707577 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.707870 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.707953 139995557732352 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:07:34.708061 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.708100 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.708130 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.708192 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.710418 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.715746 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.716001 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.718638 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.730741 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.730799 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.730835 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.730866 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.730927 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.731479 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.731556 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.731913 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.732594 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.735551 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.736170 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.736249 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.736284 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.736342 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.736469 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.736579 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.736618 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.738478 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.738574 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.740924 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.741003 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.741121 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.743344 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.745184 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.745279 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.745570 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.745661 139995557732352 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:07:34.745775 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.745815 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.745846 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.745908 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.748113 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.753386 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.753650 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.756256 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.768328 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.768387 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.768423 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.768455 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.768517 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.769062 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.769140 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.769494 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.770184 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.772664 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.773270 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.773348 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.773383 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.773442 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.773569 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.773684 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.773725 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.775528 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.775622 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.777963 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.778044 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.778153 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.780377 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.782223 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.782319 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.782609 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.782694 139995557732352 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:07:34.782801 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.782840 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.782871 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.782933 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.785128 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.790431 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.790687 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.793293 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.805682 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.805740 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.805776 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.805807 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.805868 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.806415 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.806493 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.806844 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.807510 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.809988 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.810588 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.810667 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.810702 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.810760 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.810889 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.811008 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.811048 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.812881 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.812973 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.815319 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.815401 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.815509 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.817732 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.819563 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.819661 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.819950 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.820033 139995557732352 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:07:34.820140 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.820178 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.820209 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.820270 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.822475 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.827784 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.828042 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.830673 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.842675 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.842734 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.842771 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.842802 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.842864 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.843415 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.843494 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.843848 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.844522 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.847456 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.848060 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.848139 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.848174 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.848230 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.848356 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.848462 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.848500 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.850300 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.850394 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.852728 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.852807 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.852915 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.855135 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.856948 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.857053 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.857348 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.857432 139995557732352 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:07:34.857540 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.857580 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.857611 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.857681 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.859863 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.865219 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.865477 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.868180 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.880359 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.880417 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.880455 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.880486 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.880549 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.881094 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.881171 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.881523 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.882217 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.884784 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.885395 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.885473 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.885508 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.885567 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.885705 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.885817 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.885855 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.887658 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.887751 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.890127 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.890213 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.890325 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.892589 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.894406 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.894511 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.894803 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.894887 139995557732352 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:07:34.894995 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.895034 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.895064 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.895125 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.897451 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.902730 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.902989 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.905601 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.917585 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.917650 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.917689 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.917720 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.917781 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.918328 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.918406 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.918760 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.919429 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.922065 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.922666 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.922746 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.922781 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.922840 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.922968 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.923076 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.923115 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.924928 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.925024 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.927381 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.927464 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.927574 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.929797 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.931621 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.931717 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.932018 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.932104 139995557732352 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:07:34.932212 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.932251 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.932281 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.932343 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.934563 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.939890 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.940149 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.942777 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.954705 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.954763 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.954798 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.954828 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.954889 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.955432 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.955510 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.955862 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.956535 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.959424 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.960027 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.960106 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.960142 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.960200 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.960327 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.960436 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.960475 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.962288 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.962382 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.964718 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.964798 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:34.964905 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:34.967109 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.968915 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.969009 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:34.969310 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.969391 139995557732352 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:07:34.969497 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:34.969536 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:34.969567 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:34.969628 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.971817 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:34.977119 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.977374 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:34.980005 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:34.991928 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:34.991986 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:34.992023 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:34.992053 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.992114 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.992656 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.992733 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.993081 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.993757 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.996220 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.996823 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.996900 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:34.996935 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:34.996992 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.997117 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:34.997225 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:34.997264 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:34.999068 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:34.999160 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:35.001480 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.001559 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:35.001672 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:35.003871 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:35.005705 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.005801 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:35.006095 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.006185 139995557732352 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:07:35.006295 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:35.006335 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:35.006366 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:35.006428 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.008614 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:35.013891 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.014144 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:35.016751 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:35.028869 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:35.028927 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:35.028963 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:35.028994 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.029053 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.029597 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.029684 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.030044 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.030724 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.033193 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.033812 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.033890 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:35.033925 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:35.033982 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.034108 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:35.034216 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:35.034256 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:35.036065 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.036158 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:35.038520 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.038600 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:35.038710 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:35.041091 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:35.042909 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.043005 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:35.043293 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.043382 139995557732352 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:07:35.043491 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:35.043531 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:35.043561 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:35.043624 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.045824 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:35.051082 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.051335 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:35.053971 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:35.066154 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:35.066212 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:35.066249 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:35.066281 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.066341 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.066892 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.066969 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.067321 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.067994 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.070912 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.071525 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.071604 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:35.071640 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:35.071698 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.071825 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:35.071933 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:35.071972 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:35.073797 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.073890 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:35.076223 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.076302 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:35.076409 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:35.078625 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:35.080451 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.080546 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:35.080836 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.080918 139995557732352 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:07:35.081035 139995557732352 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:07:35.081076 139995557732352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:07:35.081106 139995557732352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:07:35.081168 139995557732352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.083354 139995557732352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:07:35.088610 139995557732352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.088867 139995557732352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:07:35.091572 139995557732352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:07:35.103767 139995557732352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:07:35.103826 139995557732352 attention.py:418] Single window, no scan.
I0123 16:07:35.103862 139995557732352 transformer_layer.py:389] tlayer: self-attention.
I0123 16:07:35.103893 139995557732352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.103954 139995557732352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.104507 139995557732352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.104586 139995557732352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.104940 139995557732352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.105616 139995557732352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.108120 139995557732352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.108729 139995557732352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.108806 139995557732352 transformer_layer.py:468] tlayer: End windows.
I0123 16:07:35.108841 139995557732352 transformer_layer.py:472] tlayer: final FFN.
I0123 16:07:35.108898 139995557732352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.109022 139995557732352 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:07:35.109130 139995557732352 nn_components.py:325] mlp: activation = None
I0123 16:07:35.109169 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:35.110986 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.111080 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:35.113418 139995557732352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.113496 139995557732352 transformer_base.py:443] tbase: final FFN
I0123 16:07:35.113602 139995557732352 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:07:35.115814 139995557732352 nn_components.py:329] mlp: final activation = None
I0123 16:07:35.117624 139995557732352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.117727 139995557732352 nn_components.py:261] mlp: residual
I0123 16:07:35.118020 139995557732352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:35.118108 139995557732352 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:07:35.120908 139995557732352 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:07:51.010931 139995557732352 alphageometry.py:566] LM output (score=-1.970718): "o : C g i o 23 D g o i o 24 ;"
I0123 16:07:51.011155 139995557732352 alphageometry.py:567] Translation: "o = on_line o g i, on_bline o i g"

I0123 16:07:51.011209 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_line o g i, on_bline o i g ? perp n l l j"
I0123 16:07:51.011387 139995557732352 graph.py:498] 
I0123 16:07:51.011453 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_line o g i, on_bline o i g ? perp n l l j
I0123 16:07:55.129595 139995557732352 ddar.py:60] Depth 1/1000 time = 4.049906969070435
I0123 16:08:03.803031 139995557732352 ddar.py:60] Depth 2/1000 time = 8.673232078552246
I0123 16:08:23.415501 139995557732352 ddar.py:60] Depth 3/1000 time = 19.612269639968872
I0123 16:08:38.228027 139995557732352 ddar.py:60] Depth 4/1000 time = 14.812281131744385
I0123 16:08:53.542933 139995557732352 ddar.py:60] Depth 5/1000 time = 15.31468915939331
I0123 16:09:08.821122 139995557732352 ddar.py:60] Depth 6/1000 time = 15.277969360351562
I0123 16:09:24.404480 139995557732352 ddar.py:60] Depth 7/1000 time = 15.583149671554565
I0123 16:09:40.260618 139995557732352 ddar.py:60] Depth 8/1000 time = 15.85591435432434
I0123 16:09:56.684799 139995557732352 ddar.py:60] Depth 9/1000 time = 16.423911333084106
I0123 16:10:12.395312 139995557732352 ddar.py:60] Depth 10/1000 time = 15.709677934646606
I0123 16:10:29.722201 139995557732352 ddar.py:60] Depth 11/1000 time = 17.323439359664917
I0123 16:10:48.193674 139995557732352 ddar.py:60] Depth 12/1000 time = 18.471192121505737
I0123 16:11:06.375221 139995557732352 ddar.py:60] Depth 13/1000 time = 18.181262016296387
I0123 16:11:25.370811 139995557732352 ddar.py:60] Depth 14/1000 time = 18.995285034179688
I0123 16:11:43.934833 139995557732352 ddar.py:60] Depth 15/1000 time = 18.326729774475098
I0123 16:12:02.484308 139995557732352 ddar.py:60] Depth 16/1000 time = 18.524796962738037
I0123 16:12:02.484842 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:12:02.485004 139995557732352 alphageometry.py:566] LM output (score=-2.207390): "o : D g o i o 23 ;"
I0123 16:12:02.485045 139995557732352 alphageometry.py:567] Translation: "o = on_bline o i g"

I0123 16:12:02.485106 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_bline o i g ? perp n l l j"
I0123 16:12:02.485309 139995557732352 graph.py:498] 
I0123 16:12:02.485373 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_bline o i g ? perp n l l j
I0123 16:12:06.184756 139995557732352 ddar.py:60] Depth 1/1000 time = 3.630288600921631
I0123 16:12:14.103256 139995557732352 ddar.py:60] Depth 2/1000 time = 7.918329954147339
I0123 16:12:26.374837 139995557732352 ddar.py:60] Depth 3/1000 time = 12.271363258361816
I0123 16:12:40.596755 139995557732352 ddar.py:60] Depth 4/1000 time = 14.221627950668335
I0123 16:12:55.165960 139995557732352 ddar.py:60] Depth 5/1000 time = 14.568859100341797
I0123 16:13:09.573874 139995557732352 ddar.py:60] Depth 6/1000 time = 14.407675981521606
I0123 16:13:24.464376 139995557732352 ddar.py:60] Depth 7/1000 time = 14.890222549438477
I0123 16:13:39.080712 139995557732352 ddar.py:60] Depth 8/1000 time = 14.615459680557251
I0123 16:13:54.985224 139995557732352 ddar.py:60] Depth 9/1000 time = 15.900752544403076
I0123 16:14:12.049107 139995557732352 ddar.py:60] Depth 10/1000 time = 17.063567399978638
I0123 16:14:28.615161 139995557732352 ddar.py:60] Depth 11/1000 time = 16.565610885620117
I0123 16:14:45.450877 139995557732352 ddar.py:60] Depth 12/1000 time = 16.835327625274658
I0123 16:15:02.482771 139995557732352 ddar.py:60] Depth 13/1000 time = 17.020540237426758
I0123 16:15:19.058138 139995557732352 ddar.py:60] Depth 14/1000 time = 16.462511777877808
I0123 16:15:36.087119 139995557732352 ddar.py:60] Depth 15/1000 time = 16.895631551742554
I0123 16:15:36.087598 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:15:36.087746 139995557732352 alphageometry.py:566] LM output (score=-2.209737): "o : C b g o 23 D b o g o 24 ;"
I0123 16:15:36.087790 139995557732352 alphageometry.py:567] Translation: "o = on_line o b g, on_bline o g b"

I0123 16:15:36.087860 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_line o b g, on_bline o g b ? perp n l l j"
I0123 16:15:36.088062 139995557732352 graph.py:498] 
I0123 16:15:36.088125 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_line o b g, on_bline o g b ? perp n l l j
I0123 16:15:39.517549 139995557732352 ddar.py:60] Depth 1/1000 time = 3.3633711338043213
I0123 16:15:46.901090 139995557732352 ddar.py:60] Depth 2/1000 time = 7.38334584236145
I0123 16:15:59.067976 139995557732352 ddar.py:60] Depth 3/1000 time = 12.166638612747192
I0123 16:16:13.132201 139995557732352 ddar.py:60] Depth 4/1000 time = 14.063904047012329
I0123 16:16:27.615190 139995557732352 ddar.py:60] Depth 5/1000 time = 14.482753992080688
I0123 16:16:42.562492 139995557732352 ddar.py:60] Depth 6/1000 time = 14.946964502334595
I0123 16:16:57.677990 139995557732352 ddar.py:60] Depth 7/1000 time = 15.115051984786987
I0123 16:17:12.505401 139995557732352 ddar.py:60] Depth 8/1000 time = 14.826592683792114
I0123 16:17:28.624791 139995557732352 ddar.py:60] Depth 9/1000 time = 16.11589550971985
I0123 16:17:45.822024 139995557732352 ddar.py:60] Depth 10/1000 time = 17.19687557220459
I0123 16:18:03.172560 139995557732352 ddar.py:60] Depth 11/1000 time = 17.35008406639099
I0123 16:18:20.537849 139995557732352 ddar.py:60] Depth 12/1000 time = 17.364870309829712
I0123 16:18:38.595821 139995557732352 ddar.py:60] Depth 13/1000 time = 17.855184316635132
I0123 16:18:56.068042 139995557732352 ddar.py:60] Depth 14/1000 time = 17.4185631275177
I0123 16:18:56.068347 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:18:56.068449 139995557732352 alphageometry.py:566] LM output (score=-2.365249): "o : T h i i o 23 ;"
I0123 16:18:56.068489 139995557732352 alphageometry.py:567] Translation: "o = on_tline o i h i"

I0123 16:18:56.068537 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o i h i ? perp n l l j"
I0123 16:18:56.068743 139995557732352 graph.py:498] 
I0123 16:18:56.068809 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o i h i ? perp n l l j
I0123 16:18:59.356538 139995557732352 ddar.py:60] Depth 1/1000 time = 3.227414846420288
I0123 16:19:07.111472 139995557732352 ddar.py:60] Depth 2/1000 time = 7.75470232963562
I0123 16:19:19.781833 139995557732352 ddar.py:60] Depth 3/1000 time = 12.670071840286255
I0123 16:19:34.619248 139995557732352 ddar.py:60] Depth 4/1000 time = 14.837147235870361
I0123 16:19:49.468554 139995557732352 ddar.py:60] Depth 5/1000 time = 14.848949432373047
I0123 16:20:04.461113 139995557732352 ddar.py:60] Depth 6/1000 time = 14.99229383468628
I0123 16:20:19.590530 139995557732352 ddar.py:60] Depth 7/1000 time = 15.129146575927734
I0123 16:20:34.664131 139995557732352 ddar.py:60] Depth 8/1000 time = 15.072581052780151
I0123 16:20:51.516308 139995557732352 ddar.py:60] Depth 9/1000 time = 16.848158359527588
I0123 16:21:09.458477 139995557732352 ddar.py:60] Depth 10/1000 time = 17.941875219345093
I0123 16:21:27.089631 139995557732352 ddar.py:60] Depth 11/1000 time = 17.630829334259033
I0123 16:21:44.749168 139995557732352 ddar.py:60] Depth 12/1000 time = 17.658893585205078
I0123 16:22:02.307626 139995557732352 ddar.py:60] Depth 13/1000 time = 17.300333499908447
I0123 16:22:02.308084 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:22:02.308216 139995557732352 alphageometry.py:566] LM output (score=-2.404608): "o : T c g g o 23 ;"
I0123 16:22:02.308257 139995557732352 alphageometry.py:567] Translation: "o = on_tline o g c g"

I0123 16:22:02.308309 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o g c g ? perp n l l j"
I0123 16:22:02.308511 139995557732352 graph.py:498] 
I0123 16:22:02.308573 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o g c g ? perp n l l j
I0123 16:22:06.151606 139995557732352 ddar.py:60] Depth 1/1000 time = 3.7692482471466064
I0123 16:22:14.423319 139995557732352 ddar.py:60] Depth 2/1000 time = 8.271525859832764
I0123 16:22:26.961058 139995557732352 ddar.py:60] Depth 3/1000 time = 12.537513017654419
I0123 16:22:40.119987 139995557732352 ddar.py:60] Depth 4/1000 time = 13.15870189666748
I0123 16:22:55.165158 139995557732352 ddar.py:60] Depth 5/1000 time = 15.04492735862732
I0123 16:23:10.192887 139995557732352 ddar.py:60] Depth 6/1000 time = 15.027453660964966
I0123 16:23:25.541362 139995557732352 ddar.py:60] Depth 7/1000 time = 15.348107814788818
I0123 16:23:40.344869 139995557732352 ddar.py:60] Depth 8/1000 time = 14.802428722381592
I0123 16:23:56.446298 139995557732352 ddar.py:60] Depth 9/1000 time = 16.097914457321167
I0123 16:24:13.783868 139995557732352 ddar.py:60] Depth 10/1000 time = 17.33723735809326
I0123 16:24:31.673561 139995557732352 ddar.py:60] Depth 11/1000 time = 17.88920760154724
I0123 16:24:48.864449 139995557732352 ddar.py:60] Depth 12/1000 time = 17.190462112426758
I0123 16:25:06.658238 139995557732352 ddar.py:60] Depth 13/1000 time = 17.560067892074585
I0123 16:25:06.658596 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:25:06.658695 139995557732352 alphageometry.py:566] LM output (score=-2.575894): "o : C b i o 23 D b o i o 24 ;"
I0123 16:25:06.658734 139995557732352 alphageometry.py:567] Translation: "o = on_line o b i, on_bline o i b"

I0123 16:25:06.658778 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_line o b i, on_bline o i b ? perp n l l j"
I0123 16:25:06.658971 139995557732352 graph.py:498] 
I0123 16:25:06.659032 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_line o b i, on_bline o i b ? perp n l l j
I0123 16:25:10.551873 139995557732352 ddar.py:60] Depth 1/1000 time = 3.8271872997283936
I0123 16:25:17.635480 139995557732352 ddar.py:60] Depth 2/1000 time = 7.083329200744629
I0123 16:25:29.417650 139995557732352 ddar.py:60] Depth 3/1000 time = 11.781979084014893
I0123 16:25:43.096786 139995557732352 ddar.py:60] Depth 4/1000 time = 13.678919553756714
I0123 16:25:57.070238 139995557732352 ddar.py:60] Depth 5/1000 time = 13.97315263748169
I0123 16:26:11.469899 139995557732352 ddar.py:60] Depth 6/1000 time = 14.399301052093506
I0123 16:26:25.537330 139995557732352 ddar.py:60] Depth 7/1000 time = 14.067159414291382
I0123 16:26:39.922889 139995557732352 ddar.py:60] Depth 8/1000 time = 14.384774923324585
I0123 16:26:54.665800 139995557732352 ddar.py:60] Depth 9/1000 time = 14.739351272583008
I0123 16:27:11.433365 139995557732352 ddar.py:60] Depth 10/1000 time = 16.76732039451599
I0123 16:27:27.972572 139995557732352 ddar.py:60] Depth 11/1000 time = 16.53891372680664
I0123 16:27:44.932641 139995557732352 ddar.py:60] Depth 12/1000 time = 16.959681510925293
I0123 16:28:01.874745 139995557732352 ddar.py:60] Depth 13/1000 time = 16.713359594345093
I0123 16:28:01.875191 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:28:01.875324 139995557732352 alphageometry.py:566] LM output (score=-2.632347): "o : P h i l o 23 ;"
I0123 16:28:01.875367 139995557732352 alphageometry.py:567] Translation: "o = on_pline o l h i"

I0123 16:28:01.875427 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o l h i ? perp n l l j"
I0123 16:28:01.875636 139995557732352 graph.py:498] 
I0123 16:28:01.875704 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o l h i ? perp n l l j
I0123 16:28:05.107505 139995557732352 ddar.py:60] Depth 1/1000 time = 3.1696550846099854
I0123 16:28:12.890724 139995557732352 ddar.py:60] Depth 2/1000 time = 7.783034801483154
I0123 16:28:24.904960 139995557732352 ddar.py:60] Depth 3/1000 time = 12.013953685760498
I0123 16:28:38.566359 139995557732352 ddar.py:60] Depth 4/1000 time = 13.661037921905518
I0123 16:28:53.519393 139995557732352 ddar.py:60] Depth 5/1000 time = 14.95278024673462
I0123 16:29:07.651438 139995557732352 ddar.py:60] Depth 6/1000 time = 14.131715297698975
I0123 16:29:22.376077 139995557732352 ddar.py:60] Depth 7/1000 time = 14.724223613739014
I0123 16:29:37.228832 139995557732352 ddar.py:60] Depth 8/1000 time = 14.851763725280762
I0123 16:29:52.774205 139995557732352 ddar.py:60] Depth 9/1000 time = 15.54182243347168
I0123 16:30:10.322860 139995557732352 ddar.py:60] Depth 10/1000 time = 17.548068046569824
I0123 16:30:27.869853 139995557732352 ddar.py:60] Depth 11/1000 time = 17.546714782714844
I0123 16:30:45.073659 139995557732352 ddar.py:60] Depth 12/1000 time = 17.203494548797607
I0123 16:31:02.319156 139995557732352 ddar.py:60] Depth 13/1000 time = 17.019957542419434
I0123 16:31:02.319504 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:31:02.319611 139995557732352 alphageometry.py:566] LM output (score=-2.691446): "o : D c d d o 23 ;"
I0123 16:31:02.319647 139995557732352 alphageometry.py:567] Translation: "o = on_circle o d c"

I0123 16:31:02.319697 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_circle o d c ? perp n l l j"
I0123 16:31:02.319887 139995557732352 graph.py:498] 
I0123 16:31:02.319948 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_circle o d c ? perp n l l j
I0123 16:31:06.979486 139995557732352 ddar.py:60] Depth 1/1000 time = 4.597005605697632
I0123 16:31:17.446278 139995557732352 ddar.py:60] Depth 2/1000 time = 10.466541290283203
I0123 16:31:37.915899 139995557732352 ddar.py:60] Depth 3/1000 time = 20.469252347946167
I0123 16:32:01.339775 139995557732352 ddar.py:60] Depth 4/1000 time = 23.42348337173462
I0123 16:32:25.702443 139995557732352 ddar.py:60] Depth 5/1000 time = 24.362274885177612
I0123 16:32:50.133410 139995557732352 ddar.py:60] Depth 6/1000 time = 24.43070387840271
I0123 16:33:13.992998 139995557732352 ddar.py:60] Depth 7/1000 time = 23.85928463935852
I0123 16:33:38.242141 139995557732352 ddar.py:60] Depth 8/1000 time = 24.24815797805786
I0123 16:34:03.318965 139995557732352 ddar.py:60] Depth 9/1000 time = 25.07206416130066
I0123 16:34:30.486881 139995557732352 ddar.py:60] Depth 10/1000 time = 27.16758155822754
I0123 16:34:57.853601 139995557732352 ddar.py:60] Depth 11/1000 time = 27.36628270149231
I0123 16:35:24.829580 139995557732352 ddar.py:60] Depth 12/1000 time = 26.97554612159729
I0123 16:35:52.221547 139995557732352 ddar.py:60] Depth 13/1000 time = 27.021010398864746
I0123 16:35:52.221883 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:35:52.221988 139995557732352 alphageometry.py:566] LM output (score=-2.734814): "o : T c o g i 23 ;"
I0123 16:35:52.222028 139995557732352 alphageometry.py:567] Translation: "o = on_tline o c g i"

I0123 16:35:52.222073 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o c g i ? perp n l l j"
I0123 16:35:52.222273 139995557732352 graph.py:498] 
I0123 16:35:52.222334 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o c g i ? perp n l l j
I0123 16:35:57.761379 139995557732352 ddar.py:60] Depth 1/1000 time = 5.484951734542847
I0123 16:36:07.175025 139995557732352 ddar.py:60] Depth 2/1000 time = 9.413437604904175
I0123 16:36:21.593446 139995557732352 ddar.py:60] Depth 3/1000 time = 14.418180465698242
I0123 16:36:38.991453 139995557732352 ddar.py:60] Depth 4/1000 time = 17.397616863250732
I0123 16:36:55.644224 139995557732352 ddar.py:60] Depth 5/1000 time = 16.65241527557373
I0123 16:37:13.208820 139995557732352 ddar.py:60] Depth 6/1000 time = 17.56434440612793
I0123 16:37:30.457863 139995557732352 ddar.py:60] Depth 7/1000 time = 17.248788118362427
I0123 16:37:47.562865 139995557732352 ddar.py:60] Depth 8/1000 time = 17.104061126708984
I0123 16:38:06.207642 139995557732352 ddar.py:60] Depth 9/1000 time = 18.641201734542847
I0123 16:38:26.488452 139995557732352 ddar.py:60] Depth 10/1000 time = 20.280370235443115
I0123 16:38:46.331768 139995557732352 ddar.py:60] Depth 11/1000 time = 19.842846393585205
I0123 16:39:06.562556 139995557732352 ddar.py:60] Depth 12/1000 time = 20.230339527130127
I0123 16:39:26.638618 139995557732352 ddar.py:60] Depth 13/1000 time = 19.849971532821655
I0123 16:39:26.638920 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:39:26.639032 139995557732352 alphageometry.py:566] LM output (score=-2.890934): "o : T a g o p 23 ;"
I0123 16:39:26.639070 139995557732352 alphageometry.py:567] Translation: "ERROR: point p does not exist."

I0123 16:39:26.639108 139995557732352 alphageometry.py:566] LM output (score=-2.909452): "o : T h i h o 23 ;"
I0123 16:39:26.639134 139995557732352 alphageometry.py:567] Translation: "o = on_tline o h h i"

I0123 16:39:26.639166 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o h h i ? perp n l l j"
I0123 16:39:26.639352 139995557732352 graph.py:498] 
I0123 16:39:26.639412 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o h h i ? perp n l l j
I0123 16:39:29.975239 139995557732352 ddar.py:60] Depth 1/1000 time = 3.2681503295898438
I0123 16:39:37.847225 139995557732352 ddar.py:60] Depth 2/1000 time = 7.871785402297974
I0123 16:39:51.112306 139995557732352 ddar.py:60] Depth 3/1000 time = 13.264829397201538
I0123 16:40:06.452920 139995557732352 ddar.py:60] Depth 4/1000 time = 15.340251922607422
I0123 16:40:21.405877 139995557732352 ddar.py:60] Depth 5/1000 time = 14.952670574188232
I0123 16:40:36.694730 139995557732352 ddar.py:60] Depth 6/1000 time = 15.288499116897583
I0123 16:40:52.593664 139995557732352 ddar.py:60] Depth 7/1000 time = 15.89853024482727
I0123 16:41:07.661300 139995557732352 ddar.py:60] Depth 8/1000 time = 15.066646337509155
I0123 16:41:23.744310 139995557732352 ddar.py:60] Depth 9/1000 time = 16.079112768173218
I0123 16:41:41.915903 139995557732352 ddar.py:60] Depth 10/1000 time = 18.17123770713806
I0123 16:42:00.018005 139995557732352 ddar.py:60] Depth 11/1000 time = 18.10161542892456
I0123 16:42:17.774974 139995557732352 ddar.py:60] Depth 12/1000 time = 17.756502628326416
I0123 16:42:35.391729 139995557732352 ddar.py:60] Depth 13/1000 time = 17.357102155685425
I0123 16:42:35.392042 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:42:35.392143 139995557732352 alphageometry.py:566] LM output (score=-2.910723): "o : P c g e o 23 ;"
I0123 16:42:35.392180 139995557732352 alphageometry.py:567] Translation: "o = on_pline o e c g"

I0123 16:42:35.392222 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o e c g ? perp n l l j"
I0123 16:42:35.392412 139995557732352 graph.py:498] 
I0123 16:42:35.392472 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o e c g ? perp n l l j
I0123 16:42:39.169986 139995557732352 ddar.py:60] Depth 1/1000 time = 3.714491844177246
I0123 16:42:46.320058 139995557732352 ddar.py:60] Depth 2/1000 time = 7.1497883796691895
I0123 16:42:59.699963 139995557732352 ddar.py:60] Depth 3/1000 time = 13.379721403121948
I0123 16:43:15.385815 139995557732352 ddar.py:60] Depth 4/1000 time = 15.685644149780273
I0123 16:43:31.600265 139995557732352 ddar.py:60] Depth 5/1000 time = 16.21421766281128
I0123 16:43:48.006893 139995557732352 ddar.py:60] Depth 6/1000 time = 16.406325578689575
I0123 16:44:03.856930 139995557732352 ddar.py:60] Depth 7/1000 time = 15.849652290344238
I0123 16:44:20.281663 139995557732352 ddar.py:60] Depth 8/1000 time = 16.423770904541016
I0123 16:44:36.691744 139995557732352 ddar.py:60] Depth 9/1000 time = 16.40630030632019
I0123 16:44:56.190756 139995557732352 ddar.py:60] Depth 10/1000 time = 19.498735904693604
I0123 16:45:14.509731 139995557732352 ddar.py:60] Depth 11/1000 time = 18.318575620651245
I0123 16:45:33.429068 139995557732352 ddar.py:60] Depth 12/1000 time = 18.918926239013672
I0123 16:45:52.270283 139995557732352 ddar.py:60] Depth 13/1000 time = 18.63351798057556
I0123 16:46:10.995276 139995557732352 ddar.py:60] Depth 14/1000 time = 18.672693490982056
I0123 16:46:10.995712 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:46:10.995844 139995557732352 alphageometry.py:566] LM output (score=-2.924555): "o : P e o h i 23 ;"
I0123 16:46:10.995883 139995557732352 alphageometry.py:567] Translation: "o = on_pline o e h i"

I0123 16:46:10.995946 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o e h i ? perp n l l j"
I0123 16:46:10.996174 139995557732352 graph.py:498] 
I0123 16:46:10.996235 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o e h i ? perp n l l j
I0123 16:46:14.363127 139995557732352 ddar.py:60] Depth 1/1000 time = 3.300628900527954
I0123 16:46:21.755226 139995557732352 ddar.py:60] Depth 2/1000 time = 7.391902446746826
I0123 16:46:32.921761 139995557732352 ddar.py:60] Depth 3/1000 time = 11.166293144226074
I0123 16:46:46.695296 139995557732352 ddar.py:60] Depth 4/1000 time = 13.773202180862427
I0123 16:47:00.645344 139995557732352 ddar.py:60] Depth 5/1000 time = 13.94977855682373
I0123 16:47:15.107426 139995557732352 ddar.py:60] Depth 6/1000 time = 14.461749076843262
I0123 16:47:29.179314 139995557732352 ddar.py:60] Depth 7/1000 time = 14.071650505065918
I0123 16:47:43.731336 139995557732352 ddar.py:60] Depth 8/1000 time = 14.551148176193237
I0123 16:47:58.840289 139995557732352 ddar.py:60] Depth 9/1000 time = 15.105238437652588
I0123 16:48:15.601996 139995557732352 ddar.py:60] Depth 10/1000 time = 16.761448621749878
I0123 16:48:32.414138 139995557732352 ddar.py:60] Depth 11/1000 time = 16.811872720718384
I0123 16:48:49.340702 139995557732352 ddar.py:60] Depth 12/1000 time = 16.926199913024902
I0123 16:49:05.838896 139995557732352 ddar.py:60] Depth 13/1000 time = 16.275737047195435
I0123 16:49:05.839342 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:49:05.839472 139995557732352 alphageometry.py:566] LM output (score=-2.970483): "o : T e g g o 23 ;"
I0123 16:49:05.839509 139995557732352 alphageometry.py:567] Translation: "o = on_tline o g e g"

I0123 16:49:05.839563 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o g e g ? perp n l l j"
I0123 16:49:05.839776 139995557732352 graph.py:498] 
I0123 16:49:05.839837 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o g e g ? perp n l l j
I0123 16:49:09.729851 139995557732352 ddar.py:60] Depth 1/1000 time = 3.8198211193084717
I0123 16:49:17.195312 139995557732352 ddar.py:60] Depth 2/1000 time = 7.465276718139648
I0123 16:49:30.665807 139995557732352 ddar.py:60] Depth 3/1000 time = 13.470231771469116
I0123 16:49:46.872473 139995557732352 ddar.py:60] Depth 4/1000 time = 16.206254720687866
I0123 16:50:03.200087 139995557732352 ddar.py:60] Depth 5/1000 time = 16.327245473861694
I0123 16:50:18.911792 139995557732352 ddar.py:60] Depth 6/1000 time = 15.71143651008606
I0123 16:50:34.603173 139995557732352 ddar.py:60] Depth 7/1000 time = 15.6911039352417
I0123 16:50:50.638607 139995557732352 ddar.py:60] Depth 8/1000 time = 16.03447198867798
I0123 16:51:08.141341 139995557732352 ddar.py:60] Depth 9/1000 time = 17.498889446258545
I0123 16:51:26.466895 139995557732352 ddar.py:60] Depth 10/1000 time = 18.32525134086609
I0123 16:51:45.333437 139995557732352 ddar.py:60] Depth 11/1000 time = 18.866231441497803
I0123 16:52:03.263447 139995557732352 ddar.py:60] Depth 12/1000 time = 17.92967414855957
I0123 16:52:21.720384 139995557732352 ddar.py:60] Depth 13/1000 time = 18.197845697402954
I0123 16:52:21.720870 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:52:21.721029 139995557732352 alphageometry.py:566] LM output (score=-2.972486): "o : T a g i o 23 ;"
I0123 16:52:21.721070 139995557732352 alphageometry.py:567] Translation: "o = on_tline o i a g"

I0123 16:52:21.721124 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o i a g ? perp n l l j"
I0123 16:52:21.721354 139995557732352 graph.py:498] 
I0123 16:52:21.721433 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o i a g ? perp n l l j
I0123 16:52:25.707104 139995557732352 ddar.py:60] Depth 1/1000 time = 3.908600330352783
I0123 16:52:34.117222 139995557732352 ddar.py:60] Depth 2/1000 time = 8.40993356704712
I0123 16:52:48.245412 139995557732352 ddar.py:60] Depth 3/1000 time = 14.127969026565552
I0123 16:53:03.581194 139995557732352 ddar.py:60] Depth 4/1000 time = 15.33545207977295
I0123 16:53:19.883890 139995557732352 ddar.py:60] Depth 5/1000 time = 16.30231475830078
I0123 16:53:35.750895 139995557732352 ddar.py:60] Depth 6/1000 time = 15.86673378944397
I0123 16:53:51.653395 139995557732352 ddar.py:60] Depth 7/1000 time = 15.90222454071045
I0123 16:54:07.407009 139995557732352 ddar.py:60] Depth 8/1000 time = 15.75257134437561
I0123 16:54:24.896061 139995557732352 ddar.py:60] Depth 9/1000 time = 17.484902143478394
I0123 16:54:43.303148 139995557732352 ddar.py:60] Depth 10/1000 time = 18.40669870376587
I0123 16:55:00.956506 139995557732352 ddar.py:60] Depth 11/1000 time = 17.65306568145752
I0123 16:55:18.637158 139995557732352 ddar.py:60] Depth 12/1000 time = 17.680352210998535
I0123 16:55:37.245180 139995557732352 ddar.py:60] Depth 13/1000 time = 18.351027250289917
I0123 16:55:37.245496 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:55:37.245597 139995557732352 alphageometry.py:566] LM output (score=-3.014105): "o : T m n m o 23 ;"
I0123 16:55:37.245634 139995557732352 alphageometry.py:567] Translation: "o = on_tline o m m n"

I0123 16:55:37.245684 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o m m n ? perp n l l j"
I0123 16:55:37.245870 139995557732352 graph.py:498] 
I0123 16:55:37.245930 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o m m n ? perp n l l j
I0123 16:55:41.165663 139995557732352 ddar.py:60] Depth 1/1000 time = 3.8588082790374756
I0123 16:55:48.686782 139995557732352 ddar.py:60] Depth 2/1000 time = 7.520884037017822
I0123 16:56:00.304675 139995557732352 ddar.py:60] Depth 3/1000 time = 11.6175856590271
I0123 16:56:13.839497 139995557732352 ddar.py:60] Depth 4/1000 time = 13.534580707550049
I0123 16:56:27.785945 139995557732352 ddar.py:60] Depth 5/1000 time = 13.946177244186401
I0123 16:56:42.256590 139995557732352 ddar.py:60] Depth 6/1000 time = 14.470424175262451
I0123 16:56:57.353462 139995557732352 ddar.py:60] Depth 7/1000 time = 15.096622943878174
I0123 16:57:11.441998 139995557732352 ddar.py:60] Depth 8/1000 time = 14.087671279907227
I0123 16:57:27.621488 139995557732352 ddar.py:60] Depth 9/1000 time = 16.17498540878296
I0123 16:57:45.968991 139995557732352 ddar.py:60] Depth 10/1000 time = 18.347217082977295
I0123 16:58:04.124767 139995557732352 ddar.py:60] Depth 11/1000 time = 18.15546464920044
I0123 16:58:21.764979 139995557732352 ddar.py:60] Depth 12/1000 time = 17.639917850494385
I0123 16:58:39.352203 139995557732352 ddar.py:60] Depth 13/1000 time = 17.36522936820984
I0123 16:58:39.361115 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:58:39.361257 139995557732352 alphageometry.py:566] LM output (score=-3.071823): "o : T c g l o 23 ;"
I0123 16:58:39.361306 139995557732352 alphageometry.py:567] Translation: "o = on_tline o l c g"

I0123 16:58:39.361362 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o l c g ? perp n l l j"
I0123 16:58:39.361575 139995557732352 graph.py:498] 
I0123 16:58:39.361636 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o l c g ? perp n l l j
I0123 16:58:43.548397 139995557732352 ddar.py:60] Depth 1/1000 time = 4.100894212722778
I0123 16:58:50.418613 139995557732352 ddar.py:60] Depth 2/1000 time = 6.870046138763428
I0123 16:59:02.728562 139995557732352 ddar.py:60] Depth 3/1000 time = 12.309749841690063
I0123 16:59:16.044353 139995557732352 ddar.py:60] Depth 4/1000 time = 13.315552711486816
I0123 16:59:29.851633 139995557732352 ddar.py:60] Depth 5/1000 time = 13.806969404220581
I0123 16:59:44.878262 139995557732352 ddar.py:60] Depth 6/1000 time = 15.026394367218018
I0123 16:59:58.796729 139995557732352 ddar.py:60] Depth 7/1000 time = 13.918182849884033
I0123 17:00:13.255385 139995557732352 ddar.py:60] Depth 8/1000 time = 14.45772409439087
I0123 17:00:28.521321 139995557732352 ddar.py:60] Depth 9/1000 time = 15.262593746185303
I0123 17:00:45.127411 139995557732352 ddar.py:60] Depth 10/1000 time = 16.605844974517822
I0123 17:01:01.568006 139995557732352 ddar.py:60] Depth 11/1000 time = 16.440276384353638
I0123 17:01:18.738920 139995557732352 ddar.py:60] Depth 12/1000 time = 17.17055916786194
I0123 17:01:35.633711 139995557732352 ddar.py:60] Depth 13/1000 time = 16.66071629524231
I0123 17:01:35.634017 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:01:35.634113 139995557732352 alphageometry.py:566] LM output (score=-3.074022): "o : T b g b o 23 ;"
I0123 17:01:35.634148 139995557732352 alphageometry.py:567] Translation: "o = on_tline o b b g"

I0123 17:01:35.634190 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o b b g ? perp n l l j"
I0123 17:01:35.634370 139995557732352 graph.py:498] 
I0123 17:01:35.634432 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o b b g ? perp n l l j
I0123 17:01:39.189210 139995557732352 ddar.py:60] Depth 1/1000 time = 3.4870002269744873
I0123 17:01:47.562160 139995557732352 ddar.py:60] Depth 2/1000 time = 8.372758150100708
I0123 17:01:59.405540 139995557732352 ddar.py:60] Depth 3/1000 time = 11.843188047409058
I0123 17:02:13.529897 139995557732352 ddar.py:60] Depth 4/1000 time = 14.12411117553711
I0123 17:02:28.763757 139995557732352 ddar.py:60] Depth 5/1000 time = 15.233621597290039
I0123 17:02:44.005834 139995557732352 ddar.py:60] Depth 6/1000 time = 15.241825342178345
I0123 17:02:58.628917 139995557732352 ddar.py:60] Depth 7/1000 time = 14.62283444404602
I0123 17:03:13.860984 139995557732352 ddar.py:60] Depth 8/1000 time = 15.231321334838867
I0123 17:03:30.130818 139995557732352 ddar.py:60] Depth 9/1000 time = 16.265966653823853
I0123 17:03:48.385293 139995557732352 ddar.py:60] Depth 10/1000 time = 18.254016876220703
I0123 17:04:06.049685 139995557732352 ddar.py:60] Depth 11/1000 time = 17.663964986801147
I0123 17:04:23.819661 139995557732352 ddar.py:60] Depth 12/1000 time = 17.769691705703735
I0123 17:04:41.730373 139995557732352 ddar.py:60] Depth 13/1000 time = 17.669678926467896
I0123 17:04:41.730679 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:04:41.730779 139995557732352 alphageometry.py:566] LM output (score=-3.074162): "o : T c o e g 23 ;"
I0123 17:04:41.730815 139995557732352 alphageometry.py:567] Translation: "o = on_tline o c e g"

I0123 17:04:41.730859 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o c e g ? perp n l l j"
I0123 17:04:41.731045 139995557732352 graph.py:498] 
I0123 17:04:41.731106 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o c e g ? perp n l l j
I0123 17:04:45.347084 139995557732352 ddar.py:60] Depth 1/1000 time = 3.5320308208465576
I0123 17:04:54.045570 139995557732352 ddar.py:60] Depth 2/1000 time = 8.698304414749146
I0123 17:05:07.042824 139995557732352 ddar.py:60] Depth 3/1000 time = 12.99704647064209
I0123 17:05:23.806335 139995557732352 ddar.py:60] Depth 4/1000 time = 16.763272047042847
I0123 17:05:40.239340 139995557732352 ddar.py:60] Depth 5/1000 time = 16.432706832885742
I0123 17:05:56.691310 139995557732352 ddar.py:60] Depth 6/1000 time = 16.45165729522705
I0123 17:06:12.475615 139995557732352 ddar.py:60] Depth 7/1000 time = 15.783949851989746
I0123 17:06:28.824465 139995557732352 ddar.py:60] Depth 8/1000 time = 16.347655534744263
I0123 17:06:46.278532 139995557732352 ddar.py:60] Depth 9/1000 time = 17.449687480926514
I0123 17:07:04.975476 139995557732352 ddar.py:60] Depth 10/1000 time = 18.696636199951172
I0123 17:07:23.955868 139995557732352 ddar.py:60] Depth 11/1000 time = 18.98006796836853
I0123 17:07:43.034560 139995557732352 ddar.py:60] Depth 12/1000 time = 19.078376054763794
I0123 17:08:01.327467 139995557732352 ddar.py:60] Depth 13/1000 time = 18.033721923828125
I0123 17:08:01.327939 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:08:01.328073 139995557732352 alphageometry.py:566] LM output (score=-3.129311): "o : T e g i o 23 ;"
I0123 17:08:01.328111 139995557732352 alphageometry.py:567] Translation: "o = on_tline o i e g"

I0123 17:08:01.328166 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o i e g ? perp n l l j"
I0123 17:08:01.328379 139995557732352 graph.py:498] 
I0123 17:08:01.328439 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o i e g ? perp n l l j
I0123 17:08:04.978996 139995557732352 ddar.py:60] Depth 1/1000 time = 3.593888521194458
I0123 17:08:13.023071 139995557732352 ddar.py:60] Depth 2/1000 time = 8.0438973903656
I0123 17:08:26.709991 139995557732352 ddar.py:60] Depth 3/1000 time = 13.68669581413269
I0123 17:08:44.064345 139995557732352 ddar.py:60] Depth 4/1000 time = 17.354017734527588
I0123 17:08:59.975351 139995557732352 ddar.py:60] Depth 5/1000 time = 15.910534620285034
I0123 17:09:17.029668 139995557732352 ddar.py:60] Depth 6/1000 time = 17.05393886566162
I0123 17:09:33.495317 139995557732352 ddar.py:60] Depth 7/1000 time = 16.465383052825928
I0123 17:09:50.187424 139995557732352 ddar.py:60] Depth 8/1000 time = 16.691138982772827
I0123 17:10:07.922729 139995557732352 ddar.py:60] Depth 9/1000 time = 17.73127579689026
I0123 17:10:26.618289 139995557732352 ddar.py:60] Depth 10/1000 time = 18.695211172103882
I0123 17:10:45.302806 139995557732352 ddar.py:60] Depth 11/1000 time = 18.684009552001953
I0123 17:11:04.590635 139995557732352 ddar.py:60] Depth 12/1000 time = 19.28738760948181
I0123 17:11:23.565798 139995557732352 ddar.py:60] Depth 13/1000 time = 18.718270778656006
I0123 17:11:23.566122 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:11:23.566251 139995557732352 alphageometry.py:566] LM output (score=-3.145633): "o : T a g g o 23 ;"
I0123 17:11:23.566293 139995557732352 alphageometry.py:567] Translation: "o = on_tline o g a g"

I0123 17:11:23.566339 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o g a g ? perp n l l j"
I0123 17:11:23.566554 139995557732352 graph.py:498] 
I0123 17:11:23.566619 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o g a g ? perp n l l j
I0123 17:11:27.317933 139995557732352 ddar.py:60] Depth 1/1000 time = 3.677013397216797
I0123 17:11:35.417902 139995557732352 ddar.py:60] Depth 2/1000 time = 8.099759340286255
I0123 17:11:49.741519 139995557732352 ddar.py:60] Depth 3/1000 time = 14.323400497436523
I0123 17:12:05.891966 139995557732352 ddar.py:60] Depth 4/1000 time = 16.150155782699585
I0123 17:12:21.579269 139995557732352 ddar.py:60] Depth 5/1000 time = 15.686957120895386
I0123 17:12:37.882809 139995557732352 ddar.py:60] Depth 6/1000 time = 16.303271293640137
I0123 17:12:54.163447 139995557732352 ddar.py:60] Depth 7/1000 time = 16.280279636383057
I0123 17:13:10.559085 139995557732352 ddar.py:60] Depth 8/1000 time = 16.394433975219727
I0123 17:13:27.427610 139995557732352 ddar.py:60] Depth 9/1000 time = 16.86446523666382
I0123 17:13:46.674589 139995557732352 ddar.py:60] Depth 10/1000 time = 19.246698141098022
I0123 17:14:04.560090 139995557732352 ddar.py:60] Depth 11/1000 time = 17.885220289230347
I0123 17:14:23.605157 139995557732352 ddar.py:60] Depth 12/1000 time = 19.04475426673889
I0123 17:14:42.431072 139995557732352 ddar.py:60] Depth 13/1000 time = 18.571099281311035
I0123 17:14:42.431401 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:14:42.431512 139995557732352 alphageometry.py:566] LM output (score=-3.151426): "o : T c h c o 23 ;"
I0123 17:14:42.431548 139995557732352 alphageometry.py:567] Translation: "o = on_tline o c c h"

I0123 17:14:42.431594 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o c c h ? perp n l l j"
I0123 17:14:42.431803 139995557732352 graph.py:498] 
I0123 17:14:42.431866 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o c c h ? perp n l l j
I0123 17:14:46.212390 139995557732352 ddar.py:60] Depth 1/1000 time = 3.721672773361206
I0123 17:14:53.440786 139995557732352 ddar.py:60] Depth 2/1000 time = 7.228163242340088
I0123 17:15:06.067336 139995557732352 ddar.py:60] Depth 3/1000 time = 12.626250505447388
I0123 17:15:19.865900 139995557732352 ddar.py:60] Depth 4/1000 time = 13.798302173614502
I0123 17:15:34.467676 139995557732352 ddar.py:60] Depth 5/1000 time = 14.6014084815979
I0123 17:15:49.044295 139995557732352 ddar.py:60] Depth 6/1000 time = 14.576260566711426
I0123 17:16:04.267970 139995557732352 ddar.py:60] Depth 7/1000 time = 15.223406076431274
I0123 17:16:18.787716 139995557732352 ddar.py:60] Depth 8/1000 time = 14.518718719482422
I0123 17:16:34.923082 139995557732352 ddar.py:60] Depth 9/1000 time = 16.131291151046753
I0123 17:16:52.378927 139995557732352 ddar.py:60] Depth 10/1000 time = 17.455610752105713
I0123 17:17:09.872279 139995557732352 ddar.py:60] Depth 11/1000 time = 17.49308443069458
I0123 17:17:26.721004 139995557732352 ddar.py:60] Depth 12/1000 time = 16.848448038101196
I0123 17:17:44.212408 139995557732352 ddar.py:60] Depth 13/1000 time = 17.260168075561523
I0123 17:17:44.212885 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:17:44.213022 139995557732352 alphageometry.py:566] LM output (score=-3.156569): "o : T b h h o 23 ;"
I0123 17:17:44.213059 139995557732352 alphageometry.py:567] Translation: "o = on_tline o h b h"

I0123 17:17:44.213114 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o h b h ? perp n l l j"
I0123 17:17:44.213323 139995557732352 graph.py:498] 
I0123 17:17:44.213384 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o h b h ? perp n l l j
I0123 17:17:47.369547 139995557732352 ddar.py:60] Depth 1/1000 time = 3.097895383834839
I0123 17:17:55.959976 139995557732352 ddar.py:60] Depth 2/1000 time = 8.590243577957153
I0123 17:18:08.274646 139995557732352 ddar.py:60] Depth 3/1000 time = 12.31444001197815
I0123 17:18:22.134923 139995557732352 ddar.py:60] Depth 4/1000 time = 13.859984397888184
I0123 17:18:37.941249 139995557732352 ddar.py:60] Depth 5/1000 time = 15.805958032608032
I0123 17:18:53.173423 139995557732352 ddar.py:60] Depth 6/1000 time = 15.231915473937988
I0123 17:19:08.404188 139995557732352 ddar.py:60] Depth 7/1000 time = 15.230501651763916
I0123 17:19:23.461778 139995557732352 ddar.py:60] Depth 8/1000 time = 15.05623722076416
I0123 17:19:39.546160 139995557732352 ddar.py:60] Depth 9/1000 time = 16.080431938171387
I0123 17:19:57.574451 139995557732352 ddar.py:60] Depth 10/1000 time = 18.02801823616028
I0123 17:20:14.863304 139995557732352 ddar.py:60] Depth 11/1000 time = 17.28856062889099
I0123 17:20:32.771589 139995557732352 ddar.py:60] Depth 12/1000 time = 17.907923936843872
I0123 17:20:50.633129 139995557732352 ddar.py:60] Depth 13/1000 time = 17.600258350372314
I0123 17:20:50.633596 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:20:50.633741 139995557732352 alphageometry.py:566] LM output (score=-3.164401): "o : T g i i o 23 ;"
I0123 17:20:50.633780 139995557732352 alphageometry.py:567] Translation: "o = on_tline o i g i"

I0123 17:20:50.633836 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o i g i ? perp n l l j"
I0123 17:20:50.634048 139995557732352 graph.py:498] 
I0123 17:20:50.634107 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o i g i ? perp n l l j
I0123 17:20:55.196155 139995557732352 ddar.py:60] Depth 1/1000 time = 4.501941919326782
I0123 17:21:03.380231 139995557732352 ddar.py:60] Depth 2/1000 time = 8.183903932571411
I0123 17:21:15.163948 139995557732352 ddar.py:60] Depth 3/1000 time = 11.783528566360474
I0123 17:21:27.988525 139995557732352 ddar.py:60] Depth 4/1000 time = 12.824321985244751
I0123 17:21:43.283210 139995557732352 ddar.py:60] Depth 5/1000 time = 15.294366359710693
I0123 17:21:57.973492 139995557732352 ddar.py:60] Depth 6/1000 time = 14.690053939819336
I0123 17:22:12.633529 139995557732352 ddar.py:60] Depth 7/1000 time = 14.659743309020996
I0123 17:22:27.361090 139995557732352 ddar.py:60] Depth 8/1000 time = 14.72655963897705
I0123 17:22:43.044113 139995557732352 ddar.py:60] Depth 9/1000 time = 15.679695844650269
I0123 17:22:59.965923 139995557732352 ddar.py:60] Depth 10/1000 time = 16.9215304851532
I0123 17:23:17.475073 139995557732352 ddar.py:60] Depth 11/1000 time = 17.50877809524536
I0123 17:23:35.069881 139995557732352 ddar.py:60] Depth 12/1000 time = 17.594370126724243
I0123 17:23:52.838385 139995557732352 ddar.py:60] Depth 13/1000 time = 17.540107011795044
I0123 17:23:52.838660 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:23:52.838766 139995557732352 alphageometry.py:566] LM output (score=-3.205575): "o : P a o b c 23 ;"
I0123 17:23:52.838803 139995557732352 alphageometry.py:567] Translation: "o = on_pline o a b c"

I0123 17:23:52.838845 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o a b c ? perp n l l j"
I0123 17:23:52.839041 139995557732352 graph.py:498] 
I0123 17:23:52.839103 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o a b c ? perp n l l j
I0123 17:23:55.990904 139995557732352 ddar.py:60] Depth 1/1000 time = 3.0873279571533203
I0123 17:24:03.258285 139995557732352 ddar.py:60] Depth 2/1000 time = 7.267172574996948
I0123 17:24:14.769413 139995557732352 ddar.py:60] Depth 3/1000 time = 11.510906219482422
I0123 17:24:28.137910 139995557732352 ddar.py:60] Depth 4/1000 time = 13.368192434310913
I0123 17:24:43.038598 139995557732352 ddar.py:60] Depth 5/1000 time = 14.900436878204346
I0123 17:24:57.295028 139995557732352 ddar.py:60] Depth 6/1000 time = 14.256119012832642
I0123 17:25:11.757974 139995557732352 ddar.py:60] Depth 7/1000 time = 14.462565422058105
I0123 17:25:26.041851 139995557732352 ddar.py:60] Depth 8/1000 time = 14.283048152923584
I0123 17:25:41.417410 139995557732352 ddar.py:60] Depth 9/1000 time = 15.372177124023438
I0123 17:25:58.119979 139995557732352 ddar.py:60] Depth 10/1000 time = 16.702134132385254
I0123 17:26:14.628994 139995557732352 ddar.py:60] Depth 11/1000 time = 16.508639335632324
I0123 17:26:31.806281 139995557732352 ddar.py:60] Depth 12/1000 time = 17.17701768875122
I0123 17:26:49.175035 139995557732352 ddar.py:60] Depth 13/1000 time = 17.141069173812866
I0123 17:26:49.175363 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:26:49.175469 139995557732352 alphageometry.py:566] LM output (score=-3.249151): "o : P c o e g 23 ;"
I0123 17:26:49.175506 139995557732352 alphageometry.py:567] Translation: "o = on_pline o c e g"

I0123 17:26:49.175553 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o c e g ? perp n l l j"
I0123 17:26:49.175738 139995557732352 graph.py:498] 
I0123 17:26:49.175799 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o c e g ? perp n l l j
I0123 17:26:53.024456 139995557732352 ddar.py:60] Depth 1/1000 time = 3.789987564086914
I0123 17:27:01.220193 139995557732352 ddar.py:60] Depth 2/1000 time = 8.19551396369934
I0123 17:27:12.979897 139995557732352 ddar.py:60] Depth 3/1000 time = 11.759402990341187
I0123 17:27:26.505466 139995557732352 ddar.py:60] Depth 4/1000 time = 13.525301218032837
I0123 17:27:41.420590 139995557732352 ddar.py:60] Depth 5/1000 time = 14.914772748947144
I0123 17:27:56.496243 139995557732352 ddar.py:60] Depth 6/1000 time = 15.075403213500977
I0123 17:28:10.775680 139995557732352 ddar.py:60] Depth 7/1000 time = 14.279151916503906
I0123 17:28:26.426644 139995557732352 ddar.py:60] Depth 8/1000 time = 15.649971961975098
I0123 17:28:42.456234 139995557732352 ddar.py:60] Depth 9/1000 time = 16.025650024414062
I0123 17:29:00.606145 139995557732352 ddar.py:60] Depth 10/1000 time = 18.14964509010315
I0123 17:29:17.850962 139995557732352 ddar.py:60] Depth 11/1000 time = 17.24453067779541
I0123 17:29:35.967750 139995557732352 ddar.py:60] Depth 12/1000 time = 18.11647319793701
I0123 17:29:53.344743 139995557732352 ddar.py:60] Depth 13/1000 time = 17.1498544216156
I0123 17:29:53.345190 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:29:53.345331 139995557732352 alphageometry.py:566] LM output (score=-3.249434): "o : P c h g o 23 ;"
I0123 17:29:53.345372 139995557732352 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2570, in add_clause
    raise DepCheckFailError(
graph.DepCheckFailError: ncoll g c h
"

I0123 17:29:53.345421 139995557732352 alphageometry.py:566] LM output (score=-3.250247): "o : P g h n o 23 ;"
I0123 17:29:53.345448 139995557732352 alphageometry.py:567] Translation: "o = on_pline o n g h"

I0123 17:29:53.345481 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o n g h ? perp n l l j"
I0123 17:29:53.345715 139995557732352 graph.py:498] 
I0123 17:29:53.345778 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_pline o n g h ? perp n l l j
I0123 17:29:57.317109 139995557732352 ddar.py:60] Depth 1/1000 time = 3.9083383083343506
I0123 17:30:04.882232 139995557732352 ddar.py:60] Depth 2/1000 time = 7.5649518966674805
I0123 17:30:17.284052 139995557732352 ddar.py:60] Depth 3/1000 time = 12.401608943939209
I0123 17:30:32.070376 139995557732352 ddar.py:60] Depth 4/1000 time = 14.786026000976562
I0123 17:30:46.866374 139995557732352 ddar.py:60] Depth 5/1000 time = 14.795628309249878
I0123 17:31:02.559810 139995557732352 ddar.py:60] Depth 6/1000 time = 15.693176746368408
I0123 17:31:16.794506 139995557732352 ddar.py:60] Depth 7/1000 time = 14.234365940093994
I0123 17:31:31.710613 139995557732352 ddar.py:60] Depth 8/1000 time = 14.914946794509888
I0123 17:31:47.471921 139995557732352 ddar.py:60] Depth 9/1000 time = 15.757765769958496
I0123 17:32:05.435027 139995557732352 ddar.py:60] Depth 10/1000 time = 17.96285629272461
I0123 17:32:22.483587 139995557732352 ddar.py:60] Depth 11/1000 time = 17.048266887664795
I0123 17:32:40.371419 139995557732352 ddar.py:60] Depth 12/1000 time = 17.887439250946045
I0123 17:32:57.669134 139995557732352 ddar.py:60] Depth 13/1000 time = 17.06403923034668
I0123 17:32:57.669737 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:32:57.669880 139995557732352 alphageometry.py:566] LM output (score=-3.326705): "o : T c o i j 23 ;"
I0123 17:32:57.669918 139995557732352 alphageometry.py:567] Translation: "o = on_tline o c i j"

I0123 17:32:57.669972 139995557732352 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o c i j ? perp n l l j"
I0123 17:32:57.670179 139995557732352 graph.py:498] 
I0123 17:32:57.670241 139995557732352 graph.py:499] a b c = triangle a b c; d = circle d c a b; e = on_circle e d a, on_bline e a b; f = foot f c a b; g = on_line g c f, on_line g a e; h = on_circle h d c, on_line h f c; i = on_pline i g b a, on_line i a h; j = circle j g a i; k = foot k a d j; l = mirror l a k; m = angle_bisector m a c b, on_line m a b; n = on_line n c m, on_line n a e; o = on_tline o c i j ? perp n l l j
I0123 17:33:01.599316 139995557732352 ddar.py:60] Depth 1/1000 time = 3.8711843490600586
I0123 17:33:09.022212 139995557732352 ddar.py:60] Depth 2/1000 time = 7.42271614074707
I0123 17:33:21.245172 139995557732352 ddar.py:60] Depth 3/1000 time = 12.22271203994751
I0123 17:33:35.672033 139995557732352 ddar.py:60] Depth 4/1000 time = 14.426515817642212
I0123 17:33:51.228824 139995557732352 ddar.py:60] Depth 5/1000 time = 15.556545495986938
I0123 17:34:06.287788 139995557732352 ddar.py:60] Depth 6/1000 time = 15.058675527572632
I0123 17:34:20.537592 139995557732352 ddar.py:60] Depth 7/1000 time = 14.249284267425537
I0123 17:34:35.554241 139995557732352 ddar.py:60] Depth 8/1000 time = 15.015624284744263
I0123 17:34:51.599303 139995557732352 ddar.py:60] Depth 9/1000 time = 16.041685819625854
I0123 17:35:08.831897 139995557732352 ddar.py:60] Depth 10/1000 time = 17.23230504989624
I0123 17:35:25.850780 139995557732352 ddar.py:60] Depth 11/1000 time = 17.018485069274902
I0123 17:35:43.022840 139995557732352 ddar.py:60] Depth 12/1000 time = 17.171570539474487
I0123 17:35:43.259061 139995557732352 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:35:43.259124 139995557732352 alphageometry.py:585] Timeout.
