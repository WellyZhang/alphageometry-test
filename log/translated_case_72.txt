I0123 11:42:46.953343 140441023606784 inference_utils.py:69] Parsing gin configuration.
I0123 11:42:46.953444 140441023606784 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:42:46.953655 140441023606784 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:42:46.953690 140441023606784 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:42:46.953717 140441023606784 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:42:46.953744 140441023606784 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:42:46.953772 140441023606784 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:42:46.953798 140441023606784 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:42:46.953824 140441023606784 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:42:46.953849 140441023606784 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:42:46.953874 140441023606784 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:42:46.953899 140441023606784 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:42:46.953948 140441023606784 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:42:46.954087 140441023606784 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:42:46.954300 140441023606784 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:42:46.954411 140441023606784 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:42:46.960879 140441023606784 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:42:46.961008 140441023606784 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:42:46.961330 140441023606784 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:42:46.961436 140441023606784 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:42:46.961730 140441023606784 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:42:46.961833 140441023606784 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:42:46.962257 140441023606784 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:42:46.962362 140441023606784 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:42:46.966147 140441023606784 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:42:47.059422 140441023606784 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:42:47.060158 140441023606784 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:42:47.066929 140441023606784 training_loop.py:335] Process 0 of 1
I0123 11:42:47.066982 140441023606784 training_loop.py:336] Local device count = 1
I0123 11:42:47.067022 140441023606784 training_loop.py:337] Number of replicas = 1
I0123 11:42:47.067052 140441023606784 training_loop.py:339] Using random number seed 42
I0123 11:42:47.546500 140441023606784 training_loop.py:359] Initializing the model.
I0123 11:42:47.914550 140441023606784 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.914823 140441023606784 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:42:47.914932 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915015 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915094 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915176 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915251 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915321 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915390 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915460 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915529 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915598 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915668 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915735 140441023606784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:47.915775 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:47.915822 140441023606784 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:42:47.915936 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:47.915976 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:47.916007 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:47.918075 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.923549 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:47.934633 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.934928 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:47.939933 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:47.950824 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:47.950882 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:47.950921 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:47.950954 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.951019 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.952222 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.952301 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.953022 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.955603 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.962054 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.963429 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.963510 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:47.963546 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:47.963608 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.963739 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:47.964087 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:47.964136 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:47.966107 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.966216 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:47.969169 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.969249 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:47.969772 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:47.980157 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:47.989062 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.989163 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:47.989467 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.989549 140441023606784 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:42:47.989669 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:47.989712 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:47.989743 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:47.991639 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:47.994174 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:47.999896 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.000162 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.002873 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.006790 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.006847 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.006883 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.006916 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.006981 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.007560 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.007637 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.008003 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.008789 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.011324 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.011952 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.012035 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.012072 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.012133 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.012262 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.012590 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.012635 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.014612 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.014710 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.017207 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.017291 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.017738 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.020074 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.021982 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.022080 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.022371 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.022453 140441023606784 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:42:48.022562 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.022601 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.022631 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.024907 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.027300 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.032966 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.033233 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.035926 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.039986 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.040043 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.040079 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.040110 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.040173 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.040739 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.040816 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.041291 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.042082 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.044613 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.045291 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.045370 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.045406 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.045466 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.045598 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.045932 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.045977 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.047911 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.048007 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.050570 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.050659 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.051149 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.053444 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.055405 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.055502 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.055799 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.055882 140441023606784 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:42:48.055994 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.056034 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.056065 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.058007 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.060474 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.066205 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.066468 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.069180 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.073077 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.073134 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.073170 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.073201 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.073265 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.073836 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.073914 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.074283 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.075065 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.077663 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.078305 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.078383 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.078419 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.078480 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.078609 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.078937 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.078981 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.080917 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.081011 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.083706 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.083793 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.084232 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.086545 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.088511 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.088608 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.088903 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.088986 140441023606784 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:42:48.089097 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.089138 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.089169 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.091118 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.093579 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.099279 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.099541 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.102645 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.106445 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.106501 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.106537 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.106568 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.106632 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.107209 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.107287 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.107655 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.108440 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.111027 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.111658 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.111737 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.111773 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.111832 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.111970 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.112305 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.112349 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.114297 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.114394 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.116986 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.117071 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.117512 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.119819 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.121818 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.121916 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.122217 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.122299 140441023606784 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:42:48.122413 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.122452 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.122483 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.124356 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.126799 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.133237 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.133563 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.136305 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.140209 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.140269 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.140307 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.140339 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.140403 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.141219 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.141300 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.141669 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.142477 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.145188 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.145831 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.145909 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.145945 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.146004 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.146138 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.146474 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.146518 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.148461 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.148556 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.151185 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.151268 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.151711 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.154080 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.156050 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.156148 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.156449 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.156534 140441023606784 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:42:48.156648 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.156688 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.156719 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.158640 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.161162 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.166958 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.167222 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.169919 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.173791 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.173850 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.173887 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.173918 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.173985 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.174563 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.174641 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.175010 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.175793 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.178338 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.178986 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.179065 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.179101 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.179162 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.179294 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.179624 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.179668 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.181985 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.182085 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.184633 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.184714 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.185151 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.326185 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.328410 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.328563 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.328885 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.328977 140441023606784 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:42:48.329092 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.329133 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.329164 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.331254 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.333826 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.339698 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.339987 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.342733 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.346969 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.347028 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.347066 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.347099 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.347163 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.347966 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.348043 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.348410 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.349215 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.351899 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.352551 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.352631 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.352667 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.352731 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.352865 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.353196 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.353240 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.355210 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.355312 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.357904 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.357985 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.358477 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.360808 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.362821 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.362928 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.363227 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.363310 140441023606784 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:42:48.363422 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.363461 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.363493 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.365461 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.367907 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.373681 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.373952 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.376682 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.380545 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.380602 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.380638 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.380668 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.380731 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.381306 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.381383 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.381770 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.382566 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.385146 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.385786 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.385865 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.385901 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.385961 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.386094 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.386417 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.386460 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.388415 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.388511 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.391139 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.391226 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.391664 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.394004 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.395953 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.396050 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.396344 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.396435 140441023606784 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:42:48.396549 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.396589 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.396620 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.398551 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.400970 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.407042 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.407306 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.410065 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.413899 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.413954 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.413991 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.414023 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.414086 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.414653 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.414730 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.415096 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.415922 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.418448 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.419081 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.419160 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.419196 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.419256 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.419390 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.419722 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.419766 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.421690 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.421789 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.424397 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.424477 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.424916 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.427248 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.429240 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.429337 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.429634 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.429729 140441023606784 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:42:48.429844 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.429884 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.429916 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.431805 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.434291 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.439961 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.440218 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.442944 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.446728 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.446784 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.446826 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.446859 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.446963 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.447703 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.447780 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.448144 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.448922 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.451427 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.452231 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.452308 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.452342 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.452404 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.452533 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.452855 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.452898 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.454863 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.454960 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.457794 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.457875 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.458309 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.460676 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.462625 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.462725 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.463018 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.463100 140441023606784 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:42:48.463219 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.463260 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.463290 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.465155 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.467649 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.473356 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.473615 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.476665 140441023606784 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:48.480844 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.480902 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.480939 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.480971 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.481041 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.481617 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.481710 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.482088 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.482875 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.485398 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.486035 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.486115 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.486150 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.486211 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.486343 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.486675 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.486719 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.488735 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.488830 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.491411 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.491493 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.491928 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.494286 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.496240 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.496337 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.496634 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.496921 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.496993 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497061 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497120 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497176 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497231 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497284 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497338 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497392 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497444 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497497 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497549 140441023606784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:48.497586 140441023606784 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:42:48.501321 140441023606784 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:48.549934 140441023606784 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.550021 140441023606784 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:42:48.550077 140441023606784 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:42:48.550184 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.550223 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.550253 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.550317 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.552797 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.558401 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.558665 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.561374 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.578676 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.578734 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.578770 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.578802 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.578864 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.580011 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.580091 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.580812 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.582853 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.587727 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.589054 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.589146 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.589184 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.589243 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.589374 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.589487 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.589527 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.591504 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.591602 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.594110 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.594192 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.594304 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.596595 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.598599 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.598698 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.598996 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.599079 140441023606784 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:42:48.599191 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.599232 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.599263 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.599328 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.601650 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.607284 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.607550 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.610303 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.623759 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.623816 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.623853 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.623883 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.623947 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.624524 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.624601 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.624968 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.625695 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.628302 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.628939 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.629018 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.629059 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.629120 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.629259 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.629371 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.629411 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.631450 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.631548 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.634024 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.634106 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.634218 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.636510 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.638493 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.638591 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.638885 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.638968 140441023606784 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:42:48.639082 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.639123 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.639154 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.639219 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.641541 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.647172 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.647433 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.650211 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.663371 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.663429 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.663466 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.663498 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.663561 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.664136 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.664217 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.664589 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.665300 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.667829 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.668464 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.668545 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.668581 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.668647 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.668779 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.668890 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.668929 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.670912 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.671010 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.673516 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.673599 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.673717 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.676000 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.678053 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.678153 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.678451 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.678534 140441023606784 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:42:48.678647 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.678686 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.678718 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.678783 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.681268 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.686880 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.687146 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.689895 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.702994 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.703051 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.703088 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.703120 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.703187 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.703762 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.703840 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.704209 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.704926 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.707505 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.708149 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.708229 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.708265 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.708327 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.708468 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.708586 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.708626 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.710920 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.711019 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.713520 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.713602 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.713722 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.716024 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.717966 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.718065 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.718362 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.718444 140441023606784 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:42:48.718556 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.718597 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.718628 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.718693 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.721070 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.726706 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.726981 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.729765 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.743059 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.743117 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.743154 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.743185 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.743250 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.743816 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.743893 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.744260 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.744962 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.747578 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.748225 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.748303 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.748339 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.748398 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.748535 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.748648 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.748688 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.750634 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.750731 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.753205 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.753285 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.753398 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.755762 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.757688 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.757787 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.758081 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.758163 140441023606784 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:42:48.758277 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.758316 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.758346 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.758409 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.760708 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.766290 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.766555 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.769315 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.782401 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.782458 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.782494 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.782523 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.782587 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.783154 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.783232 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.783600 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.784309 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.786866 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.787503 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.787582 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.787619 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.787679 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.787814 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.787933 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.787973 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.793240 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.793385 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.796171 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.796253 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.796375 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.798780 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.800730 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.800829 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.801126 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.801213 140441023606784 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:42:48.801332 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.801375 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.801407 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.801476 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.803833 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.809887 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.810159 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.812871 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.826460 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.826519 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.826556 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.826588 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.826652 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.827245 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.827323 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.827692 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.828404 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.831006 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.831695 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.831775 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.831811 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.831871 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.832003 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.832115 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.832162 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.834120 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.834218 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.836698 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.836780 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.836892 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.839175 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.841155 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.841253 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.841550 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.841634 140441023606784 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:42:48.841758 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.841798 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.841829 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.841894 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.844196 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.849783 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.850062 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.852829 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.865965 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.866023 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.866059 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.866090 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.866154 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.866775 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.866855 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.867225 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.867934 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.870476 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.871113 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.871192 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.871229 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.871290 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.871427 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.871540 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.871586 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.873536 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.873633 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.876191 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.876273 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.876385 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.878679 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.880611 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.880710 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.881001 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.881084 140441023606784 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:42:48.881197 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.881237 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.881268 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.881333 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.883643 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.889326 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.889593 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.892303 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.905425 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.905482 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.905517 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.905548 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.905611 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.906199 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.906276 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.906645 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.907349 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.910079 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.910940 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.911019 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.911054 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.911113 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.911249 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.911363 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.911402 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.913328 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.913424 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.915931 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.916015 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.916127 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.918499 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.920480 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.920578 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.920873 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.920957 140441023606784 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:42:48.921070 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.921110 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.921141 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.921206 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.923537 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.929145 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.929409 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.932476 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.945560 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.945617 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.945663 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.945697 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.945762 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.946380 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.946458 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.946831 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.947538 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.950124 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.950764 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.950845 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.950880 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.950939 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.951076 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.951189 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.951228 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.953167 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.953270 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.955828 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.955911 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.956022 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.958319 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.960224 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.960321 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.960614 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.960698 140441023606784 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:42:48.960811 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:48.960851 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:48.960882 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:48.960946 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.963262 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:48.968949 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.969213 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:48.971911 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:48.985049 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:48.985110 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:48.985147 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:48.985179 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.985244 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.985832 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.985915 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.986297 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.987034 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.989594 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.990304 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.990387 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:48.990425 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:48.990488 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.990628 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:48.990747 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:48.990788 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.992745 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.992848 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:48.995411 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.995494 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:48.995609 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:48.997871 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:48.999891 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:48.999989 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.000281 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.000365 140441023606784 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:42:49.000478 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.000519 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.000551 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.000616 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.002979 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.008554 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.008819 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.011749 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.025030 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.025087 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.025124 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.025156 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.025221 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.025804 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.025884 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.026255 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.026977 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.029609 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.030260 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.030340 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.030376 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.030436 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.030573 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.030685 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.030725 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.032662 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.032759 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.035267 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.035349 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.035467 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.038164 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.040096 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.040194 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.040488 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.040580 140441023606784 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:42:49.043583 140441023606784 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:49.100678 140441023606784 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.100766 140441023606784 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:42:49.100824 140441023606784 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:42:49.100932 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.100971 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.101001 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.101066 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.103482 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.109018 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.109284 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.112047 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.124869 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.124927 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.124963 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.124994 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.125057 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.125622 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.125710 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.126075 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.126774 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.129322 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.129957 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.130037 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.130074 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.130135 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.130266 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.130388 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.130428 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.132314 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.132412 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.134884 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.134966 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.135079 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.137383 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.139626 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.139725 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.140022 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.140106 140441023606784 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:42:49.140219 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.140260 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.140292 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.140356 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.142647 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.148197 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.148463 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.151202 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.163944 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.164003 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.164040 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.164072 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.164136 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.164705 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.164784 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.165150 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.165853 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.168429 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.169064 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.169144 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.169180 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.169241 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.169373 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.169485 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.169531 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.171434 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.171531 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.174004 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.174086 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.174199 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.176519 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.178428 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.178528 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.178823 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.178907 140441023606784 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:42:49.179018 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.179059 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.179090 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.179155 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.181459 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.187020 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.187286 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.190046 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.202803 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.202861 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.202898 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.202929 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.202995 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.203562 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.203639 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.203999 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.204695 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.207703 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.208341 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.208422 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.208457 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.208518 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.208650 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.208762 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.208802 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.210702 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.210802 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.213244 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.213326 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.213440 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.215752 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.217653 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.217754 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.218051 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.218136 140441023606784 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:42:49.218250 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.218291 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.218322 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.218387 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.220684 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.226201 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.226467 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.229195 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.242340 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.242397 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.242436 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.242477 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.242542 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.243120 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.243198 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.243564 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.244256 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.246864 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.247501 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.247579 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.247614 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.247674 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.247805 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.247917 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.247958 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.249898 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.249993 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.252447 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.252526 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.252636 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.254984 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.256897 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.256993 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.257285 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.257367 140441023606784 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:42:49.257478 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.257517 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.257548 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.257611 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.259896 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.265441 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.265716 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.268483 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.281385 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.281441 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.281475 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.281505 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.281567 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.282139 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.282216 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.282579 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.283279 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.285859 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.286488 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.286564 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.286598 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.286656 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.286784 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.286894 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.286932 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.288847 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.288948 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.291409 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.291491 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.291602 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.293941 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.295830 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.295927 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.296216 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.296298 140441023606784 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:42:49.296408 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.296447 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.296477 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.296540 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.298833 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.304379 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.304645 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.307419 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.320348 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.320404 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.320438 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.320468 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.320530 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.321097 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.321172 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.321544 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.322255 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.325257 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.325900 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.325979 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.326013 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.326072 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.326202 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.326311 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.326349 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.328269 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.328368 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.330815 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.330895 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.331005 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.333323 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.335228 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.335325 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.335615 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.335696 140441023606784 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:42:49.335806 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.335844 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.335874 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.335937 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.338231 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.344140 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.344401 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.347122 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.359951 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.360006 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.360041 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.360070 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.360132 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.360712 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.360788 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.361154 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.361868 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.364457 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.365096 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.365174 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.365208 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.365268 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.365395 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.365504 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.365541 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.367448 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.367542 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.370002 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.370083 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.370193 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.372519 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.374431 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.374528 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.374819 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.374902 140441023606784 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:42:49.375010 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.375049 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.375079 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.375141 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.377416 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.383010 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.383278 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.386035 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.398968 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.399023 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.399058 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.399088 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.399151 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.399719 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.399796 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.400162 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.400870 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.403461 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.404095 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.404173 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.404208 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.404271 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.404402 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.404511 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.404549 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.406476 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.406572 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.409024 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.409110 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.409222 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.411555 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.413437 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.413533 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.413831 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.413915 140441023606784 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:42:49.414024 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.414063 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.414093 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.414156 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.416425 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.421948 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.422208 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.424925 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.437749 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.437804 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.437839 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.437869 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.437931 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.438506 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.438581 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.438939 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.439639 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.442652 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.443400 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.443476 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.443510 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.443568 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.443696 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.443810 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.443849 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.445906 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.446002 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.448475 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.448560 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.448824 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.451176 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.453085 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.453182 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.453477 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.453560 140441023606784 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:42:49.453677 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.453718 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.453749 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.453814 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.456112 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.461997 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.462257 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.464976 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.478192 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.478248 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.478283 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.478313 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.478375 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.479105 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.479184 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.479548 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.480242 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.483027 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.483655 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.483734 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.483769 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.483826 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.483957 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.484066 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.484105 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.486555 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.486849 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.489290 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.489369 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.489485 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.491764 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.493638 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.493743 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.494031 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.494112 140441023606784 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:42:49.494220 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.494260 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.494289 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.494351 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.496630 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.502154 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.502414 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.505137 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.518006 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.518063 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.518098 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.518128 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.518194 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.518762 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.518838 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.519195 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.519895 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.522506 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.523148 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.523226 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.523261 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.523319 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.523451 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.523561 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.523599 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.525513 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.525607 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.528060 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.528141 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.528249 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.530596 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.532495 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.532591 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.532882 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.532964 140441023606784 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:42:49.533072 140441023606784 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:49.533111 140441023606784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:49.533141 140441023606784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:49.533202 140441023606784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.535478 140441023606784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:49.540966 140441023606784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.541229 140441023606784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:49.543965 140441023606784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:49.556868 140441023606784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:49.556922 140441023606784 attention.py:418] Single window, no scan.
I0123 11:42:49.556957 140441023606784 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:49.556987 140441023606784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.557048 140441023606784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.557614 140441023606784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.557698 140441023606784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.558064 140441023606784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.558772 140441023606784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.561724 140441023606784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.562358 140441023606784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.562437 140441023606784 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:49.562472 140441023606784 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:49.562530 140441023606784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.562659 140441023606784 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:49.562774 140441023606784 nn_components.py:325] mlp: activation = None
I0123 11:42:49.562813 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.564719 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.564813 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.567276 140441023606784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.567357 140441023606784 transformer_base.py:443] tbase: final FFN
I0123 11:42:49.567465 140441023606784 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:49.569809 140441023606784 nn_components.py:329] mlp: final activation = None
I0123 11:42:49.571720 140441023606784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.571815 140441023606784 nn_components.py:261] mlp: residual
I0123 11:42:49.572105 140441023606784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:49.572190 140441023606784 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:42:49.575113 140441023606784 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:53.955252 140441023606784 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:42:54.511766 140441023606784 training_loop.py:409] No working directory specified.
I0123 11:42:54.511896 140441023606784 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:42:54.512662 140441023606784 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:42:57.500776 140441023606784 training_loop.py:447] Only restoring trainable parameters.
I0123 11:42:57.501477 140441023606784 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:42:57.501535 140441023606784 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.501582 140441023606784 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.501625 140441023606784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.501674 140441023606784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.501714 140441023606784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.501753 140441023606784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.501791 140441023606784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.501829 140441023606784 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.501866 140441023606784 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.501903 140441023606784 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.501940 140441023606784 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.501976 140441023606784 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.502012 140441023606784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.502048 140441023606784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502084 140441023606784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.502121 140441023606784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502159 140441023606784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502195 140441023606784 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.502231 140441023606784 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.502281 140441023606784 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502319 140441023606784 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.502355 140441023606784 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.502391 140441023606784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.502428 140441023606784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502464 140441023606784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.502501 140441023606784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502537 140441023606784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502573 140441023606784 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.502609 140441023606784 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.502644 140441023606784 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502681 140441023606784 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.502717 140441023606784 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.502753 140441023606784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.502790 140441023606784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502826 140441023606784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.502862 140441023606784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502898 140441023606784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.502934 140441023606784 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.502970 140441023606784 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.503005 140441023606784 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503041 140441023606784 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.503078 140441023606784 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.503114 140441023606784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.503151 140441023606784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503187 140441023606784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.503227 140441023606784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503264 140441023606784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503300 140441023606784 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.503336 140441023606784 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.503371 140441023606784 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503407 140441023606784 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.503443 140441023606784 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.503479 140441023606784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.503514 140441023606784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503550 140441023606784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.503585 140441023606784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503620 140441023606784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503655 140441023606784 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.503692 140441023606784 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.503727 140441023606784 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503763 140441023606784 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.503799 140441023606784 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.503834 140441023606784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.503870 140441023606784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503906 140441023606784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.503941 140441023606784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.503976 140441023606784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504012 140441023606784 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.504048 140441023606784 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.504084 140441023606784 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504120 140441023606784 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.504156 140441023606784 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.504197 140441023606784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.504234 140441023606784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504270 140441023606784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.504305 140441023606784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504340 140441023606784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504375 140441023606784 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.504410 140441023606784 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.504445 140441023606784 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504480 140441023606784 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.504516 140441023606784 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.504552 140441023606784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.504588 140441023606784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504623 140441023606784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.504657 140441023606784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504692 140441023606784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504727 140441023606784 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.504762 140441023606784 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.504798 140441023606784 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504834 140441023606784 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.504869 140441023606784 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.504905 140441023606784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.504940 140441023606784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.504976 140441023606784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.505012 140441023606784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505048 140441023606784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505084 140441023606784 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.505119 140441023606784 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.505159 140441023606784 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505196 140441023606784 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.505231 140441023606784 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.505267 140441023606784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.505304 140441023606784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505340 140441023606784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.505377 140441023606784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505412 140441023606784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505447 140441023606784 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.505482 140441023606784 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.505518 140441023606784 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505554 140441023606784 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.505589 140441023606784 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:57.505625 140441023606784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:57.505668 140441023606784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505706 140441023606784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.505742 140441023606784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505778 140441023606784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505815 140441023606784 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:57.505851 140441023606784 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:57.505887 140441023606784 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:57.505923 140441023606784 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:57.505951 140441023606784 training_loop.py:725] Total parameters: 152072288
I0123 11:42:57.506164 140441023606784 training_loop.py:739] Total state size: 0
I0123 11:42:57.527063 140441023606784 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:42:57.527306 140441023606784 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:42:57.527853 140441023606784 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:42:57.528170 140441023606784 training_loop.py:89] registering functions: dict_keys([])
I0123 11:42:57.543836 140441023606784 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = mirror e c b; f = angle_bisector f a b e, on_line f a e; g = on_circle g d b, on_line g f b ? cong a g c g
I0123 11:42:57.700826 140441023606784 ddar.py:60] Depth 1/1000 time = 0.1407909393310547
I0123 11:42:57.701497 140441023606784 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G : Points
DA = DC [00]
DB = DA [01]
C,B,E are collinear [02]
ABF = FBE [03]
DG = DB [04]
F,B,G are collinear [05]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. DA = DC [00] & DB = DA [01] & DG = DB [04]   C,B,G,A are concyclic [06]
002. F,B,G are collinear [05] & ABF = FBE [03] & C,B,E are collinear [02]   CBG = GBA [07]
003. C,B,G,A are concyclic [06] & CBG = GBA [07]   CG = GA
==========================

