I0123 21:38:37.619692 140127958315008 inference_utils.py:69] Parsing gin configuration.
I0123 21:38:37.619799 140127958315008 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 21:38:37.620019 140127958315008 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 21:38:37.620054 140127958315008 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 21:38:37.620084 140127958315008 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 21:38:37.620110 140127958315008 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 21:38:37.620137 140127958315008 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 21:38:37.620163 140127958315008 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 21:38:37.620189 140127958315008 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 21:38:37.620215 140127958315008 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 21:38:37.620240 140127958315008 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 21:38:37.620265 140127958315008 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 21:38:37.620310 140127958315008 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 21:38:37.620449 140127958315008 resource_reader.py:55] Path not found: base_htrans.gin
I0123 21:38:37.620661 140127958315008 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 21:38:37.620762 140127958315008 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 21:38:37.627158 140127958315008 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 21:38:37.627278 140127958315008 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 21:38:37.627600 140127958315008 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 21:38:37.627702 140127958315008 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 21:38:37.627979 140127958315008 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 21:38:37.628079 140127958315008 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 21:38:37.628483 140127958315008 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 21:38:37.628583 140127958315008 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 21:38:37.632313 140127958315008 training_loop.py:334] ==== Training loop: initializing model ====
I0123 21:38:37.742844 140127958315008 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 21:38:37.743547 140127958315008 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 21:38:37.750141 140127958315008 training_loop.py:335] Process 0 of 1
I0123 21:38:37.750195 140127958315008 training_loop.py:336] Local device count = 1
I0123 21:38:37.750234 140127958315008 training_loop.py:337] Number of replicas = 1
I0123 21:38:37.750264 140127958315008 training_loop.py:339] Using random number seed 42
I0123 21:38:38.238265 140127958315008 training_loop.py:359] Initializing the model.
I0123 21:38:38.634664 140127958315008 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.634910 140127958315008 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 21:38:38.635014 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635094 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635172 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635252 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635326 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635396 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635466 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635535 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635603 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635671 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635739 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635806 140127958315008 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 21:38:38.635845 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:38.635890 140127958315008 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:38:38.636006 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:38.636045 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:38.636076 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:38.638118 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.643480 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:38.654229 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.654509 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:38.658907 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:38.669947 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:38.670004 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:38.670042 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:38.670076 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.670138 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.671326 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.671402 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.672125 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.674613 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.680848 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.682174 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.682254 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:38.682289 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:38.682350 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.682480 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:38.682813 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:38.682862 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.684806 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.684906 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.687811 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.687892 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:38.688583 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:38.699047 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.708026 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.708125 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.708427 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.708507 140127958315008 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:38:38.708619 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:38.708659 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:38.708691 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:38.710584 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.713093 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:38.718789 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.719053 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:38.721749 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:38.725588 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:38.725649 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:38.725687 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:38.725720 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.725784 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.726352 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.726428 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.726789 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.727571 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.730124 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.730748 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.730826 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:38.730862 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:38.730920 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.731047 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:38.731377 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:38.731420 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.733387 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.733480 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.736006 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.736085 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:38.736524 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:38.738875 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.740799 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.740894 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.741205 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.741285 140127958315008 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:38:38.741398 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:38.741437 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:38.741469 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:38.743726 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.746126 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:38.751740 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.752004 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:38.754697 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:38.758549 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:38.758605 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:38.758641 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:38.758673 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.758735 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.759294 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.759369 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.759731 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.760504 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.763033 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.763696 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.763775 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:38.763811 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:38.763870 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.764001 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:38.764321 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:38.764363 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.766290 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.766384 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.768905 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.768999 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:38.769486 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:38.771789 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.773734 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.773830 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.774122 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.774201 140127958315008 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:38:38.774312 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:38.774351 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:38.774382 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:38.776284 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.778683 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:38.784351 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.784619 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:38.787269 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:38.791273 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:38.791332 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:38.791368 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:38.791400 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.791462 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.792022 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.792097 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.792466 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.793242 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.795842 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.796476 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.796555 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:38.796591 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:38.796651 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.796778 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:38.797105 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:38.797148 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.799068 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.799161 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.801777 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.801863 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:38.802298 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:38.804570 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.806493 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.806593 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.806891 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.806970 140127958315008 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:38:38.807080 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:38.807119 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:38.807151 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:38.809077 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.811525 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:38.817180 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.817444 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:38.820489 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:38.824287 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:38.824342 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:38.824377 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:38.824409 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.824469 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.825037 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.825111 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.825472 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.826962 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.829635 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.830323 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.830404 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:38.830439 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:38.830500 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.830643 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:38.830994 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:38.831040 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.832985 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.833079 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.835701 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.835781 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:38.836215 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:38.838509 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.840486 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.840580 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.840882 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.840964 140127958315008 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:38:38.841076 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:38.841115 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:38.841147 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:38.843043 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.845458 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:38.851152 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.851411 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:38.854152 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:38.857948 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:38.858005 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:38.858041 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:38.858073 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.858138 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.858735 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.858811 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.859176 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.860070 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.862581 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.863201 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.863276 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:38.863311 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:38.863369 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.863501 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:38.863823 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:38.863866 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.865787 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.865888 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.868442 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.868520 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:38.868951 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:38.871275 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.873195 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.873290 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.873591 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.873679 140127958315008 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:38:38.873791 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:38.873832 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:38.873863 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:38.875731 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.878207 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:38.883867 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.884127 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:38.886795 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:38.890594 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:38.890649 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:38.890684 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:38.890715 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.890777 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.891342 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.891416 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.891774 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.892554 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.895060 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.895678 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.895754 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:38.895789 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:38.895845 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.895974 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:38.896294 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:38.896337 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:38.898640 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.898736 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:38.901248 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:38.901331 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:38.901772 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.040599 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.042802 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.042954 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.043272 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.043363 140127958315008 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:38:39.043478 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.043519 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.043549 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.045595 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.048134 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.054199 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.054468 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.057115 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:39.061022 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.061078 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.061115 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.061147 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.061211 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.061827 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.061902 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.062269 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.063048 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.065621 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.066272 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.066349 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.066388 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.066446 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.066574 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.066905 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.066948 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.068865 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.068957 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.071537 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.071619 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.072106 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.074431 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.076377 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.076478 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.076776 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.076858 140127958315008 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:38:39.076972 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.077012 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.077044 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.078995 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.081413 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.087162 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.087424 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.090162 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:39.093986 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.094041 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.094077 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.094108 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.094170 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.094733 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.094808 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.095177 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.095948 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.098519 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.099141 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.099217 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.099252 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.099310 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.099436 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.099761 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.099804 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.101733 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.101825 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.104390 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.104469 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.104896 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.107190 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.109107 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.109202 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.109494 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.109579 140127958315008 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:38:39.109694 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.109735 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.109767 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.111679 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.114082 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.120089 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.120354 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.123083 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:39.126846 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.126900 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.126938 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.126970 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.127032 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.127596 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.127671 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.128032 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.128851 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.131360 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.131973 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.132049 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.132084 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.132143 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.132276 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.132600 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.132643 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.134558 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.134652 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.137202 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.137283 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.137736 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.140007 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.141983 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.142077 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.142374 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.142461 140127958315008 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:38:39.142573 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.142612 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.142643 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.144488 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.146944 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.152613 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.152879 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.155679 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:39.159444 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.159501 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.159536 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.159568 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.159669 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.160240 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.160315 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.160674 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.161446 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.163964 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.164580 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.164658 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.164692 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.164750 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.164876 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.165202 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.165245 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.167237 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.167332 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.170119 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.170198 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.170635 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.172974 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.174909 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.175005 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.175301 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.175381 140127958315008 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:38:39.175498 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.175538 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.175570 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.177428 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.179910 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.185562 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.185823 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.188491 140127958315008 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:38:39.192634 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.192689 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.192725 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.192756 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.192817 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.193387 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.193462 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.193836 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.194621 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.197142 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.197772 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.197849 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.197884 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.197943 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.198074 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.198401 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.198446 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.200423 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.200518 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.203063 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.203143 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.203583 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.205924 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.207871 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.207970 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.208263 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.208542 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.208613 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.208681 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.208740 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.208796 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.208852 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.208905 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.208957 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.209009 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.209061 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.209114 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.209167 140127958315008 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 21:38:39.209206 140127958315008 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:38:39.212760 140127958315008 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:38:39.261081 140127958315008 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.261166 140127958315008 decoder_stack.py:333] dstack: autoregressive generator.
I0123 21:38:39.261416 140127958315008 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:38:39.261522 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.261560 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.261591 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.261664 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.264132 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.269856 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.270117 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.272801 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.289724 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.289780 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.289817 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.289848 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.289908 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.291022 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.291097 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.291803 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.293801 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.298493 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.299790 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.299873 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.299908 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.299966 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.300094 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.300201 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.300239 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.302112 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.302205 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.304618 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.304696 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.304803 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.307021 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.308942 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.309036 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.309331 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.309413 140127958315008 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:38:39.309520 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.309558 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.309588 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.309657 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.311892 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.317335 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.317588 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.320264 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.333205 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.333261 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.333296 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.333326 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.333387 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.333951 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.334027 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.334390 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.335097 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.337593 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.338215 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.338290 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.338330 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.338387 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.338515 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.338624 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.338663 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.340589 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.340683 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.343099 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.343178 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.343285 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.345483 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.347405 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.347500 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.347790 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.347870 140127958315008 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:38:39.347979 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.348017 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.348047 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.348109 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.350350 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.355782 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.356040 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.358748 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.371300 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.371355 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.371390 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.371419 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.371479 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.372029 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.372103 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.372457 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.373142 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.375633 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.376253 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.376328 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.376362 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.376423 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.376550 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.376657 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.376694 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.378712 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.378806 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.381274 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.381358 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.381466 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.383675 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.385573 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.385673 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.385961 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.386040 140127958315008 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:38:39.386148 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.386186 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.386216 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.386278 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.388504 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.393940 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.394196 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.396863 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.409452 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.409506 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.409542 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.409571 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.409631 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.410181 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.410254 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.410606 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.411295 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.413783 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.414403 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.414478 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.414511 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.414569 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.414700 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.414813 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.414852 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.417049 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.417143 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.419552 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.419631 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.419737 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.421944 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.423799 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.423893 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.424179 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.424260 140127958315008 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:38:39.424368 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.424406 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.424436 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.424497 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.426771 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.432192 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.432454 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.435052 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.447662 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.447716 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.447751 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.447780 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.447839 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.448394 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.448468 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.448828 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.449537 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.452096 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.452719 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.452795 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.452829 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.452886 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.453021 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.453133 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.453172 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.455066 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.455161 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.457568 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.457651 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.457759 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.460025 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.461899 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.461995 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.462281 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.462360 140127958315008 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:38:39.462468 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.462506 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.462537 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.462600 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.464851 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.470273 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.470531 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.473232 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.490796 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.490875 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.490913 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.490944 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.491019 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.491619 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.491696 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.492068 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.492782 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.495350 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.495972 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.496048 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.496081 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.496145 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.496277 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.496396 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.496435 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.498451 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.498545 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.501020 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.501099 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.501208 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.503462 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.505330 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.505424 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.505716 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.505800 140127958315008 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:38:39.505908 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.505948 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.505978 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.506042 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.508263 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.513767 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.514024 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.516663 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.529574 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.529629 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.529671 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.529702 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.529764 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.530337 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.530417 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.530780 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.531460 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.533963 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.534636 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.534714 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.534748 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.534810 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.534938 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.535046 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.535088 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.536974 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.537068 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.539488 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.539567 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.539674 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.541888 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.543809 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.543905 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.544195 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.544276 140127958315008 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:38:39.544385 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.544424 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.544453 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.544515 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.546773 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.552226 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.552495 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.555186 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.567842 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.567898 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.567934 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.567965 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.568027 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.568635 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.568710 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.569069 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.569782 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.572282 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.572903 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.572979 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.573014 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.573072 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.573202 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.573312 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.573354 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.575253 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.575346 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.577808 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.577888 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.578003 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.580208 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.582070 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.582165 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.582453 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.582532 140127958315008 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:38:39.582640 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.582679 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.582709 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.582772 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.585016 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.590515 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.590772 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.593402 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.606050 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.606105 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.606141 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.606171 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.606231 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.606792 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.606866 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.607226 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.607915 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.610428 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.611093 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.611170 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.611205 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.611263 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.611395 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.611503 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.611541 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.613430 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.613522 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.615931 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.616009 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.616114 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.618338 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.620270 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.620366 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.620657 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.620738 140127958315008 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:38:39.620847 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.620884 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.620914 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.620975 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.623216 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.628606 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.628865 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.631843 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.644423 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.644478 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.644513 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.644543 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.644603 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.645203 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.645281 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.645647 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.646346 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.648822 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.649443 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.649519 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.649553 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.649610 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.649746 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.649855 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.649892 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.651777 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.651875 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.654360 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.654440 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.654547 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.656757 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.658607 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.658702 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.658987 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.659066 140127958315008 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:38:39.659175 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.659214 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.659245 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.659307 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.661528 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.667019 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.667279 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.669918 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.682430 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.682485 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.682520 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.682550 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.682609 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.683157 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.683233 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.683585 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.684267 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.686749 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.687404 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.687479 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.687513 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.687567 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.687695 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.687802 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.687839 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.689715 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.689815 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.692216 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.692296 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.692404 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.694599 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.696498 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.696592 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.696878 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.696958 140127958315008 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:38:39.697066 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.697104 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.697134 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.697196 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.699424 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.704902 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.705163 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.707820 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.720424 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.720479 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.720514 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.720545 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.720606 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.721167 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.721241 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.721600 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.722291 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.724838 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.725451 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.725526 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.725560 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.725616 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.725752 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.725866 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.725903 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.727766 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.727859 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.730279 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.730358 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.730464 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.733054 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.734924 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.735020 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.735308 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.735397 140127958315008 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:38:39.738309 140127958315008 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:38:39.793908 140127958315008 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.793994 140127958315008 decoder_stack.py:333] dstack: autoregressive generator.
I0123 21:38:39.794050 140127958315008 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:38:39.794158 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.794197 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.794229 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.794293 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.796697 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.802169 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.802433 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.805072 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.817547 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.817603 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.817639 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.817677 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.817737 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.818288 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.818362 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.818722 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.819397 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.821945 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.822561 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.822637 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.822671 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.822729 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.822856 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.822974 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.823015 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.824879 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.824973 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.827393 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.827472 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.827581 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.829849 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.831721 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.831817 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.832109 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.832190 140127958315008 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:38:39.832297 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.832335 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.832366 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.832429 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.834692 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.840095 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.840355 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.843044 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.855401 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.855458 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.855494 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.855526 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.855587 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.856137 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.856211 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.856568 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.857251 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.859811 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.860433 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.860509 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.860545 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.860606 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.860736 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.860846 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.860890 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.862766 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.862861 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.865288 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.865367 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.865478 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.867780 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.869648 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.869746 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.870042 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.870122 140127958315008 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:38:39.870232 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.870271 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.870302 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.870365 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.872633 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.878114 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.878381 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.881083 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.893526 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.893581 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.893617 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.893655 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.893719 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.894275 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.894350 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.894709 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.895393 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.898379 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.898994 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.899071 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.899107 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.899168 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.899296 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.899407 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.899446 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.901329 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.901424 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.903864 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.903944 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.904053 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.906319 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.908189 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.908284 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.908573 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.908654 140127958315008 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:38:39.908762 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.908800 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.908831 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.908894 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.911161 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.916579 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.916841 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.919522 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.932015 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.932069 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.932107 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.932151 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.932214 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.932762 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.932836 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.933196 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.933899 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.936466 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.937086 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.937161 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.937196 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.937254 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.937380 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.937488 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.937528 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.939427 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.939519 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.941961 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.942039 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.942145 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.944450 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.946334 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.946429 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.946718 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.946799 140127958315008 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:38:39.946906 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.946943 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.946974 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.947037 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.949301 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.954768 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.955026 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.957768 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:39.970462 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:39.970516 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:39.970551 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:39.970581 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.970641 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.971193 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.971267 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.971629 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.972315 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.974877 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.975506 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.975580 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:39.975614 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:39.975671 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.975796 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:39.975902 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:39.975939 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.977846 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.977945 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.980371 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.980450 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:39.980560 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:39.982861 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:39.984732 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.984825 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:39.985116 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.985194 140127958315008 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:38:39.985301 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:39.985339 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:39.985368 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:39.985430 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.987704 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:39.993149 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:39.993406 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:39.996160 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:40.008786 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:40.008839 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:40.008874 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:40.008905 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.008965 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.009515 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.009588 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.009950 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.010636 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.013644 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.014264 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.014338 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:40.014371 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:40.014428 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.014554 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:40.014661 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:40.014698 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.016592 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.016691 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.019134 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.019212 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:40.019321 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:40.021634 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.023497 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.023590 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.023879 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.023957 140127958315008 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:38:40.024064 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:40.024101 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:40.024130 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:40.024192 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.026441 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:40.031894 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.032149 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:40.034880 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:40.047514 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:40.047568 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:40.047602 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:40.047632 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.047692 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.048248 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.048321 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.048674 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.049360 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.051927 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.052549 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.052623 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:40.052657 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:40.052714 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.052839 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:40.052945 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:40.052982 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.054886 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.054979 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.057411 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.057489 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:40.057595 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:40.059890 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.061755 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.061849 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.062138 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.062217 140127958315008 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:38:40.062323 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:40.062360 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:40.062390 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:40.062451 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.064713 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:40.070181 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.070442 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:40.073167 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:40.085771 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:40.085824 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:40.085858 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:40.085887 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.085947 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.086506 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.086580 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.086942 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.087636 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.090205 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.090824 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.090898 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:40.090931 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:40.090992 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.091116 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:40.091226 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:40.091263 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.093139 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.093231 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.095661 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.095743 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:40.095856 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:40.098161 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.100028 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.100121 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.100411 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.100491 140127958315008 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:38:40.100598 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:40.100636 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:40.100665 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:40.100727 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.102984 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:40.108455 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.108712 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:40.111431 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:40.124045 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:40.124100 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:40.124135 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:40.124166 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.124226 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.124788 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.124861 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.125218 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.125911 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.128853 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.129467 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.129541 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:40.129575 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:40.129631 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.129763 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:40.129873 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:40.129910 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.131782 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.131873 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.134310 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.134392 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:40.134502 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:40.136788 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.138658 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.138751 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.139036 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.139114 140127958315008 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:38:40.139220 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:40.139257 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:40.139287 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:40.139348 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.141608 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:40.147058 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.147314 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:40.150018 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:40.162694 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:40.162748 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:40.162782 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:40.162812 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.162871 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.163438 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.163511 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.163869 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.164555 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.167126 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.167748 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.167822 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:40.167855 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:40.167911 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.168038 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:40.168146 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:40.168182 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.170530 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.170624 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.173035 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.173112 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:40.173225 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:40.175485 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.177324 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.177417 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.177711 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.177790 140127958315008 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:38:40.177896 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:40.177933 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:40.177963 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:40.178023 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.180258 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:40.185674 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.185929 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:40.188626 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:40.201129 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:40.201183 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:40.201218 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:40.201247 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.201307 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.201871 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.201944 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.202295 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.202975 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.205547 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.206168 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.206242 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:40.206275 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:40.206331 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.206457 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:40.206564 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:40.206601 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.208613 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.208704 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.211148 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.211226 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:40.211332 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:40.213609 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.215473 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.215566 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.215853 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.215931 140127958315008 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:38:40.216037 140127958315008 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:38:40.216074 140127958315008 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:38:40.216104 140127958315008 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:38:40.216165 140127958315008 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.218514 140127958315008 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:38:40.223922 140127958315008 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.224179 140127958315008 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:38:40.226889 140127958315008 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:38:40.239510 140127958315008 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:38:40.239563 140127958315008 attention.py:418] Single window, no scan.
I0123 21:38:40.239598 140127958315008 transformer_layer.py:389] tlayer: self-attention.
I0123 21:38:40.239629 140127958315008 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.239689 140127958315008 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.240242 140127958315008 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.240315 140127958315008 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.240667 140127958315008 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.241361 140127958315008 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.244272 140127958315008 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.244893 140127958315008 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.244967 140127958315008 transformer_layer.py:468] tlayer: End windows.
I0123 21:38:40.245001 140127958315008 transformer_layer.py:472] tlayer: final FFN.
I0123 21:38:40.245058 140127958315008 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.245186 140127958315008 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:38:40.245297 140127958315008 nn_components.py:325] mlp: activation = None
I0123 21:38:40.245334 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.247216 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.247308 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.249710 140127958315008 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.249789 140127958315008 transformer_base.py:443] tbase: final FFN
I0123 21:38:40.249897 140127958315008 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:38:40.252188 140127958315008 nn_components.py:329] mlp: final activation = None
I0123 21:38:40.254055 140127958315008 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.254150 140127958315008 nn_components.py:261] mlp: residual
I0123 21:38:40.254437 140127958315008 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:40.254520 140127958315008 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:38:40.257350 140127958315008 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:38:44.673551 140127958315008 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 21:38:45.208685 140127958315008 training_loop.py:409] No working directory specified.
I0123 21:38:45.208796 140127958315008 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 21:38:45.209532 140127958315008 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 21:38:48.415973 140127958315008 training_loop.py:447] Only restoring trainable parameters.
I0123 21:38:48.416583 140127958315008 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 21:38:48.416658 140127958315008 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.416707 140127958315008 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.416749 140127958315008 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.416790 140127958315008 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.416830 140127958315008 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.416869 140127958315008 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.416907 140127958315008 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.416945 140127958315008 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.416982 140127958315008 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.417019 140127958315008 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417056 140127958315008 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.417092 140127958315008 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.417129 140127958315008 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.417165 140127958315008 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417201 140127958315008 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.417237 140127958315008 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417272 140127958315008 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417307 140127958315008 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.417342 140127958315008 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.417388 140127958315008 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417426 140127958315008 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.417461 140127958315008 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.417496 140127958315008 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.417531 140127958315008 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417566 140127958315008 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.417601 140127958315008 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417636 140127958315008 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417688 140127958315008 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.417724 140127958315008 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.417759 140127958315008 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417794 140127958315008 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.417829 140127958315008 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.417865 140127958315008 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.417899 140127958315008 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.417935 140127958315008 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.417970 140127958315008 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418005 140127958315008 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418039 140127958315008 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.418074 140127958315008 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.418109 140127958315008 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418144 140127958315008 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.418179 140127958315008 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.418215 140127958315008 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.418250 140127958315008 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418286 140127958315008 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.418327 140127958315008 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418364 140127958315008 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418400 140127958315008 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.418435 140127958315008 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.418469 140127958315008 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418504 140127958315008 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.418539 140127958315008 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.418573 140127958315008 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.418608 140127958315008 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418643 140127958315008 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.418679 140127958315008 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418714 140127958315008 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418748 140127958315008 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.418782 140127958315008 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.418816 140127958315008 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418852 140127958315008 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.418887 140127958315008 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.418921 140127958315008 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.418956 140127958315008 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.418991 140127958315008 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.419026 140127958315008 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419062 140127958315008 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419097 140127958315008 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.419131 140127958315008 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.419166 140127958315008 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419200 140127958315008 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.419235 140127958315008 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.419275 140127958315008 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.419311 140127958315008 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419346 140127958315008 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.419380 140127958315008 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419415 140127958315008 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419449 140127958315008 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.419484 140127958315008 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.419518 140127958315008 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419553 140127958315008 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.419587 140127958315008 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.419622 140127958315008 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.419656 140127958315008 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419692 140127958315008 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.419728 140127958315008 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419762 140127958315008 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419797 140127958315008 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.419831 140127958315008 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.419866 140127958315008 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.419901 140127958315008 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.419935 140127958315008 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.419970 140127958315008 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.420005 140127958315008 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420041 140127958315008 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.420077 140127958315008 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420113 140127958315008 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420149 140127958315008 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.420184 140127958315008 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.420224 140127958315008 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420260 140127958315008 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.420295 140127958315008 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.420330 140127958315008 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.420365 140127958315008 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420400 140127958315008 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.420435 140127958315008 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420470 140127958315008 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420504 140127958315008 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.420539 140127958315008 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.420573 140127958315008 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420608 140127958315008 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.420643 140127958315008 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 21:38:48.420678 140127958315008 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 21:38:48.420712 140127958315008 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420747 140127958315008 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.420781 140127958315008 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420816 140127958315008 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420850 140127958315008 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 21:38:48.420885 140127958315008 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 21:38:48.420919 140127958315008 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 21:38:48.420954 140127958315008 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 21:38:48.420982 140127958315008 training_loop.py:725] Total parameters: 152072288
I0123 21:38:48.421188 140127958315008 training_loop.py:739] Total state size: 0
I0123 21:38:48.445867 140127958315008 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 21:38:48.446091 140127958315008 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 21:38:48.446616 140127958315008 training_loop.py:652] Compiling mode beam_search with jit.
I0123 21:38:48.446935 140127958315008 training_loop.py:89] registering functions: dict_keys([])
I0123 21:38:48.463556 140127958315008 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d c; f = angle_bisector f e b a, on_line f e a; g = foot g c b f; h = mirror h c g; i = angle_bisector i a c e, on_line i a e; j = foot j b c i; k = mirror k b j; l = on_line l c k, on_line l b h ? cong a l e l
I0123 21:38:49.399952 140127958315008 ddar.py:60] Depth 1/1000 time = 0.9050040245056152
I0123 21:38:51.082927 140127958315008 ddar.py:60] Depth 2/1000 time = 1.6828131675720215
I0123 21:38:52.899408 140127958315008 ddar.py:60] Depth 3/1000 time = 1.8163189888000488
I0123 21:38:54.743423 140127958315008 ddar.py:60] Depth 4/1000 time = 1.8438477516174316
I0123 21:38:56.606725 140127958315008 ddar.py:60] Depth 5/1000 time = 1.8629798889160156
I0123 21:38:58.374713 140127958315008 ddar.py:60] Depth 6/1000 time = 1.7670633792877197
I0123 21:39:00.756914 140127958315008 ddar.py:60] Depth 7/1000 time = 2.3820183277130127
I0123 21:39:03.152209 140127958315008 ddar.py:60] Depth 8/1000 time = 2.3933260440826416
I0123 21:39:06.054178 140127958315008 ddar.py:60] Depth 9/1000 time = 2.8776023387908936
I0123 21:39:08.836333 140127958315008 ddar.py:60] Depth 10/1000 time = 2.781963586807251
I0123 21:39:12.469140 140127958315008 ddar.py:60] Depth 11/1000 time = 3.632589101791382
I0123 21:39:15.951397 140127958315008 ddar.py:60] Depth 12/1000 time = 3.4820120334625244
I0123 21:39:19.517068 140127958315008 ddar.py:60] Depth 13/1000 time = 3.5553629398345947
I0123 21:39:19.531892 140127958315008 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L : Points
DB = DC [00]
DA = DB [01]
DE = DC [02]
∠EBF = ∠FBA [03]
A,F,E are collinear [04]
G,F,B are collinear [05]
CG ⟂ BF [06]
GC = GH [07]
C,G,H are collinear [08]
∠ACI = ∠ICE [09]
C,I,J are collinear [10]
BJ ⟂ CI [11]
K,J,B are collinear [12]
JB = JK [13]
L,C,K are collinear [14]
L,H,B are collinear [15]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. DE = DC [02] & DB = DC [00] & DA = DB [01] ⇒  DA = DE [16]
002. DE = DC [02] & DB = DC [00] & DA = DB [01] ⇒  A,C,E,B are concyclic [17]
003. A,C,E,B are concyclic [17] ⇒  ∠AEC = ∠ABC [18]
004. A,C,E,B are concyclic [17] ⇒  ∠ACE = ∠ABE [19]
005. DA = DE [16] ⇒  ∠DAE = ∠AED [20]
006. C,I,J are collinear [10] & F,B,G are collinear [05] & CG ⟂ BF [06] & BJ ⟂ CI [11] ⇒  ∠CJB = ∠CGB [21]
007. ∠CJB = ∠CGB [21] ⇒  G,C,J,B are concyclic [22]
008. G,C,J,B are concyclic [22] ⇒  ∠GJC = ∠GBC [23]
009. G,C,J,B are concyclic [22] ⇒  ∠CGJ = ∠CBJ [24]
010. ∠GJC = ∠GBC [23] & C,I,J are collinear [10] & G,F,B are collinear [05] ⇒  ∠(GJ-CI) = ∠FBC [25]
011. ∠EBF = ∠FBA [03] & ∠ACI = ∠ICE [09] & ∠AEC = ∠ABC [18] & ∠ACE = ∠ABE [19] & ∠DAE = ∠AED [20] & ∠(GJ-CI) = ∠FBC [25] (Angle chase)⇒  ∠(AD-GJ) = ∠(GJ-DE) [26]
012. ∠EBF = ∠FBA [03] & ∠ACI = ∠ICE [09] & ∠AEC = ∠ABC [18] & ∠ACE = ∠ABE [19] & ∠(GJ-CI) = ∠FBC [25] (Angle chase)⇒  AE ⟂ GJ [27]
013. B,K,J are collinear [12] & A,F,E are collinear [04] & AE ⟂ GJ [27] & BJ ⟂ CI [11] ⇒  ∠(CI-KJ) = ∠(AF-JG) [28]
014. B,K,J are collinear [12] & ∠CGJ = ∠CBJ [24] ⇒  ∠CGJ = ∠(CB-KJ) [29]
015. ∠(CI-KJ) = ∠(AF-JG) [28] & ∠CGJ = ∠(CB-KJ) [29] ⇒  ∠ICB = ∠(AF-CG) [30]
016. DB = DC [00] & DA = DB [01] ⇒  DA = DC [31]
017. DA = DC [31] ⇒  ∠DAC = ∠ACD [32]
018. DA = DB [01] ⇒  ∠DAB = ∠ABD [33]
019. DE = DC [02] ⇒  ∠DCE = ∠CED [34]
020. ∠EBF = ∠FBA [03] & CG ⟂ BF [06] & ∠ACE = ∠ABE [19] & ∠DAC = ∠ACD [32] & ∠DAE = ∠AED [20] & ∠DAB = ∠ABD [33] & ∠DCE = ∠CED [34] (Angle chase)⇒  ∠FBD = ∠(AE-CG) [35]
021. F,B,G are collinear [05] & ∠ICB = ∠(AF-CG) [30] & A,F,E are collinear [04] & ∠FBD = ∠(AE-CG) [35] ⇒  ∠(GF-BD) = ∠ICB [36]
022. B,J,K are collinear [12] & C,I,J are collinear [10] & BJ ⟂ CI [11] ⇒  ∠KJC = ∠CJB [37]
023. JB = JK [13] & ∠KJC = ∠CJB [37] (SAS)⇒  ∠KCJ = ∠JCB [38]
024. JB = JK [13] & ∠KJC = ∠CJB [37] (SAS)⇒  ∠JKC = ∠CBJ [39]
025. F,B,G are collinear [05] & L,C,K are collinear [14] & ∠KCJ = ∠JCB [38] & C,I,J are collinear [10] & ∠GJC = ∠GBC [23] ⇒  ∠FGJ = ∠ICL [40]
026. ∠(GF-BD) = ∠ICB [36] & ∠FGJ = ∠ICL [40] ⇒  ∠(BD-JG) = ∠BCL [41]
027. F,B,G are collinear [05] & H,C,G are collinear [08] & CG ⟂ BF [06] ⇒  ∠CGB = ∠BGH [42]
028. GC = GH [07] & ∠CGB = ∠BGH [42] (SAS)⇒  ∠GCB = ∠BHG [43]
029. ∠GCB = ∠BHG [43] & C,G,H are collinear [08] ⇒  ∠GCB = ∠(BH-CG) [44]
030. ∠JKC = ∠CBJ [39] & K,J,B are collinear [12] ⇒  ∠(BJ-CK) = ∠CBJ [45]
031. ∠EBF = ∠FBA [03] & CG ⟂ BF [06] & ∠ACI = ∠ICE [09] & BJ ⟂ CI [11] & ∠ACE = ∠ABE [19] & ∠DAC = ∠ACD [32] & ∠DAB = ∠ABD [33] & ∠GCB = ∠(BH-CG) [44] & ∠(BJ-CK) = ∠CBJ [45] (Angle chase)⇒  ∠CDB = ∠(CK-BH) [46]
032. L,C,K are collinear [14] & L,H,B are collinear [15] & ∠CDB = ∠(CK-BH) [46] ⇒  ∠CDB = ∠CLB [47]
033. ∠CDB = ∠CLB [47] ⇒  L,C,B,D are concyclic [48]
034. L,C,B,D are concyclic [48] ⇒  ∠LCB = ∠LDB [49]
035. L,C,K are collinear [14] & ∠(BD-JG) = ∠BCL [41] & ∠LCB = ∠LDB [49] ⇒  ∠DLC = ∠(JG-LC) [50]
036. ∠DLC = ∠(JG-LC) [50] ⇒  LD ∥ JG [51]
037. ∠(AD-GJ) = ∠(GJ-DE) [26] & DL ∥ GJ [51] ⇒  ∠ADL = ∠LDE [52]
038. DA = DE [16] & ∠ADL = ∠LDE [52] (SAS)⇒  LA = LE
==========================

