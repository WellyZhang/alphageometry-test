I0123 12:42:57.504987 140324316274688 inference_utils.py:69] Parsing gin configuration.
I0123 12:42:57.505195 140324316274688 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:42:57.505667 140324316274688 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:42:57.505707 140324316274688 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:42:57.505738 140324316274688 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:42:57.505766 140324316274688 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:42:57.505793 140324316274688 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:42:57.505819 140324316274688 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:42:57.505847 140324316274688 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:42:57.505873 140324316274688 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:42:57.505898 140324316274688 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:42:57.505924 140324316274688 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:42:57.505994 140324316274688 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:42:57.506235 140324316274688 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:42:57.506595 140324316274688 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:42:57.506721 140324316274688 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:42:57.513704 140324316274688 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:42:57.513857 140324316274688 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:42:57.514174 140324316274688 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:42:57.514286 140324316274688 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:42:57.514566 140324316274688 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:42:57.514667 140324316274688 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:42:57.515069 140324316274688 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:42:57.515169 140324316274688 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:42:57.519521 140324316274688 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:42:57.624281 140324316274688 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:42:57.625267 140324316274688 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:42:57.631931 140324316274688 training_loop.py:335] Process 0 of 1
I0123 12:42:57.631989 140324316274688 training_loop.py:336] Local device count = 1
I0123 12:42:57.632030 140324316274688 training_loop.py:337] Number of replicas = 1
I0123 12:42:57.632061 140324316274688 training_loop.py:339] Using random number seed 42
I0123 12:42:58.128227 140324316274688 training_loop.py:359] Initializing the model.
I0123 12:42:58.512518 140324316274688 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.513052 140324316274688 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:42:58.513168 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513303 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513384 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513467 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513538 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513607 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513683 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513752 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513820 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513890 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.513959 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.514028 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:42:58.514070 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:58.514121 140324316274688 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:42:58.514240 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:58.514280 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:58.514311 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:58.516328 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.521748 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:58.536708 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.537554 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:58.542083 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:58.553341 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:58.553424 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:58.553466 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:58.553502 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.553562 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.554980 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.555070 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.555767 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.558350 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.564489 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.565743 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.565828 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:58.565864 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:58.565927 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.566053 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:58.566419 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:58.566466 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.568330 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.568436 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.571295 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.571379 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:58.571827 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:58.581749 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.590301 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.590405 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.590697 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.590801 140324316274688 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:42:58.590916 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:58.590956 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:58.590987 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:58.592992 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.595344 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:58.600841 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.601102 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:58.603695 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:58.607459 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:58.607516 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:58.607552 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:58.607583 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.607648 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.608226 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.608303 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.608656 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.609409 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.611901 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.612576 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.612654 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:58.612688 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:58.612746 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.612872 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:58.613200 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:58.613243 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.615110 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.615208 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.617672 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.617752 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:58.618237 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:58.620455 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.622304 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.622400 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.622685 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.622764 140324316274688 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:42:58.622873 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:58.622912 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:58.622942 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:58.624844 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.627153 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:58.632992 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.633250 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:58.636014 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:58.639825 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:58.639880 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:58.639915 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:58.639945 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.640007 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.640560 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.640636 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.640987 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.641745 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.644264 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.644884 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.644960 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:58.644995 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:58.645053 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.645177 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:58.645496 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:58.645539 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.647409 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.647505 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.650007 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.650092 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:58.650523 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:58.652758 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.654626 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.654719 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.655005 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.655085 140324316274688 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:42:58.655192 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:58.655231 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:58.655261 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:58.657147 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.659500 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:58.665042 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.665312 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:58.667964 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:58.671643 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:58.671699 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:58.671734 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:58.671763 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.671825 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.672380 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.672459 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.672811 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.673571 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.676075 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.676693 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.676771 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:58.676806 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:58.676870 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.676995 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:58.677321 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:58.677363 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.679213 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.679306 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.681811 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.681897 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:58.682334 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:58.684549 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.686477 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.686573 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.686861 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.686941 140324316274688 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:42:58.687048 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:58.687086 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:58.687116 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:58.688946 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.691297 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:58.696766 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.697024 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:58.699692 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:58.703370 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:58.703426 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:58.703462 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:58.703492 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.703553 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.704472 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.704550 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.704907 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.705685 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.708132 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.708763 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.708839 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:58.708875 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:58.708934 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.709065 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:58.709391 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:58.709434 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.711289 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.711382 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.713886 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.713966 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:58.714401 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:58.716659 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.718533 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.718628 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.718913 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.718993 140324316274688 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:42:58.719101 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:58.719139 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:58.719170 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:58.721024 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.723422 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:58.728898 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.729148 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:58.731734 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:58.735652 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:58.735708 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:58.735744 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:58.735774 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.735835 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.736394 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.736471 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.736823 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.737590 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.740020 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.740639 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.740715 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:58.740750 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:58.740809 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.740934 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:58.741258 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:58.741301 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.743222 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.743319 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.745767 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.745846 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:58.746274 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:58.748546 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.750410 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.750508 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.750788 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.750868 140324316274688 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:42:58.750976 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:58.751015 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:58.751045 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:58.752928 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.755238 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:58.760731 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.760996 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:58.763564 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:58.767298 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:58.767354 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:58.767390 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:58.767420 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.767484 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.768032 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.768106 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.768457 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.769249 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.771665 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.772329 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.772407 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:58.772441 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:58.772500 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.772624 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:58.772946 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:58.772990 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.774847 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.774940 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.777364 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.777444 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:58.778247 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:58.780461 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:58.782320 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.782426 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:58.782710 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:58.782790 140324316274688 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:42:58.782898 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:58.782937 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:58.782967 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.155976 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.159563 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.165826 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.166180 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.169185 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:59.174404 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.174470 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.174512 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.174546 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.174805 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.175559 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.175641 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.176035 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.176862 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.179609 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.180291 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.180372 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.180411 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.180479 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.180616 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.180971 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.181016 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.182987 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.183086 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.185749 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.185832 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.186298 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.188754 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.190809 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.190925 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.191219 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.191306 140324316274688 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:42:59.191422 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.191473 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.191505 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.193469 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.195960 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.201660 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.201937 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.204628 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:59.208548 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.208604 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.208642 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.208673 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.208733 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.209363 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.209442 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.209808 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.210618 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.213118 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.213771 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.213850 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.213885 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.213944 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.214072 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.214402 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.214445 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.216338 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.216437 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.219012 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.219091 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.219526 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.221892 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.223839 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.223935 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.224228 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.224318 140324316274688 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:42:59.224431 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.224471 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.224502 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.226370 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.228851 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.234882 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.235147 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.237763 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:59.241608 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.241671 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.241708 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.241740 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.241801 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.242374 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.242451 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.242802 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.243581 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.246067 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.246699 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.246777 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.246811 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.246870 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.246998 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.247324 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.247368 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.249334 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.249427 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.251901 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.251982 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.252425 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.254754 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.256658 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.256753 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.257040 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.257128 140324316274688 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:42:59.257241 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.257280 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.257310 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.259259 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.261653 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.267310 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.267589 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.270205 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:59.274070 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.274126 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.274162 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.274194 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.274256 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.275011 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.275087 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.275442 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.276302 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.278769 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.279442 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.279520 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.279555 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.279614 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.279744 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.280068 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.280112 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.281998 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.282093 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.285470 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.285550 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.286050 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.288288 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.290163 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.290261 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.290552 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.290634 140324316274688 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:42:59.290750 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.290789 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.290820 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.292727 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.295071 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.300584 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.300842 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.303488 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:42:59.307274 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.307331 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.307368 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.307399 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.307459 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.308027 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.308104 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.308457 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.309216 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.312043 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.312677 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.312756 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.312792 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.312853 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.312979 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.313308 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.313352 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.315231 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.315325 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.317863 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.317944 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.318378 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.320600 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.322487 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.322582 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.322869 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.323193 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323266 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323332 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323391 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323445 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323499 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323553 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323606 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323658 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323710 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323763 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323816 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:42:59.323855 140324316274688 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:42:59.327443 140324316274688 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:42:59.375144 140324316274688 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.375232 140324316274688 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:42:59.375288 140324316274688 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:42:59.375394 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.375434 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.375464 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.375532 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.378187 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.383546 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.383806 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.386427 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.403105 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.403162 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.403199 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.403230 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.403292 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.404432 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.404510 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.405202 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.407203 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.411934 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.413236 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.413326 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.413362 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.413422 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.413553 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.413669 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.413710 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.415559 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.415654 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.418033 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.418115 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.418222 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.420411 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.422348 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.422446 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.422737 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.422820 140324316274688 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:42:59.422931 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.422970 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.423001 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.423067 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.425285 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.430683 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.430943 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.433594 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.446492 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.446549 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.446584 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.446614 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.446675 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.447226 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.447304 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.447659 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.448342 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.450785 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.451395 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.451473 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.451513 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.451572 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.451699 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.451810 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.451849 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.453753 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.453850 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.456199 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.456279 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.456387 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.458577 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.460462 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.460558 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.460839 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.460920 140324316274688 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:42:59.461029 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.461068 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.461100 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.461165 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.463371 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.468659 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.468917 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.471541 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.484436 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.484494 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.484530 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.484560 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.484621 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.485183 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.485260 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.485611 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.486304 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.488734 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.489355 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.489432 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.489467 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.489530 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.489664 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.489774 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.489813 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.491703 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.491796 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.494188 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.494268 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.494379 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.496557 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.498459 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.498555 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.498837 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.498916 140324316274688 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:42:59.499025 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.499065 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.499096 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.499161 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.501361 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.506704 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.506961 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.509590 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.522226 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.522282 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.522317 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.522347 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.522410 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.522967 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.523043 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.523390 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.524074 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.526530 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.527158 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.527235 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.527269 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.527328 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.527462 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.527569 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.527607 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.529516 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.529611 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.531994 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.532074 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.532182 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.534379 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.536203 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.536298 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.536577 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.536659 140324316274688 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:42:59.536768 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.536807 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.536838 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.536901 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.539447 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.544795 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.545060 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.547652 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.560233 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.560290 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.560326 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.560357 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.560417 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.560969 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.561048 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.561401 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.562094 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.564570 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.565195 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.565273 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.565308 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.565366 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.565500 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.565611 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.565657 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.567495 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.567589 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.569964 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.570045 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.570153 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.572412 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.574255 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.574353 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.574635 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.574716 140324316274688 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:42:59.574824 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.574863 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.574892 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.574957 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.577149 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.582510 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.582768 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.585430 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.597947 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.598004 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.598040 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.598071 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.598136 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.598692 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.598768 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.599117 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.599796 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.602230 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.602852 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.602929 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.602963 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.603021 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.603146 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.603258 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.603295 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.605181 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.605274 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.607644 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.607725 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.607831 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.610021 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.611843 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.611940 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.612221 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.612301 140324316274688 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:42:59.612408 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.612447 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.612477 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.612540 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.614729 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.620129 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.620386 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.622929 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.635391 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.635447 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.635483 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.635513 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.635572 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.636125 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.636200 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.636550 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.637220 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.639636 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.640630 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.640708 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.640743 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.640800 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.640926 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.641032 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.641075 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.642919 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.643013 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.645345 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.645425 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.645531 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.647705 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.649581 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.649686 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.649972 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.650054 140324316274688 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:42:59.650164 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.650204 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.650235 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.650298 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.652488 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.657768 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.658035 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.660635 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.673051 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.673115 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.673151 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.673181 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.673242 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.673853 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.673930 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.674282 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.674957 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.677375 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.678012 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.678090 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.678124 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.678186 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.678315 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.678426 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.678471 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.680306 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.680401 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.682822 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.682904 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.683012 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.685364 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.687199 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.687297 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.687582 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.687664 140324316274688 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:42:59.687772 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.687811 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.687841 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.687904 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.690112 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.695475 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.695734 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.698302 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.710800 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.710860 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.710895 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.710925 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.710985 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.711539 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.711615 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.711960 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.712633 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.715051 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.715717 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.715795 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.715830 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.715888 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.716017 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.716125 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.716163 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.717998 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.718092 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.720437 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.720516 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.720622 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.722787 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.724673 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.724768 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.725052 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.725131 140324316274688 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:42:59.725238 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.725276 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.725306 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.725368 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.727561 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.732848 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.733106 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.735729 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.748428 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.748485 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.748521 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.748552 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.748613 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.749215 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.749291 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.749649 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.750326 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.752765 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.753384 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.753461 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.753503 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.753560 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.753695 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.753806 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.753844 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.755678 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.755779 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.758188 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.758269 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.758377 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.760551 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.762356 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.762451 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.762732 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.762813 140324316274688 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:42:59.762920 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.762958 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.762988 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.763048 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.765222 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.770582 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.770837 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.773394 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.785775 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.785831 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.785866 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.785897 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.785958 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.786509 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.786585 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.786932 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.787601 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.790352 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.791018 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.791096 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.791130 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.791188 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.791312 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.791418 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.791457 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.793298 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.793398 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.795767 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.795848 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.795961 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.798112 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.799979 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.800075 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.800354 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.800437 140324316274688 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:42:59.800545 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.800584 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.800615 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.800678 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.802879 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.808157 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.808409 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.811027 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.823398 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.823456 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.823492 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.823521 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.823581 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.824132 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.824207 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.824556 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.825268 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.827709 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.828320 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.828396 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.828431 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.828487 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.828616 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.828722 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.828761 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.830605 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.830699 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.833066 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.833145 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.833253 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.835490 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.837297 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.837393 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.837680 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.837767 140324316274688 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:42:59.840571 140324316274688 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:42:59.895291 140324316274688 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.895381 140324316274688 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:42:59.895436 140324316274688 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:42:59.895538 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.895576 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.895606 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.895668 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.898276 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.903513 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.903770 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.906272 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.918371 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.918428 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.918464 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.918494 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.918555 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.919102 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.919179 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.919526 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.920176 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.922602 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.923212 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.923290 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.923325 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.923384 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.923512 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.923625 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.923665 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.925446 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.925539 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.927849 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.927929 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.928037 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.930235 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.932030 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.932126 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.932404 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.932484 140324316274688 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:42:59.932591 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.932629 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.932659 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.932723 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.934911 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.940212 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.940465 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.943063 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.955188 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.955244 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.955279 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.955309 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.955370 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.955917 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.955993 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.956338 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.956998 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.959463 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.960084 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.960161 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.960197 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.960256 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.960385 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.960493 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.960538 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.962561 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.962656 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.965107 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.965187 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:42:59.965296 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:42:59.967548 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.969361 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.969458 140324316274688 nn_components.py:261] mlp: residual
I0123 12:42:59.969746 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.969828 140324316274688 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:42:59.969936 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:42:59.969974 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:42:59.970006 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:42:59.970070 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.972260 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:42:59.977555 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.977825 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:42:59.980472 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:42:59.992788 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:42:59.992845 140324316274688 attention.py:418] Single window, no scan.
I0123 12:42:59.992881 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:42:59.992912 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.992973 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.993529 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.993606 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.993965 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.994631 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.997091 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.997716 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.997794 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:42:59.997830 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:42:59.997890 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:42:59.998019 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:42:59.998128 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:42:59.998167 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:42:59.999999 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.000094 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.002448 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.002529 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.002638 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.005312 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.007156 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.007254 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.007537 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.007619 140324316274688 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:43:00.007726 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:43:00.007765 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:43:00.007796 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:43:00.007860 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.010085 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:43:00.015373 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.015633 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:43:00.018283 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:43:00.030509 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:43:00.030566 140324316274688 attention.py:418] Single window, no scan.
I0123 12:43:00.030609 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:43:00.030650 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.030714 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.031267 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.031342 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.031686 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.032352 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.034813 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.035438 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.035514 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:43:00.035546 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:43:00.035603 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.035727 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:43:00.035832 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:43:00.035870 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.037714 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.037806 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.040118 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.040196 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.040300 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.042520 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.044321 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.044412 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.044688 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.044765 140324316274688 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:43:00.044870 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:43:00.044907 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:43:00.044935 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:43:00.044997 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.047178 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:43:00.052433 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.052684 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:43:00.055316 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:43:00.067658 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:43:00.067713 140324316274688 attention.py:418] Single window, no scan.
I0123 12:43:00.067747 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:43:00.067776 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.067835 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.068379 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.068453 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.068800 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.069468 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.071944 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.072561 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.072636 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:43:00.072669 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:43:00.072726 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.072849 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:43:00.072954 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:43:00.072991 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.074837 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.074934 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.077279 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.077356 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.077462 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.079703 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.081523 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.081616 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.081900 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.081980 140324316274688 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:43:00.082086 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:43:00.082123 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:43:00.082151 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:43:00.082212 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.084383 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:43:00.089622 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.089883 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:43:00.092506 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:43:00.104772 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:43:00.104827 140324316274688 attention.py:418] Single window, no scan.
I0123 12:43:00.104861 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:43:00.104890 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.104948 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.105494 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.105570 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.105920 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.106590 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.109063 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.109681 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.109756 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:43:00.109789 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:43:00.109844 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.109968 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:43:00.110072 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:43:00.110109 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.111920 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.112016 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.114331 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.114409 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.114519 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.117151 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.118967 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.119061 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.119338 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.119416 140324316274688 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:43:00.119520 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:43:00.119557 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:43:00.119586 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:43:00.119647 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.121815 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:43:00.127068 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.127320 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:43:00.129937 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:43:00.142214 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:43:00.142269 140324316274688 attention.py:418] Single window, no scan.
I0123 12:43:00.142303 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:43:00.142332 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.142394 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.142940 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.143014 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.143361 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.144028 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.146492 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.147112 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.147187 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:43:00.147219 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:43:00.147274 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.147397 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:43:00.147500 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:43:00.147536 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.149349 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.149440 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.151755 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.151833 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.151936 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.154165 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.155964 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.156058 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.156334 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.156412 140324316274688 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:43:00.156516 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:43:00.156553 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:43:00.156580 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:43:00.156640 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.158805 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:43:00.164071 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.164323 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:43:00.166938 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:43:00.179167 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:43:00.179222 140324316274688 attention.py:418] Single window, no scan.
I0123 12:43:00.179255 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:43:00.179284 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.179343 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.179894 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.179969 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.180314 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.180984 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.183461 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.184078 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.184153 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:43:00.184186 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:43:00.184243 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.184367 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:43:00.184472 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:43:00.184509 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.186328 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.186420 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.188729 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.188811 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.188917 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.191151 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.192963 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.193057 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.193333 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.193411 140324316274688 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:43:00.193517 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:43:00.193554 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:43:00.193583 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:43:00.193650 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.195818 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:43:00.201072 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.201323 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:43:00.203932 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:43:00.216115 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:43:00.216170 140324316274688 attention.py:418] Single window, no scan.
I0123 12:43:00.216205 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:43:00.216233 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.216292 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.216838 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.216912 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.217257 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.217926 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.220406 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.221013 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.221088 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:43:00.221121 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:43:00.221176 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.221300 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:43:00.221404 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:43:00.221440 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.223431 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.223523 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.226008 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.226090 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.226198 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.228816 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.230639 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.230733 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.231009 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.231088 140324316274688 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:43:00.231192 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:43:00.231230 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:43:00.231258 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:43:00.231318 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.233520 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:43:00.238754 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.239005 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:43:00.241616 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:43:00.253774 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:43:00.253828 140324316274688 attention.py:418] Single window, no scan.
I0123 12:43:00.253863 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:43:00.253891 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.253950 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.254500 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.254574 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.254917 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.255579 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.258020 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.258632 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.258707 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:43:00.258740 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:43:00.258795 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.258918 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:43:00.259021 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:43:00.259057 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.261200 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.261292 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.263594 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.263671 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.263780 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.265966 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.267763 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.267857 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.268132 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.268210 140324316274688 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:43:00.268314 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:43:00.268350 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:43:00.268379 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:43:00.268437 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.270592 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:43:00.275855 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.276107 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:43:00.278729 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:43:00.290944 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:43:00.290999 140324316274688 attention.py:418] Single window, no scan.
I0123 12:43:00.291032 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:43:00.291061 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.291125 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.291674 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.291748 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.292093 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.292765 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.295257 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.295879 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.295957 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:43:00.295990 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:43:00.296046 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.296170 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:43:00.296274 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:43:00.296311 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.298151 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.298241 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.300539 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.300617 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.300721 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.302947 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.304742 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.304833 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.305109 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.305186 140324316274688 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:43:00.305291 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:43:00.305328 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:43:00.305357 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:43:00.305417 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.307590 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:43:00.312816 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.313063 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:43:00.315679 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:43:00.328018 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:43:00.328072 140324316274688 attention.py:418] Single window, no scan.
I0123 12:43:00.328108 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:43:00.328136 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.328199 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.328746 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.328820 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.329166 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.329844 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.332316 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.332929 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.333004 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:43:00.333037 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:43:00.333092 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.333214 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:43:00.333318 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:43:00.333354 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.335177 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.335269 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.337583 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.337670 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:43:00.337777 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:43:00.340360 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:43:00.342182 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.342276 140324316274688 nn_components.py:261] mlp: residual
I0123 12:43:00.342551 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:00.342636 140324316274688 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:43:00.345381 140324316274688 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:43:05.041932 140324316274688 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:43:05.598815 140324316274688 training_loop.py:409] No working directory specified.
I0123 12:43:05.598970 140324316274688 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:43:05.599967 140324316274688 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:43:08.826207 140324316274688 training_loop.py:447] Only restoring trainable parameters.
I0123 12:43:08.827165 140324316274688 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:43:08.827230 140324316274688 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.827277 140324316274688 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.827321 140324316274688 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.827361 140324316274688 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.827400 140324316274688 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.827439 140324316274688 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.827477 140324316274688 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.827515 140324316274688 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.827553 140324316274688 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.827589 140324316274688 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.827626 140324316274688 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.827663 140324316274688 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.827700 140324316274688 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.827736 140324316274688 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.827773 140324316274688 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.827810 140324316274688 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.827845 140324316274688 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.827882 140324316274688 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.827918 140324316274688 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.827976 140324316274688 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828015 140324316274688 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.828051 140324316274688 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.828087 140324316274688 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.828124 140324316274688 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828161 140324316274688 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.828198 140324316274688 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828235 140324316274688 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828271 140324316274688 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.828307 140324316274688 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.828343 140324316274688 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828379 140324316274688 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.828415 140324316274688 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.828451 140324316274688 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.828486 140324316274688 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828521 140324316274688 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.828557 140324316274688 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828592 140324316274688 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828627 140324316274688 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.828662 140324316274688 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.828697 140324316274688 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828733 140324316274688 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.828768 140324316274688 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.828803 140324316274688 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.828839 140324316274688 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828873 140324316274688 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.828914 140324316274688 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828951 140324316274688 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.828986 140324316274688 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.829021 140324316274688 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.829056 140324316274688 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829091 140324316274688 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.829126 140324316274688 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.829161 140324316274688 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.829197 140324316274688 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829231 140324316274688 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.829267 140324316274688 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829302 140324316274688 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829338 140324316274688 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.829374 140324316274688 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.829410 140324316274688 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829444 140324316274688 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.829480 140324316274688 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.829515 140324316274688 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.829550 140324316274688 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829585 140324316274688 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.829620 140324316274688 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829665 140324316274688 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829703 140324316274688 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.829739 140324316274688 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.829772 140324316274688 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829808 140324316274688 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.829843 140324316274688 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.829885 140324316274688 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.829921 140324316274688 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.829956 140324316274688 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.829992 140324316274688 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830027 140324316274688 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830062 140324316274688 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.830098 140324316274688 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.830133 140324316274688 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830169 140324316274688 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.830204 140324316274688 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.830240 140324316274688 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.830276 140324316274688 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830312 140324316274688 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.830348 140324316274688 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830383 140324316274688 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830418 140324316274688 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.830452 140324316274688 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.830487 140324316274688 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830522 140324316274688 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.830558 140324316274688 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.830593 140324316274688 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.830628 140324316274688 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830663 140324316274688 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.830698 140324316274688 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830733 140324316274688 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830769 140324316274688 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.830804 140324316274688 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.830843 140324316274688 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.830880 140324316274688 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.830917 140324316274688 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.830952 140324316274688 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.830987 140324316274688 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.831023 140324316274688 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.831058 140324316274688 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.831093 140324316274688 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.831128 140324316274688 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.831164 140324316274688 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.831199 140324316274688 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.831234 140324316274688 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.831269 140324316274688 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:43:08.831304 140324316274688 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:43:08.831340 140324316274688 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.831375 140324316274688 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.831411 140324316274688 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.831446 140324316274688 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.831481 140324316274688 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:43:08.831515 140324316274688 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:43:08.831549 140324316274688 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:43:08.831584 140324316274688 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:43:08.831614 140324316274688 training_loop.py:725] Total parameters: 152072288
I0123 12:43:08.831953 140324316274688 training_loop.py:739] Total state size: 0
I0123 12:43:08.853412 140324316274688 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:43:08.853866 140324316274688 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:43:08.854340 140324316274688 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:43:08.854815 140324316274688 training_loop.py:89] registering functions: dict_keys([])
I0123 12:43:08.877243 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f ? coll d o m
I0123 12:43:18.380486 140324316274688 ddar.py:60] Depth 1/1000 time = 9.46443486213684
I0123 12:43:26.789116 140324316274688 ddar.py:60] Depth 2/1000 time = 8.40835165977478
I0123 12:43:36.758139 140324316274688 ddar.py:60] Depth 3/1000 time = 9.968522310256958
I0123 12:43:47.599494 140324316274688 ddar.py:60] Depth 4/1000 time = 10.840965986251831
I0123 12:43:59.944649 140324316274688 ddar.py:60] Depth 5/1000 time = 12.344883918762207
I0123 12:44:13.578631 140324316274688 ddar.py:60] Depth 6/1000 time = 13.63370394706726
I0123 12:44:26.934961 140324316274688 ddar.py:60] Depth 7/1000 time = 13.356035947799683
I0123 12:44:45.037575 140324316274688 ddar.py:60] Depth 8/1000 time = 18.10228657722473
I0123 12:45:08.623009 140324316274688 ddar.py:60] Depth 9/1000 time = 23.585039615631104
I0123 12:45:36.959535 140324316274688 ddar.py:60] Depth 10/1000 time = 28.336103677749634
I0123 12:46:04.873760 140324316274688 ddar.py:60] Depth 11/1000 time = 27.91376519203186
I0123 12:46:32.237395 140324316274688 ddar.py:60] Depth 12/1000 time = 27.363086700439453
I0123 12:46:59.428789 140324316274688 ddar.py:60] Depth 13/1000 time = 27.190160989761353
I0123 12:47:27.251591 140324316274688 ddar.py:60] Depth 14/1000 time = 27.7286958694458
I0123 12:47:27.323302 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:47:27.323454 140324316274688 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 12:47:27.323496 140324316274688 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a b e 02 C c d e 03 ; f : C d e f 04 ; g : C b c g 05 T b c f g 06 ; h : C a c h 07 T a c f h 08 ; i : C a b i 09 D b g b i 10 ; j : C a b j 11 D b g b j 12 ; k : C a b k 13 D a h a k 14 ; l : C a b l 15 D a h a l 16 ; m : C g k m 17 C h j m 18 ; n : C g j n 19 C h k n 20 ; o : C f n o 21 D f n f o 22 ? C d o m {F1} x00
I0123 12:47:27.323528 140324316274688 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a b e 02 C c d e 03 ; f : C d e f 04 ; g : C b c g 05 T b c f g 06 ; h : C a c h 07 T a c f h 08 ; i : C a b i 09 D b g b i 10 ; j : C a b j 11 D b g b j 12 ; k : C a b k 13 D a h a k 14 ; l : C a b l 15 D a h a l 16 ; m : C g k m 17 C h j m 18 ; n : C g j n 19 C h k n 20 ; o : C f n o 21 D f n f o 22 ? C d o m {F1} x00
I0123 12:47:27.481347 140324316274688 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.481573 140324316274688 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:47:27.481691 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.481769 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.481840 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.481907 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.481975 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.482042 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.482107 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.482172 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.482236 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.482316 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.482383 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.482448 140324316274688 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:47:27.482491 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.482541 140324316274688 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:47:27.482651 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.482689 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.482718 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.484721 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.487303 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.493056 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.493339 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.496020 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.500028 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.500086 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.500123 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.500167 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.500231 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.500889 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.500965 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.501321 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.502112 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.504672 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.505304 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.505381 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.505415 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.505477 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.505604 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.505939 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.505985 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.507874 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.507967 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.510408 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.510487 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.510913 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.513331 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.515260 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.515354 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.515639 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.515718 140324316274688 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:47:27.515824 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.515861 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.515890 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.517702 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.519983 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.525568 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.525834 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.528364 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.532054 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.532109 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.532145 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.532175 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.532235 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.532847 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.532923 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.533273 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.534034 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.536439 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.537056 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.537132 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.537166 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.537223 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.537350 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.537672 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.537714 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.539674 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.539766 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.542204 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.542283 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.542707 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.544947 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.546959 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.547060 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.547344 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.547424 140324316274688 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:47:27.547531 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.547569 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.547599 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.549491 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.551786 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.557309 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.557566 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.560084 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.563771 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.563826 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.563862 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.563893 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.563954 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.564511 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.564586 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.564936 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.565705 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.568122 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.568734 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.568811 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.568846 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.568905 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.569031 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.569753 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.569798 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.571698 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.571789 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.574225 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.574305 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.574729 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.576972 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.578958 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.579054 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.579347 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.579428 140324316274688 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:47:27.579535 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.579573 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.579604 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.581400 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.583691 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.589272 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.589525 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.592029 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.595713 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.595769 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.595803 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.595833 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.595893 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.596503 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.596579 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.596936 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.597702 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.600106 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.600723 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.600800 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.600833 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.600890 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.601018 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.601332 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.601373 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.603341 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.603432 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.605844 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.605922 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.606341 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.608569 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.610455 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.610548 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.610831 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.610918 140324316274688 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:47:27.611027 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.611065 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.611095 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.612961 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.615257 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.620694 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.620950 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.623492 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.627166 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.627221 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.627256 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.627286 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.627347 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.627896 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.627972 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.628321 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.629069 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.631477 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.632093 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.632168 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.632202 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.632259 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.632384 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.632752 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.632797 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.634678 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.634770 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.637181 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.637259 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.637696 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.639941 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.641904 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.641999 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.642284 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.642365 140324316274688 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:47:27.642479 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.642518 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.642548 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.644336 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.646630 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.652217 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.652472 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.655007 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.658660 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.658715 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.658750 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.658779 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.658841 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.659442 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.659518 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.659864 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.660609 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.663017 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.663632 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.663708 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.663742 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.663798 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.663943 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.664260 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.664301 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.666255 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.666347 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.668769 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.668846 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.669271 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.671513 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.673381 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.673474 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.673767 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.673847 140324316274688 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:47:27.673953 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.673996 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.674027 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.676216 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.678501 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.684005 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.684261 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.686796 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.690492 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.690547 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.690582 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.690613 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.690673 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.691222 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.691297 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.691644 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.692391 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.694810 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.695419 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.695496 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.695530 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.695587 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.695714 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.696077 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.696120 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.698017 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.698110 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.700521 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.700600 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.701021 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.703248 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.705213 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.705307 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.705592 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.705680 140324316274688 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:47:27.705790 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.705829 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.705865 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.707657 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.709933 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.715497 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.715755 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.718276 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.721894 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.721949 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.721984 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.722016 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.722077 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.722685 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.722761 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.723111 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.723863 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.726273 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.726885 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.726961 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.726995 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.727053 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.727178 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.727489 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.727530 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.729452 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.729542 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.731941 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.732019 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.732439 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.734640 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.736500 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.736593 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.736872 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.736951 140324316274688 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:47:27.737056 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.737094 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.737123 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.738986 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.741250 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.746730 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.746983 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.749540 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.753176 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.753231 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.753266 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.753296 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.753357 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.753922 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.753999 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.754350 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.755095 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.757484 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.758095 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.758173 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.758207 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.758264 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.758388 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.758752 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.758794 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.760693 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.760783 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.763204 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.763282 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.763699 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.765914 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.767785 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.767877 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.768157 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.768237 140324316274688 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:47:27.768342 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.768380 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.768410 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.770253 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.772516 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.777971 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.778225 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.780715 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.784402 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.784457 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.784492 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.784523 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.784585 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.785140 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.785214 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.785564 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.786319 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.788730 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.789342 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.789418 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.789452 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.789510 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.789636 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.789961 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.790003 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.792304 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.792397 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.794835 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.794914 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.795331 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.797535 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.799398 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.799492 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.799774 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.799853 140324316274688 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:47:27.799959 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.799996 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.800027 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.801894 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.804169 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.809683 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.809944 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.812501 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.816184 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.816239 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.816274 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.816304 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.816365 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.816916 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.816991 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.817333 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.818083 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.820461 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.821072 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.821148 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.821181 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.821238 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.821363 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.821687 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.821731 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.823649 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.823740 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.826141 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.826220 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.826641 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.828849 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.830736 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.830829 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.831111 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.831190 140324316274688 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:47:27.831296 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.831333 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.831363 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.833222 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.835508 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.840979 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.841232 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.843738 140324316274688 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:47:27.847384 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.847439 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.847474 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.847505 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.847565 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.848116 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.848191 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.848539 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.849285 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.851717 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.852329 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.852406 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.852440 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.852499 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.852626 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.852941 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.852983 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.854935 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.855028 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.857437 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.857515 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.857942 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.860170 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.862082 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.862176 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.862458 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.862707 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.862774 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.862831 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.862885 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.862938 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.862990 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.863052 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.863106 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.863160 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.863212 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.863264 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.863315 140324316274688 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:47:27.863351 140324316274688 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:47:27.866279 140324316274688 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:47:27.910767 140324316274688 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.910852 140324316274688 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:47:27.910903 140324316274688 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:47:27.911004 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.911041 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.911069 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.911131 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.913448 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.918676 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.918932 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.921426 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:27.934142 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.934198 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.934231 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.934262 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.934323 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.934891 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.934967 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.935327 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.936016 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.938546 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.939170 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.939246 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.939279 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.939335 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.939460 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.939566 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.939610 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.941418 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.941509 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.943871 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.943950 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.944056 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.946275 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.948086 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.948179 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.948462 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.948540 140324316274688 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:47:27.948646 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.948683 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.948712 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.948773 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.950953 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.956250 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.956506 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.959118 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:27.971315 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:27.971370 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:27.971404 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:27.971434 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.971495 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.972051 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.972126 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.972473 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.973578 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.976019 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.976642 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.976718 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:27.976752 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:27.976811 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.976942 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:27.977051 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:27.977088 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.978947 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.979040 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.981424 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.981503 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:27.981611 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:27.983842 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:27.985805 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.985898 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:27.986184 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.986262 140324316274688 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:47:27.986369 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:27.986406 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:27.986436 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:27.986499 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.988696 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:27.994017 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:27.994274 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:27.996909 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.009241 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.009296 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.009331 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.009362 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.009422 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.009981 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.010059 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.010409 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.011140 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.013574 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.014197 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.014274 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.014308 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.014366 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.014494 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.014600 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.014637 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.016489 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.016589 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.018990 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.019071 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.019179 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.021408 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.023244 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.023339 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.023625 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.023705 140324316274688 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:47:28.023811 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.023849 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.023880 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.023941 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.026149 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.031510 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.031768 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.034407 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.046730 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.046785 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.046819 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.046849 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.046910 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.047464 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.047539 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.047890 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.048615 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.051067 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.051682 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.051758 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.051791 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.051849 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.051974 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.052079 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.052116 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.053961 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.055206 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.057586 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.057675 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.057783 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.060012 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.061872 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.061967 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.062253 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.062332 140324316274688 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:47:28.062437 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.062475 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.062506 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.062567 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.064754 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.070070 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.070332 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.072956 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.085453 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.085508 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.085543 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.085574 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.085635 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.086195 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.086272 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.086618 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.087739 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.090171 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.090789 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.090866 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.090900 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.090958 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.091090 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.091197 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.091235 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.093056 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.093148 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.095521 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.095601 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.095706 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.097938 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.099753 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.099846 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.100132 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.100212 140324316274688 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:47:28.100319 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.100357 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.100387 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.100449 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.102656 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.107954 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.108212 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.110836 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.123186 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.123242 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.123277 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.123307 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.123368 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.123920 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.124103 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.124451 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.125173 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.127583 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.128194 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.128270 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.128304 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.128363 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.128490 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.128597 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.128635 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.130458 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.130551 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.132908 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.132992 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.133100 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.135352 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.137170 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.137263 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.137550 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.137629 140324316274688 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:47:28.137744 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.137781 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.137811 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.137872 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.140062 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.145395 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.145659 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.148275 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.160705 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.160760 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.160794 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.160824 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.160885 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.161442 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.161519 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.161879 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.162612 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.165036 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.165652 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.165729 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.165763 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.165821 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.165947 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.166055 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.166091 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.167938 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.168029 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.170432 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.170516 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.170624 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.172867 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.174717 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.174812 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.175097 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.175176 140324316274688 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:47:28.175283 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.175320 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.175350 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.175412 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.177618 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.183014 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.183273 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.185903 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.198189 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.198243 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.198278 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.198309 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.198370 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.198925 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.199001 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.199354 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.200485 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.202935 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.203551 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.203628 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.203662 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.203722 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.203851 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.203960 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.203998 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.205837 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.205929 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.208302 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.208386 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.208496 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.210742 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.212579 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.212672 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.212958 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.213039 140324316274688 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:47:28.213146 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.213184 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.213215 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.213278 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.215499 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.220810 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.221068 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.223707 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.236284 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.236338 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.236374 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.236404 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.236465 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.237014 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.237091 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.237445 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.238130 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.240599 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.241210 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.241286 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.241320 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.241377 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.241504 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.241613 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.241657 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.243500 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.243592 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.245954 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.246032 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.246150 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.248368 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.250190 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.250284 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.250568 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.250647 140324316274688 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:47:28.250754 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.250791 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.250822 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.250884 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.253069 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.258360 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.258617 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.261236 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.273517 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.273572 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.273607 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.273637 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.273707 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.274262 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.274337 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.274684 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.275352 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.277810 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.278416 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.278491 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.278524 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.278580 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.278706 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.278810 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.278847 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.280662 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.280753 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.283105 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.283184 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.283292 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.285524 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.287372 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.287465 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.287750 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.287830 140324316274688 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:47:28.287935 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.287971 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.288001 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.288064 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.290264 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.295568 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.295826 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.298439 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.310727 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.310782 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.310817 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.310848 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.310908 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.311461 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.311537 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.311883 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.312550 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.315432 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.316046 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.316122 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.316156 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.316214 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.316340 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.316445 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.316482 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.318315 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.318407 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.320750 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.320828 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.320934 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.323169 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.325132 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.325226 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.325510 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.325590 140324316274688 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:47:28.325704 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.325743 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.325775 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.325838 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.328037 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.333311 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.333566 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.336177 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.348461 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.348517 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.348552 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.348582 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.348642 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.349192 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.349267 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.349617 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.350302 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.352757 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.353358 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.353433 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.353467 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.353523 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.353653 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.353760 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.353797 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.355605 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.355695 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.358034 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.358112 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.358217 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.360412 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.362213 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.362311 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.362593 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.362677 140324316274688 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:47:28.365440 140324316274688 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:47:28.415179 140324316274688 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.415286 140324316274688 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:47:28.415339 140324316274688 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:47:28.415441 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.415478 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.415507 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.415569 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.417807 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.423175 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.423429 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.425967 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.438401 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.438458 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.438494 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.438525 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.438587 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.439140 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.439216 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.439565 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.440234 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.442646 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.443250 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.443326 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.443360 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.443419 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.443546 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.443652 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.443690 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.445735 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.445827 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.448186 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.448279 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.448388 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.450548 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.452360 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.452454 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.452740 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.452820 140324316274688 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:47:28.452923 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.452959 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.452990 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.453052 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.455233 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.460579 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.460837 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.463377 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.475690 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.475745 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.475779 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.475808 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.475869 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.476418 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.476494 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.476844 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.477505 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.479894 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.480509 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.480584 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.480618 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.480676 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.480801 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.480905 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.480942 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.482834 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.482926 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.485262 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.485346 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.485455 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.487602 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.489405 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.489498 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.489791 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.489872 140324316274688 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:47:28.489976 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.490013 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.490043 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.490105 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.492269 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.498121 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.498377 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.500905 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.513156 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.513212 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.513247 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.513278 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.513339 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.513901 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.513978 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.514329 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.514994 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.517365 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.517983 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.518060 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.518094 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.518152 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.518278 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.518385 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.518422 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.520327 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.520419 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.522765 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.522845 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.522957 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.525100 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.526914 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.527008 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.527292 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.527373 140324316274688 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:47:28.527480 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.527518 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.527548 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.527610 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.529800 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.535151 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.535406 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.537921 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.550446 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.550501 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.550535 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.550565 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.550625 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.551170 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.551245 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.551591 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.552256 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.554646 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.555252 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.555329 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.555363 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.555420 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.555547 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.555652 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.555690 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.557570 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.557666 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.560002 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.560079 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.560198 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.562345 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.564142 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.564234 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.564516 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.564595 140324316274688 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:47:28.564701 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.564738 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.564768 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.564829 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.566996 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.572342 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.572600 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.575118 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.587324 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.587380 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.587414 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.587444 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.587503 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.588051 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.588125 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.588472 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.589131 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.591514 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.592116 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.592190 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.592224 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.592281 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.592405 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.592509 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.592547 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.594441 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.594533 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.596869 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.596947 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.597053 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.599211 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.601020 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.601114 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.601398 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.601478 140324316274688 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:47:28.601584 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.601622 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.601663 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.601728 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.603896 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.609681 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.609939 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.612453 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.624794 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.624850 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.624885 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.624915 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.624976 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.625526 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.625602 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.625958 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.626622 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.629014 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.629616 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.629700 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.629735 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.629793 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.629919 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.630026 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.630063 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.631958 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.632048 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.634383 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.634462 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.634568 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.636725 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.638547 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.638641 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.638927 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.639006 140324316274688 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:47:28.639112 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.639150 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.639181 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.639244 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.641415 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.646954 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.647209 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.649735 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.662046 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.662102 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.662136 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.662166 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.662227 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.662773 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.662847 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.663194 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.663855 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.666248 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.666856 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.666932 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.666966 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.667024 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.667151 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.667258 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.667295 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.669154 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.669246 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.671592 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.671670 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.671776 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.673923 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.675718 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.675818 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.676100 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.676179 140324316274688 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:47:28.676283 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.676321 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.676351 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.676414 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.678587 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.683933 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.684187 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.686712 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.699015 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.699072 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.699107 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.699136 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.699196 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.699743 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.699817 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.700163 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.700824 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.703220 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.703826 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.703900 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.703935 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.703993 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.704119 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.704224 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.704261 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.706165 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.706256 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.708598 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.708675 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.708783 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.710949 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.712790 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.712888 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.713175 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.713254 140324316274688 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:47:28.713361 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.713399 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.713429 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.713494 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.715701 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.721478 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.721745 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.724266 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.736620 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.736676 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.736711 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.736740 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.736799 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.737348 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.737421 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.737778 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.738443 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.740837 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.741443 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.741519 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.741553 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.741611 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.741746 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.741853 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.741890 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.743795 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.743885 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.746349 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.746426 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.746533 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.748666 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.750474 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.750566 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.750856 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.750935 140324316274688 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:47:28.751041 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.751079 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.751109 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.751170 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.753324 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.758677 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.758928 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.761419 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.773731 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.773786 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.773822 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.773853 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.773912 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.774459 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.774532 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.774878 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.775539 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.777933 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.778535 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.778609 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.778642 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.778700 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.778826 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.778934 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.778972 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.780837 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.780926 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.783265 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.783343 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.783450 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.785594 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.787409 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.787502 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.787784 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.787869 140324316274688 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:47:28.787976 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.788014 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.788044 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.788106 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.790281 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.795622 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.795876 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.798388 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.810604 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.810660 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.810695 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.810725 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.810784 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.811332 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.811406 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.811755 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.812418 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.814802 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.815408 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.815482 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.815516 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.815573 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.815698 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.815804 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.815840 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.817738 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.817874 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.820266 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.820343 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.820451 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.822615 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.824429 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.824520 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.824805 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.824887 140324316274688 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:47:28.824994 140324316274688 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:47:28.825031 140324316274688 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:47:28.825062 140324316274688 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:47:28.825124 140324316274688 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.827295 140324316274688 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:47:28.833053 140324316274688 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.833307 140324316274688 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:47:28.835858 140324316274688 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:47:28.848214 140324316274688 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:47:28.848269 140324316274688 attention.py:418] Single window, no scan.
I0123 12:47:28.848305 140324316274688 transformer_layer.py:389] tlayer: self-attention.
I0123 12:47:28.848335 140324316274688 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.848395 140324316274688 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.848943 140324316274688 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.849016 140324316274688 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.849362 140324316274688 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.850032 140324316274688 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.852388 140324316274688 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.852990 140324316274688 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.853064 140324316274688 transformer_layer.py:468] tlayer: End windows.
I0123 12:47:28.853097 140324316274688 transformer_layer.py:472] tlayer: final FFN.
I0123 12:47:28.853154 140324316274688 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.853278 140324316274688 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:47:28.853385 140324316274688 nn_components.py:325] mlp: activation = None
I0123 12:47:28.853423 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.855306 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.855397 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.857745 140324316274688 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.857823 140324316274688 transformer_base.py:443] tbase: final FFN
I0123 12:47:28.857929 140324316274688 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:47:28.860064 140324316274688 nn_components.py:329] mlp: final activation = None
I0123 12:47:28.861895 140324316274688 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.861987 140324316274688 nn_components.py:261] mlp: residual
I0123 12:47:28.862268 140324316274688 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:28.862350 140324316274688 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:47:28.865131 140324316274688 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:47:44.004273 140324316274688 alphageometry.py:566] LM output (score=-1.948536): "p : C h k p 23 D h p k p 24 ;"
I0123 12:47:44.004484 140324316274688 alphageometry.py:567] Translation: "p = on_line p h k, on_bline p k h"

I0123 12:47:44.004529 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p h k, on_bline p k h ? coll d o m"
I0123 12:47:44.004714 140324316274688 graph.py:498] 
I0123 12:47:44.004770 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p h k, on_bline p k h ? coll d o m
I0123 12:47:52.826223 140324316274688 ddar.py:60] Depth 1/1000 time = 8.77928113937378
I0123 12:48:04.261467 140324316274688 ddar.py:60] Depth 2/1000 time = 11.435006380081177
I0123 12:48:17.712922 140324316274688 ddar.py:60] Depth 3/1000 time = 13.451207160949707
I0123 12:48:32.804365 140324316274688 ddar.py:60] Depth 4/1000 time = 15.091200828552246
I0123 12:48:51.417488 140324316274688 ddar.py:60] Depth 5/1000 time = 18.612881898880005
I0123 12:49:11.192883 140324316274688 ddar.py:60] Depth 6/1000 time = 19.775127410888672
I0123 12:49:31.186904 140324316274688 ddar.py:60] Depth 7/1000 time = 19.99374032020569
I0123 12:49:57.112881 140324316274688 ddar.py:60] Depth 8/1000 time = 25.925659894943237
I0123 12:50:28.853677 140324316274688 ddar.py:60] Depth 9/1000 time = 31.740330934524536
I0123 12:51:07.017986 140324316274688 ddar.py:60] Depth 10/1000 time = 38.16374087333679
I0123 12:51:44.089844 140324316274688 ddar.py:60] Depth 11/1000 time = 37.07133412361145
I0123 12:52:22.323116 140324316274688 ddar.py:60] Depth 12/1000 time = 38.232609033584595
I0123 12:53:00.591904 140324316274688 ddar.py:60] Depth 13/1000 time = 38.26748728752136
I0123 12:53:38.646610 140324316274688 ddar.py:60] Depth 14/1000 time = 37.97001647949219
I0123 12:54:16.530263 140324316274688 ddar.py:60] Depth 15/1000 time = 37.864513874053955
I0123 12:54:16.605747 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:54:16.605976 140324316274688 alphageometry.py:566] LM output (score=-2.034154): "p : C h l p 23 D h p l p 24 ;"
I0123 12:54:16.606016 140324316274688 alphageometry.py:567] Translation: "p = on_line p h l, on_bline p l h"

I0123 12:54:16.606075 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p h l, on_bline p l h ? coll d o m"
I0123 12:54:16.606306 140324316274688 graph.py:498] 
I0123 12:54:16.606369 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p h l, on_bline p l h ? coll d o m
I0123 12:54:25.292402 140324316274688 ddar.py:60] Depth 1/1000 time = 8.615309000015259
I0123 12:54:35.526309 140324316274688 ddar.py:60] Depth 2/1000 time = 10.233633518218994
I0123 12:54:48.091612 140324316274688 ddar.py:60] Depth 3/1000 time = 12.564945220947266
I0123 12:55:01.330000 140324316274688 ddar.py:60] Depth 4/1000 time = 13.238127708435059
I0123 12:55:17.209986 140324316274688 ddar.py:60] Depth 5/1000 time = 15.87967038154602
I0123 12:55:34.074898 140324316274688 ddar.py:60] Depth 6/1000 time = 16.864548206329346
I0123 12:55:51.542208 140324316274688 ddar.py:60] Depth 7/1000 time = 17.467012643814087
I0123 12:56:14.940855 140324316274688 ddar.py:60] Depth 8/1000 time = 23.398321628570557
I0123 12:56:42.617946 140324316274688 ddar.py:60] Depth 9/1000 time = 27.676735877990723
I0123 12:57:18.563637 140324316274688 ddar.py:60] Depth 10/1000 time = 35.945292949676514
I0123 12:57:53.184118 140324316274688 ddar.py:60] Depth 11/1000 time = 34.62003564834595
I0123 12:58:29.284019 140324316274688 ddar.py:60] Depth 12/1000 time = 36.09922766685486
I0123 12:59:05.298433 140324316274688 ddar.py:60] Depth 13/1000 time = 36.013206243515015
I0123 12:59:41.904424 140324316274688 ddar.py:60] Depth 14/1000 time = 36.50162220001221
I0123 12:59:41.977064 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:59:41.977220 140324316274688 alphageometry.py:566] LM output (score=-2.134670): "p : D f p g p 23 D g p h p 24 ;"
I0123 12:59:41.977262 140324316274688 alphageometry.py:567] Translation: "p = on_bline p g f, on_bline p h g"

I0123 12:59:41.977313 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_bline p g f, on_bline p h g ? coll d o m"
I0123 12:59:41.977524 140324316274688 graph.py:498] 
I0123 12:59:41.977588 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_bline p g f, on_bline p h g ? coll d o m
I0123 12:59:49.210311 140324316274688 ddar.py:60] Depth 1/1000 time = 7.1562628746032715
I0123 12:59:59.105144 140324316274688 ddar.py:60] Depth 2/1000 time = 9.894593954086304
I0123 13:00:14.855847 140324316274688 ddar.py:60] Depth 3/1000 time = 15.750454664230347
I0123 13:00:33.102363 140324316274688 ddar.py:60] Depth 4/1000 time = 18.246224403381348
I0123 13:00:53.236845 140324316274688 ddar.py:60] Depth 5/1000 time = 20.134109020233154
I0123 13:01:15.049648 140324316274688 ddar.py:60] Depth 6/1000 time = 21.812324047088623
I0123 13:01:37.139717 140324316274688 ddar.py:60] Depth 7/1000 time = 22.08961844444275
I0123 13:02:04.532883 140324316274688 ddar.py:60] Depth 8/1000 time = 27.39280867576599
I0123 13:02:38.213196 140324316274688 ddar.py:60] Depth 9/1000 time = 33.679903745651245
I0123 13:03:18.675043 140324316274688 ddar.py:60] Depth 10/1000 time = 40.46136927604675
I0123 13:03:57.661820 140324316274688 ddar.py:60] Depth 11/1000 time = 38.986300230026245
I0123 13:04:36.872583 140324316274688 ddar.py:60] Depth 12/1000 time = 39.21023893356323
I0123 13:05:14.874700 140324316274688 ddar.py:60] Depth 13/1000 time = 38.000951051712036
I0123 13:05:54.619228 140324316274688 ddar.py:60] Depth 14/1000 time = 39.61847114562988
I0123 13:05:54.705341 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:05:54.705525 140324316274688 alphageometry.py:566] LM output (score=-2.198547): "p : C l m p 23 D l p m p 24 ;"
I0123 13:05:54.705564 140324316274688 alphageometry.py:567] Translation: "p = on_line p l m, on_bline p m l"

I0123 13:05:54.705655 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p l m, on_bline p m l ? coll d o m"
I0123 13:05:54.705887 140324316274688 graph.py:498] 
I0123 13:05:54.705950 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p l m, on_bline p m l ? coll d o m
I0123 13:06:01.998306 140324316274688 ddar.py:60] Depth 1/1000 time = 7.234247207641602
I0123 13:06:12.480612 140324316274688 ddar.py:60] Depth 2/1000 time = 10.482057571411133
I0123 13:06:24.255159 140324316274688 ddar.py:60] Depth 3/1000 time = 11.774286031723022
I0123 13:06:37.000947 140324316274688 ddar.py:60] Depth 4/1000 time = 12.745476484298706
I0123 13:06:52.083461 140324316274688 ddar.py:60] Depth 5/1000 time = 15.082149982452393
I0123 13:07:08.288080 140324316274688 ddar.py:60] Depth 6/1000 time = 16.204342365264893
I0123 13:07:24.637794 140324316274688 ddar.py:60] Depth 7/1000 time = 16.34934091567993
I0123 13:07:46.448450 140324316274688 ddar.py:60] Depth 8/1000 time = 21.810139894485474
I0123 13:08:13.312347 140324316274688 ddar.py:60] Depth 9/1000 time = 26.863404035568237
I0123 13:08:45.291011 140324316274688 ddar.py:60] Depth 10/1000 time = 31.978241682052612
I0123 13:09:16.839432 140324316274688 ddar.py:60] Depth 11/1000 time = 31.54787039756775
I0123 13:09:47.609514 140324316274688 ddar.py:60] Depth 12/1000 time = 30.769481420516968
I0123 13:10:18.991604 140324316274688 ddar.py:60] Depth 13/1000 time = 31.380757331848145
I0123 13:10:50.295878 140324316274688 ddar.py:60] Depth 14/1000 time = 31.20018434524536
I0123 13:10:50.368823 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:10:50.369035 140324316274688 alphageometry.py:566] LM output (score=-2.292715): "p : C i j p 23 D i p j p 24 ;"
I0123 13:10:50.369080 140324316274688 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2635, in add_clause
    raise PointTooCloseError()
graph.PointTooCloseError
"

I0123 13:10:50.369126 140324316274688 alphageometry.py:566] LM output (score=-2.307819): "p : D f p g p 23 ;"
I0123 13:10:50.369156 140324316274688 alphageometry.py:567] Translation: "p = on_bline p g f"

I0123 13:10:50.369195 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_bline p g f ? coll d o m"
I0123 13:10:50.369403 140324316274688 graph.py:498] 
I0123 13:10:50.369468 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_bline p g f ? coll d o m
I0123 13:10:58.726639 140324316274688 ddar.py:60] Depth 1/1000 time = 8.313645601272583
I0123 13:11:08.379235 140324316274688 ddar.py:60] Depth 2/1000 time = 9.652330160140991
I0123 13:11:19.318204 140324316274688 ddar.py:60] Depth 3/1000 time = 10.938668489456177
I0123 13:11:31.391424 140324316274688 ddar.py:60] Depth 4/1000 time = 12.07285189628601
I0123 13:11:44.879607 140324316274688 ddar.py:60] Depth 5/1000 time = 13.48790454864502
I0123 13:11:59.871180 140324316274688 ddar.py:60] Depth 6/1000 time = 14.99128246307373
I0123 13:12:15.620653 140324316274688 ddar.py:60] Depth 7/1000 time = 15.749183177947998
I0123 13:12:34.960980 140324316274688 ddar.py:60] Depth 8/1000 time = 19.34000015258789
I0123 13:13:00.620227 140324316274688 ddar.py:60] Depth 9/1000 time = 25.658855199813843
I0123 13:13:31.190371 140324316274688 ddar.py:60] Depth 10/1000 time = 30.569592237472534
I0123 13:14:01.830060 140324316274688 ddar.py:60] Depth 11/1000 time = 30.639036178588867
I0123 13:14:32.854365 140324316274688 ddar.py:60] Depth 12/1000 time = 31.023679733276367
I0123 13:15:03.034688 140324316274688 ddar.py:60] Depth 13/1000 time = 30.179214239120483
I0123 13:15:33.863963 140324316274688 ddar.py:60] Depth 14/1000 time = 30.822304725646973
I0123 13:16:05.303682 140324316274688 ddar.py:60] Depth 15/1000 time = 31.346450805664062
I0123 13:16:05.375211 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:16:05.375401 140324316274688 alphageometry.py:566] LM output (score=-2.337214): "p : C k l p 23 D k p l p 24 ;"
I0123 13:16:05.375440 140324316274688 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2635, in add_clause
    raise PointTooCloseError()
graph.PointTooCloseError
"

I0123 13:16:05.375501 140324316274688 alphageometry.py:566] LM output (score=-2.437339): "p : T d e e p 23 ;"
I0123 13:16:05.375530 140324316274688 alphageometry.py:567] Translation: "p = on_tline p e d e"

I0123 13:16:05.375566 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_tline p e d e ? coll d o m"
I0123 13:16:05.375988 140324316274688 graph.py:498] 
I0123 13:16:05.376052 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_tline p e d e ? coll d o m
I0123 13:16:12.828993 140324316274688 ddar.py:60] Depth 1/1000 time = 7.3421852588653564
I0123 13:16:22.328984 140324316274688 ddar.py:60] Depth 2/1000 time = 9.499760627746582
I0123 13:16:33.736293 140324316274688 ddar.py:60] Depth 3/1000 time = 11.407062530517578
I0123 13:16:46.022600 140324316274688 ddar.py:60] Depth 4/1000 time = 12.28604793548584
I0123 13:17:00.955120 140324316274688 ddar.py:60] Depth 5/1000 time = 14.932257175445557
I0123 13:17:16.721230 140324316274688 ddar.py:60] Depth 6/1000 time = 15.765844345092773
I0123 13:17:32.674008 140324316274688 ddar.py:60] Depth 7/1000 time = 15.952480792999268
I0123 13:17:52.958685 140324316274688 ddar.py:60] Depth 8/1000 time = 20.284278869628906
I0123 13:18:19.016702 140324316274688 ddar.py:60] Depth 9/1000 time = 26.057404041290283
I0123 13:18:51.662355 140324316274688 ddar.py:60] Depth 10/1000 time = 32.64504647254944
I0123 13:19:23.150696 140324316274688 ddar.py:60] Depth 11/1000 time = 31.487805366516113
I0123 13:19:54.975842 140324316274688 ddar.py:60] Depth 12/1000 time = 31.82451057434082
I0123 13:20:26.873791 140324316274688 ddar.py:60] Depth 13/1000 time = 31.89672303199768
I0123 13:20:58.534618 140324316274688 ddar.py:60] Depth 14/1000 time = 31.556137323379517
I0123 13:20:58.607738 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:20:58.607929 140324316274688 alphageometry.py:566] LM output (score=-2.510789): "p : C f g p 23 D f p g p 24 ;"
I0123 13:20:58.607969 140324316274688 alphageometry.py:567] Translation: "p = on_line p f g, on_bline p g f"

I0123 13:20:58.608037 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p f g, on_bline p g f ? coll d o m"
I0123 13:20:58.608267 140324316274688 graph.py:498] 
I0123 13:20:58.608330 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p f g, on_bline p g f ? coll d o m
I0123 13:21:07.611999 140324316274688 ddar.py:60] Depth 1/1000 time = 8.889180660247803
I0123 13:21:17.457663 140324316274688 ddar.py:60] Depth 2/1000 time = 9.84529972076416
I0123 13:21:29.527234 140324316274688 ddar.py:60] Depth 3/1000 time = 12.069256067276001
I0123 13:21:42.539632 140324316274688 ddar.py:60] Depth 4/1000 time = 13.012017965316772
I0123 13:21:57.025220 140324316274688 ddar.py:60] Depth 5/1000 time = 14.485331535339355
I0123 13:22:13.356761 140324316274688 ddar.py:60] Depth 6/1000 time = 16.331212759017944
I0123 13:22:29.302055 140324316274688 ddar.py:60] Depth 7/1000 time = 15.94488000869751
I0123 13:22:51.108931 140324316274688 ddar.py:60] Depth 8/1000 time = 21.80657935142517
I0123 13:23:18.769166 140324316274688 ddar.py:60] Depth 9/1000 time = 27.659886598587036
I0123 13:23:52.039819 140324316274688 ddar.py:60] Depth 10/1000 time = 33.270259618759155
I0123 13:24:24.828344 140324316274688 ddar.py:60] Depth 11/1000 time = 32.78799915313721
I0123 13:24:57.624878 140324316274688 ddar.py:60] Depth 12/1000 time = 32.79591250419617
I0123 13:25:30.550654 140324316274688 ddar.py:60] Depth 13/1000 time = 32.924471855163574
I0123 13:26:02.779142 140324316274688 ddar.py:60] Depth 14/1000 time = 32.226391315460205
I0123 13:26:35.846348 140324316274688 ddar.py:60] Depth 15/1000 time = 32.96830630302429
I0123 13:26:35.919729 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:26:35.919889 140324316274688 alphageometry.py:566] LM output (score=-2.517966): "p : C c e p 23 D c p e p 24 ;"
I0123 13:26:35.919930 140324316274688 alphageometry.py:567] Translation: "p = on_line p c e, on_bline p e c"

I0123 13:26:35.919981 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p c e, on_bline p e c ? coll d o m"
I0123 13:26:35.920182 140324316274688 graph.py:498] 
I0123 13:26:35.920249 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p c e, on_bline p e c ? coll d o m
I0123 13:26:48.997681 140324316274688 ddar.py:60] Depth 1/1000 time = 13.004283666610718
I0123 13:27:04.456805 140324316274688 ddar.py:60] Depth 2/1000 time = 15.458866357803345
I0123 13:27:21.291615 140324316274688 ddar.py:60] Depth 3/1000 time = 16.834529876708984
I0123 13:27:39.243386 140324316274688 ddar.py:60] Depth 4/1000 time = 17.95145034790039
I0123 13:27:59.551052 140324316274688 ddar.py:60] Depth 5/1000 time = 20.30722141265869
I0123 13:28:21.891811 140324316274688 ddar.py:60] Depth 6/1000 time = 22.340349435806274
I0123 13:28:42.912626 140324316274688 ddar.py:60] Depth 7/1000 time = 21.020439624786377
I0123 13:29:10.812872 140324316274688 ddar.py:60] Depth 8/1000 time = 27.8997962474823
I0123 13:29:42.781272 140324316274688 ddar.py:60] Depth 9/1000 time = 31.967971086502075
I0123 13:30:22.808651 140324316274688 ddar.py:60] Depth 10/1000 time = 40.02681350708008
I0123 13:31:02.658564 140324316274688 ddar.py:60] Depth 11/1000 time = 39.84950351715088
I0123 13:31:40.333868 140324316274688 ddar.py:60] Depth 12/1000 time = 37.674755334854126
I0123 13:32:19.466205 140324316274688 ddar.py:60] Depth 13/1000 time = 39.13099455833435
I0123 13:32:58.348760 140324316274688 ddar.py:60] Depth 14/1000 time = 38.77148461341858
I0123 13:32:58.424216 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:32:58.424372 140324316274688 alphageometry.py:566] LM output (score=-2.527939): "p : C h n p 23 D h p k p 24 ;"
I0123 13:32:58.424411 140324316274688 alphageometry.py:567] Translation: "p = on_line p h n, on_bline p k h"

I0123 13:32:58.424460 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p h n, on_bline p k h ? coll d o m"
I0123 13:32:58.424659 140324316274688 graph.py:498] 
I0123 13:32:58.424722 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p h n, on_bline p k h ? coll d o m
I0123 13:33:07.537840 140324316274688 ddar.py:60] Depth 1/1000 time = 9.070294857025146
I0123 13:33:20.702692 140324316274688 ddar.py:60] Depth 2/1000 time = 13.164604663848877
I0123 13:33:36.641329 140324316274688 ddar.py:60] Depth 3/1000 time = 15.938383102416992
I0123 13:33:54.510510 140324316274688 ddar.py:60] Depth 4/1000 time = 17.868913650512695
I0123 13:34:14.845360 140324316274688 ddar.py:60] Depth 5/1000 time = 20.334587812423706
I0123 13:34:36.629614 140324316274688 ddar.py:60] Depth 6/1000 time = 21.78396701812744
I0123 13:34:58.285277 140324316274688 ddar.py:60] Depth 7/1000 time = 21.655343532562256
I0123 13:35:26.120439 140324316274688 ddar.py:60] Depth 8/1000 time = 27.834855556488037
I0123 13:36:00.108161 140324316274688 ddar.py:60] Depth 9/1000 time = 33.98733043670654
I0123 13:36:40.582085 140324316274688 ddar.py:60] Depth 10/1000 time = 40.473358154296875
I0123 13:37:19.637399 140324316274688 ddar.py:60] Depth 11/1000 time = 39.05477714538574
I0123 13:38:00.149312 140324316274688 ddar.py:60] Depth 12/1000 time = 40.511457443237305
I0123 13:38:40.205296 140324316274688 ddar.py:60] Depth 13/1000 time = 40.05483078956604
I0123 13:39:20.728643 140324316274688 ddar.py:60] Depth 14/1000 time = 40.43670082092285
I0123 13:40:00.991121 140324316274688 ddar.py:60] Depth 15/1000 time = 40.24297070503235
I0123 13:40:01.066442 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:40:01.066637 140324316274688 alphageometry.py:566] LM output (score=-2.537578): "p : C i k p 23 D i p k p 24 ;"
I0123 13:40:01.066681 140324316274688 alphageometry.py:567] Translation: "p = on_line p i k, on_bline p k i"

I0123 13:40:01.066739 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p i k, on_bline p k i ? coll d o m"
I0123 13:40:01.066949 140324316274688 graph.py:498] 
I0123 13:40:01.067021 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p i k, on_bline p k i ? coll d o m
I0123 13:40:14.991018 140324316274688 ddar.py:60] Depth 1/1000 time = 13.750774621963501
I0123 13:40:30.930161 140324316274688 ddar.py:60] Depth 2/1000 time = 15.938842058181763
I0123 13:40:48.136973 140324316274688 ddar.py:60] Depth 3/1000 time = 17.206488370895386
I0123 13:41:07.163049 140324316274688 ddar.py:60] Depth 4/1000 time = 19.025697946548462
I0123 13:41:28.205954 140324316274688 ddar.py:60] Depth 5/1000 time = 21.042644500732422
I0123 13:41:51.245309 140324316274688 ddar.py:60] Depth 6/1000 time = 23.03901219367981
I0123 13:42:13.888720 140324316274688 ddar.py:60] Depth 7/1000 time = 22.64290738105774
I0123 13:42:42.603906 140324316274688 ddar.py:60] Depth 8/1000 time = 28.71471619606018
I0123 13:43:17.197966 140324316274688 ddar.py:60] Depth 9/1000 time = 34.59366416931152
I0123 13:43:57.813737 140324316274688 ddar.py:60] Depth 10/1000 time = 40.615267276763916
I0123 13:44:39.730893 140324316274688 ddar.py:60] Depth 11/1000 time = 41.91669583320618
I0123 13:45:19.356262 140324316274688 ddar.py:60] Depth 12/1000 time = 39.62490487098694
I0123 13:45:58.545013 140324316274688 ddar.py:60] Depth 13/1000 time = 39.18666982650757
I0123 13:46:39.934466 140324316274688 ddar.py:60] Depth 14/1000 time = 41.32664394378662
I0123 13:47:19.429756 140324316274688 ddar.py:60] Depth 15/1000 time = 39.494662046432495
I0123 13:48:01.361865 140324316274688 ddar.py:60] Depth 16/1000 time = 41.89213824272156
I0123 13:48:01.438326 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:48:01.438544 140324316274688 alphageometry.py:566] LM output (score=-2.538986): "p : C i l p 23 D i p l p 24 ;"
I0123 13:48:01.438586 140324316274688 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2635, in add_clause
    raise PointTooCloseError()
graph.PointTooCloseError
"

I0123 13:48:01.438642 140324316274688 alphageometry.py:566] LM output (score=-2.548369): "p : C e g p 23 D e p g p 24 ;"
I0123 13:48:01.438671 140324316274688 alphageometry.py:567] Translation: "p = on_line p e g, on_bline p g e"

I0123 13:48:01.438707 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p e g, on_bline p g e ? coll d o m"
I0123 13:48:01.438937 140324316274688 graph.py:498] 
I0123 13:48:01.439024 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p e g, on_bline p g e ? coll d o m
I0123 13:48:11.261669 140324316274688 ddar.py:60] Depth 1/1000 time = 9.757036447525024
I0123 13:48:21.552463 140324316274688 ddar.py:60] Depth 2/1000 time = 10.290550708770752
I0123 13:48:33.930885 140324316274688 ddar.py:60] Depth 3/1000 time = 12.378161191940308
I0123 13:48:48.144220 140324316274688 ddar.py:60] Depth 4/1000 time = 14.213011503219604
I0123 13:49:03.380354 140324316274688 ddar.py:60] Depth 5/1000 time = 15.23576545715332
I0123 13:49:20.143703 140324316274688 ddar.py:60] Depth 6/1000 time = 16.763079404830933
I0123 13:49:37.145807 140324316274688 ddar.py:60] Depth 7/1000 time = 17.001800060272217
I0123 13:49:59.211457 140324316274688 ddar.py:60] Depth 8/1000 time = 22.065325021743774
I0123 13:50:25.990929 140324316274688 ddar.py:60] Depth 9/1000 time = 26.779067039489746
I0123 13:50:59.287314 140324316274688 ddar.py:60] Depth 10/1000 time = 33.29593324661255
I0123 13:51:31.020778 140324316274688 ddar.py:60] Depth 11/1000 time = 31.733011484146118
I0123 13:52:03.674864 140324316274688 ddar.py:60] Depth 12/1000 time = 32.6535587310791
I0123 13:52:34.755049 140324316274688 ddar.py:60] Depth 13/1000 time = 31.078890323638916
I0123 13:53:06.367765 140324316274688 ddar.py:60] Depth 14/1000 time = 31.514615774154663
I0123 13:53:06.439393 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:53:06.439580 140324316274688 alphageometry.py:566] LM output (score=-2.611334): "p : C j k p 23 D j p k p 24 ;"
I0123 13:53:06.439620 140324316274688 alphageometry.py:567] Translation: "p = on_line p j k, on_bline p k j"

I0123 13:53:06.439684 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p j k, on_bline p k j ? coll d o m"
I0123 13:53:06.439920 140324316274688 graph.py:498] 
I0123 13:53:06.439984 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_line p j k, on_bline p k j ? coll d o m
I0123 13:53:20.413667 140324316274688 ddar.py:60] Depth 1/1000 time = 13.844520807266235
I0123 13:53:36.502718 140324316274688 ddar.py:60] Depth 2/1000 time = 16.088781595230103
I0123 13:53:53.800789 140324316274688 ddar.py:60] Depth 3/1000 time = 17.29779815673828
I0123 13:54:13.585593 140324316274688 ddar.py:60] Depth 4/1000 time = 19.784522771835327
I0123 13:54:35.584177 140324316274688 ddar.py:60] Depth 5/1000 time = 21.99827265739441
I0123 13:54:59.854321 140324316274688 ddar.py:60] Depth 6/1000 time = 24.26983332633972
I0123 13:55:23.277479 140324316274688 ddar.py:60] Depth 7/1000 time = 23.422836303710938
I0123 13:55:55.438348 140324316274688 ddar.py:60] Depth 8/1000 time = 32.160512924194336
I0123 13:56:34.539859 140324316274688 ddar.py:60] Depth 9/1000 time = 39.10100317001343
I0123 13:57:21.004092 140324316274688 ddar.py:60] Depth 10/1000 time = 46.46376419067383
I0123 13:58:06.872031 140324316274688 ddar.py:60] Depth 11/1000 time = 45.86741924285889
I0123 13:58:52.425307 140324316274688 ddar.py:60] Depth 12/1000 time = 45.55271625518799
I0123 13:59:37.932466 140324316274688 ddar.py:60] Depth 13/1000 time = 45.504831314086914
I0123 14:00:24.021103 140324316274688 ddar.py:60] Depth 14/1000 time = 46.007935523986816
I0123 14:01:11.329626 140324316274688 ddar.py:60] Depth 15/1000 time = 47.29030179977417
I0123 14:01:56.931295 140324316274688 ddar.py:60] Depth 16/1000 time = 45.56881308555603
I0123 14:01:57.014327 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:01:57.014484 140324316274688 alphageometry.py:566] LM output (score=-2.615401): "p : T e i e p 23 ;"
I0123 14:01:57.014526 140324316274688 alphageometry.py:567] Translation: "p = on_tline p e e i"

I0123 14:01:57.014586 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_tline p e e i ? coll d o m"
I0123 14:01:57.014806 140324316274688 graph.py:498] 
I0123 14:01:57.014873 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_tline p e e i ? coll d o m
I0123 14:02:06.477882 140324316274688 ddar.py:60] Depth 1/1000 time = 9.369748592376709
I0123 14:02:16.455070 140324316274688 ddar.py:60] Depth 2/1000 time = 9.976887226104736
I0123 14:02:28.696649 140324316274688 ddar.py:60] Depth 3/1000 time = 12.241196870803833
I0123 14:02:42.972084 140324316274688 ddar.py:60] Depth 4/1000 time = 14.27518105506897
I0123 14:02:58.129954 140324316274688 ddar.py:60] Depth 5/1000 time = 15.157557249069214
I0123 14:03:15.023177 140324316274688 ddar.py:60] Depth 6/1000 time = 16.892840147018433
I0123 14:03:31.505360 140324316274688 ddar.py:60] Depth 7/1000 time = 16.481910467147827
I0123 14:03:53.643556 140324316274688 ddar.py:60] Depth 8/1000 time = 22.137894868850708
I0123 14:04:22.434618 140324316274688 ddar.py:60] Depth 9/1000 time = 28.79070734977722
I0123 14:04:56.261412 140324316274688 ddar.py:60] Depth 10/1000 time = 33.82627511024475
I0123 14:05:30.412175 140324316274688 ddar.py:60] Depth 11/1000 time = 34.15009140968323
I0123 14:06:05.117678 140324316274688 ddar.py:60] Depth 12/1000 time = 34.70486330986023
I0123 14:06:39.503656 140324316274688 ddar.py:60] Depth 13/1000 time = 34.38472294807434
I0123 14:07:13.063184 140324316274688 ddar.py:60] Depth 14/1000 time = 33.45436096191406
I0123 14:07:13.139645 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:07:13.139801 140324316274688 alphageometry.py:566] LM output (score=-2.644234): "p : D a p c p 23 D c p d p 24 ;"
I0123 14:07:13.139839 140324316274688 alphageometry.py:567] Translation: "p = on_bline p c a, on_bline p d c"

I0123 14:07:13.139886 140324316274688 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_bline p c a, on_bline p d c ? coll d o m"
I0123 14:07:13.140086 140324316274688 graph.py:498] 
I0123 14:07:13.140150 140324316274688 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = on_line e c d, on_line e b a; f = on_line f e d; g = foot g f c b; h = foot h f c a; i = on_circle i b g, on_line i b a; j = on_circle j b g, on_line j b a; k = on_circle k a h, on_line k b a; l = on_circle l a h, on_line l b a; m = on_line m g k, on_line m h j; n = on_line n g j, on_line n h k; o = mirror o n f; p = on_bline p c a, on_bline p d c ? coll d o m
I0123 14:07:22.289180 140324316274688 ddar.py:60] Depth 1/1000 time = 9.099037647247314
I0123 14:07:33.423534 140324316274688 ddar.py:60] Depth 2/1000 time = 11.134091854095459
I0123 14:07:45.822162 140324316274688 ddar.py:60] Depth 3/1000 time = 12.398367643356323
I0123 14:07:59.333638 140324316274688 ddar.py:60] Depth 4/1000 time = 13.511147260665894
I0123 14:08:14.811983 140324316274688 ddar.py:60] Depth 5/1000 time = 15.477953433990479
I0123 14:08:31.094895 140324316274688 ddar.py:60] Depth 6/1000 time = 16.282633304595947
I0123 14:08:47.905001 140324316274688 ddar.py:60] Depth 7/1000 time = 16.80978488922119
I0123 14:09:09.899105 140324316274688 ddar.py:60] Depth 8/1000 time = 21.993799924850464
I0123 14:09:35.939234 140324316274688 ddar.py:60] Depth 9/1000 time = 26.039786100387573
I0123 14:10:09.940441 140324316274688 ddar.py:60] Depth 10/1000 time = 34.00067353248596
I0123 14:10:42.475348 140324316274688 ddar.py:60] Depth 11/1000 time = 32.53433060646057
I0123 14:11:14.560206 140324316274688 ddar.py:60] Depth 12/1000 time = 32.08432745933533
I0123 14:11:45.696356 140324316274688 ddar.py:60] Depth 13/1000 time = 31.13490867614746
I0123 14:12:20.656692 140324316274688 ddar.py:60] Depth 14/1000 time = 34.95408344268799
I0123 14:13:01.769384 140324316274688 ddar.py:60] Depth 15/1000 time = 41.112005949020386
I0123 14:13:43.592259 140324316274688 ddar.py:60] Depth 16/1000 time = 41.82224631309509
I0123 14:14:23.168205 140324316274688 ddar.py:60] Depth 17/1000 time = 39.57546615600586
I0123 14:14:23.237992 140324316274688 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:14:23.238046 140324316274688 alphageometry.py:585] Timeout.
