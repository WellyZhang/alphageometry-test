I0123 12:32:57.781037 139820135366656 inference_utils.py:69] Parsing gin configuration.
I0123 12:32:57.781144 139820135366656 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:32:57.781358 139820135366656 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:32:57.781391 139820135366656 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:32:57.781420 139820135366656 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:32:57.781448 139820135366656 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:32:57.781475 139820135366656 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:32:57.781501 139820135366656 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:32:57.781527 139820135366656 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:32:57.781553 139820135366656 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:32:57.781579 139820135366656 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:32:57.781605 139820135366656 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:32:57.781660 139820135366656 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:32:57.781800 139820135366656 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:32:57.782015 139820135366656 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:32:57.782121 139820135366656 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:32:57.788422 139820135366656 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:32:57.788546 139820135366656 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:32:57.788861 139820135366656 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:32:57.788966 139820135366656 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:32:57.789241 139820135366656 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:32:57.789340 139820135366656 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:32:57.789762 139820135366656 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:32:57.789865 139820135366656 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:32:57.793614 139820135366656 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:32:57.893230 139820135366656 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:32:57.893942 139820135366656 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:32:57.900474 139820135366656 training_loop.py:335] Process 0 of 1
I0123 12:32:57.900528 139820135366656 training_loop.py:336] Local device count = 1
I0123 12:32:57.900569 139820135366656 training_loop.py:337] Number of replicas = 1
I0123 12:32:57.900604 139820135366656 training_loop.py:339] Using random number seed 42
I0123 12:32:58.378115 139820135366656 training_loop.py:359] Initializing the model.
I0123 12:32:58.801786 139820135366656 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.802049 139820135366656 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:32:58.802157 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802238 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802317 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802400 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802474 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802546 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802618 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802688 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802758 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802829 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802900 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.802969 139820135366656 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:58.803009 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:58.803055 139820135366656 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:58.803170 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:58.803210 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:58.803241 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:58.805284 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.811218 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:58.821968 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.822261 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:58.826657 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:58.837426 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:58.837487 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:58.837527 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:58.837560 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.837630 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.838855 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.838935 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.839647 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.842151 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.847898 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.849630 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.849723 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:58.849760 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:58.849822 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.849954 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:58.850292 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:58.850340 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:58.852265 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.852370 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:58.855280 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.855362 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:58.855859 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:58.866016 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:58.874767 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.874867 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:58.875165 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.875251 139820135366656 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:58.875364 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:58.875404 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:58.875435 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:58.877314 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.879800 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:58.885400 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.885679 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:58.888305 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:58.892463 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:58.892522 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:58.892561 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:58.892593 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.892658 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.893247 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.893324 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.893853 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.894664 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.897174 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.897819 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.897899 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:58.897935 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:58.897996 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.898127 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:58.898455 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:58.898499 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:58.900436 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.900536 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:58.903067 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.903151 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:58.903587 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:58.905936 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:58.907844 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.907940 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:58.908236 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.908319 139820135366656 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:58.908431 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:58.908473 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:58.908505 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:58.910448 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.912815 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:58.918783 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.919045 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:58.921696 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:58.925583 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:58.925644 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:58.925684 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:58.925716 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.925779 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.926349 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.926427 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.926784 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.927551 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.930214 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.930891 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.930971 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:58.931007 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:58.931068 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.931195 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:58.931524 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:58.931568 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:58.933506 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.933602 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:58.936133 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.936219 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:58.936707 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:58.938991 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:58.940912 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.941007 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:58.941300 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.941382 139820135366656 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:58.941494 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:58.941534 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:58.941565 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:58.943509 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.945906 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:58.951545 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.951812 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:58.954442 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:58.958269 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:58.958326 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:58.958361 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:58.958392 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.958455 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.959028 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.959106 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.959461 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.960231 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.962791 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.963417 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.963499 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:58.963534 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:58.963597 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.963728 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:58.964054 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:58.964099 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:58.966003 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.966098 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:58.968630 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.968717 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:58.969148 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:58.971417 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:58.973295 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.973389 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:58.973681 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.973764 139820135366656 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:58.973875 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:58.973915 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:58.973946 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:58.975857 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.978257 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:58.983882 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.984141 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:58.986798 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:58.990587 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:58.990643 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:58.990679 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:58.990710 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.990778 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.991350 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.991427 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.991788 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.992561 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.995620 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.996244 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.996326 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:58.996363 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:58.996590 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.996723 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:58.997049 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:58.997093 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:58.999024 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:58.999119 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.001670 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.001751 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.002185 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.004486 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.006482 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.006579 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.006874 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.006957 139820135366656 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:59.007068 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.007107 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.007138 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.009021 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.011424 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.017082 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.017347 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.020040 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:59.023825 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.023881 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.023918 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.023950 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.024013 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.024631 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.024709 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.025067 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.025851 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.028334 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.028957 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.029036 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.029072 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.029136 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.029266 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.029594 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.029644 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.031552 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.031647 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.034217 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.034299 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.034745 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.037061 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.038996 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.039093 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.039384 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.039467 139820135366656 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:59.039579 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.039619 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.039651 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.041512 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.043987 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.049648 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.049919 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.052544 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:59.056356 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.056412 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.056448 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.056480 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.056543 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.057106 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.057184 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.057539 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.058318 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.060764 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.061393 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.061471 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.061508 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.061568 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.061703 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.062033 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.062079 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.064050 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.064148 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.066657 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.066739 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.067175 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.069838 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.071765 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.071866 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.072157 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.072243 139820135366656 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:59.072356 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.072396 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.072426 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.212505 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.215615 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.221596 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.221903 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.224636 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:59.228677 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.228738 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.228778 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.228811 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.228879 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.229505 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.229588 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.229966 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.230773 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.233626 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.234297 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.234379 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.234419 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.234486 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.234618 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.234966 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.235010 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.236943 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.237038 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.239656 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.239742 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.240189 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.242542 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.244484 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.244592 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.244892 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.244979 139820135366656 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:59.245093 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.245134 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.245167 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.247182 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.249597 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.255363 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.255632 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.258351 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:59.262262 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.262319 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.262358 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.262390 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.262454 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.263036 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.263116 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.263474 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.264249 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.266844 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.267482 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.267560 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.267597 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.267659 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.267789 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.268114 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.268158 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.270092 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.270190 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.272807 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.272889 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.273334 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.275667 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.277657 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.277754 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.278045 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.278138 139820135366656 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:59.278253 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.278293 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.278324 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.280191 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.282675 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.288227 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.288494 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.291513 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:59.295331 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.295389 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.295426 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.295458 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.295520 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.296129 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.296208 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.296565 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.297343 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.299803 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.300436 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.300514 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.300550 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.300610 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.300737 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.301067 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.301112 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.303036 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.303130 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.305697 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.305779 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.306214 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.308524 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.310451 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.310547 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.310837 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.310927 139820135366656 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:59.311042 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.311082 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.311114 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.312973 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.315626 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.321222 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.321484 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.324096 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:59.327921 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.327977 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.328014 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.328045 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.328108 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.328684 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.328761 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.329113 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.329892 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.332319 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.332951 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.333029 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.333065 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.333124 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.333255 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.333578 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.333622 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.335562 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.335657 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.338406 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.338487 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.338921 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.341245 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.343167 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.343265 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.343556 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.343639 139820135366656 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:59.343756 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.343796 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.343828 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.345747 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.348127 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.353733 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.353992 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.356584 139820135366656 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:59.360375 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.360431 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.360468 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.360500 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.360562 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.361128 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.361205 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.361562 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.362328 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.364771 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.365763 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.365843 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.365879 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.365939 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.366067 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.366385 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.366429 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.368309 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.368402 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.370876 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.370956 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.371436 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.373682 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.375589 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.375684 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.375970 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.376255 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376325 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376390 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376447 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376501 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376554 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376609 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376662 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376714 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376766 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376819 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376871 139820135366656 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:59.376908 139820135366656 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:59.380429 139820135366656 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:59.428393 139820135366656 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.428478 139820135366656 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:59.428534 139820135366656 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:59.428637 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.428676 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.428706 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.428776 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.431172 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.436596 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.436858 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.439499 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.456078 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.456134 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.456171 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.456202 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.456269 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.457404 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.457483 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.458199 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.460192 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.464899 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.469372 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.469505 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.469544 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.469623 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.469773 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.469901 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.469943 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.472055 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.472151 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.474660 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.474742 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.474858 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.477156 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.479149 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.479247 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.479536 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.479622 139820135366656 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:59.479734 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.479776 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.479808 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.479878 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.482176 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.487677 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.487938 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.490681 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.504118 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.504177 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.504215 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.504246 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.504308 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.504905 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.504982 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.505355 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.506057 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.508575 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.509208 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.509289 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.509332 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.509393 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.509528 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.509645 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.509686 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.511639 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.511734 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.514151 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.514238 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.514352 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.516615 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.518565 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.518663 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.518951 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.519034 139820135366656 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:59.519145 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.519185 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.519217 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.519280 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.521529 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.527043 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.527473 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.530157 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.542995 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.543052 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.543089 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.543121 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.543183 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.543747 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.543827 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.544187 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.544878 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.547363 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.547990 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.548068 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.548103 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.548168 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.548301 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.548410 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.548449 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.550379 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.550475 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.552885 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.552965 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.553074 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.555311 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.557235 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.557331 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.557619 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.557710 139820135366656 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:59.557822 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.557861 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.557892 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.557954 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.560201 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.565613 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.565877 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.568544 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.581382 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.581439 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.581476 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.581507 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.581570 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.582147 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.582224 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.582581 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.583271 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.585738 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.586363 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.586441 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.586477 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.586536 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.586673 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.586784 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.586823 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.588778 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.588873 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.591273 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.591353 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.591466 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.593688 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.595568 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.595666 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.595952 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.596035 139820135366656 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:59.596146 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.596186 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.596216 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.596280 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.598875 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.604365 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.604629 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.607235 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.620038 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.620095 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.620132 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.620164 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.620227 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.620792 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.620870 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.621226 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.621927 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.624446 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.625076 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.625156 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.625192 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.625252 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.625386 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.625497 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.625536 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.627437 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.627532 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.629947 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.630032 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.630141 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.632408 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.634292 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.634391 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.634679 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.634763 139820135366656 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:59.634875 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.634914 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.634946 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.635009 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.637367 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.642832 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.643091 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.645786 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.658551 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.658607 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.658646 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.658678 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.658740 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.659303 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.659382 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.659736 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.660428 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.662881 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.663502 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.663579 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.663614 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.663673 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.663802 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.663920 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.663961 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.665913 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.666008 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.668410 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.668491 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.668599 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.670844 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.672710 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.672805 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.673091 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.673172 139820135366656 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:59.673284 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.673324 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.673355 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.673418 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.675659 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.681220 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.681480 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.684078 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.696882 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.696941 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.696978 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.697010 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.697077 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.697656 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.697736 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.698091 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.698774 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.701246 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.702249 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.702330 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.702367 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.702427 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.702562 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.702673 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.702717 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.704607 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.704701 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.707114 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.707195 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.707303 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.709506 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.711431 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.711527 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.711811 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.711894 139820135366656 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:59.712005 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.712044 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.712077 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.712141 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.714378 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.719811 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.720081 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.722755 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.735514 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.735571 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.735608 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.735639 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.735702 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.736445 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.736524 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.736883 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.737750 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.740218 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.740841 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.740921 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.740956 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.741015 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.741145 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.741254 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.741299 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.743198 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.743293 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.745742 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.745822 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.745932 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.748158 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.750024 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.750120 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.750404 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.750488 139820135366656 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:59.750598 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.750638 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.750669 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.750731 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.753157 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.759188 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.759453 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.762073 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.774772 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.774830 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.774867 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.774899 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.774961 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.775525 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.775601 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.775955 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.776648 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.779109 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.779781 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.779859 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.779895 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.779958 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.780086 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.780195 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.780234 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.782105 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.782200 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.784591 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.784670 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.784777 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.787010 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.788941 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.789038 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.789323 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.789407 139820135366656 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:59.789517 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.789556 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.789587 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.789657 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.791905 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.797325 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.797586 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.800249 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.813233 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.813289 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.813326 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.813357 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.813420 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.814029 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.814109 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.814468 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.815157 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.817596 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.818231 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.818309 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.818345 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.818404 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.818536 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.818645 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.818683 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.820546 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.820647 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.823109 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.823190 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.823299 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.825514 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.827378 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.827476 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.827761 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.827845 139820135366656 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:59.827956 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.827996 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.828028 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.828094 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.830326 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.835788 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.836047 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.838660 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.851349 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.851406 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.851443 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.851474 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.851535 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.852099 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.852176 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.852525 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.853210 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.855665 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.856453 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.856532 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.856567 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.856626 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.856754 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.856863 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.856901 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.858793 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.858898 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.861368 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.861600 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.861719 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.863965 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.865900 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.866000 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.866296 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.866383 139820135366656 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:59.866499 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.866541 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.866573 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.866640 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.868912 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.874437 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.874694 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.877352 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.890131 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.890190 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.890228 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.890261 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.890326 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.890916 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.890996 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.891364 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.892128 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.894631 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.895276 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.895357 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.895394 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.895455 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.895591 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.895706 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.895747 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.897596 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.897698 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.900154 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.900233 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.900341 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:59.902653 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.904574 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.904671 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.904956 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.905047 139820135366656 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:59.907920 139820135366656 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:59.963754 139820135366656 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.963843 139820135366656 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:59.963900 139820135366656 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:59.964008 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:59.964047 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:59.964079 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:59.964144 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.966836 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:59.972310 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.972573 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:59.975193 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:59.987847 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:59.987906 139820135366656 attention.py:418] Single window, no scan.
I0123 12:32:59.987944 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:59.987977 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.988040 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.988598 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.988676 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.989033 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.989730 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.992254 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.992881 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.992962 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:59.992999 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:59.993061 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.993191 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:59.993308 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:32:59.993349 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:32:59.995210 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.995307 139820135366656 nn_components.py:261] mlp: residual
I0123 12:32:59.997722 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:59.997803 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:32:59.997914 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.000188 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.002080 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.002178 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.002463 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.002547 139820135366656 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:33:00.002657 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.002697 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.002729 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.002794 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.005053 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.010514 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.010779 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.013446 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.025836 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.025893 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.025929 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.025961 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.026024 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.026580 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.026657 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.027008 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.027678 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.030164 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.030789 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.030867 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.030903 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.030964 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.031092 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.031201 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.031246 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.033083 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.033177 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.035547 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.035628 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.035739 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.037994 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.039839 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.039936 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.040217 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.040300 139820135366656 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:33:00.040409 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.040448 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.040479 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.040541 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.042756 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.048103 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.048362 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.050997 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.063460 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.063516 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.063554 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.063586 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.063650 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.064205 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.064283 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.064630 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.065294 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.067780 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.068396 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.068474 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.068510 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.068569 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.068695 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.068803 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.068842 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.070678 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.070773 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.073130 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.073210 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.073319 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.076009 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.077859 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.077957 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.078242 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.078325 139820135366656 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:33:00.078433 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.078473 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.078504 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.078568 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.080767 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.086118 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.086374 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.088991 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.101421 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.101478 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.101516 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.101556 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.101620 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.102199 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.102275 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.102629 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.103310 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.105803 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.106418 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.106494 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.106528 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.106587 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.106711 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.106818 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.106858 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.108719 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.108812 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.111209 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.111288 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.111396 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.113705 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.115565 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.115659 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.115941 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.116023 139820135366656 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:33:00.116132 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.116170 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.116200 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.116263 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.118490 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.123856 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.124114 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.126770 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.139429 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.139484 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.139518 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.139547 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.139612 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.140168 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.140244 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.140601 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.141279 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.143774 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.144397 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.144473 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.144507 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.144565 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.144690 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.144797 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.144834 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.146771 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.146871 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.149232 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.149310 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.149417 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.151700 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.153534 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.153628 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.153917 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.153998 139820135366656 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:33:00.154107 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.154145 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.154175 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.154237 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.156460 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.161847 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.162107 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.164777 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.177348 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.177402 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.177436 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.177466 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.177527 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.178100 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.178177 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.178532 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.179210 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.181729 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.182348 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.182424 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.182459 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.182516 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.182641 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.182751 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.182789 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.184638 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.184739 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.187136 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.187216 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.187324 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.190017 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.191875 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.191971 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.192252 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.192333 139820135366656 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:33:00.192441 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.192478 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.192508 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.192570 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.194778 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.200138 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.200396 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.203063 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.215700 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.215756 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.215790 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.215819 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.215881 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.216445 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.216522 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.216875 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.217559 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.220111 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.220746 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.220823 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.220857 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.220915 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.221044 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.221152 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.221189 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.223043 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.223137 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.225529 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.225606 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.225719 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.228007 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.229879 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.229976 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.230256 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.230337 139820135366656 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:33:00.230445 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.230483 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.230513 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.230575 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.232787 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.238190 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.238446 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.241092 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.253886 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.253941 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.253979 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.254009 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.254071 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.254633 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.254709 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.255065 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.255751 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.258270 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.258900 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.258976 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.259011 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.259073 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.259201 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.259308 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.259346 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.261218 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.261310 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.263691 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.263777 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.263886 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.266180 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.268035 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.268130 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.268415 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.268497 139820135366656 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:33:00.268606 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.268644 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.268674 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.268738 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.270954 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.276329 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.276585 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.279245 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.291855 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.291910 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.291945 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.291977 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.292039 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.292599 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.292676 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.293024 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.293710 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.296227 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.296851 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.296927 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.296962 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.297020 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.297148 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.297256 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.297294 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.299167 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.299261 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.301624 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.301715 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.301824 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.304482 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.306357 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.306453 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.306740 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.306821 139820135366656 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:33:00.306929 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.306967 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.306997 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.307059 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.309286 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.314763 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.315021 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.317671 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.330167 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.330222 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.330256 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.330286 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.330347 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.330911 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.330986 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.331341 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.332024 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.334533 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.335167 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.335244 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.335278 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.335334 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.335457 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.335562 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.335599 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.337930 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.338025 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.340381 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.340458 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.340573 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.342823 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.344654 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.344749 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.345030 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.345112 139820135366656 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:33:00.345220 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.345258 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.345289 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.345351 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.347582 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.352946 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.353201 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.355853 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.368405 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.368458 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.368493 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.368524 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.368584 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.369145 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.369223 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.369575 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.370264 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.372786 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.373409 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.373485 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.373518 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.373575 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.373709 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.373817 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.373855 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.375713 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.375805 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.378161 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.378240 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.378348 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.380612 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.382458 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.382553 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.382836 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.382917 139820135366656 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:33:00.383026 139820135366656 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:33:00.383065 139820135366656 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:33:00.383094 139820135366656 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:33:00.383154 139820135366656 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.385371 139820135366656 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:33:00.390771 139820135366656 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.391026 139820135366656 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:33:00.393662 139820135366656 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:33:00.406217 139820135366656 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:33:00.406271 139820135366656 attention.py:418] Single window, no scan.
I0123 12:33:00.406306 139820135366656 transformer_layer.py:389] tlayer: self-attention.
I0123 12:33:00.406337 139820135366656 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.406398 139820135366656 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.406959 139820135366656 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.407036 139820135366656 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.407386 139820135366656 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.408076 139820135366656 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.410587 139820135366656 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.411391 139820135366656 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.411467 139820135366656 transformer_layer.py:468] tlayer: End windows.
I0123 12:33:00.411502 139820135366656 transformer_layer.py:472] tlayer: final FFN.
I0123 12:33:00.411559 139820135366656 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.411687 139820135366656 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:33:00.411799 139820135366656 nn_components.py:325] mlp: activation = None
I0123 12:33:00.411838 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.413705 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.413798 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.416157 139820135366656 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.416234 139820135366656 transformer_base.py:443] tbase: final FFN
I0123 12:33:00.416340 139820135366656 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:33:00.418998 139820135366656 nn_components.py:329] mlp: final activation = None
I0123 12:33:00.420858 139820135366656 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.420952 139820135366656 nn_components.py:261] mlp: residual
I0123 12:33:00.421233 139820135366656 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:00.421320 139820135366656 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:33:00.424135 139820135366656 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:33:04.786375 139820135366656 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:33:05.306625 139820135366656 training_loop.py:409] No working directory specified.
I0123 12:33:05.306748 139820135366656 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:33:05.307504 139820135366656 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:33:08.232472 139820135366656 training_loop.py:447] Only restoring trainable parameters.
I0123 12:33:08.233183 139820135366656 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:33:08.233243 139820135366656 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.233289 139820135366656 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.233332 139820135366656 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.233372 139820135366656 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.233412 139820135366656 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.233453 139820135366656 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.233492 139820135366656 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.233531 139820135366656 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.233569 139820135366656 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.233606 139820135366656 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.233649 139820135366656 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.233689 139820135366656 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.233727 139820135366656 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.233765 139820135366656 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.233802 139820135366656 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.233839 139820135366656 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.233877 139820135366656 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.233914 139820135366656 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.233952 139820135366656 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.234002 139820135366656 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234042 139820135366656 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.234079 139820135366656 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.234116 139820135366656 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.234153 139820135366656 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234189 139820135366656 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.234226 139820135366656 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234262 139820135366656 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234299 139820135366656 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.234335 139820135366656 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.234371 139820135366656 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234407 139820135366656 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.234443 139820135366656 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.234480 139820135366656 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.234517 139820135366656 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234552 139820135366656 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.234588 139820135366656 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234624 139820135366656 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234660 139820135366656 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.234696 139820135366656 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.234732 139820135366656 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234768 139820135366656 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.234805 139820135366656 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.234840 139820135366656 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.234876 139820135366656 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234910 139820135366656 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.234952 139820135366656 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.234989 139820135366656 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235025 139820135366656 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.235060 139820135366656 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.235095 139820135366656 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235131 139820135366656 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.235166 139820135366656 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.235203 139820135366656 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.235240 139820135366656 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235277 139820135366656 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.235313 139820135366656 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235349 139820135366656 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235388 139820135366656 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.235426 139820135366656 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.235463 139820135366656 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235499 139820135366656 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.235535 139820135366656 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.235573 139820135366656 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.235608 139820135366656 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235644 139820135366656 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.235681 139820135366656 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235717 139820135366656 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235752 139820135366656 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.235788 139820135366656 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.235823 139820135366656 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.235859 139820135366656 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.235894 139820135366656 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.235935 139820135366656 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.235972 139820135366656 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236008 139820135366656 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.236044 139820135366656 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236079 139820135366656 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236114 139820135366656 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.236150 139820135366656 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.236185 139820135366656 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236220 139820135366656 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.236256 139820135366656 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.236291 139820135366656 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.236326 139820135366656 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236361 139820135366656 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.236397 139820135366656 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236432 139820135366656 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236468 139820135366656 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.236503 139820135366656 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.236538 139820135366656 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236574 139820135366656 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.236609 139820135366656 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.236644 139820135366656 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.236680 139820135366656 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236715 139820135366656 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.236750 139820135366656 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236786 139820135366656 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236821 139820135366656 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.236856 139820135366656 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.236897 139820135366656 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.236934 139820135366656 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.236970 139820135366656 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.237007 139820135366656 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.237044 139820135366656 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.237081 139820135366656 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.237118 139820135366656 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.237153 139820135366656 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.237189 139820135366656 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.237225 139820135366656 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.237261 139820135366656 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.237298 139820135366656 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.237333 139820135366656 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:33:08.237368 139820135366656 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:33:08.237403 139820135366656 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.237439 139820135366656 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.237474 139820135366656 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.237510 139820135366656 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.237545 139820135366656 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:33:08.237580 139820135366656 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:33:08.237616 139820135366656 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:33:08.237656 139820135366656 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:33:08.237687 139820135366656 training_loop.py:725] Total parameters: 152072288
I0123 12:33:08.237894 139820135366656 training_loop.py:739] Total state size: 0
I0123 12:33:08.262263 139820135366656 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:33:08.262556 139820135366656 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:33:08.262937 139820135366656 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:33:08.263280 139820135366656 training_loop.py:89] registering functions: dict_keys([])
I0123 12:33:08.280718 139820135366656 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d a, on_line e d a; f = on_circle f e b, on_line f a e; g = on_circle g e b, on_line g a e; h = foot h b g a; i = mirror i b h; j = midpoint j g a; k = lc_tangent k j g, on_line k i g; l = foot l a g k; m = mirror m a l; n = midpoint n g k; o = lc_tangent o n g, on_line o m g; p = midpoint p o d; q = mirror q g p; r = on_line r b k, on_line r e q ? coll d o r
I0123 12:33:25.115867 139820135366656 ddar.py:60] Depth 1/1000 time = 16.714972734451294
I0123 12:34:07.658240 139820135366656 ddar.py:60] Depth 2/1000 time = 42.542115211486816
I0123 12:35:28.950736 139820135366656 ddar.py:60] Depth 3/1000 time = 81.29207825660706
I0123 12:37:55.027126 139820135366656 ddar.py:60] Depth 4/1000 time = 146.0759391784668
I0123 12:41:15.264416 139820135366656 ddar.py:60] Depth 5/1000 time = 200.2368392944336
I0123 12:48:36.864650 139820135366656 ddar.py:60] Depth 6/1000 time = 441.59980630874634
I0123 12:59:21.488307 139820135366656 ddar.py:60] Depth 7/1000 time = 644.6231994628906
I0123 12:59:23.264151 139820135366656 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E G H I J K L M N O P Q R : Points
DA = DB [00]
DB = DC [01]
E,A,D are collinear [02]
DE = DA [03]
G,E,A are collinear [04]
EG = EB [05]
G,H,A are collinear [06]
HB  GA [07]
H,B,I are collinear [08]
HB = HI [09]
(GI-AD) = (GI-AD) [10]
G,A,J are collinear [11]
JG = JA [12]
JG  JK [13]
G,I,K are collinear [14]
IK:GK = IK:GK [15]
LA  GK [16]
G,K,L are collinear [17]
M,A,L are collinear [18]
LA = LM [19]
G,K,N are collinear [20]
NG = NK [21]
NG  NO [22]
G,M,O are collinear [23]
PO = PD [24]
O,P,D are collinear [25]
PG = PQ [26]
G,P,Q are collinear [27]
K,B,R are collinear [28]
E,R,Q are collinear [29]

 * Auxiliary Constructions:
F : Points
E,A,F are collinear [30]
EF = EB [31]

 * Proof steps:
001. P,O,D are collinear [25] & P,Q,G are collinear [27]   OPQ = DPG [32]
002. P,O,D are collinear [25] & P,Q,G are collinear [27]   OPG = DPQ [33]
003. PO = PD [24] & PG = PQ [26] & OPQ = DPG [32] (SAS)  QO = GD [34]
004. H,B,I are collinear [08] & HB = HI [09]   H is midpoint of BI [35]
005. G,H,A are collinear [06] & G,E,A are collinear [04] & E,A,D are collinear [02] & I,B,H are collinear [08] & HB  GA [07]   GH  BI [36]
006. H is midpoint of BI [35] & GH  BI [36]   GB = GI [37]
007. DA = DB [00] & DE = DA [03]   D is the circumcenter of \Delta ABE [38]
008. DA = DB [00] & DE = DA [03]   DB = DE [39]
009. D is the circumcenter of \Delta ABE [38] & E,A,D are collinear [02]   AB  BE [40]
010. G,E,A are collinear [04] & E,A,D are collinear [02] & AB  BE [40] & HB  GA [07] & JG  JK [13] & G,A,J are collinear [11]   (KJ-GA) = EBA [41]
011. LA  GK [16] & G,I,K are collinear [14] & NG  NO [22] & G,K,N are collinear [20]   ON  IG [42]
012. G,E,A are collinear [04] & E,A,D are collinear [02] & HB  GA [07] & JG  JK [13] & G,A,J are collinear [11]   KJ  GA [43]
013. ON  IG [42] & KJ  GA [43]   (ON-KJ) = IGA [44]
014. H,B,I are collinear [08] & G,H,A are collinear [06] & G,E,A are collinear [04] & E,A,D are collinear [02] & HB  GA [07]   IHG = GHB [45]
015. HB = HI [09] & IHG = GHB [45] (SAS)  IGH = HGB [46]
016. HB = HI [09] & IHG = GHB [45] (SAS)  HIG = GBH [47]
017. EG = EB [05]   EBG = BGE [48]
018. (ON-KJ) = IGA [44] & G,E,A are collinear [04] & E,A,D are collinear [02] & NG  NO [22] & LA  GK [16] & G,I,K are collinear [14] & G,K,N are collinear [20] & JG  JK [13] & HB  GA [07] & G,A,J are collinear [11] & IGH = HGB [46] & G,H,A are collinear [06] & EBG = BGE [48]   GBE = (ON-KJ) [49]
019. (KJ-GA) = EBA [41] & GBE = (ON-KJ) [49]   ABG = (GA-ON) [50]
020. M,A,L are collinear [18] & G,K,L are collinear [17] & G,I,K are collinear [14] & LA  GK [16]   MLG = GLA [51]
021. LA = LM [19] & MLG = GLA [51] (SAS)  MGL = LGA [52]
022. G,K,N are collinear [20] & G,I,K are collinear [14] & LA  GK [16] & NG  NO [22]   KNO = ONG [53]
023. NG = NK [21] & KNO = ONG [53] (SAS)  NKO = OGN [54]
024. G,E,A are collinear [04] & E,A,D are collinear [02] & MGL = LGA [52] & G,K,L are collinear [17] & G,I,K are collinear [14] & NKO = OGN [54] & G,K,N are collinear [20] & G,M,O are collinear [23]   (OK-IG) = AGI [55]
025. (OK-IG) = AGI [55]   OK  GA [56]
026. G,A,J are collinear [11] & G,E,A are collinear [04] & E,A,D are collinear [02] & HB  GA [07] & JG  JK [13]   GJK = KJA [57]
027. JG = JA [12] & GJK = KJA [57] (SAS)  JGK = KAJ [58]
028. JG = JA [12] & GJK = KJA [57] (SAS)  GKJ = JKA [59]
029. G,E,A are collinear [04] & E,A,D are collinear [02] & IGH = HGB [46] & G,H,A are collinear [06] & JGK = KAJ [58] & G,A,J are collinear [11] & G,I,K are collinear [14]   KAG = BGA [60]
030. KAG = BGA [60]   KA  GB [61]
031. DA = DB [00]   BAD = DBA [62]
032. M,A,L are collinear [18] & ABG = (GA-ON) [50] & G,E,A are collinear [04] & E,A,D are collinear [02] & OK  GA [56] & AK  BG [61] & NG  NO [22] & LA  GK [16] & G,I,K are collinear [14] & G,K,N are collinear [20] & BAD = DBA [62]   DBA = KAM [63]
033. M,A,L are collinear [18] & G,K,L are collinear [17] & G,I,K are collinear [14] & LA  GK [16]   MLK = KLA [64]
034. LA = LM [19] & MLK = KLA [64] (SAS)  LMK = KAL [65]
035. M,A,L are collinear [18] & LMK = KAL [65] & ABG = (GA-ON) [50] & G,E,A are collinear [04] & E,A,D are collinear [02] & OK  GA [56] & AK  BG [61] & NG  NO [22] & LA  GK [16] & G,I,K are collinear [14] & G,K,N are collinear [20]   DAB = KMA [66]
036. DBA = KAM [63] & DAB = KMA [66] (Similar Triangles)  BD:BA = AK:AM [67]
037. DBA = KAM [63] & DAB = KMA [66] (Similar Triangles)  BD:AK = BA:AM [68]
038. G,H,A are collinear [06] & G,E,A are collinear [04] & E,A,D are collinear [02] & I,B,H are collinear [08] & HB  GA [07]   AH  BI [69]
039. H is midpoint of BI [35] & AH  BI [69]   AB = AI [70]
040. M,A,L are collinear [18] & LA = LM [19]   L is midpoint of AM [71]
041. G,K,L are collinear [17] & G,I,K are collinear [14] & M,A,L are collinear [18] & LA  GK [16]   KL  MA [72]
042. L is midpoint of AM [71] & KL  MA [72]   KM = KA [73]
043. G,H,A are collinear [06] & E,A,F are collinear [30] & E,A,D are collinear [02] & G,E,A are collinear [04] & I,B,H are collinear [08] & HB  GA [07]   FH  BI [74]
044. H is midpoint of BI [35] & FH  BI [74]   FB = FI [75]
045. AB = AI [70] & FB = FI [75] (SSS)  AIF = FBA [76]
046. G,H,A are collinear [06] & G,E,A are collinear [04] & E,A,D are collinear [02] & I,B,H are collinear [08] & HB  GA [07]   EH  BI [77]
047. H is midpoint of BI [35] & EH  BI [77]   EB = EI [78]
048. EB = EI [78] & EG = EB [05] & EF = EB [31]   E is the circumcenter of \Delta GIF [79]
049. EB = EI [78] & EG = EB [05] & EF = EB [31]   G,I,B,F are concyclic [80]
050. E,A,F are collinear [30] & E,A,D are collinear [02] & G,E,A are collinear [04]   E,G,F are collinear [81]
051. E is the circumcenter of \Delta GIF [79] & E,G,F are collinear [81]   GI  IF [82]
052. EG = EB [05] & EF = EB [31]   E is the circumcenter of \Delta GBF [83]
053. E is the circumcenter of \Delta GBF [83] & E,G,F are collinear [81]   BG  BF [84]
054. AB  BE [40] & BG  BF [84]   FBG = ABE [85]
055. G,E,A are collinear [04] & E,A,D are collinear [02] & EBG = BGE [48]   AGB = GBE [86]
056. FBG = ABE [85] & AGB = GBE [86]   (BF-GA) = ABG [87]
057. G,A,J are collinear [11] & G,E,A are collinear [04] & E,A,D are collinear [02] & G,K,L are collinear [17] & G,I,K are collinear [14] & LA  GK [16] & HB  GA [07] & JG  JK [13]   KJA = KLA [88]
058. KJA = KLA [88]   K,A,L,J are concyclic [89]
059. K,A,L,J are concyclic [89]   AKL = AJL [90]
060. K,A,L,J are concyclic [89]   AKJ = ALJ [91]
061. J,A,G are collinear [11] & JG = JA [12]   J is midpoint of AG [92]
062. J is midpoint of AG [92] & L is midpoint of AM [71]   JL  GM [93]
063. G,M,O are collinear [23] & G,K,I are collinear [14] & M,A,L are collinear [18] & AIF = FBA [76] & GI  IF [82] & LA  GK [16] & (BF-GA) = ABG [87] & G,E,A are collinear [04] & E,A,D are collinear [02] & AKL = AJL [90] & G,K,L are collinear [17] & G,A,J are collinear [11] & AK  BG [61] & OK  GA [56] & JL  GM [93]   OGK = IAM [94]
064. M,A,L are collinear [18] & G,K,L are collinear [17] & G,I,K are collinear [14] & LA  GK [16]   MLI = ILA [95]
065. LA = LM [19] & MLI = ILA [95] (SAS)  LMI = IAL [96]
066. G,K,I are collinear [14] & M,A,L are collinear [18] & LMI = IAL [96] & AIF = FBA [76] & GI  IF [82] & LA  GK [16] & (BF-GA) = ABG [87] & G,E,A are collinear [04] & E,A,D are collinear [02] & IGH = HGB [46] & G,H,A are collinear [06] & OK  GA [56]   OKG = IMA [97]
067. OGK = IAM [94] & OKG = IMA [97] (Similar Triangles)  GO:AI = GK:AM [98]
068. OGK = IAM [94] & OKG = IMA [97] (Similar Triangles)  GO:GK = AI:AM [99]
069. J is midpoint of AG [92] & KJ  GA [43]   KG = KA [100]
070. G,K,L are collinear [17] & G,I,K are collinear [14] & M,A,L are collinear [18] & LA  GK [16]   GL  MA [101]
071. L is midpoint of AM [71] & GL  MA [101]   GM = GA [102]
072. G,K,I are collinear [14] & G,E,A are collinear [04] & E,A,D are collinear [02] & JGK = KAJ [58] & G,A,J are collinear [11] & MGL = LGA [52] & G,K,L are collinear [17]   KGM = KAG [103]
073. KG = KA [100] & GM = GA [102] & KGM = KAG [103] (SAS)  MK = GK [104]
074. G,K,L are collinear [17] & G,I,K are collinear [14] & M,A,L are collinear [18] & LA  GK [16]   IL  MA [105]
075. L is midpoint of AM [71] & IL  MA [105]   IM = IA [106]
076. BD:BA = AK:AM [67] & ED = BD [39] & AB = AI [70] & KM = KA [73] & GO:AI = GK:AM [98] & MK = GK [104] & MI = IA [106]   GO:BA = ED:MI [107]
077. IM = IA [106] & AB = AI [70]   BA = MI [108]
078. GO:BA = ED:MI [107] & BA = MI [108]   GO = ED [109]
079. H,B,I are collinear [08] & G,H,A are collinear [06] & G,E,A are collinear [04] & E,A,D are collinear [02] & HB  GA [07]   IHE = EHB [110]
080. HB = HI [09] & IHE = EHB [110] (SAS)  IEH = HEB [111]
081. HB = HI [09] & IHE = EHB [110] (SAS)  HIE = EBH [112]
082. DE = DB [39]   DEB = EBD [113]
083. E,A,D are collinear [02] & IEH = HEB [111] & G,H,A are collinear [06] & G,E,A are collinear [04] & DEB = EBD [113]   DBE = DEI [114]
084. DE = DB [39] & EB = EI [78] & DBE = DEI [114] (SAS)  ED = ID [115]
085. GO = ED [109] & ED = ID [115]   ID = GO [116]
086. EBG = BGE [48] & G,E,A are collinear [04] & E,A,D are collinear [02] & IGH = HGB [46] & G,H,A are collinear [06] & MGL = LGA [52] & G,K,L are collinear [17] & G,I,K are collinear [14] & GM  JL [93]   (IG-LJ) = EBG [117]
087. G,E,A are collinear [04] & E,A,D are collinear [02] & DEB = EBD [113]   (GA-BE) = EBD [118]
088. G,E,A are collinear [04] & E,A,D are collinear [02] & EBG = BGE [48] & IGH = HGB [46] & G,H,A are collinear [06]   IGA = GBE [119]
089. (GA-BE) = EBD [118] & IGA = GBE [119]   (BE-GI) = DBG [120]
090. G,H,A are collinear [06] & G,E,A are collinear [04] & E,A,D are collinear [02] & I,B,H are collinear [08] & HB  GA [07]   DH  BI [121]
091. H is midpoint of BI [35] & DH  BI [121]   DB = DI [122]
092. DB = DI [122] & GB = GI [37] (SSS)  DIG = GBD [123]
093. (BE-GI) = DBG [120] & AK  BG [61] & DIG = GBD [123]   GID = (BE-IG) [124]
094. (IG-LJ) = EBG [117] & GID = (BE-IG) [124]   (BG-JL) = GID [125]
095. (IG-LJ) = EBG [117] & GID = (BE-IG) [124]   BGI = (JL-DI) [126]
096. HIE = EBH [112] & H,B,I are collinear [08] & JG  JK [13] & HB  GA [07] & G,E,A are collinear [04] & E,A,D are collinear [02] & G,A,J are collinear [11]   (BE-KJ) = (KJ-IE) [127]
097. GBE = (ON-KJ) [49] & (BE-KJ) = (KJ-IE) [127]   (NO-EI) = (BG-JK) [128]
098. AKJ = ALJ [91] & AK  BG [61] & JG  JK [13] & HB  GA [07] & G,E,A are collinear [04] & E,A,D are collinear [02] & G,A,J are collinear [11] & JL  GM [93] & (NO-EI) = (BG-JK) [128] & NG  NO [22] & LA  GK [16] & G,I,K are collinear [14] & G,K,N are collinear [20]   (IE-ON) = (LJ-ON) [129]
099. (IE-ON) = (LJ-ON) [129]   IE  LJ [130]
100. G,M,O are collinear [23] & (BG-JL) = GID [125] & AK  BG [61] & JL  GM [93] & EI  JL [130]   BGO = GID [131]
101. GB = GI [37] & ID = GO [116] & BGO = GID [131] (SAS)  OB = DG [132]
102. QO = GD [34] & OB = DG [132]   OB = OQ [133]
103. PO = PD [24] & PG = PQ [26] & OPG = DPQ [33] (SAS)  GO = QD [134]
104. BD:AK = BA:AM [68] & ED = BD [39] & KM = KA [73] & AB = AI [70] & GO:GK = AI:AM [99] & MK = GK [104] & DB = DC [01] & DA = DB [00] & DE = DA [03]   GK:GO = GK:CD [135]
105. IK:GK = IK:GK [15] & GK:GO = GK:CD [135]   GO = CD [136]
106. G,K,L are collinear [17] & G,I,K are collinear [14] & G,H,A are collinear [06] & G,E,A are collinear [04] & E,A,D are collinear [02] & H,B,I are collinear [08] & LA  GK [16] & HB  GA [07]   ALG = GHI [137]
107. G,E,A are collinear [04] & E,A,D are collinear [02] & G,K,L are collinear [17] & G,I,K are collinear [14] & G,H,A are collinear [06]   AGL = HGI [138]
108. ALG = GHI [137] & AGL = HGI [138] (Similar Triangles)  GA:GI = GL:GH [139]
109. GA:GI = GL:GH [139] & GM = GA [102] & GB = GI [37]   GB:GA = GH:GL [140]
110. G,E,A are collinear [04] & E,A,D are collinear [02] & G,H,A are collinear [06] & G,K,L are collinear [17] & G,I,K are collinear [14] & IGH = HGB [46]   BGA = HGL [141]
111. GB:GA = GH:GL [140] & BGA = HGL [141] (Similar Triangles)  GBA = GHL [142]
112. GBA = GHL [142] & G,H,A are collinear [06] & G,E,A are collinear [04] & E,A,D are collinear [02] & AK  BG [61] & OK  GA [56] & (BF-GA) = ABG [87]   (BF-OK) = (HL-OK) [143]
113. (BF-OK) = (HL-OK) [143]   BF  HL [144]
114. G,K,I are collinear [14] & G,E,A are collinear [04] & E,A,D are collinear [02] & IGH = HGB [46] & G,H,A are collinear [06]   KGA = EGB [145]
115. G,E,A are collinear [04] & E,A,D are collinear [02] & EBG = BGE [48] & IGH = HGB [46] & G,H,A are collinear [06] & JGK = KAJ [58] & G,A,J are collinear [11] & G,I,K are collinear [14]   GAK = GBE [146]
116. KGA = EGB [145] & GAK = GBE [146] (Similar Triangles)  GK:GE = GA:GB [147]
117. GK:GE = GA:GB [147] & MK = GK [104] & EG = EB [05] & GM = GA [102] & GB = GI [37] & GA:GI = GL:GH [139]   GL:GH = GK:GE [148]
118. G,K,L are collinear [17] & G,I,K are collinear [14] & G,H,A are collinear [06] & G,E,A are collinear [04] & E,A,D are collinear [02] & (GI-AD) = (GI-AD) [10]   LGH = KGE [149]
119. GL:GH = GK:GE [148] & LGH = KGE [149] (Similar Triangles)  GLH = GKE [150]
120. BG  BF [84] & BF  HL [144] & GLH = GKE [150] & G,K,L are collinear [17] & G,I,K are collinear [14] & BG  AK [61]   EK  KA [151]
121. E,A,D are collinear [02] & DE = DA [03]   D is midpoint of AE [152]
122. EK  KA [151] & D is midpoint of AE [152]   ED = KD [153]
123. G,E,A are collinear [04] & E,A,D are collinear [02] & AB  BE [40] & HB  GA [07] & JG  JK [13] & G,A,J are collinear [11]   (KJ-GA) = ABE [154]
124. (KJ-GA) = ABE [154] & IGA = GBE [119]   (JK-GI) = ABG [155]
125. G,I,K are collinear [14] & I,B,H are collinear [08] & (JK-GI) = ABG [155] & JG  JK [13] & HB  GA [07] & G,E,A are collinear [04] & E,A,D are collinear [02] & G,A,J are collinear [11] & AK  BG [61]   KIB = KAB [156]
126. KIB = KAB [156]   K,B,I,A are concyclic [157]
127. DB = DC [01] & DA = DB [00] & DE = DA [03] & GO = QD [134] & GO = CD [136] & ED = KD [153] & K,B,I,A are concyclic [157] & DB = DI [122]   I,B,Q,E are concyclic [158]
128. I,B,Q,E are concyclic [158]   IBQ = IEQ [159]
129. N,G,K are collinear [20] & NG = NK [21]   N is midpoint of GK [160]
130. KJ  JG [13] & N is midpoint of GK [160]   NK = NJ [161]
131. NK = NJ [161] & DB = DI [122] & DB = DC [01] & GO = CD [136] & GO = QD [134]   NK:NJ = DQ:DI [162]
132. J is midpoint of AG [92] & N is midpoint of GK [160]   JN  AK [163]
133. O,P,D are collinear [25] & PO = PD [24]   P is midpoint of OD [164]
134. G,P,Q are collinear [27] & PG = PQ [26]   P is midpoint of GQ [165]
135. P is midpoint of OD [164] & P is midpoint of GQ [165]   OG  DQ [166]
136. P is midpoint of OD [164] & P is midpoint of GQ [165]   OQ  DG [167]
137. G,K,N are collinear [20] & G,I,K are collinear [14] & BGI = (JL-DI) [126] & AK  BG [61] & EI  JL [130] & AK  JN [163] & JL  GM [93] & OG  DQ [166] & G,M,O are collinear [23]   KNJ = IDQ [168]
138. NK:NJ = DQ:DI [162] & KNJ = IDQ [168] (Similar Triangles)  NKJ = IQD [169]
139. G,K,I are collinear [14] & NO  GI [42]   ON  GK [170]
140. N is midpoint of GK [160] & ON  GK [170]   OG = OK [171]
141. BD:BA = AK:AM [67] & ED = BD [39] & AB = AI [70] & KM = KA [73] & GO:AI = GK:AM [98] & MK = GK [104] & OK = GO [171] & MI = IA [106]   OK:MI = ED:BA [172]
142. OK:MI = ED:BA [172] & BA = MI [108]   OK = ED [173]
143. E,A,D are collinear [02] & O,M,G are collinear [23] & OG  DQ [166] & OK  GA [56] & G,E,A are collinear [04]   QDE = GOK [174]
144. GO = QD [134] & OK = ED [173] & QDE = GOK [174] (SAS)  DQE = OGK [175]
145. DB = DI [122] & EB = EI [78]   DE  IB [176]
146. GB = GI [37] & BA = MI [108] & GM = GA [102] (SSS)  GBA = GIM [177]
147. (JK-GI) = ABG [155] & JG  JK [13] & HB  GA [07] & G,E,A are collinear [04] & E,A,D are collinear [02] & G,A,J are collinear [11] & AK  BG [61] & GBA = GIM [177]   MIG = (KJ-IG) [178]
148. MIG = (KJ-IG) [178]   MI  KJ [179]
149. DB = DC [01] & DA = DB [00] & DE = DA [03] & GO = QD [134] & GO = CD [136] & ED = KD [153] & K,B,I,A are concyclic [157] & DB = DI [122]   I,B,K,Q are concyclic [180]
150. I,B,K,Q are concyclic [180]   IKB = IQB [181]
151. I,B,K,Q are concyclic [180]   IBQ = IKQ [182]
152. K,B,R are collinear [28] & E,R,Q are collinear [29] & IBQ = IEQ [159] & H,B,I are collinear [08] & NKJ = IQD [169] & G,K,N are collinear [20] & G,I,K are collinear [14] & DQE = OGK [175] & G,M,O are collinear [23] & OG  DQ [166] & JG  JK [13] & HB  GA [07] & G,E,A are collinear [04] & E,A,D are collinear [02] & G,A,J are collinear [11] & DE  IB [176] & OK  GA [56] & IM  JK [179] & EI  JL [130] & JL  GM [93] & IKB = IQB [181]   RBQ = BQR [183]
153. RBQ = BQR [183]   RB = RQ [184]
154. OB = OQ [133] & RB = RQ [184]   BQ  OR [185]
155. BD:AK = BA:AM [68] & ED = BD [39] & KM = KA [73] & AB = AI [70] & GO:GK = AI:AM [99] & MK = GK [104] & AD = ED [03]   GO:GK = AD:AK [186]
156. G,M,O are collinear [23] & G,K,I are collinear [14] & JGK = KAJ [58] & G,A,J are collinear [11] & G,E,A are collinear [04] & E,A,D are collinear [02] & MGL = LGA [52] & G,K,L are collinear [17]   OGK = DAK [187]
157. GO:GK = AD:AK [186] & OGK = DAK [187] (Similar Triangles)  KG:KA = KO:KD [188]
158. KG:KA = KO:KD [188] & KG = KA [100]   KO = KD [189]
159. PO = PD [24] & KO = KD [189]   DO  KP [190]
160. O,P,D are collinear [25] & E,A,D are collinear [02] & OK  GA [56] & G,E,A are collinear [04]   POK = PDE [191]
161. PO = PD [24] & OK = ED [173] & POK = PDE [191] (SAS)  OPK = DPE [192]
162. E,G,F are collinear [81] & EF = EB [31] & EG = EB [05]   E is midpoint of GF [193]
163. E is midpoint of GF [193] & P is midpoint of GQ [165]   EP  FQ [194]
164. G,K,N are collinear [20] & G,I,K are collinear [14] & I,B,H are collinear [08] & JG  JK [13] & HB  GA [07] & G,E,A are collinear [04] & E,A,D are collinear [02] & G,A,J are collinear [11]   NKJ = GIB [195]
165. I,B,H are collinear [08] & HIG = GBH [47] & GKJ = JKA [59] & G,I,K are collinear [14] & JG  JK [13] & HB  GA [07] & G,E,A are collinear [04] & E,A,D are collinear [02] & G,A,J are collinear [11] & AK  JN [163]   NJK = GBI [196]
166. NKJ = GIB [195] & NJK = GBI [196] (Similar Triangles)  KNJ = IGB [197]
167. BD:BA = AK:AM [67] & ED = BD [39] & AB = AI [70] & KM = KA [73] & GO:AI = GK:AM [98] & MK = GK [104] & DB = DC [01] & DA = DB [00] & DE = DA [03] & MI = IA [106] & OK = GO [171]   CD:MI = OK:BA [198]
168. CD:MI = OK:BA [198] & BA = MI [108]   CD = OK [199]
169. DB = DC [01] & DA = DB [00] & CD = OK [199]   OK = DA [200]
170. OK  GA [56] & G,E,A are collinear [04] & E,A,D are collinear [02]   OKD = ADK [201]
171. OK = DA [200] & OKD = ADK [201] (SAS)  (KO-AD) = (DO-AK) [202]
172. O,P,D are collinear [25] & BG  BF [84] & BF  HL [144] & KNJ = IGB [197] & G,K,N are collinear [20] & G,I,K are collinear [14] & JN  AK [163] & (KO-AD) = (DO-AK) [202] & OK  GA [56] & G,E,A are collinear [04] & E,A,D are collinear [02]   HL  OP [203]
173. O,P,D are collinear [25] & DO  KP [190] & OPK = DPE [192] & EP  FQ [194]   OP  QF [204]
174. HL  OP [203] & OP  QF [204]   HL  QF [205]
175. OQ  DG [167] & G,E,A are collinear [04] & E,A,D are collinear [02] & OK  GA [56]   OK  OQ [206]
176. OK  OQ [206]   O,K,Q are collinear [207]
177. G,I,B,F are concyclic [80]   GIB = GFB [208]
178. IBQ = IKQ [182] & H,B,I are collinear [08] & G,I,K are collinear [14] & O,K,Q are collinear [207] & DE  IB [176] & HB  GA [07] & G,E,A are collinear [04] & E,A,D are collinear [02] & OK  GA [56] & JG  JK [13] & G,A,J are collinear [11] & IM  JK [179] & GIB = GFB [208] & G,E,F are collinear [81] & BF  HL [144]   (HL-KJ) = (BQ-KJ) [209]
179. (HL-KJ) = (BQ-KJ) [209]   HL  BQ [210]
180. BQ  OR [185] & DO  KP [190] & OPK = DPE [192] & O,P,D are collinear [25] & EP  FQ [194] & HL  FQ [205] & HL  BQ [210]   O,R,D are collinear
==========================

