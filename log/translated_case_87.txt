I0123 16:33:04.831857 140702311272448 inference_utils.py:69] Parsing gin configuration.
I0123 16:33:04.832023 140702311272448 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 16:33:04.832270 140702311272448 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 16:33:04.832303 140702311272448 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 16:33:04.832332 140702311272448 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 16:33:04.832357 140702311272448 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 16:33:04.832384 140702311272448 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 16:33:04.832412 140702311272448 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 16:33:04.832439 140702311272448 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 16:33:04.832466 140702311272448 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 16:33:04.832492 140702311272448 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 16:33:04.832517 140702311272448 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 16:33:04.832576 140702311272448 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 16:33:04.832757 140702311272448 resource_reader.py:55] Path not found: base_htrans.gin
I0123 16:33:04.833017 140702311272448 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 16:33:04.833130 140702311272448 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 16:33:04.839629 140702311272448 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 16:33:04.839759 140702311272448 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 16:33:04.840086 140702311272448 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 16:33:04.840192 140702311272448 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 16:33:04.840470 140702311272448 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 16:33:04.840573 140702311272448 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 16:33:04.840983 140702311272448 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 16:33:04.841083 140702311272448 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 16:33:04.844849 140702311272448 training_loop.py:334] ==== Training loop: initializing model ====
I0123 16:33:04.943107 140702311272448 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 16:33:04.944035 140702311272448 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 16:33:04.950766 140702311272448 training_loop.py:335] Process 0 of 1
I0123 16:33:04.950822 140702311272448 training_loop.py:336] Local device count = 1
I0123 16:33:04.950862 140702311272448 training_loop.py:337] Number of replicas = 1
I0123 16:33:04.950894 140702311272448 training_loop.py:339] Using random number seed 42
I0123 16:33:05.454634 140702311272448 training_loop.py:359] Initializing the model.
I0123 16:33:05.881480 140702311272448 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.882110 140702311272448 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 16:33:05.882215 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882293 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882369 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882450 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882518 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882585 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882652 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882718 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882784 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882852 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882920 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.882987 140702311272448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:33:05.883035 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:05.883084 140702311272448 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:33:05.883210 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:05.883252 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:05.883283 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:05.885341 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.890852 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:05.901475 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.901776 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:05.906034 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:05.916772 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:05.916831 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:05.916869 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:05.916901 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.916964 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.918231 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.918309 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.919005 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.921452 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.927112 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.928821 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.928903 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:05.928939 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:05.929000 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.929129 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:05.929462 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:05.929510 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:05.931421 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.931519 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:05.934478 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.934558 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:05.935057 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:05.945142 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:05.954099 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.954200 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:05.954493 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.954574 140702311272448 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:33:05.954684 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:05.954724 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:05.954755 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:05.956617 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.959071 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:05.964579 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.964849 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:05.967470 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:05.971328 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:05.971384 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:05.971420 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:05.971451 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.971515 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.972105 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.972182 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.972536 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.973292 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.975752 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.976373 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.976449 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:05.976483 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:05.976541 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.976667 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:05.976987 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:05.977031 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:05.978986 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.979079 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:05.981537 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.981618 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:05.982059 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:05.984354 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:05.986241 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.986337 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:05.986623 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.986703 140702311272448 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:33:05.986813 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:05.986852 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:05.986882 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:05.988779 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.991137 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:05.997018 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:05.997281 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:05.999887 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.003712 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.003767 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.003803 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.003835 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.003895 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.004457 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.004533 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.004888 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.005648 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.008108 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.008770 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.008847 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.008881 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.008939 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.009069 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.009397 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.009441 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.011358 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.011451 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.013921 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.014004 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.014489 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.016743 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.018658 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.018758 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.019047 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.019127 140702311272448 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:33:06.019237 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.019277 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.019308 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.021221 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.023571 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.029137 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.029399 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.032006 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.035772 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.035829 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.035865 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.035896 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.035959 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.036520 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.036595 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.036947 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.037728 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.040211 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.040826 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.040903 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.040937 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.040995 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.041128 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.041449 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.041492 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.043381 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.043474 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.046000 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.046084 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.046622 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.048852 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.050744 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.050842 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.051125 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.051205 140702311272448 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:33:06.051315 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.051354 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.051385 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.053300 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.055668 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.061212 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.061477 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.064143 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.067894 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.067952 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.067991 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.068024 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.068088 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.068649 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.068725 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.069081 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.069854 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.072674 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.073289 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.073369 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.073405 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.073464 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.073595 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.073934 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.073978 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.075849 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.075944 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.078435 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.078514 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.078948 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.081185 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.083122 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.083218 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.083504 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.083584 140702311272448 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:33:06.083693 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.083732 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.083763 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.085611 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.087930 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.093455 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.093719 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.096325 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.100173 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.100231 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.100266 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.100297 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.100358 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.100958 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.101034 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.101383 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.102205 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.104730 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.105337 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.105413 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.105447 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.105503 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.105634 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.105966 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.106009 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.107866 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.107960 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.110468 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.110547 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.110980 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.113262 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.115175 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.115269 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.115553 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.115633 140702311272448 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:33:06.115741 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.115780 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.115810 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.117665 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.120079 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.125604 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.125875 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.128468 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.132234 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.132289 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.132324 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.132355 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.132416 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.132965 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.133038 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.133392 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.134164 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.136563 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.137178 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.137253 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.137287 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.137346 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.137478 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.137805 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.137849 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.139777 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.139870 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.142297 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.142378 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.142811 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.145433 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.147335 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.147437 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.147726 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.147808 140702311272448 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:33:06.147917 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.147956 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.147986 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.292035 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.295377 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.301251 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.301553 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.304236 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.308219 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.308279 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.308316 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.308348 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.308410 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.309041 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.309118 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.309479 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.310277 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.312829 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.313465 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.313542 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.313577 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.313638 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.313775 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.314119 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.314163 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.316074 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.316168 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.318727 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.318805 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.319247 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.321549 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.323444 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.323565 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.323852 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.323939 140702311272448 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:33:06.324051 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.324091 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.324123 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.326080 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.328423 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.333995 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.334263 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.336909 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.340654 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.340710 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.340745 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.340776 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.340838 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.341408 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.341484 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.341844 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.342611 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.345111 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.345742 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.345820 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.345855 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.345914 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.346042 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.346369 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.346413 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.348283 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.348376 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.350899 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.350979 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.351407 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.353661 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.355755 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.355850 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.356138 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.356225 140702311272448 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:33:06.356339 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.356379 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.356411 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.358422 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.360820 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.366354 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.366615 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.369596 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.373336 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.373391 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.373426 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.373457 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.373520 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.374133 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.374217 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.374575 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.375349 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.377782 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.378407 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.378486 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.378522 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.378580 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.378710 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.379031 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.379074 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.380955 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.381047 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.383543 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.383625 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.384054 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.386330 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.388217 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.388316 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.388601 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.388686 140702311272448 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:33:06.388797 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.388836 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.388866 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.390721 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.393118 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.398645 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.398898 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.401488 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.405278 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.405333 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.405369 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.405400 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.405461 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.406035 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.406112 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.406466 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.407243 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.409688 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.410314 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.410389 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.410423 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.410481 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.410610 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.410931 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.410974 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.412894 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.412988 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.415698 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.415778 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.416204 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.418517 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.420373 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.420465 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.420753 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.420835 140702311272448 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:33:06.420949 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.420989 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.421020 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.422914 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.425231 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.430721 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.430980 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.433543 140702311272448 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:33:06.437306 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.437362 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.437397 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.437429 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.437495 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.438069 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.438147 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.438495 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.439252 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.441676 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.442652 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.442731 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.442767 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.442826 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.442957 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.443285 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.443328 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.445193 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.445286 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.447757 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.447838 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.448318 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.450532 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.452399 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.452492 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.452779 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.453060 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453127 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453191 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453248 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453301 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453354 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453406 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453457 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453508 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453559 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453610 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453669 140702311272448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:33:06.453709 140702311272448 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:33:06.457383 140702311272448 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:33:06.509688 140702311272448 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.509809 140702311272448 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:33:06.509865 140702311272448 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:33:06.509976 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.510016 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.510046 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.510116 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.512557 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.518110 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.518370 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.520983 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.537688 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.537748 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.537784 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.537814 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.537875 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.539021 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.539098 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.539795 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.541791 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.546496 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.547806 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.547891 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.547926 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.547986 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.548120 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.548232 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.548272 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.550185 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.550279 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.552684 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.552773 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.552884 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.555120 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.557046 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.557146 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.557503 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.557585 140702311272448 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:33:06.557704 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.557745 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.557776 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.557839 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.560086 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.565465 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.565731 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.568371 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.581524 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.581583 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.581618 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.581656 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.581720 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.582282 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.582357 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.582710 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.583393 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.585848 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.586466 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.586544 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.586590 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.586647 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.586776 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.586884 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.586923 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.588852 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.588946 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.591312 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.591393 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.591501 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.593707 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.595611 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.595706 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.595991 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.596073 140702311272448 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:33:06.596184 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.596224 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.596254 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.596318 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.598538 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.603907 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.604164 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.606812 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.619523 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.619579 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.619613 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.619644 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.619704 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.620259 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.620336 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.620690 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.621376 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.623805 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.624424 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.624500 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.624534 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.624598 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.624725 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.624833 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.624871 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.626792 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.626885 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.629268 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.629347 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.629454 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.631659 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.633559 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.633661 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.633944 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.634024 140702311272448 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:33:06.634133 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.634173 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.634203 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.634265 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.636598 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.641952 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.642209 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.644832 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.657518 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.657574 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.657609 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.657648 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.657711 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.658262 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.658338 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.658838 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.659520 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.661939 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.662740 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.662817 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.662851 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.662908 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.663041 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.663152 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.663190 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.665109 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.665201 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.667579 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.667658 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.667767 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.669958 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.671805 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.671899 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.672178 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.672259 140702311272448 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:33:06.672367 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.672406 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.672436 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.672498 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.675064 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.680510 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.680775 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.683371 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.696029 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.696084 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.696119 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.696149 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.696210 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.696760 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.696840 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.697188 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.697875 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.700366 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.700990 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.701066 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.701100 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.701157 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.701292 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.701402 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.701440 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.703327 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.703420 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.705791 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.705868 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.705974 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.708220 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.710067 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.710160 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.710436 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.710516 140702311272448 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:33:06.710622 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.710659 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.710690 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.710752 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.712937 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.718334 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.718590 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.721228 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.733903 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.733958 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.733993 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.734024 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.734084 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.734645 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.734721 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.735070 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.735758 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.738209 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.738825 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.738902 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.738937 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.738994 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.739123 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.739237 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.739275 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.741191 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.741283 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.743646 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.743725 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.743834 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.746017 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.747855 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.747949 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.748230 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.748310 140702311272448 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:33:06.748419 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.748457 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.748487 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.748549 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.750763 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.756233 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.756487 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.759047 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.771890 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.771944 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.771979 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.772009 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.772069 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.772628 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.772701 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.773045 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.773734 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.776154 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.777132 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.777209 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.777243 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.777300 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.777430 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.777539 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.777583 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.779459 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.779551 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.781918 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.781996 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.782102 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.784284 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.786209 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.786305 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.786585 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.786666 140702311272448 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:33:06.786775 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.786813 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.786843 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.786904 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.789118 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.794532 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.794800 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.797434 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.810023 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.810076 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.810111 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.810142 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.810205 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.810810 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.810886 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.811239 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.811917 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.814339 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.814964 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.815041 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.815076 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.815134 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.815270 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.815378 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.815423 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.817275 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.817367 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.819793 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.819873 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.819979 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.822161 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.823989 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.824082 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.824360 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.824441 140702311272448 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:33:06.824550 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.824589 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.824619 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.824680 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.826875 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.832355 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.832612 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.835192 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.847855 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.847912 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.847948 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.847979 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.848038 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.848598 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.848674 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.849025 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.849714 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.852135 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.852790 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.852867 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.852901 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.852958 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.853088 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.853199 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.853238 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.855086 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.855181 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.857510 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.857587 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.857701 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.859863 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.861780 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.861875 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.862156 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.862236 140702311272448 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:33:06.862344 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.862383 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.862413 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.862474 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.864755 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.870090 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.870343 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.872940 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.885907 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.885962 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.885996 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.886027 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.886088 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.886685 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.886760 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.887108 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.887786 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.890210 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.890823 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.890899 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.890934 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.890991 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.891122 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.891230 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.891269 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.893110 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.893207 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.895609 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.895690 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.895798 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.897999 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.899814 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.899907 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.900185 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.900265 140702311272448 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:33:06.900373 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.900411 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.900440 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.900502 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.902671 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.908082 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.908335 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.910892 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.923547 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.923603 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.923638 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.923668 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.923728 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.924282 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.924361 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.924710 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.925393 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.927829 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.928495 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.928571 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.928606 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.928663 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.928791 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.928899 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.928937 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.930812 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.930911 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.933273 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.933351 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.933457 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.935642 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.937543 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.937636 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.937925 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.938006 140702311272448 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:33:06.938114 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:06.938153 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:06.938183 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:06.938243 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.940424 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:06.945758 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.946015 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:06.948632 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:06.961177 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:06.961232 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:06.961266 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:06.961296 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.961356 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.961936 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.962012 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.962361 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.963078 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.965503 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.966131 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.966208 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:06.966243 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:06.966301 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.966431 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:06.966538 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:06.966576 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.968410 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.968502 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.970873 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.970950 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:06.971054 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:06.973285 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:06.975129 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.975223 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:06.975500 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:06.975587 140702311272448 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:33:06.978389 140702311272448 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:33:07.033013 140702311272448 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.033100 140702311272448 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:33:07.033154 140702311272448 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:33:07.033257 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.033294 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.033324 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.033383 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.036000 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.041300 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.041556 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.044094 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.056458 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.056513 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.056547 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.056576 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.056636 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.057190 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.057266 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.057614 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.058285 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.060708 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.061314 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.061391 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.061426 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.061483 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.061610 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.061733 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.061773 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.063572 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.063664 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.066014 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.066094 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.066202 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.068389 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.070219 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.070313 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.070593 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.070673 140702311272448 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:33:07.070779 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.070817 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.070847 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.070907 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.073090 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.078348 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.078602 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.081193 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.093410 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.093466 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.093500 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.093530 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.093590 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.094145 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.094221 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.094565 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.095223 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.097661 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.098266 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.098342 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.098376 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.098435 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.098560 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.098665 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.098710 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.100510 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.100602 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.102949 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.103028 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.103136 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.105343 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.107165 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.107260 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.107539 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.107620 140702311272448 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:33:07.107726 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.107764 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.107794 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.107855 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.110023 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.115272 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.115525 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.118114 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.130341 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.130398 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.130433 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.130463 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.130523 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.131073 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.131147 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.131492 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.132149 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.134577 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.135181 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.135256 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.135290 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.135350 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.135477 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.135583 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.135621 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.137414 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.137506 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.139844 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.139922 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.140029 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.142693 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.144491 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.144586 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.144870 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.144952 140702311272448 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:33:07.145059 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.145098 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.145128 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.145190 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.147353 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.152608 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.152863 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.155452 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.167798 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.167855 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.167897 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.167939 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.168002 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.168555 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.168628 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.168972 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.169646 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.172120 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.172727 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.172800 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.172832 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.172888 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.173010 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.173116 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.173155 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.174999 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.175089 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.177414 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.177490 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.177600 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.179856 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.181695 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.181791 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.182074 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.182153 140702311272448 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:33:07.182259 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.182296 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.182325 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.182385 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.184565 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.189902 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.190153 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.192748 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.205163 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.205216 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.205249 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.205277 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.205335 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.205892 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.205967 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.206317 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.206983 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.209453 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.210073 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.210149 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.210182 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.210237 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.210359 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.210468 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.210505 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.212336 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.212432 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.214779 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.214855 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.214960 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.217195 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.219015 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.219107 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.219385 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.219463 140702311272448 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:33:07.219570 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.219607 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.219635 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.219694 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.221858 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.227167 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.227417 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.230032 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.242463 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.242517 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.242551 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.242579 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.242638 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.243187 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.243260 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.243605 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.244271 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.246755 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.247368 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.247442 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.247475 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.247530 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.247652 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.247756 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.247792 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.249608 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.249712 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.252034 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.252110 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.252214 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.254876 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.256699 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.256791 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.257069 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.257147 140702311272448 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:33:07.257251 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.257288 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.257316 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.257375 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.259551 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.264822 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.265072 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.267696 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.280266 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.280319 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.280352 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.280380 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.280439 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.280988 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.281063 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.281411 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.282239 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.284710 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.285324 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.285398 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.285430 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.285485 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.285608 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.285724 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.285762 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.287595 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.287686 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.290029 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.290109 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.290215 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.292445 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.294274 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.294367 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.294643 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.294722 140702311272448 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:33:07.294825 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.294862 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.294891 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.294949 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.297122 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.302452 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.302706 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.305339 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.317790 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.317842 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.317876 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.317904 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.317963 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.318510 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.318584 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.318938 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.319613 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.322095 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.322714 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.322789 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.322821 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.322877 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.323001 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.323106 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.323142 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.324973 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.325063 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.327389 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.327471 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.327577 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.329808 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.331618 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.331709 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.331982 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.332060 140702311272448 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:33:07.332163 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.332200 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.332228 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.332286 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.334451 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.339751 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.340001 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.342620 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.355076 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.355130 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.355162 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.355192 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.355252 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.355804 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.355878 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.356222 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.356898 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.359374 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.359982 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.360055 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.360088 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.360143 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.360266 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.360372 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.360408 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.362254 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.362344 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.364658 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.364739 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.364845 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.367488 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.369307 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.369398 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.369685 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.369765 140702311272448 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:33:07.369870 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.369907 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.369936 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.369995 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.372161 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.377466 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.377728 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.380323 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.392983 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.393037 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.393070 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.393099 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.393156 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.393722 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.393797 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.394148 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.394828 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.397298 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.397921 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.397995 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.398028 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.398082 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.398204 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.398309 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.398346 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.400579 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.400671 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.402989 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.403065 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.403176 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.405370 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.407176 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.407268 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.407542 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.407621 140702311272448 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:33:07.407726 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.407762 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.407790 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.407849 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.410017 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.415276 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.415528 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.418121 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.430453 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.430505 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.430539 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.430568 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.430627 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.431174 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.431249 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.431595 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.432265 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.434739 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.435353 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.435428 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.435461 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.435515 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.435642 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.435747 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.435786 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.437608 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.437706 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.440034 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.440113 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.440219 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.442460 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.444277 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.444369 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.444645 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.444724 140702311272448 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:33:07.444829 140702311272448 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:33:07.444865 140702311272448 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:33:07.444893 140702311272448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:33:07.444951 140702311272448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.447133 140702311272448 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:33:07.452449 140702311272448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.452701 140702311272448 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:33:07.455337 140702311272448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:33:07.467756 140702311272448 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:33:07.467810 140702311272448 attention.py:418] Single window, no scan.
I0123 16:33:07.467843 140702311272448 transformer_layer.py:389] tlayer: self-attention.
I0123 16:33:07.467872 140702311272448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.467931 140702311272448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.468481 140702311272448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.468555 140702311272448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.468895 140702311272448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.469567 140702311272448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.472036 140702311272448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.472708 140702311272448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.472793 140702311272448 transformer_layer.py:468] tlayer: End windows.
I0123 16:33:07.472833 140702311272448 transformer_layer.py:472] tlayer: final FFN.
I0123 16:33:07.472897 140702311272448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.473036 140702311272448 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:33:07.473167 140702311272448 nn_components.py:325] mlp: activation = None
I0123 16:33:07.473212 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.475147 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.475237 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.477543 140702311272448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.477622 140702311272448 transformer_base.py:443] tbase: final FFN
I0123 16:33:07.477736 140702311272448 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:33:07.480336 140702311272448 nn_components.py:329] mlp: final activation = None
I0123 16:33:07.482161 140702311272448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.482253 140702311272448 nn_components.py:261] mlp: residual
I0123 16:33:07.482528 140702311272448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:07.482659 140702311272448 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:33:07.485398 140702311272448 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:33:11.897179 140702311272448 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 16:33:12.456638 140702311272448 training_loop.py:409] No working directory specified.
I0123 16:33:12.456789 140702311272448 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 16:33:12.457685 140702311272448 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 16:33:15.483350 140702311272448 training_loop.py:447] Only restoring trainable parameters.
I0123 16:33:15.484266 140702311272448 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 16:33:15.484329 140702311272448 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.484375 140702311272448 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.484416 140702311272448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.484637 140702311272448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.484675 140702311272448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.484714 140702311272448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.484752 140702311272448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.484789 140702311272448 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.484826 140702311272448 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.484861 140702311272448 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.484898 140702311272448 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.484934 140702311272448 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.484970 140702311272448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.485005 140702311272448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485040 140702311272448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.485074 140702311272448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485110 140702311272448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485145 140702311272448 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.485179 140702311272448 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.485237 140702311272448 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485277 140702311272448 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.485314 140702311272448 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.485350 140702311272448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.485385 140702311272448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485420 140702311272448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.485455 140702311272448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485490 140702311272448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485524 140702311272448 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.485560 140702311272448 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.485594 140702311272448 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485628 140702311272448 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.485685 140702311272448 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.485725 140702311272448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.485761 140702311272448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485796 140702311272448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.485831 140702311272448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485866 140702311272448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.485899 140702311272448 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.485933 140702311272448 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.485967 140702311272448 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486001 140702311272448 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.486036 140702311272448 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.486071 140702311272448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.486105 140702311272448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486139 140702311272448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.486179 140702311272448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486215 140702311272448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486250 140702311272448 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.486285 140702311272448 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.486320 140702311272448 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486355 140702311272448 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.486390 140702311272448 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.486424 140702311272448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.486458 140702311272448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486493 140702311272448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.486528 140702311272448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486562 140702311272448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486597 140702311272448 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.486632 140702311272448 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.486666 140702311272448 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486701 140702311272448 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.486735 140702311272448 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.486769 140702311272448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.486803 140702311272448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486838 140702311272448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.486872 140702311272448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486906 140702311272448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.486939 140702311272448 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.486973 140702311272448 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.487007 140702311272448 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487041 140702311272448 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.487075 140702311272448 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.487114 140702311272448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.487149 140702311272448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487183 140702311272448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.487217 140702311272448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487251 140702311272448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487286 140702311272448 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.487319 140702311272448 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.487353 140702311272448 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487386 140702311272448 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.487421 140702311272448 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.487454 140702311272448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.487489 140702311272448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487523 140702311272448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.487558 140702311272448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487593 140702311272448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487627 140702311272448 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.487662 140702311272448 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.487697 140702311272448 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487732 140702311272448 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.487767 140702311272448 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.487802 140702311272448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.487837 140702311272448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487871 140702311272448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.487906 140702311272448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487940 140702311272448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.487974 140702311272448 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.488008 140702311272448 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.488048 140702311272448 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.488084 140702311272448 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.488119 140702311272448 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.488153 140702311272448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.488187 140702311272448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.488222 140702311272448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.488256 140702311272448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.488290 140702311272448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.488324 140702311272448 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.488358 140702311272448 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.488393 140702311272448 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.488427 140702311272448 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.488461 140702311272448 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:33:15.488495 140702311272448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:33:15.488529 140702311272448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.488563 140702311272448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.488596 140702311272448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.488630 140702311272448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.488664 140702311272448 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:33:15.488697 140702311272448 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:33:15.488731 140702311272448 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:33:15.488765 140702311272448 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:33:15.488792 140702311272448 training_loop.py:725] Total parameters: 152072288
I0123 16:33:15.489046 140702311272448 training_loop.py:739] Total state size: 0
I0123 16:33:15.514884 140702311272448 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 16:33:15.515131 140702311272448 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 16:33:15.515508 140702311272448 training_loop.py:652] Compiling mode beam_search with jit.
I0123 16:33:15.515880 140702311272448 training_loop.py:89] registering functions: dict_keys([])
I0123 16:33:15.538036 140702311272448 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = mirror f b e; g = on_line g e f; h = circle h c e g; i = on_circle i h c, on_line i b c; j = circle j g e a; k = on_circle k j a, on_line k b a ? coll k g i
I0123 16:33:16.782181 140702311272448 ddar.py:60] Depth 1/1000 time = 1.1841871738433838
I0123 16:33:18.757382 140702311272448 ddar.py:60] Depth 2/1000 time = 1.9749741554260254
I0123 16:33:20.723169 140702311272448 ddar.py:60] Depth 3/1000 time = 1.9655518531799316
I0123 16:33:20.725383 140702311272448 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K : Points
DB = DA [00]
DC = DB [01]
DE = DA [02]
F,E,B are collinear [03]
F,E,G are collinear [04]
HC = HE [05]
HE = HG [06]
HI = HC [07]
C,B,I are collinear [08]
JE = JA [09]
JG = JE [10]
JK = JA [11]
K,B,A are collinear [12]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. HC = HE [05] & HI = HC [07] & HE = HG [06]   C,G,E,I are concyclic [13]
002. C,G,E,I are concyclic [13]   CEG = CIG [14]
003. DB = DA [00] & DE = DA [02] & DC = DB [01]   C,E,B,A are concyclic [15]
004. C,E,B,A are concyclic [15]   BCE = BAE [16]
005. JE = JA [09] & JK = JA [11] & JG = JE [10]   K,G,E,A are concyclic [17]
006. K,G,E,A are concyclic [17]   KGE = KAE [18]
007. F,E,G are collinear [04] & F,E,B are collinear [03] & CEG = CIG [14] & C,B,I are collinear [08] & BCE = BAE [16] & KGE = KAE [18] & K,B,A are collinear [12]   KGF = IGF [19]
008. KGF = IGF [19]   KG  IG [20]
009. GK  GI [20]   I,K,G are collinear
==========================

