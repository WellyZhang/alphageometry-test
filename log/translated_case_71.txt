I0123 11:29:36.513366 140658285101056 inference_utils.py:69] Parsing gin configuration.
I0123 11:29:36.513470 140658285101056 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:29:36.513689 140658285101056 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:29:36.513723 140658285101056 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:29:36.513754 140658285101056 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:29:36.513783 140658285101056 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:29:36.513812 140658285101056 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:29:36.513838 140658285101056 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:29:36.513865 140658285101056 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:29:36.513891 140658285101056 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:29:36.513917 140658285101056 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:29:36.513944 140658285101056 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:29:36.513990 140658285101056 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:29:36.514132 140658285101056 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:29:36.514347 140658285101056 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:29:36.514453 140658285101056 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:29:36.520845 140658285101056 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:29:36.520971 140658285101056 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:29:36.521305 140658285101056 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:29:36.521414 140658285101056 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:29:36.521712 140658285101056 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:29:36.521817 140658285101056 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:29:36.522223 140658285101056 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:29:36.522325 140658285101056 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:29:36.526041 140658285101056 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:29:36.628551 140658285101056 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:29:36.629669 140658285101056 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:29:36.640150 140658285101056 training_loop.py:335] Process 0 of 1
I0123 11:29:36.640246 140658285101056 training_loop.py:336] Local device count = 1
I0123 11:29:36.640316 140658285101056 training_loop.py:337] Number of replicas = 1
I0123 11:29:36.640373 140658285101056 training_loop.py:339] Using random number seed 42
I0123 11:29:37.112320 140658285101056 training_loop.py:359] Initializing the model.
I0123 11:29:37.484462 140658285101056 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.484735 140658285101056 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:29:37.484840 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.484921 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485000 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485083 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485158 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485231 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485305 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485375 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485444 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485515 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485584 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485661 140658285101056 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:29:37.485704 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.485752 140658285101056 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:29:37.485869 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.485908 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.485940 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.488051 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.493441 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.504287 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.504561 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.509011 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.519826 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.519884 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.519922 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.519955 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.520018 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.521209 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.521291 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.522010 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.524500 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.530730 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.532048 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.532129 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.532164 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.532224 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.532353 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.532694 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.532743 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.534660 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.534765 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.537634 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.537722 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.538233 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.548410 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.558250 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.558405 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.558706 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.558793 140658285101056 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:29:37.558907 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.558947 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.558977 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.560906 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.563374 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.568948 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.569226 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.571862 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.575772 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.575832 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.575868 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.575899 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.575960 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.576548 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.576624 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.576981 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.577750 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.580238 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.580860 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.580936 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.580969 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.581027 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.581157 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.581483 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.581526 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.583479 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.583577 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.586052 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.586131 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.586573 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.588886 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.590791 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.590887 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.591180 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.591262 140658285101056 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:29:37.591371 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.591410 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.591440 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.593722 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.596107 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.601730 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.601994 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.604640 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.608505 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.608561 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.608596 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.608626 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.608689 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.609249 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.609326 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.609694 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.610469 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.612961 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.613628 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.613712 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.613746 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.613802 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.613932 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.614258 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.614301 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.616213 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.616305 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.618812 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.618900 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.619391 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.621693 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.623608 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.623703 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.623994 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.624074 140658285101056 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:29:37.624183 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.624222 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.624252 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.626187 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.628589 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.634221 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.634488 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.637162 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.640969 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.641024 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.641058 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.641088 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.641148 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.641720 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.641797 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.642154 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.642925 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.645464 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.646090 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.646168 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.646202 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.646266 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.646393 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.646719 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.646766 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.648662 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.648755 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.651325 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.651411 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.651858 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.654137 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.656045 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.656140 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.656427 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.656511 140658285101056 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:29:37.656620 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.656659 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.656689 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.658648 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.661125 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.666765 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.667031 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.670096 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.673885 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.673942 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.673977 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.674008 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.674070 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.674636 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.674718 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.675073 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.675849 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.678402 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.679028 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.679111 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.679145 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.679208 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.679340 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.679672 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.679716 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.681616 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.681717 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.684280 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.684359 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.684799 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.687079 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.689020 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.689114 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.689407 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.689489 140658285101056 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:29:37.689601 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.689646 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.689678 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.691525 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.693898 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.699467 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.699733 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.702433 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.706171 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.706227 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.706262 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.706293 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.706354 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.706968 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.707046 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.707402 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.708182 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.710655 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.711275 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.711353 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.711386 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.711445 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.711575 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.711898 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.711941 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.713842 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.713936 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.716460 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.716543 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.716983 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.719279 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.721189 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.721284 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.721579 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.721669 140658285101056 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:29:37.721787 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.721826 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.722027 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.723896 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.726355 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.731924 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.732185 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.734817 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.738614 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.738670 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.738705 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.738735 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.738796 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.739357 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.739433 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.739788 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.740558 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.743041 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.743666 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.743743 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.743777 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.743834 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.743959 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.744294 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.744338 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.746628 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.746723 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.749221 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.749300 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.749743 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.891899 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.894132 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.894289 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.894619 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.894715 140658285101056 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:29:37.894839 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.894882 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.894916 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.897043 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.899662 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.905507 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.905798 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.908567 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.912612 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.912671 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.912711 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.912744 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.912806 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.913422 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.913499 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.913872 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.914683 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.917296 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.917941 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.918020 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.918055 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.918116 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.918245 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.918580 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.918623 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.920535 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.920632 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.923238 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.923321 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.923814 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.926159 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.928082 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.928184 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.928476 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.928558 140658285101056 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:29:37.928668 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.928707 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.928737 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.930713 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.933106 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.938783 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.939064 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.941781 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.945556 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.945611 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.945652 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.945686 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.945753 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.946333 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.946412 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.946778 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.947568 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.950117 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.950749 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.950829 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.950864 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.950926 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.951055 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.951396 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.951441 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.953357 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.953450 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.956064 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.956144 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.956584 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.958879 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.960791 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.960885 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.961178 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.961265 140658285101056 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:29:37.961377 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.961416 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.961447 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.963372 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.965736 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:37.971712 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.971977 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:37.974642 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:37.978386 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:37.978441 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:37.978477 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:37.978507 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.978567 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.979124 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.979201 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.979553 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.980361 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.982976 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.983603 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.983680 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:37.983713 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:37.983771 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.983899 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:37.984226 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:37.984269 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.986162 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.986255 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.988775 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.988854 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:37.989283 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:37.991536 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:37.993485 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.993580 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:37.993877 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.993965 140658285101056 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:29:37.994078 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:37.994117 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:37.994147 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:37.995998 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:37.998434 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.003968 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.004226 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.006897 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:38.010662 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.010720 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.010755 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.010785 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.010887 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.011461 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.011538 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.011895 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.012671 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.015141 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.015769 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.015847 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.015881 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.015939 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.016069 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.016395 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.016439 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.018369 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.018464 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.021196 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.021276 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.021714 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.024024 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.025917 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.026017 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.026306 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.026387 140658285101056 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:29:38.026506 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.026546 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.026576 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.028420 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.030878 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.036484 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.036742 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.039386 140658285101056 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:29:38.043545 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.043602 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.043638 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.043668 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.043728 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.044342 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.044424 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.044800 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.045619 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.048470 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.049133 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.049213 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.049249 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.049320 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.049454 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.049816 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.049864 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.051896 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.051993 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.054617 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.054705 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.055154 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.057655 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.059639 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.059738 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.060040 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.060351 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060426 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060494 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060554 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060612 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060667 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060721 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060776 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060830 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060884 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060938 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.060992 140658285101056 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:29:38.061031 140658285101056 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:29:38.064762 140658285101056 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:29:38.120311 140658285101056 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.120490 140658285101056 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:29:38.120559 140658285101056 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:29:38.120697 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.120747 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.120786 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.120880 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.124026 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.130420 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.130706 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.133677 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.152210 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.152274 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.152312 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.152346 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.152413 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.153715 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.153802 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.154585 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.156833 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.162119 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.163568 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.163665 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.163702 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.163767 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.163911 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.164034 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.164077 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.166196 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.166301 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.168954 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.169041 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.169159 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.171648 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.173815 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.173918 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.174233 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.174324 140658285101056 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:29:38.174444 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.174487 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.174519 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.174588 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.177067 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.183182 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.183469 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.186455 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.201261 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.201323 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.201360 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.201392 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.201457 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.202069 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.202150 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.202531 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.203269 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.205909 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.206571 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.206652 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.206694 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.206756 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.206895 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.207010 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.207051 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.209071 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.209169 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.211688 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.211772 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.211884 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.218252 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.220430 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.220545 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.221031 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.221121 140658285101056 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:29:38.221242 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.221286 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.221318 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.221390 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.224026 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.229815 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.230095 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.233003 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.246666 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.246728 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.246767 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.246799 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.246864 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.247479 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.247565 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.247947 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.248686 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.251325 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.251989 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.252070 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.252105 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.252171 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.252310 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.252425 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.252465 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.254496 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.254595 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.257116 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.257198 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.257313 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.259659 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.261713 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.261814 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.262112 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.262197 140658285101056 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:29:38.262313 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.262353 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.262385 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.262452 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.264798 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.270565 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.270839 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.273700 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.287181 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.287241 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.287277 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.287309 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.287376 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.287967 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.288047 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.288418 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.289154 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.291774 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.292452 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.292534 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.292570 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.292634 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.292779 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.292902 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.292943 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.295281 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.295382 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.297931 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.298016 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.298130 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.300468 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.302439 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.302537 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.302825 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.302909 140658285101056 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:29:38.303020 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.303060 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.303092 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.303157 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.305474 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.311000 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.311270 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.313909 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.327000 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.327056 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.327091 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.327121 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.327184 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.327760 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.327837 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.328194 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.328899 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.331472 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.332113 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.332191 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.332226 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.332285 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.332423 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.332539 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.332578 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.334476 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.334573 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.337010 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.337090 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.337199 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.339512 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.341419 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.341516 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.341811 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.341894 140658285101056 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:29:38.342004 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.342043 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.342074 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.342140 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.344408 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.349917 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.350176 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.352882 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.365723 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.365780 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.365815 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.365845 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.365906 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.366478 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.366555 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.366918 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.367627 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.370130 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.370767 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.370846 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.370879 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.370938 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.371073 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.371191 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.371231 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.373184 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.373281 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.375713 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.375793 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.375901 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.378158 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.380034 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.380130 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.380413 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.380494 140658285101056 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:29:38.380603 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.380641 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.380671 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.380736 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.382985 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.388554 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.388811 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.391439 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.404579 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.404636 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.404671 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.404702 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.404762 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.405327 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.405404 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.405765 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.406462 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.408981 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.409664 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.409744 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.409778 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.409837 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.409971 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.410083 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.410127 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.412029 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.412124 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.414535 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.414615 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.414729 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.416952 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.418903 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.419001 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.419286 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.419369 140658285101056 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:29:38.419479 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.419518 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.419549 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.419614 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.421859 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.427298 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.427565 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.430251 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.443009 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.443066 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.443102 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.443132 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.443194 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.443833 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.443912 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.444269 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.444969 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.447474 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.448115 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.448194 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.448228 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.448286 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.448418 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.448529 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.448572 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.450478 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.450574 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.453040 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.453125 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.453237 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.455469 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.457345 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.457443 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.457741 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.457824 140658285101056 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:29:38.457934 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.457973 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.458003 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.458067 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.460305 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.465852 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.466110 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.468731 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.481650 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.481710 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.481745 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.481774 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.481835 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.482405 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.482483 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.482841 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.483535 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.486027 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.486707 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.486785 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.486818 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.486874 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.487001 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.487110 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.487148 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.489027 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.489120 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.491531 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.491610 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.491721 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.493933 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.495857 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.495952 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.496234 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.496317 140658285101056 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:29:38.496428 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.496467 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.496497 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.496562 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.498829 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.504284 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.504545 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.507546 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.520272 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.520329 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.520364 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.520395 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.520457 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.521068 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.521145 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.521501 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.522193 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.524674 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.525296 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.525373 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.525407 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.525463 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.525594 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.525709 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.525749 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.527632 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.527733 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.530185 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.530269 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.530377 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.532587 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.534470 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.534567 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.534850 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.534931 140658285101056 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:29:38.535042 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.535081 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.535112 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.535177 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.537409 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.542965 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.543227 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.545901 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.558784 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.558840 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.558876 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.558906 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.558968 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.559536 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.559613 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.559966 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.560660 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.563143 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.563818 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.563895 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.563929 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.563986 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.564112 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.564222 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.564261 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.566150 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.566250 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.568666 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.568744 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.568851 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.571036 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.572964 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.573059 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.573342 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.573423 140658285101056 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:29:38.573532 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.573570 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.573600 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.573671 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.575898 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.581311 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.581574 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.584229 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.597015 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.597072 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.597107 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.597138 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.597200 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.597772 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.597850 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.598206 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.598902 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.601436 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.602070 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.602149 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.602183 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.602240 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.602373 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.602485 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.602524 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.604396 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.604490 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.607040 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.607121 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.607229 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.609885 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.611814 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.611911 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.612197 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.612290 140658285101056 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:29:38.615162 140658285101056 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:29:38.670995 140658285101056 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.671081 140658285101056 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:29:38.671135 140658285101056 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:29:38.671239 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.671276 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.671305 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.671367 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.673705 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.679052 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.679316 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.681873 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.694232 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.694290 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.694326 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.694356 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.694418 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.694973 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.695050 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.695402 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.696078 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.698576 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.699196 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.699275 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.699310 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.699368 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.699498 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.699616 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.699655 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.701488 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.701582 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.703988 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.704068 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.704177 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.706746 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.708586 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.708683 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.708966 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.709048 140658285101056 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:29:38.709156 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.709195 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.709225 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.709289 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.711511 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.716846 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.717107 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.719751 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.732116 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.732171 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.732206 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.732236 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.732297 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.732853 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.732930 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.733286 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.733973 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.736450 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.737072 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.737150 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.737185 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.737243 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.737371 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.737482 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.737526 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.739378 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.739474 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.741877 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.741956 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.742066 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.744322 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.746190 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.746289 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.746574 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.746656 140658285101056 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:29:38.746765 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.746804 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.746835 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.746899 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.749125 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.754493 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.754754 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.757387 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.769686 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.769744 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.769779 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.769809 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.769870 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.770425 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.770502 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.770853 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.771528 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.774462 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.775081 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.775161 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.775195 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.775253 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.775381 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.775491 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.775530 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.777380 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.777474 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.779856 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.779937 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.780047 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.782290 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.784131 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.784225 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.784509 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.784591 140658285101056 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:29:38.784699 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.784738 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.784769 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.784832 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.787043 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.792345 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.792602 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.795230 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.807723 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.807780 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.807816 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.807854 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.807918 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.808481 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.808556 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.809083 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.809772 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.812289 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.812907 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.812982 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.813014 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.813071 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.813198 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.813306 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.813345 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.815200 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.815294 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.817649 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.817728 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.817835 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.820104 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.821945 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.822040 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.822320 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.822399 140658285101056 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:29:38.822506 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.822544 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.822573 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.822634 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.824821 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.830136 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.830391 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.833049 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.845460 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.845515 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.845548 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.845577 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.845636 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.846194 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.846268 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.846619 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.847297 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.849802 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.850423 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.850499 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.850532 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.850590 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.850714 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.850821 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.850859 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.852721 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.852819 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.855201 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.855279 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.855387 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.857653 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.859541 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.859639 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.859929 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.860012 140658285101056 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:29:38.860124 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.860163 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.860194 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.860258 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.862730 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.868310 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.868567 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.871297 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.883917 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.883972 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.884006 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.884036 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.884097 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.884650 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.884726 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.885087 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.885796 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.888842 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.889488 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.889568 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.889602 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.889669 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.889809 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.889921 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.889960 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.891867 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.891965 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.894340 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.894422 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.894531 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.896801 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.898689 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.898787 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.899075 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.899157 140658285101056 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:29:38.899267 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.899305 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.899334 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.899396 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.901582 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.906985 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.907253 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.909912 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.922462 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.922519 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.922554 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.922585 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.922647 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.923236 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.923315 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.923666 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.924347 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.926885 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.927518 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.927594 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.927628 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.927685 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.927808 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.927919 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.927957 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.929816 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.929909 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.932330 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.932410 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.932518 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.934784 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.936626 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.936720 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.937000 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.937080 140658285101056 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:29:38.937186 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.937225 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.937254 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.937316 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.939588 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.944992 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.945250 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.947958 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.960486 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.960542 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.960575 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.960603 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.960663 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.961218 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.961293 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.961648 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.962358 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.965075 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.965706 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.965783 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:38.965815 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:38.965872 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.965997 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:38.966105 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:38.966143 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.968148 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.968240 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.970641 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.970728 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:38.970841 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:38.973112 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:38.974962 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.975057 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:38.975341 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.975420 140658285101056 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:29:38.975527 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:38.975565 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:38.975594 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:38.975655 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.977847 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:38.983208 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.983474 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:38.986187 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:38.998617 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:38.998673 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:38.998707 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:38.998737 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.998802 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.999367 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.999443 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:38.999795 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.000476 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.003383 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.004003 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.004080 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:39.004112 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:39.004168 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.004292 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:39.004398 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:39.004434 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:39.006280 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.006373 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:39.008753 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.008836 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:39.008944 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:39.011212 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:39.013056 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.013149 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:39.013428 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.013508 140658285101056 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:29:39.013618 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:39.013664 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:39.013695 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:39.013758 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.016014 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:39.021373 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.021627 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:39.024278 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:39.036736 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:39.036791 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:39.036825 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:39.036854 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.036912 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.037469 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.037544 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.037903 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.038583 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.041126 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.041762 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.041842 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:39.041876 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:39.041931 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.042055 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:39.042161 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:39.042199 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:39.044514 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.044608 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:39.046967 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.047046 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:39.047158 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:39.049375 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:39.051207 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.051301 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:39.051589 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.051669 140658285101056 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:29:39.051774 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:39.051811 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:39.051840 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:39.051900 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.054105 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:39.059399 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.059654 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:39.062294 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:39.074862 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:39.074916 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:39.074950 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:39.074979 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.075040 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.075598 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.075673 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.076023 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.076698 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.079210 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.079824 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.079899 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:39.079933 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:39.079988 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.080113 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:39.080219 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:39.080256 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:39.082144 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.082240 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:39.084632 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.084709 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:39.084813 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:39.087074 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:39.088893 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.088986 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:39.089264 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.089342 140658285101056 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:29:39.089447 140658285101056 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:29:39.089485 140658285101056 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:29:39.089514 140658285101056 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:29:39.089575 140658285101056 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.091770 140658285101056 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:29:39.097107 140658285101056 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.097364 140658285101056 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:29:39.099995 140658285101056 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:29:39.112433 140658285101056 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:29:39.112488 140658285101056 attention.py:418] Single window, no scan.
I0123 11:29:39.112521 140658285101056 transformer_layer.py:389] tlayer: self-attention.
I0123 11:29:39.112550 140658285101056 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.112609 140658285101056 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.113160 140658285101056 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.113235 140658285101056 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.113592 140658285101056 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.114284 140658285101056 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.117155 140658285101056 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.117792 140658285101056 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.117870 140658285101056 transformer_layer.py:468] tlayer: End windows.
I0123 11:29:39.117903 140658285101056 transformer_layer.py:472] tlayer: final FFN.
I0123 11:29:39.117959 140658285101056 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.118084 140658285101056 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:29:39.118192 140658285101056 nn_components.py:325] mlp: activation = None
I0123 11:29:39.118229 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:39.120076 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.120169 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:39.122525 140658285101056 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.122603 140658285101056 transformer_base.py:443] tbase: final FFN
I0123 11:29:39.122708 140658285101056 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:29:39.124965 140658285101056 nn_components.py:329] mlp: final activation = None
I0123 11:29:39.126806 140658285101056 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.126900 140658285101056 nn_components.py:261] mlp: residual
I0123 11:29:39.127177 140658285101056 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:39.127263 140658285101056 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:29:39.130060 140658285101056 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:29:43.543819 140658285101056 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:29:44.042332 140658285101056 training_loop.py:409] No working directory specified.
I0123 11:29:44.042451 140658285101056 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:29:44.043241 140658285101056 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:29:47.129261 140658285101056 training_loop.py:447] Only restoring trainable parameters.
I0123 11:29:47.130040 140658285101056 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:29:47.130099 140658285101056 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.130146 140658285101056 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.130191 140658285101056 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.130233 140658285101056 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.130273 140658285101056 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.130313 140658285101056 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.130352 140658285101056 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.130390 140658285101056 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.130429 140658285101056 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.130465 140658285101056 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.130504 140658285101056 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.130542 140658285101056 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.130579 140658285101056 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.130617 140658285101056 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.130654 140658285101056 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.130691 140658285101056 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.130728 140658285101056 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.130766 140658285101056 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.130802 140658285101056 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.130851 140658285101056 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.130890 140658285101056 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.130930 140658285101056 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.130968 140658285101056 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.131005 140658285101056 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131043 140658285101056 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.131081 140658285101056 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131118 140658285101056 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131155 140658285101056 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.131191 140658285101056 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.131227 140658285101056 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131263 140658285101056 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.131302 140658285101056 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.131340 140658285101056 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.131377 140658285101056 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131415 140658285101056 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.131451 140658285101056 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131488 140658285101056 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131524 140658285101056 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.131561 140658285101056 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.131597 140658285101056 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131634 140658285101056 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.131670 140658285101056 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.131707 140658285101056 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.131744 140658285101056 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131780 140658285101056 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.131823 140658285101056 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131861 140658285101056 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.131898 140658285101056 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.131936 140658285101056 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.131971 140658285101056 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132008 140658285101056 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.132044 140658285101056 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.132080 140658285101056 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.132117 140658285101056 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132153 140658285101056 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.132189 140658285101056 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132226 140658285101056 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132262 140658285101056 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.132298 140658285101056 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.132334 140658285101056 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132370 140658285101056 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.132406 140658285101056 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.132443 140658285101056 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.132479 140658285101056 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132515 140658285101056 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.132551 140658285101056 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132589 140658285101056 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132625 140658285101056 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.132661 140658285101056 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.132696 140658285101056 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132733 140658285101056 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.132769 140658285101056 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.132811 140658285101056 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.132849 140658285101056 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132887 140658285101056 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.132923 140658285101056 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132959 140658285101056 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.132995 140658285101056 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.133031 140658285101056 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.133067 140658285101056 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.133103 140658285101056 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.133139 140658285101056 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.133174 140658285101056 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.133210 140658285101056 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.133247 140658285101056 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.133283 140658285101056 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.133320 140658285101056 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.133356 140658285101056 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.133392 140658285101056 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.133428 140658285101056 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.133465 140658285101056 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.133501 140658285101056 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.133537 140658285101056 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.133573 140658285101056 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.133610 140658285101056 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.133654 140658285101056 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.133694 140658285101056 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.133732 140658285101056 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.133769 140658285101056 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.133811 140658285101056 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.133850 140658285101056 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.133888 140658285101056 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.133925 140658285101056 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.133963 140658285101056 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.134000 140658285101056 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.134037 140658285101056 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.134074 140658285101056 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.134111 140658285101056 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.134147 140658285101056 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.134184 140658285101056 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.134221 140658285101056 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.134259 140658285101056 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:29:47.134296 140658285101056 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:29:47.134334 140658285101056 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.134370 140658285101056 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.134408 140658285101056 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.134444 140658285101056 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.134480 140658285101056 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:29:47.134516 140658285101056 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:29:47.134552 140658285101056 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:29:47.134588 140658285101056 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:29:47.134617 140658285101056 training_loop.py:725] Total parameters: 152072288
I0123 11:29:47.134842 140658285101056 training_loop.py:739] Total state size: 0
I0123 11:29:47.155071 140658285101056 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:29:47.155371 140658285101056 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:29:47.155884 140658285101056 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:29:47.156219 140658285101056 training_loop.py:89] registering functions: dict_keys([])
I0123 11:29:47.172137 140658285101056 graph.py:499] a b c = triangle a b c; d = midpoint d c a; e = on_line e a d; f = on_circle f a c, on_line f a b; g = on_circle g a c, on_line g a b; h = on_circle h e c, on_line h a b; i = on_circle i e c, on_line i a b; j = on_circle j i g, on_circle j e c; k = on_circle k i g, on_circle k e c; l = on_circle l a c, on_line l k j; m = on_circle m a c, on_line m k j; n = on_circle n e c, on_line n m c; o = on_circle o e c, on_line o l c; p = on_circle p i g, on_line p e i; q = on_circle q i g, on_line q e i ? perp n p p i
I0123 11:29:57.165992 140658285101056 ddar.py:60] Depth 1/1000 time = 9.727293014526367
I0123 11:30:26.361752 140658285101056 ddar.py:60] Depth 2/1000 time = 29.195499658584595
I0123 11:31:01.811378 140658285101056 ddar.py:60] Depth 3/1000 time = 35.44925904273987
I0123 11:31:40.097148 140658285101056 ddar.py:60] Depth 4/1000 time = 38.285436391830444
I0123 11:32:24.489365 140658285101056 ddar.py:60] Depth 5/1000 time = 44.391491413116455
I0123 11:33:09.073051 140658285101056 ddar.py:60] Depth 6/1000 time = 44.583033084869385
I0123 11:33:54.185370 140658285101056 ddar.py:60] Depth 7/1000 time = 45.110376834869385
I0123 11:34:44.137356 140658285101056 ddar.py:60] Depth 8/1000 time = 49.425358057022095
I0123 11:35:34.186561 140658285101056 ddar.py:60] Depth 9/1000 time = 50.04815673828125
I0123 11:36:25.310832 140658285101056 ddar.py:60] Depth 10/1000 time = 51.123230934143066
I0123 11:37:16.965333 140658285101056 ddar.py:60] Depth 11/1000 time = 51.65351676940918
I0123 11:38:08.867818 140658285101056 ddar.py:60] Depth 12/1000 time = 51.90134263038635
I0123 11:39:00.942809 140658285101056 ddar.py:60] Depth 13/1000 time = 52.0742027759552
I0123 11:39:53.343655 140658285101056 ddar.py:60] Depth 14/1000 time = 52.247495889663696
I0123 11:40:46.256712 140658285101056 ddar.py:60] Depth 15/1000 time = 52.83633637428284
I0123 11:41:44.263925 140658285101056 ddar.py:60] Depth 16/1000 time = 58.00628471374512
I0123 11:41:44.486451 140658285101056 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E G I J K M N P : Points
D,A,C are collinear [00]
D,E,A are collinear [01]
AG = AC [02]
A,B,G are collinear [03]
EI = EC [04]
A,B,I are collinear [05]
EJ = EC [06]
IJ = IG [07]
EK = EC [08]
IK = IG [09]
AM = AC [10]
J,M,K are collinear [11]
ACM = ACM [12]
EN = EC [13]
N,M,C are collinear [14]
P,E,I are collinear [15]
IP = IG [16]

 * Auxiliary Constructions:
F H L O Q : Points
AF = AC [17]
A,B,F are collinear [18]
EH = EC [19]
A,B,H are collinear [20]
L,J,K are collinear [21]
AL = AC [22]
ACL = ACL [23]
EO = EC [24]
L,O,C are collinear [25]
IQ = IG [26]
E,Q,I are collinear [27]

 * Proof steps:
001. EN = EC [13] & EO = EC [24]   EO = EN [28]
002. EI = EC [04] & EN = EC [13] & EH = EC [19]   N,H,C,I are concyclic [29]
003. EI = EC [04] & EO = EC [24] & EN = EC [13] & N,H,C,I are concyclic [29] & EJ = EC [06] & EH = EC [19]   N,C,O,J are concyclic [30]
004. EI = EC [04] & EO = EC [24] & EN = EC [13] & N,H,C,I are concyclic [29] & N,C,O,J are concyclic [30]   N,J,O,I are concyclic [31]
005. EI = EC [04] & EK = EC [08] & EH = EC [19]   K,H,C,I are concyclic [32]
006. N,H,C,I are concyclic [29] & EI = EC [04] & EJ = EC [06] & EH = EC [19] & K,H,C,I are concyclic [32]   N,J,K,I are concyclic [33]
007. N,H,C,I are concyclic [29] & EI = EC [04] & EJ = EC [06] & EH = EC [19] & K,H,C,I are concyclic [32]   N,J,K,C are concyclic [34]
008. N,H,C,I are concyclic [29] & EI = EC [04] & EJ = EC [06] & EH = EC [19] & K,H,C,I are concyclic [32]   N,J,K,H are concyclic [35]
009. N,J,K,I are concyclic [33]   NKJ = NIJ [36]
010. N,J,K,I are concyclic [33]   INK = IJK [37]
011. N,J,K,I are concyclic [33]   JNI = JKI [38]
012. N,J,K,I are concyclic [33]   NJK = NIK [39]
013. L,J,K are collinear [21] & NKJ = NIJ [36]   (NK-LJ) = NIJ [40]
014. IK = IG [09] & IJ = IG [07]   IJ = IK [41]
015. IJ = IK [41]   IJK = JKI [42]
016. INK = IJK [37] & IJK = JKI [42] & JNI = JKI [38]   JNI = INK [43]
017. (NK-LJ) = NIJ [40] & JNI = INK [43]   (LJ-NI) = IJN [44]
018. AM = AC [10] & AL = AC [22] & EO = EN [28]   AL:AM = EO:EN [45]
019. EO = EC [24]   EOC = OCE [46]
020. EOC = OCE [46] & L,O,C are collinear [25] & D,E,A are collinear [01] & D,A,C are collinear [00]   (EO-CL) = LCA [47]
021. AL = AC [22]   ALC = LCA [48]
022. L,C,O are collinear [25] & (EO-CL) = LCA [47] & ALC = LCA [48]   ALO = EOL [49]
023. ALO = EOL [49]   LA  EO [50]
024. EN = EC [13]   ENC = NCE [51]
025. AM = AC [10]   MCA = AMC [52]
026. N,M,C are collinear [14] & ENC = NCE [51] & D,E,A are collinear [01] & D,A,C are collinear [00] & MCA = AMC [52]   AMN = ENM [53]
027. AMN = ENM [53]   AM  EN [54]
028. AL  EO [50] & AM  EN [54]   LAM = OEN [55]
029. AL:AM = EO:EN [45] & LAM = OEN [55] (Similar Triangles)  ALM = EON [56]
030. (LJ-NI) = IJN [44] & L,J,K are collinear [21] & ALM = EON [56] & J,M,K are collinear [11] & AL  EO [50]   NJI = INO [57]
031. N,J,O,I are concyclic [31] & NJI = INO [57]   NI = IO [58]
032. EO = EN [28] & NI = IO [58]   EI  ON [59]
033. EJ = EC [06] & EK = EC [08]   EK = EJ [60]
034. EK = EJ [60] & IJ = IK [41]   JK  EI [61]
035. AM = AC [10] & AG = AC [02] & AF = AC [17]   A is the circumcenter of \Delta GMF [62]
036. A,B,F are collinear [18] & A,B,G are collinear [03]   A,G,F are collinear [63]
037. A is the circumcenter of \Delta GMF [62] & A,G,F are collinear [63]   GM  FM [64]
038. P,E,I are collinear [15] & L,J,K are collinear [21] & JK  EI [61] & GM  FM [64]   GMF = (PE-LJ) [65]
039. AL = AC [22] & AM = AC [10] & AG = AC [02]   L,G,M,C are concyclic [66]
040. AG = AC [02] & AL = AC [22] & AF = AC [17]   A is the circumcenter of \Delta GLF [67]
041. L,G,M,C are concyclic [66] & AG = AC [02] & AL = AC [22] & AF = AC [17]   L,G,M,F are concyclic [68]
042. G,M,L,F are concyclic [68]   GLM = GFM [69]
043. G,M,L,F are concyclic [68]   GML = GFL [70]
044. A,B,G are collinear [03] & L,J,K are collinear [21] & GLM = GFM [69] & J,M,K are collinear [11] & A,B,F are collinear [18]   (FM-AG) = JLG [71]
045. GMF = (PE-LJ) [65] & (FM-AG) = JLG [71]   MGA = (PE-LG) [72]
046. AL = AC [22] & AG = AC [02]   AG = AL [73]
047. AG = AL [73]   AGL = GLA [74]
048. AGL = GLA [74] & A,B,G are collinear [03]   (AB-GL) = GLA [75]
049. P,E,I are collinear [15] & MGA = (PE-LG) [72] & A,B,G are collinear [03] & (AB-GL) = GLA [75] & AL  EO [50]   (LG-EO) = (MG-PE) [76]
050. A,B,G are collinear [03] & L,J,K are collinear [21] & GML = GFL [70] & J,M,K are collinear [11] & A,B,F are collinear [18]   (LF-AG) = (LJ-MG) [77]
051. AM = AC [10] & AG = AC [02]   AG = AM [78]
052. AG = AM [78]   AGM = GMA [79]
053. (LF-AG) = (LJ-MG) [77] & GMA = AGM [79]   (LJ-AM) = (LF-MG) [80]
054. A is the circumcenter of \Delta GLF [67] & A,G,F are collinear [63]   GL  FL [81]
055. L,J,K are collinear [21] & P,E,I are collinear [15] & JK  EI [61]   LJ  PE [82]
056. GL  FL [81] & LJ  PE [82]   (LG-PE) = FLJ [83]
057. P,E,I are collinear [15] & (LJ-AM) = (LF-MG) [80] & L,J,K are collinear [21] & (LG-PE) = FLJ [83]   (LG-PE) = GMA [84]
058. (LG-EO) = (MG-PE) [76] & (LG-PE) = GMA [84]   (PE-AM) = OEP [85]
059. P,E,I are collinear [15] & (PE-AM) = OEP [85] & AL  EO [50] & AM  EN [54]   OEP = PEN [86]
060. EO = EN [28] & OEP = PEN [86] (SAS)  OPE = EPN [87]
061. N,J,O,C are concyclic [30]   NJO = NCO [88]
062. N,J,O,C are concyclic [30]   NJC = NOC [89]
063. NJO = NCO [88] & N,M,C are collinear [14] & L,O,C are collinear [25]   NJO = MCL [90]
064. N,J,O,I are concyclic [31]   NJO = NIO [91]
065. AG = AC [02]   AGC = GCA [92]
066. AGC = GCA [92] & A,B,G are collinear [03]   (AB-CG) = GCA [93]
067. EO = EN [28]   EON = ONE [94]
068. EI = EC [04] & EO = EC [24]   EO = EI [95]
069. EO = EI [95]   EOI = OIE [96]
070. EI = EC [04] & EN = EC [13]   EN = EI [97]
071. EN = EI [97]   ENI = NIE [98]
072. NJO = MCL [90] & NJO = NIO [91] & (AB-GL) = GLA [75] & (AB-CG) = GCA [93] & ALC = LCA [48] & EON = ONE [94] & EOI = OIE [96] & (EO-CL) = LCA [47] & ENI = NIE [98] (Angle chase)  (NO-CM) = LGC [99]
073. L,C,O are collinear [25] & (EO-CL) = LCA [47] & ALC = LCA [48]   ALC = EOC [100]
074. D,E,A are collinear [01] & D,A,C are collinear [00] & L,O,C are collinear [25] & ACL = ACL [23]   ACL = ECO [101]
075. ALC = EOC [100] & ACL = ECO [101] (Similar Triangles)  LA:LC = OE:OC [102]
076. LA:LC = OE:OC [102] & AL = AC [22] & EO = EC [24]   AC:LC = EC:OC [103]
077. N,M,C are collinear [14] & ENC = NCE [51] & D,E,A are collinear [01] & D,A,C are collinear [00] & MCA = AMC [52]   AMC = ENC [104]
078. D,E,A are collinear [01] & D,A,C are collinear [00] & N,M,C are collinear [14] & ACM = ACM [12]   ACM = ECN [105]
079. AMC = ENC [104] & ACM = ECN [105] (Similar Triangles)  MA:MC = NE:NC [106]
080. MA:MC = NE:NC [106] & AM = AC [10] & EN = EC [13]   AC:MC = EC:NC [107]
081. EI = EC [04] & EO = EC [24] & EN = EC [13] & N,H,C,I are concyclic [29] & K,H,C,I are concyclic [32]   N,C,O,K are concyclic [108]
082. EI = EC [04] & EO = EC [24] & EN = EC [13] & N,H,C,I are concyclic [29] & N,C,O,J are concyclic [30] & N,C,O,K are concyclic [108]   J,K,O,C are concyclic [109]
083. J,K,O,C are concyclic [109]   JKO = JCO [110]
084. L,C,O are collinear [25] & J,M,K are collinear [11] & JKO = JCO [110]   COK = CJM [111]
085. N,K,O,C are concyclic [108]   NOK = NCK [112]
086. J,M,K are collinear [11] & NOK = NCK [112] & N,M,C are collinear [14] & ALM = EON [56] & L,J,K are collinear [21] & AL  EO [50]   CKO = CMJ [113]
087. COK = CJM [111] & CKO = CMJ [113] (Similar Triangles)  OC:KC = JC:MC [114]
088. EI = EC [04] & EO = EC [24] & EN = EC [13] & N,H,C,I are concyclic [29] & EJ = EC [06] & EH = EC [19] & K,H,C,I are concyclic [32]   N,J,K,O are concyclic [115]
089. N,J,K,O are concyclic [115]   NKJ = NOJ [116]
090. N,J,K,C are concyclic [34]   NKJ = NCJ [117]
091. NKJ = NCJ [117] & N,M,C are collinear [14]   NKJ = MCJ [118]
092. N,H,C,I are concyclic [29] & EI = EC [04] & EJ = EC [06] & EH = EC [19]   N,J,C,I are concyclic [119]
093. N,H,C,I are concyclic [29] & EI = EC [04] & EJ = EC [06] & EH = EC [19]   N,J,H,I are concyclic [120]
094. N,J,C,I are concyclic [119]   NJC = NIC [121]
095. IJ = IG [07] & IQ = IG [26] & IP = IG [16] & IK = IG [09]   P,Q,G,K are concyclic [122]
096. P,Q,K,G are concyclic [122]   PQK = PGK [123]
097. PQK = PGK [123] & E,Q,I are collinear [27] & P,E,I are collinear [15]   (EI-KQ) = PGK [124]
098. IP = IG [16]   IPG = PGI [125]
099. IPG = PGI [125] & P,E,I are collinear [15] & A,B,I are collinear [05] & A,B,G are collinear [03]   (EI-GP) = (GP-AB) [126]
100. NKJ = NOJ [116] & NKJ = MCJ [118] & NJO = MCL [90] & NJC = NIC [121] & (EI-KQ) = PGK [124] & (AB-CG) = GCA [93] & EON = ONE [94] & (EO-CL) = LCA [47] & ENI = NIE [98] & (EI-GP) = (GP-AB) [126] (Angle chase)  (CI-KQ) = CGK [127]
101. H,K,C,I are concyclic [32] & EI = EC [04] & EJ = EC [06] & EH = EC [19]   J,K,C,I are concyclic [128]
102. J,K,C,I are concyclic [128]   KJC = KIC [129]
103. L,J,K are collinear [21] & KJC = KIC [129]   LJC = KIC [130]
104. IJ = IG [07] & IQ = IG [26] & IP = IG [16] & P,Q,G,K are concyclic [122]   P,Q,J,K are concyclic [131]
105. P,Q,J,K are concyclic [131]   PQK = PJK [132]
106. P,Q,J,K are concyclic [131]   PJQ = PKQ [133]
107. P,Q,J,K are concyclic [131]   PQJ = PKJ [134]
108. IK = IG [09] & IQ = IG [26]   IQ = IK [135]
109. IQ = IK [135]   IQK = QKI [136]
110. L,J,K are collinear [21] & PQK = PJK [132] & E,Q,I are collinear [27] & P,E,I are collinear [15] & IQK = QKI [136]   QKI = PJL [137]
111. LJC = KIC [130] & QKI = PJL [137]   (CI-KQ) = CJP [138]
112. (CI-KQ) = CGK [127] & (CI-KQ) = CJP [138]   CJP = CGK [139]
113. NKJ = NOJ [116] & NKJ = MCJ [118] & NJO = MCL [90] & NJC = NIC [121] & (AB-CG) = GCA [93] & EON = ONE [94] & (EO-CL) = LCA [47] & ENI = NIE [98] & (EI-GP) = (GP-AB) [126] (Angle chase)  ICA = PGC [140]
114. EI = EC [04]   EIC = ICE [141]
115. P,E,I are collinear [15] & ICA = PGC [140] & EIC = ICE [141] & D,E,A are collinear [01] & D,A,C are collinear [00]   PIC = PGC [142]
116. PIC = PGC [142]   P,G,C,I are concyclic [143]
117. P,G,C,I are concyclic [143]   PGI = PCI [144]
118. K,H,C,I are concyclic [32]   KHI = KCI [145]
119. PGI = PCI [144] & A,B,I are collinear [05] & A,B,G are collinear [03] & KHI = KCI [145] & A,B,H are collinear [20]   CKH = CPG [146]
120. N,J,K,H are concyclic [35]   NKJ = NHJ [147]
121. N,J,K,H are concyclic [35]   NJK = NHK [148]
122. N,J,H,I are concyclic [120]   NJH = NIH [149]
123. NJH = NIH [149] & A,B,I are collinear [05] & A,B,H are collinear [20]   NJH = (IN-AB) [150]
124. PQJ = PKJ [134] & E,Q,I are collinear [27] & P,E,I are collinear [15]   (EI-JQ) = PKJ [151]
125. IJ = IG [07] & IP = IG [16]   IP = IJ [152]
126. IP = IJ [152]   IPJ = PJI [153]
127. IPJ = PJI [153] & P,E,I are collinear [15]   (EI-JP) = PJI [154]
128. NKJ = NHJ [147] & NKJ = NIJ [36] & NJK = NHK [148] & NJH = (IN-AB) [150] & PJQ = PKQ [133] & (EI-JQ) = PKJ [151] & (EI-KQ) = PGK [124] & (EI-JP) = PJI [154] & (EI-GP) = (GP-AB) [126] (Angle chase)  GPJ = HKG [155]
129. CKH = CPG [146] & GPJ = HKG [155]   CKG = CPJ [156]
130. CJP = CGK [139] & CKG = CPJ [156] (Similar Triangles)  JC:PC = GC:KC [157]
131. AL = AC [22] & AM = AC [10] & EN = EC [13] & EO = EC [24] & AC:LC = EC:OC [103] & AC:MC = EC:NC [107] & OC:KC = JC:MC [114] & JC:PC = GC:KC [157] (Ratio chase)  LC:GC = PC:NC [158]
132. N,H,C,I are concyclic [29]   NHI = NCI [159]
133. N,M,C are collinear [14] & NHI = NCI [159] & A,B,I are collinear [05] & A,B,H are collinear [20] & PGI = PCI [144] & A,B,G are collinear [03]   GPC = HNM [160]
134. G,M,L,C are concyclic [66]   GLM = GCM [161]
135. G,M,L,C are concyclic [66]   GML = GCL [162]
136. GLM = GCM [161] & J,M,K are collinear [11] & L,J,K are collinear [21]   (GL-JK) = GCM [163]
137. IK = IG [09] & IP = IG [16]   IP = IK [164]
138. IP = IK [164]   IPK = PKI [165]
139. IPK = PKI [165] & P,E,I are collinear [15]   (EI-KP) = PKI [166]
140. IJ = IG [07] & IQ = IG [26]   IQ = IJ [167]
141. IQ = IJ [167]   IQJ = QJI [168]
142. IQJ = QJI [168] & E,Q,I are collinear [27]   (EI-JQ) = QJI [169]
143. NKJ = NHJ [147] & NKJ = NIJ [36] & NJK = NIK [39] & NJO = MCL [90] & NJO = NIO [91] & NJH = (IN-AB) [150] & (GL-JK) = GCM [163] & (EI-JQ) = PKJ [151] & (AB-GL) = GLA [75] & ALC = LCA [48] & EOI = OIE [96] & (EO-CL) = LCA [47] & (EI-GP) = (GP-AB) [126] & (EI-KP) = PKI [166] & (EI-JQ) = QJI [169] (Angle chase)  (CL-GP) = (CG-HN) [170]
144. L,C,O are collinear [25] & (CL-GP) = (CG-HN) [170]   (LO-PG) = (GC-NH) [171]
145. GPC = HNM [160] & (LO-PG) = (GC-NH) [171]   (NM-GC) = (PC-LO) [172]
146. N,M,C are collinear [14] & (NM-GC) = (PC-LO) [172] & L,O,C are collinear [25]   LCG = PCN [173]
147. LC:GC = PC:NC [158] & LCG = PCN [173] (Similar Triangles)  LGC = PNC [174]
148. N,M,C are collinear [14] & (NO-CM) = LGC [99] & LGC = PNC [174]   PNM = ONM [175]
149. PNM = ONM [175]   PN  NO [176]
150. OC:KC = JC:MC [114] & JC:PC = GC:KC [157] (Ratio chase)  OC:PC = GC:MC [177]
151. L,O,C are collinear [25] & (NM-GC) = (PC-LO) [172] & N,M,C are collinear [14]   GCM = OCP [178]
152. OC:PC = GC:MC [177] & GCM = OCP [178] (Similar Triangles)  CGM = COP [179]
153. L,C,O are collinear [25] & NJC = NOC [89]   ONJ = (LO-JC) [180]
154. L,C,O are collinear [25] & GML = GCL [162] & J,M,K are collinear [11] & L,J,K are collinear [21] & ALM = EON [56] & AL  EO [50]   (GC-LO) = (MG-NO) [181]
155. ONJ = (LO-JC) [180] & (GC-LO) = (MG-NO) [181]   NJC = MGC [182]
156. L,C,O are collinear [25] & CGM = COP [179] & NJC = MGC [182] & NJC = NOC [89]   NOL = POL [183]
157. NOL = POL [183]   NO  PO [184]
158. EI  ON [59] & OPE = EPN [87] & P,E,I are collinear [15] & NP  NO [176] & NO  OP [184]   NP  PI
==========================

