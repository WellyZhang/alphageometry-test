I0123 17:03:30.586300 140046437961728 inference_utils.py:69] Parsing gin configuration.
I0123 17:03:30.586396 140046437961728 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 17:03:30.586587 140046437961728 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 17:03:30.586620 140046437961728 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 17:03:30.586651 140046437961728 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 17:03:30.586680 140046437961728 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 17:03:30.586706 140046437961728 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 17:03:30.586733 140046437961728 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 17:03:30.586759 140046437961728 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 17:03:30.586785 140046437961728 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 17:03:30.586810 140046437961728 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 17:03:30.586834 140046437961728 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 17:03:30.586877 140046437961728 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 17:03:30.587008 140046437961728 resource_reader.py:55] Path not found: base_htrans.gin
I0123 17:03:30.587202 140046437961728 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 17:03:30.587299 140046437961728 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 17:03:30.593566 140046437961728 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 17:03:30.593690 140046437961728 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 17:03:30.594012 140046437961728 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 17:03:30.594114 140046437961728 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 17:03:30.594388 140046437961728 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 17:03:30.594484 140046437961728 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 17:03:30.594887 140046437961728 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 17:03:30.594985 140046437961728 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 17:03:30.598561 140046437961728 training_loop.py:334] ==== Training loop: initializing model ====
I0123 17:03:30.698792 140046437961728 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 17:03:30.699521 140046437961728 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 17:03:30.706040 140046437961728 training_loop.py:335] Process 0 of 1
I0123 17:03:30.706094 140046437961728 training_loop.py:336] Local device count = 1
I0123 17:03:30.706133 140046437961728 training_loop.py:337] Number of replicas = 1
I0123 17:03:30.706166 140046437961728 training_loop.py:339] Using random number seed 42
I0123 17:03:31.188653 140046437961728 training_loop.py:359] Initializing the model.
I0123 17:03:31.608081 140046437961728 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.608311 140046437961728 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 17:03:31.608411 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.608487 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.608562 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.608639 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.608708 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.608776 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.608843 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.608911 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.608978 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.609045 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.609112 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.609178 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 17:03:31.609216 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:31.609260 140046437961728 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:03:31.609371 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:31.609410 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:31.609440 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:31.611440 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.616659 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:31.627205 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.627481 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:31.631869 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:31.642682 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:31.642740 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:31.642778 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:31.642811 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.642874 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.644063 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.644141 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.644861 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.647360 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.653571 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.654888 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.654970 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:31.655007 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:31.655069 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.655197 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:31.655535 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:31.655585 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.657516 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.657616 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.660490 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.660570 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:31.661083 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:31.671315 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.680114 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.680212 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.680508 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.680588 140046437961728 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:03:31.680698 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:31.680738 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:31.680771 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:31.682634 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.685120 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:31.690723 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.690982 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:31.693608 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:31.697383 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:31.697438 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:31.697474 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:31.697504 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.697566 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.698135 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.698210 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.698567 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.699328 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.701802 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.702421 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.702497 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:31.702533 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:31.702592 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.702717 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:31.703041 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:31.703084 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.705016 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.705113 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.707593 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.707675 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:31.708106 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:31.710402 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.712278 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.712370 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.712657 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.712736 140046437961728 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:03:31.712844 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:31.712883 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:31.712915 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:31.715156 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.717519 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:31.723071 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.723335 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:31.726596 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:31.730641 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:31.730703 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:31.730739 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:31.730771 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.730840 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.731409 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.731484 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.731848 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.732622 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.735126 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.735792 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.735872 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:31.735907 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:31.735965 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.736089 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:31.736416 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:31.736460 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.738484 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.738576 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.741070 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.741155 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:31.741638 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:31.743914 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.745832 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.745926 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.746218 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.746302 140046437961728 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:03:31.746413 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:31.746452 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:31.746484 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:31.748375 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.750769 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:31.756365 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.756630 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:31.759268 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:31.763043 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:31.763098 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:31.763134 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:31.763165 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.763226 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.763792 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.763867 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.764226 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.764996 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.767535 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.768162 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.768239 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:31.768275 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:31.768334 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.768463 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:31.768784 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:31.768827 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.770724 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.770817 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.773384 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.773469 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:31.773910 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:31.776169 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.778084 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.778179 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.778472 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.778552 140046437961728 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:03:31.778661 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:31.778700 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:31.778732 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:31.780619 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.783004 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:31.788592 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.788849 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:31.791878 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:31.795605 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:31.795659 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:31.795696 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:31.795727 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.795790 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.796355 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.796431 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.796790 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.797568 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.800119 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.800744 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.800821 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:31.800857 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:31.800917 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.801051 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:31.801374 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:31.801416 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.803332 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.803425 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.805971 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.806056 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:31.806483 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:31.808730 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.810690 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.810784 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.811082 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.811163 140046437961728 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:03:31.811273 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:31.811312 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:31.811344 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:31.813186 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.815547 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:31.821147 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.821408 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:31.824069 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:31.827748 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:31.827803 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:31.827840 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:31.827871 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.827934 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.828526 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.828601 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.828962 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.829742 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.832217 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.832827 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.832908 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:31.832944 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:31.833003 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.833134 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:31.833459 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:31.833502 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.835382 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.835474 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.838002 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.838086 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:31.838520 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:31.840787 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.842703 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.842798 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.843088 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.843167 140046437961728 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:03:31.843276 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:31.843315 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:31.843346 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:31.845175 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.847607 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:31.853177 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.853436 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:31.856054 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:31.859853 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:31.859911 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:31.859949 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:31.859983 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.860049 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.860630 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.860707 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.861081 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.861893 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.864473 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.865090 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.865167 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:31.865203 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:31.865264 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.865391 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:31.865720 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:31.865764 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:31.868103 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.868196 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:31.870707 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:31.870790 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:31.871232 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.010593 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.012772 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.012922 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.013233 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.013324 140046437961728 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:03:32.013439 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.013480 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.013514 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.015772 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.018302 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.023982 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.024260 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.026954 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:32.030836 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.030893 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.030931 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.030964 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.031026 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.031634 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.031716 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.032085 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.032863 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.035437 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.036068 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.036146 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.036183 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.036243 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.036370 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.036696 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.036740 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.038666 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.038763 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.041258 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.041338 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.041832 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.044120 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.046023 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.046125 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.046418 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.046498 140046437961728 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:03:32.046608 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.046648 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.046680 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.048585 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.050974 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.056592 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.056859 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.059547 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:32.063283 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.063339 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.063375 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.063407 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.063469 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.064038 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.064117 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.064471 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.065228 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.067752 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.068370 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.068447 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.068483 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.068543 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.068669 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.068994 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.069037 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.070920 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.071012 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.073540 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.073620 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.074060 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.076293 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.078181 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.078276 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.078564 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.078650 140046437961728 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:03:32.078762 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.078801 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.078832 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.080726 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.083128 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.089047 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.089309 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.091985 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:32.095664 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.095719 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.095755 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.095786 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.095847 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.096406 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.096481 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.096837 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.097667 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.100150 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.100764 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.100841 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.100876 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.100934 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.101059 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.101376 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.101419 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.103316 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.103408 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.105934 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.106017 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.106447 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.108687 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.110635 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.110730 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.111017 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.111104 140046437961728 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:03:32.111214 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.111253 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.111284 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.113116 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.115660 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.121315 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.121579 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.124263 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:32.127964 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.128018 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.128054 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.128086 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.128189 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.128747 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.128823 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.129176 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.129953 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.132397 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.133015 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.133095 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.133131 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.133190 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.133317 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.133632 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.133682 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.135637 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.135730 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.138453 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.138533 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.138960 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.141249 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.143149 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.143244 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.143536 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.143618 140046437961728 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:03:32.143735 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.143776 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.143807 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.145649 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.148086 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.153690 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.153947 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.156571 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:03:32.160678 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.160734 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.160771 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.160803 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.160864 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.161426 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.161501 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.161863 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.162627 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.165102 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.165726 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.165804 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.165840 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.165899 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.166023 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.166351 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.166395 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.168340 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.168433 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.170921 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.171001 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.171442 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.173738 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.175638 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.175737 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.176028 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.176305 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176375 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176441 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176498 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176551 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176604 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176657 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176709 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176761 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176812 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176864 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176915 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 17:03:32.176953 140046437961728 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:03:32.180438 140046437961728 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:03:32.228286 140046437961728 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.228376 140046437961728 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:03:32.228430 140046437961728 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:03:32.228534 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.228572 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.228603 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.228667 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.231115 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.236645 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.236906 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.239579 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.256208 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.256265 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.256301 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.256331 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.256394 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.257537 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.257615 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.258340 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.260365 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.265187 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.266595 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.266686 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.266724 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.266788 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.266922 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.267041 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.267083 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.269031 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.269126 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.271580 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.271661 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.271775 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.274042 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.275990 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.276087 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.276381 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.276463 140046437961728 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:03:32.276573 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.276613 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.276645 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.276712 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.278982 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.284505 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.284768 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.287491 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.300620 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.300677 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.300713 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.300745 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.300806 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.301365 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.301441 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.301809 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.302511 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.305002 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.305613 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.305699 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.305740 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.305801 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.305933 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.306042 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.306081 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.308022 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.308116 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.310557 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.310638 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.310746 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.312974 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.314913 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.315010 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.315299 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.315381 140046437961728 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:03:32.315490 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.315530 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.315561 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.315624 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.318025 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.323518 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.323778 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.326492 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.339348 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.339404 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.339441 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.339473 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.339536 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.340094 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.340174 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.340530 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.341229 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.343742 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.344361 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.344437 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.344472 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.344536 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.344667 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.344779 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.344818 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.346779 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.346873 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.349309 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.349388 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.349495 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.351729 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.353655 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.353751 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.354041 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.354122 140046437961728 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:03:32.354231 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.354270 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.354301 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.354366 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.356626 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.362106 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.362367 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.365074 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.377779 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.377835 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.377871 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.377902 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.377969 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.378519 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.378595 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.378954 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.379653 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.386542 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.387276 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.387358 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.387393 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.387468 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.387614 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.387740 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.387779 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.390146 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.390241 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.392733 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.392812 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.392922 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.395187 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.397087 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.397181 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.397469 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.397553 140046437961728 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:03:32.397674 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.397717 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.397749 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.397819 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.400156 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.405622 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.405898 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.408537 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.421397 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.421454 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.421491 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.421523 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.421585 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.422149 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.422229 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.422595 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.423284 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.425845 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.426471 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.426551 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.426587 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.426647 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.426784 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.426892 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.426931 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.428954 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.429047 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.431587 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.431668 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.431777 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.434072 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.435939 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.436033 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.436316 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.436396 140046437961728 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:03:32.436504 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.436542 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.436574 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.436638 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.438887 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.444322 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.444578 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.447272 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.459973 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.460030 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.460066 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.460097 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.460159 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.460718 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.460795 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.461153 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.461844 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.464325 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.464944 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.465020 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.465054 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.465118 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.465245 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.465359 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.465398 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.467350 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.467444 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.469854 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.469935 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.470044 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.472270 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.474130 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.474225 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.474509 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.474590 140046437961728 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:03:32.474699 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.474739 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.474771 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.474836 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.477068 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.482606 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.482862 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.485438 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.498456 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.498513 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.498549 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.498580 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.498642 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.499196 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.499273 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.499630 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.500330 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.502838 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.503503 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.503581 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.503616 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.503677 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.503807 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.503915 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.503959 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.505869 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.505964 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.508359 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.508437 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.508545 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.510757 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.512688 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.512783 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.513072 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.513154 140046437961728 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:03:32.513262 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.513302 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.513333 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.513397 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.515641 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.521085 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.521358 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.524062 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.536763 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.536819 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.536855 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.536886 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.536952 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.537544 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.537620 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.537985 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.538673 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.541157 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.541783 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.541859 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.541894 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.541954 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.542085 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.542194 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.542240 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.544137 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.544231 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.546685 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.546769 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.546878 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.549099 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.550987 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.551083 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.551367 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.551448 140046437961728 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:03:32.551556 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.551596 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.551627 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.551691 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.553924 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.559442 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.559701 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.562350 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.575163 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.575219 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.575256 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.575287 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.575351 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.575909 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.575984 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.576344 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.577031 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.579530 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.580194 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.580272 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.580307 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.580367 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.580497 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.580605 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.580644 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.582545 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.582640 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.585011 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.585090 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.585196 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.587413 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.589336 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.589431 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.589723 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.589805 140046437961728 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:03:32.589913 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.589951 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.589983 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.590045 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.592277 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.597727 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.597983 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.600943 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.613473 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.613530 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.613565 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.613596 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.613670 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.614274 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.614351 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.614703 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.615389 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.617864 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.618483 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.618560 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.618595 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.618655 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.618782 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.618890 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.618929 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.620806 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.620906 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.623354 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.623434 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.623541 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.625736 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.627656 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.627755 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.628049 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.628129 140046437961728 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:03:32.628239 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.628277 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.628308 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.628372 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.630599 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.636085 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.636345 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.638968 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.651632 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.651687 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.651722 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.651753 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.651814 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.652369 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.652445 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.652812 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.653515 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.656050 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.656712 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.656788 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.656824 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.656883 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.657012 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.657123 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.657162 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.659067 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.659166 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.661586 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.661673 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.661781 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.664003 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.665936 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.666033 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.666321 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.666403 140046437961728 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:03:32.666511 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.666551 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.666583 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.666647 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.668895 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.674393 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.674653 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.677285 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.689971 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.690027 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.690062 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.690094 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.690156 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.690718 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.690793 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.691155 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.691851 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.694414 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.695029 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.695105 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.695140 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.695199 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.695332 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.695443 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.695483 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.697362 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.697455 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.699887 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.699968 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.700075 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.702685 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.704560 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.704655 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.704947 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.705036 140046437961728 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:03:32.707915 140046437961728 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:03:32.763246 140046437961728 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.763333 140046437961728 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:03:32.763387 140046437961728 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:03:32.763492 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.763531 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.763561 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.763627 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.765978 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.771341 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.771602 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.774175 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.786448 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.786504 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.786539 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.786570 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.786632 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.787188 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.787264 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.787618 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.788293 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.790820 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.791428 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.791505 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.791541 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.791599 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.791726 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.791841 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.791880 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.793729 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.793823 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.796204 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.796283 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.796391 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.798622 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.800450 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.800545 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.800828 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.800910 140046437961728 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:03:32.801017 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.801055 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.801087 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.801151 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.803372 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.808689 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.808953 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.811602 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.823765 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.823822 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.823858 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.823891 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.823953 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.824500 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.824576 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.824930 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.825613 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.828137 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.828745 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.828822 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.828859 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.828919 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.829047 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.829155 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.829200 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.831057 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.831150 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.833531 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.833611 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.833737 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.835970 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.837800 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.837895 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.838183 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.838265 140046437961728 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:03:32.838374 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.838413 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.838444 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.838509 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.840720 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.846110 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.846370 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.849030 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.861332 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.861390 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.861428 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.861460 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.861525 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.862110 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.862191 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.862564 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.863267 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.866265 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.866904 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.866986 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.867023 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.867086 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.867220 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.867332 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.867373 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.869315 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.869412 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.871894 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.871975 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.872087 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.874381 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.876275 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.876369 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.876657 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.876738 140046437961728 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:03:32.876846 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.876884 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.876915 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.876980 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.879275 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.884736 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.884994 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.887751 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.900291 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.900346 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.900383 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.900430 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.900495 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.901053 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.901128 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.901496 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.902202 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.904762 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.905371 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.905445 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.905479 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.905538 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.905670 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.905778 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.905820 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.907733 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.907825 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.910243 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.910325 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.910434 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.912719 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.914604 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.914702 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.914995 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.915075 140046437961728 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:03:32.915182 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.915219 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.915329 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.915391 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.917667 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.923158 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.923417 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.926103 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.938772 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.938828 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.938863 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.938894 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.938958 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.939508 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.939582 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.939938 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.940630 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.943181 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.943794 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.943868 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.943901 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.943959 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.944084 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.944190 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.944228 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.946105 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.946203 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.948600 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.948679 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.948787 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.951057 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.952914 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.953008 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.953292 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.953372 140046437961728 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:03:32.953477 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.953515 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.953546 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.953608 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.955842 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.961230 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.961487 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:32.964166 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:32.976614 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:32.976668 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:32.976702 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:32.976733 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.976793 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.977339 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.977413 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.977782 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.978461 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.981423 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.982043 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.982120 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:32.982154 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:32.982212 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.982337 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:32.982444 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:32.982481 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.984336 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.984433 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.986829 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.986907 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:32.987015 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:32.989268 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:32.991112 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.991207 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:32.991488 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.991567 140046437961728 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:03:32.991672 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:32.991709 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:32.991739 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:32.991801 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.994004 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:32.999392 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:32.999646 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:33.002304 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:33.014761 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:33.014815 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:33.014849 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:33.014879 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.014941 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.015501 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.015579 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.015934 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.016619 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.019296 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.019915 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.019990 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:33.020024 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:33.020083 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.020209 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:33.020315 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:33.020352 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.022229 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.022322 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.024713 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.024791 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:33.024902 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:33.027178 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.029017 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.029110 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.029398 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.029479 140046437961728 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:03:33.029585 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:33.029622 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:33.029659 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:33.029725 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.031958 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:33.037379 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.037633 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:33.040304 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:33.052755 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:33.052809 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:33.052843 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:33.052872 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.052933 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.053486 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.053560 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.053925 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.054605 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.057108 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.057746 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.057823 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:33.057857 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:33.057915 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.058040 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:33.058150 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:33.058187 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.060048 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.060139 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.062535 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.062619 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:33.062726 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:33.064987 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.066822 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.066915 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.067199 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.067279 140046437961728 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:03:33.067385 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:33.067421 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:33.067451 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:33.067512 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.069727 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:33.075114 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.075373 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:33.078054 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:33.090541 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:33.090594 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:33.090629 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:33.090659 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.090720 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.091277 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.091351 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.091704 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.092387 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.095306 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.095924 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.095999 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:33.096033 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:33.096090 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.096234 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:33.096342 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:33.096379 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.098235 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.098328 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.100810 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.100893 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:33.101001 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:33.103285 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.105134 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.105226 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.105508 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.105588 140046437961728 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:03:33.105701 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:33.105740 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:33.105769 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:33.105832 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.108061 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:33.113420 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.113683 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:33.116365 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:33.128841 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:33.128895 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:33.128931 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:33.128962 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.129029 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.129582 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.129667 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.130025 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.130700 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.133226 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.133857 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.133936 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:33.133968 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:33.134026 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.134149 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:33.134255 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:33.134292 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.136646 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.136740 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.139122 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.139200 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:33.139314 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:33.141526 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.143345 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.143437 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.143716 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.143795 140046437961728 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:03:33.143899 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:33.143935 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:33.143964 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:33.144026 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.146236 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:33.151558 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.151812 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:33.154465 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:33.166830 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:33.166884 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:33.166919 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:33.166948 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.167010 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.167563 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.167638 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.167993 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.168674 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.171214 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.171840 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.171916 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:33.171950 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:33.172007 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.172132 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:33.172239 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:33.172276 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.174149 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.174239 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.176617 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.176694 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:33.176800 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:33.179077 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.180918 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.181011 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.181296 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.181376 140046437961728 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:03:33.181482 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:03:33.181519 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:03:33.181549 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:03:33.181612 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.183823 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:03:33.189193 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.189453 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:03:33.192122 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:03:33.204646 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:03:33.204701 140046437961728 attention.py:418] Single window, no scan.
I0123 17:03:33.204735 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:03:33.204765 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.204826 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.205374 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.205448 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.205806 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.206495 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.209374 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.209995 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.210071 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:03:33.210106 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:03:33.210162 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.210289 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:03:33.210396 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:03:33.210433 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.212291 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.212381 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.214759 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.214837 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:03:33.214943 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:03:33.217194 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:03:33.219066 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.219159 140046437961728 nn_components.py:261] mlp: residual
I0123 17:03:33.219443 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:33.219527 140046437961728 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:03:33.222329 140046437961728 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:03:37.600183 140046437961728 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 17:03:38.131182 140046437961728 training_loop.py:409] No working directory specified.
I0123 17:03:38.131302 140046437961728 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 17:03:38.132072 140046437961728 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 17:03:41.475467 140046437961728 training_loop.py:447] Only restoring trainable parameters.
I0123 17:03:41.476062 140046437961728 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 17:03:41.476135 140046437961728 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.476184 140046437961728 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.476226 140046437961728 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.476267 140046437961728 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.476307 140046437961728 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.476346 140046437961728 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.476386 140046437961728 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.476425 140046437961728 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.476464 140046437961728 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.476501 140046437961728 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.476538 140046437961728 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.476574 140046437961728 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.476611 140046437961728 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.476647 140046437961728 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.476682 140046437961728 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.476717 140046437961728 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.476753 140046437961728 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.476789 140046437961728 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.476825 140046437961728 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.476873 140046437961728 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.476911 140046437961728 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.476948 140046437961728 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.476984 140046437961728 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.477019 140046437961728 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477055 140046437961728 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.477090 140046437961728 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477127 140046437961728 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477163 140046437961728 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.477199 140046437961728 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.477237 140046437961728 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477274 140046437961728 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.477310 140046437961728 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.477345 140046437961728 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.477380 140046437961728 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477416 140046437961728 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.477451 140046437961728 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477485 140046437961728 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477520 140046437961728 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.477555 140046437961728 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.477590 140046437961728 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477624 140046437961728 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.477671 140046437961728 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.477708 140046437961728 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.477744 140046437961728 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477780 140046437961728 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.477821 140046437961728 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477858 140046437961728 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.477893 140046437961728 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.477928 140046437961728 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.477965 140046437961728 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478002 140046437961728 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.478038 140046437961728 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.478075 140046437961728 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.478111 140046437961728 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478147 140046437961728 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.478182 140046437961728 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478218 140046437961728 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478253 140046437961728 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.478287 140046437961728 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.478321 140046437961728 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478355 140046437961728 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.478390 140046437961728 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.478424 140046437961728 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.478459 140046437961728 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478493 140046437961728 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.478528 140046437961728 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478564 140046437961728 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478599 140046437961728 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.478633 140046437961728 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.478668 140046437961728 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478703 140046437961728 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.478737 140046437961728 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.478777 140046437961728 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.478813 140046437961728 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478849 140046437961728 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.478884 140046437961728 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478920 140046437961728 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.478955 140046437961728 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.478991 140046437961728 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.479027 140046437961728 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479062 140046437961728 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.479097 140046437961728 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.479132 140046437961728 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.479166 140046437961728 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479200 140046437961728 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.479235 140046437961728 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479270 140046437961728 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479303 140046437961728 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.479338 140046437961728 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.479372 140046437961728 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479407 140046437961728 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.479441 140046437961728 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.479476 140046437961728 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.479511 140046437961728 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479545 140046437961728 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.479580 140046437961728 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479615 140046437961728 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479650 140046437961728 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.479685 140046437961728 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.479724 140046437961728 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479760 140046437961728 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.479794 140046437961728 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.479829 140046437961728 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.479864 140046437961728 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479898 140046437961728 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.479934 140046437961728 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.479969 140046437961728 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.480005 140046437961728 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.480041 140046437961728 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.480077 140046437961728 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.480113 140046437961728 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.480149 140046437961728 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 17:03:41.480183 140046437961728 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 17:03:41.480218 140046437961728 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.480252 140046437961728 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.480287 140046437961728 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.480322 140046437961728 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.480357 140046437961728 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 17:03:41.480391 140046437961728 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 17:03:41.480425 140046437961728 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 17:03:41.480459 140046437961728 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 17:03:41.480487 140046437961728 training_loop.py:725] Total parameters: 152072288
I0123 17:03:41.480690 140046437961728 training_loop.py:739] Total state size: 0
I0123 17:03:41.503536 140046437961728 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 17:03:41.503756 140046437961728 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 17:03:41.504252 140046437961728 training_loop.py:652] Compiling mode beam_search with jit.
I0123 17:03:41.504564 140046437961728 training_loop.py:89] registering functions: dict_keys([])
I0123 17:03:41.520576 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l ? para e f m n
I0123 17:03:43.363318 140046437961728 ddar.py:60] Depth 1/1000 time = 1.6883327960968018
I0123 17:03:48.238489 140046437961728 ddar.py:60] Depth 2/1000 time = 4.874988317489624
I0123 17:03:55.533902 140046437961728 ddar.py:60] Depth 3/1000 time = 7.295197486877441
I0123 17:04:02.741123 140046437961728 ddar.py:60] Depth 4/1000 time = 7.206980228424072
I0123 17:04:11.640485 140046437961728 ddar.py:60] Depth 5/1000 time = 8.899120092391968
I0123 17:04:27.258825 140046437961728 ddar.py:60] Depth 6/1000 time = 15.234807968139648
I0123 17:04:42.723369 140046437961728 ddar.py:60] Depth 7/1000 time = 15.464301824569702
I0123 17:05:03.263916 140046437961728 ddar.py:60] Depth 8/1000 time = 20.540311098098755
I0123 17:05:29.800700 140046437961728 ddar.py:60] Depth 9/1000 time = 26.53648352622986
I0123 17:05:58.510134 140046437961728 ddar.py:60] Depth 10/1000 time = 28.708826780319214
I0123 17:06:29.631705 140046437961728 ddar.py:60] Depth 11/1000 time = 31.121262073516846
I0123 17:07:01.128237 140046437961728 ddar.py:60] Depth 12/1000 time = 31.467024326324463
I0123 17:07:33.944693 140046437961728 ddar.py:60] Depth 13/1000 time = 32.45895552635193
I0123 17:08:13.744446 140046437961728 ddar.py:60] Depth 14/1000 time = 39.79935073852539
I0123 17:08:53.874010 140046437961728 ddar.py:60] Depth 15/1000 time = 40.10607981681824
I0123 17:09:35.433284 140046437961728 ddar.py:60] Depth 16/1000 time = 41.45770716667175
I0123 17:09:35.433613 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:09:35.433709 140046437961728 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 17:09:35.433746 140046437961728 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a b d 00 ; e : C c d e 01 ; f : ^ a b a e a f a c 02 ^ c a c f c e c b 03 ; g : D a g b g 04 D b g e g 05 ; h : D a h c h 06 D a h e h 07 ; i : D b i c i 08 D b i e i 09 ; j : D a j b j 10 D b j f j 11 ; k : D a k c k 12 D a k f k 13 ; l : D b l c l 14 D b l f l 15 ; m : D g m h m 16 D h m i m 17 ; n : D j n k n 18 D k n l n 19 ? P e f m n {F1} x00
I0123 17:09:35.433778 140046437961728 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : C a b d 00 ; e : C c d e 01 ; f : ^ a b a e a f a c 02 ^ c a c f c e c b 03 ; g : D a g b g 04 D b g e g 05 ; h : D a h c h 06 D a h e h 07 ; i : D b i c i 08 D b i e i 09 ; j : D a j b j 10 D b j f j 11 ; k : D a k c k 12 D a k f k 13 ; l : D b l c l 14 D b l f l 15 ; m : D g m h m 16 D h m i m 17 ; n : D j n k n 18 D k n l n 19 ? P e f m n {F1} x00
I0123 17:09:35.577401 140046437961728 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.577589 140046437961728 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 17:09:35.577694 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.577770 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.577839 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.577906 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.577970 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.578036 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.578100 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.578164 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.578227 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.578302 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.578367 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.578431 140046437961728 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 17:09:35.578469 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.578513 140046437961728 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:09:35.578619 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.578655 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.578683 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.580885 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.583304 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.588858 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.589121 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.591672 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.595430 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.595484 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.595520 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.595553 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.595613 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.596268 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.596342 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.596695 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.597444 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.599925 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.600540 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.600615 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.600648 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.600707 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.600831 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.601152 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.601193 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.603139 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.603231 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.605695 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.605776 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.606194 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.608479 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.610385 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.610477 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.610760 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.610837 140046437961728 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:09:35.610940 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.610977 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.611006 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.612839 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.615097 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.620557 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.620808 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.623387 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.626950 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.627003 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.627036 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.627066 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.627126 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.627673 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.627747 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.628098 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.628837 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.631259 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.631917 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.631992 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.632025 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.632080 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.632205 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.632514 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.632555 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.634455 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.634547 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.636962 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.637039 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.637456 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.639732 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.641605 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.641708 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.641996 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.642075 140046437961728 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:09:35.642180 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.642217 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.642247 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.644009 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.646290 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.651845 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.652096 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.654627 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.658157 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.658210 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.658244 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.658273 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.658334 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.658927 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.659000 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.659347 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.660086 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.662502 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.663103 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.663177 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.663211 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.663269 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.663392 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.663701 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.663742 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.665679 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.665770 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.668177 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.668256 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.668669 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.670900 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.672776 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.672874 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.673163 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.673241 140046437961728 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:09:35.673347 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.673384 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.673413 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.675271 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.677550 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.683066 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.683317 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.685853 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.689813 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.689867 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.689901 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.689932 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.689993 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.690543 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.690621 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.690978 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.691737 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.694178 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.694784 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.694859 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.694892 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.694949 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.695073 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.695432 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.695474 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.697369 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.697460 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.699894 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.699973 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.700391 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.702611 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.704539 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.704631 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.704917 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.705001 140046437961728 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:09:35.705108 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.705146 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.705175 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.706928 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.709192 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.714789 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.715042 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.717592 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.721170 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.721225 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.721259 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.721288 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.721349 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.721956 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.722031 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.722384 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.723135 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.725562 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.726181 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.726256 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.726289 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.726346 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.726470 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.726784 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.726825 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.728747 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.728837 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.731250 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.731329 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.731747 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.733953 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.735822 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.735913 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.736196 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.736283 140046437961728 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:09:35.736392 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.736429 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.736458 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.738293 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.740547 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.746017 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.746268 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.748779 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.752393 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.752446 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.752480 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.752511 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.752572 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.753114 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.753189 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.753538 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.754299 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.756689 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.757292 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.757366 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.757399 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.757456 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.757597 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.757965 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.758007 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.759875 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.759964 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.762374 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.762453 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.762868 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.765079 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.767030 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.767122 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.767403 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.767481 140046437961728 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:09:35.767592 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.767629 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.767658 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.769405 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.771654 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.777184 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.777433 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.779952 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.783501 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.783554 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.783589 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.783619 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.783679 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.784274 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.784348 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.784696 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.785436 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.787843 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.788444 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.788518 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.788552 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.788608 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.788731 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.789038 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.789078 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.791327 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.791420 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.793843 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.793921 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.794333 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.796522 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.798394 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.798486 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.798769 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.798846 140046437961728 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:09:35.798950 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.798993 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.799024 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.800843 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.803107 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.808540 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.808790 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.811287 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.814889 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.814942 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.814976 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.815005 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.815066 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.815610 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.815684 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.816030 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.816766 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.819180 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.819783 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.819858 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.819891 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.819947 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.820072 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.820432 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.820473 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.822341 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.822431 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.824807 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.824883 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.825295 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.827497 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.829439 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.829531 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.829826 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.829905 140046437961728 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:09:35.830010 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.830046 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.830081 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.831820 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.834087 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.839597 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.839847 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.842360 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.845913 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.845966 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.846000 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.846030 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.846090 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.846695 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.846768 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.847121 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.847872 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.850306 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.850914 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.850989 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.851022 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.851079 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.851204 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.851515 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.851555 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.853433 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.853523 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.855993 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.856071 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.856484 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.858697 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.860623 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.860715 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.861002 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.861081 140046437961728 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:09:35.861185 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.861221 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.861251 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.863056 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.865433 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.870972 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.871236 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.873829 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.877393 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.877445 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.877479 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.877508 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.877568 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.878188 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.878267 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.878637 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.879419 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.881851 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.882479 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.882557 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.882590 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.882648 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.882778 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.883103 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.883145 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.885025 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.885115 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.887651 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.887729 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.888146 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.890403 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.892357 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.892448 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.892731 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.892809 140046437961728 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:09:35.892915 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.892951 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.892981 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.894782 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.897172 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.902692 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.902954 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.905506 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.909103 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.909156 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.909189 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.909219 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.909280 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.910264 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.910343 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.910711 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.911493 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.913953 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.914560 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.914635 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.914669 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.914726 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.914851 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.915163 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.915203 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.917077 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.917166 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.919738 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.919817 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.920231 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.922473 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.924343 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.924434 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.924723 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.924801 140046437961728 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:09:35.924906 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:35.924944 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:35.924973 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:35.926729 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.929095 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:35.934589 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.934842 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:35.937374 140046437961728 transformer_layer.py:213] tlayer: windowed attention.
I0123 17:09:35.940946 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:35.940999 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:35.941032 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:35.941062 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.941123 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.941731 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.941808 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.942157 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.942899 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.945320 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.945930 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.946005 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:35.946039 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:35.946096 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.946219 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:35.946528 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:35.946569 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.948426 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.948514 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.950972 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.951050 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:35.951467 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:35.953698 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:35.955574 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.955665 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:35.955949 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:35.956187 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956254 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956308 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956360 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956413 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956465 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956522 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956574 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956625 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956675 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956725 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956774 140046437961728 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 17:09:35.956809 140046437961728 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:09:35.959662 140046437961728 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 17:09:36.003631 140046437961728 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.003714 140046437961728 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:09:36.003766 140046437961728 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:09:36.003866 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.003902 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.003932 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.003992 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.006314 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.011632 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.011892 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.014428 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.027013 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.027067 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.027102 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.027132 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.027194 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.027751 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.027828 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.028194 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.028882 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.031404 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.032021 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.032097 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.032130 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.032190 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.032317 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.032431 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.032469 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.034292 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.034383 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.036748 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.036825 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.036930 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.039155 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.040959 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.041050 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.041336 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.041414 140046437961728 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:09:36.041520 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.041557 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.041586 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.041653 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.043840 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.049131 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.049390 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.052033 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.064260 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.064313 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.064348 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.064378 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.064439 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.064988 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.065063 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.065413 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.066153 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.068565 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.069168 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.069243 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.069277 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.069334 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.069460 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.069564 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.069608 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.071449 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.071539 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.073911 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.073990 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.074096 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.076318 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.078166 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.078259 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.078549 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.078628 140046437961728 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:09:36.078734 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.078771 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.078801 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.078863 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.081075 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.086410 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.086667 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.089658 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.282001 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.282127 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.282169 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.282201 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.282284 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.282911 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.282990 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.283360 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.284065 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.286615 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.287248 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.287324 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.287358 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.287421 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.287550 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.287662 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.287699 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.289775 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.289868 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.292333 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.292411 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.292520 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.294755 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.296602 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.296695 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.296983 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.297065 140046437961728 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:09:36.297171 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.297208 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.297238 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.297303 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.299607 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.304908 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.305166 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.307739 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.320264 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.320318 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.320352 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.320382 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.320442 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.320994 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.321067 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.321414 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.322093 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.324611 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.325225 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.325300 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.325334 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.325392 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.325518 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.325625 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.325670 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.327502 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.327601 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.329966 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.330044 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.330151 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.332378 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.334225 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.334319 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.334606 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.334686 140046437961728 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:09:36.334792 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.334829 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.334858 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.334919 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.337124 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.342538 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.342794 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.345354 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.357709 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.357763 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.357798 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.357827 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.357890 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.358439 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.358515 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.358867 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.359534 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.362028 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.362631 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.362706 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.362741 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.362798 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.362923 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.363029 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.363065 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.364914 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.365014 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.367393 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.367472 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.367579 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.370187 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.372036 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.372128 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.372415 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.372494 140046437961728 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:09:36.372600 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.372638 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.372669 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.372732 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.374970 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.380385 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.380641 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.383219 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.395485 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.395539 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.395572 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.395602 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.395663 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.396212 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.396287 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.396636 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.397303 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.399790 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.400391 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.400466 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.400499 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.400555 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.400679 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.400783 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.400820 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.402649 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.402741 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.405114 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.405190 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.405295 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.407523 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.409345 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.409436 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.409729 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.409809 140046437961728 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:09:36.409915 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.409951 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.409981 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.410043 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.412235 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.417623 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.417886 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.420402 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.432595 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.432650 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.432683 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.432713 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.432774 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.433320 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.433395 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.433754 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.434415 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.436879 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.437486 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.437561 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.437594 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.437660 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.437790 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.437896 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.437932 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.439749 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.439840 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.442199 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.442283 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.442391 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.444608 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.446438 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.446531 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.446818 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.446897 140046437961728 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:09:36.447004 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.447041 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.447072 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.447134 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.449338 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.454720 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.455005 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.457542 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.470181 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.470235 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.470268 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.470298 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.470358 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.470906 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.470980 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.471333 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.472003 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.474484 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.475090 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.475164 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.475198 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.475254 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.475378 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.475484 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.475521 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.477355 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.477445 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.479824 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.479908 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.480016 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.482255 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.484090 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.484184 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.484472 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.484552 140046437961728 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:09:36.484661 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.484699 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.484728 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.484790 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.487005 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.492408 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.492661 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.495213 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.507420 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.507474 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.507508 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.507538 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.507600 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.508146 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.508221 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.508572 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.509241 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.511708 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.512313 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.512387 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.512421 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.512477 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.512603 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.512709 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.512746 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.514573 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.514665 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.517024 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.517102 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.517217 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.519449 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.521287 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.521379 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.521672 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.521753 140046437961728 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:09:36.521861 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.521898 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.521928 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.521991 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.524191 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.529583 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.529847 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.532406 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.544674 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.544728 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.544764 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.544794 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.544856 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.545408 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.545483 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.545848 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.546518 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.549308 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.549924 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.550000 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.550033 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.550091 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.550215 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.550321 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.550358 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.552183 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.552272 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.554643 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.554721 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.554830 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.557053 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.558892 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.558987 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.559278 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.559358 140046437961728 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:09:36.559465 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.559503 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.559534 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.559597 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.561812 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.567311 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.567577 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.570126 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.582368 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.582422 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.582457 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.582488 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.582549 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.583092 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.583166 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.583516 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.584180 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.586648 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.587246 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.587320 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.587353 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.587410 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.587534 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.587640 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.587677 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.589484 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.589574 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.591927 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.592005 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.592112 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.594316 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.596126 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.596218 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.596506 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.596586 140046437961728 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:09:36.596693 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.596729 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.596759 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.596821 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.599014 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.604383 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.604635 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.607199 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.619379 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.619433 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.619467 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.619498 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.619558 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.620099 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.620172 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.620521 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.621184 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.623658 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.624264 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.624339 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.624372 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.624429 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.624552 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.624658 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.624695 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.626558 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.626653 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.629040 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.629117 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.629223 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.631453 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.633283 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.633375 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.633672 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.633760 140046437961728 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:09:36.636562 140046437961728 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 17:09:36.686870 140046437961728 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.686955 140046437961728 decoder_stack.py:333] dstack: autoregressive generator.
I0123 17:09:36.687008 140046437961728 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 17:09:36.687108 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.687144 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.687173 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.687235 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.689551 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.694858 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.695109 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.697646 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.710242 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.710296 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.710330 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.710361 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.710422 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.710974 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.711048 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.711398 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.712066 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.714550 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.715153 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.715228 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.715262 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.715319 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.715444 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.715550 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.715587 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.717402 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.717492 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.719889 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.719968 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.720074 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.722279 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.724089 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.724181 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.724468 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.724547 140046437961728 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 17:09:36.724653 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.724691 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.724721 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.724784 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.726997 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.732290 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.732544 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.735177 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.747313 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.747367 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.747401 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.747430 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.747491 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.748096 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.748171 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.748526 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.749194 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.751594 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.752202 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.752277 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.752311 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.752369 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.752496 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.752602 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.752640 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.754460 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.754552 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.756979 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.757063 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.757173 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.759329 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.761153 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.761245 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.761531 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.761610 140046437961728 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 17:09:36.761721 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.761759 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.761789 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.761850 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.764035 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.769420 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.769686 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.772226 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.784416 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.784470 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.784504 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.784533 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.784594 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.785140 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.785213 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.785564 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.786243 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.788672 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.789334 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.789409 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.789443 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.789500 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.789623 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.789736 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.789774 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.791591 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.791682 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.794051 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.794134 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.794244 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.796390 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.798292 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.798386 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.798677 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.798756 140046437961728 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 17:09:36.798860 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.798897 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.798926 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.798987 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.801179 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.806591 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.806848 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.809386 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.821929 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.821983 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.822018 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.822047 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.822108 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.822647 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.822721 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.823070 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.823737 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.826196 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.826792 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.826866 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.826899 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.826957 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.827080 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.827185 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.827222 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.829021 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.829111 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.831473 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.831550 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.831663 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.833886 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.835697 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.835789 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.836076 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.836156 140046437961728 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 17:09:36.836262 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.836298 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.836328 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.836391 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.838599 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.843877 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.844133 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.846744 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.858767 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.858821 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.858855 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.858886 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.858946 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.859641 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.859735 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.860208 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.861017 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.863730 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.864330 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.864405 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.864439 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.864496 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.864622 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.864727 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.864764 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.866621 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.866717 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.869153 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.869230 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.869337 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.871572 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.873384 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.873476 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.873774 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.873854 140046437961728 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 17:09:36.873957 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.873994 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.874023 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.874083 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.876251 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.881597 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.881862 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.884377 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.896484 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.896538 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.896572 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.896602 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.896663 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.897205 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.897278 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.897623 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.898312 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.900771 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.901428 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.901505 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.901539 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.901597 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.901736 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.901841 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.901879 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.903691 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.903784 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.906136 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.906214 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.906319 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.908464 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.910731 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.910825 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.911117 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.911196 140046437961728 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 17:09:36.911301 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.911339 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.911368 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.911430 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.913630 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.918909 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.919167 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.921695 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.933754 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.933807 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.933841 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.933871 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.933932 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.934477 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.934551 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.934902 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.935562 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.938034 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.938635 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.938710 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.938744 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.938801 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.938926 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.939030 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.939067 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.940885 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.940975 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.943333 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.943412 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.943517 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.945712 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.947535 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.947628 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.947913 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.947992 140046437961728 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 17:09:36.948096 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.948133 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.948163 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.948225 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.950419 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.955651 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.955909 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.958509 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:36.970733 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:36.970787 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:36.970821 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:36.970852 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.970912 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.971458 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.971533 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.971884 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.972547 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.975002 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.975604 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.975678 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:36.975712 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:36.975768 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.975891 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:36.975996 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:36.976033 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.977845 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.977936 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.980274 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.980349 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:36.980456 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:36.982683 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:36.984486 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.984583 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:36.984872 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.984950 140046437961728 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 17:09:36.985056 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:36.985093 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:36.985123 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:36.985184 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.987357 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:36.992564 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:36.992817 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:36.995404 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:37.007421 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:37.007475 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:37.007509 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:37.007538 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.007596 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.008133 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.008204 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.008551 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.009206 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.011682 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.012282 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.012355 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:37.012387 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:37.012443 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.012565 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:37.012668 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:37.012705 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:37.014502 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.014591 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:37.016917 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.016993 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:37.017098 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:37.019713 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:37.021526 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.021622 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:37.021925 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.022003 140046437961728 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 17:09:37.022109 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:37.022146 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:37.022175 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:37.022236 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.024411 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:37.029670 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.029925 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:37.032538 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:37.044627 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:37.044680 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:37.044715 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:37.044745 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.044804 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.045341 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.045413 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.045767 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.046431 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.048878 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.049474 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.049547 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:37.049581 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:37.049638 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.049768 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:37.049872 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:37.049909 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:37.051715 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.051803 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:37.054159 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.054235 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:37.054341 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:37.056541 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:37.058366 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.058457 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:37.058750 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.058828 140046437961728 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 17:09:37.058932 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:37.058969 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:37.058998 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:37.059059 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.061242 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:37.066508 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.066761 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:37.069339 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:37.081406 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:37.081458 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:37.081492 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:37.081522 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.081581 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.082128 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.082202 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.082561 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.083226 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.085705 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.086309 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.086382 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:37.086416 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:37.086473 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.086596 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:37.086701 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:37.086738 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:37.088558 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.088646 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:37.091000 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.091077 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:37.091186 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:37.093403 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:37.095218 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.095309 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:37.095593 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.095677 140046437961728 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 17:09:37.095785 140046437961728 transformer_layer.py:154] tlayer: recurrent = False
I0123 17:09:37.095823 140046437961728 transformer_layer.py:155] tlayer: compute_importance = False
I0123 17:09:37.095853 140046437961728 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 17:09:37.095916 140046437961728 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.098109 140046437961728 transformer_base.py:161] kvq: pre_attn dropout.
I0123 17:09:37.103399 140046437961728 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.103651 140046437961728 transformer_base.py:194] kvq: normalize keys, queries.
I0123 17:09:37.106255 140046437961728 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 17:09:37.118322 140046437961728 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 17:09:37.118376 140046437961728 attention.py:418] Single window, no scan.
I0123 17:09:37.118411 140046437961728 transformer_layer.py:389] tlayer: self-attention.
I0123 17:09:37.118441 140046437961728 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.118500 140046437961728 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.119046 140046437961728 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.119119 140046437961728 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.119467 140046437961728 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.120131 140046437961728 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.122619 140046437961728 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.123220 140046437961728 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.123293 140046437961728 transformer_layer.py:468] tlayer: End windows.
I0123 17:09:37.123327 140046437961728 transformer_layer.py:472] tlayer: final FFN.
I0123 17:09:37.123383 140046437961728 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.123505 140046437961728 transformer_base.py:410] tbase: post-attention MLP.
I0123 17:09:37.123610 140046437961728 nn_components.py:325] mlp: activation = None
I0123 17:09:37.123647 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:37.125455 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.125542 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:37.127893 140046437961728 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.127969 140046437961728 transformer_base.py:443] tbase: final FFN
I0123 17:09:37.128075 140046437961728 nn_components.py:320] mlp: hidden 4096, relu
I0123 17:09:37.130706 140046437961728 nn_components.py:329] mlp: final activation = None
I0123 17:09:37.132513 140046437961728 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.132602 140046437961728 nn_components.py:261] mlp: residual
I0123 17:09:37.132887 140046437961728 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:37.132974 140046437961728 decoder_stack.py:344] dstack: Final layernorm.
I0123 17:09:37.135769 140046437961728 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 17:09:50.121242 140046437961728 alphageometry.py:566] LM output (score=-2.220050): "o : C l n o 20 D l n n o 21 ;"
I0123 17:09:50.121392 140046437961728 alphageometry.py:567] Translation: "o = on_line o l n, on_circle o n l"

I0123 17:09:50.121440 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_line o l n, on_circle o n l ? para e f m n"
I0123 17:09:50.121596 140046437961728 graph.py:498] 
I0123 17:09:50.121654 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_line o l n, on_circle o n l ? para e f m n
I0123 17:09:52.839701 140046437961728 ddar.py:60] Depth 1/1000 time = 2.553419351577759
I0123 17:09:58.708681 140046437961728 ddar.py:60] Depth 2/1000 time = 5.8687989711761475
I0123 17:10:07.047045 140046437961728 ddar.py:60] Depth 3/1000 time = 8.338165283203125
I0123 17:10:15.330613 140046437961728 ddar.py:60] Depth 4/1000 time = 8.283378601074219
I0123 17:10:25.335902 140046437961728 ddar.py:60] Depth 5/1000 time = 10.005094528198242
I0123 17:10:35.368407 140046437961728 ddar.py:60] Depth 6/1000 time = 10.032089948654175
I0123 17:10:52.409727 140046437961728 ddar.py:60] Depth 7/1000 time = 16.649707555770874
I0123 17:11:09.272229 140046437961728 ddar.py:60] Depth 8/1000 time = 16.86229634284973
I0123 17:11:31.985445 140046437961728 ddar.py:60] Depth 9/1000 time = 22.712975025177002
I0123 17:12:01.610089 140046437961728 ddar.py:60] Depth 10/1000 time = 29.624397039413452
I0123 17:12:32.285545 140046437961728 ddar.py:60] Depth 11/1000 time = 30.67517852783203
I0123 17:13:05.530468 140046437961728 ddar.py:60] Depth 12/1000 time = 33.24462866783142
I0123 17:13:39.139912 140046437961728 ddar.py:60] Depth 13/1000 time = 33.58048725128174
I0123 17:14:14.544638 140046437961728 ddar.py:60] Depth 14/1000 time = 35.03807258605957
I0123 17:14:56.259267 140046437961728 ddar.py:60] Depth 15/1000 time = 41.71416735649109
I0123 17:15:38.824031 140046437961728 ddar.py:60] Depth 16/1000 time = 42.5331711769104
I0123 17:16:22.405226 140046437961728 ddar.py:60] Depth 17/1000 time = 43.47009587287903
I0123 17:16:22.405646 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:16:22.405751 140046437961728 alphageometry.py:566] LM output (score=-2.256182): "o : C b f o 20 D b o f o 21 ;"
I0123 17:16:22.405788 140046437961728 alphageometry.py:567] Translation: "o = on_line o b f, on_bline o f b"

I0123 17:16:22.405827 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_line o b f, on_bline o f b ? para e f m n"
I0123 17:16:22.406009 140046437961728 graph.py:498] 
I0123 17:16:22.406066 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_line o b f, on_bline o f b ? para e f m n
I0123 17:16:24.416122 140046437961728 ddar.py:60] Depth 1/1000 time = 1.832864761352539
I0123 17:16:30.197587 140046437961728 ddar.py:60] Depth 2/1000 time = 5.781285285949707
I0123 17:16:38.266466 140046437961728 ddar.py:60] Depth 3/1000 time = 8.068596363067627
I0123 17:16:46.692527 140046437961728 ddar.py:60] Depth 4/1000 time = 8.425747156143188
I0123 17:16:56.703732 140046437961728 ddar.py:60] Depth 5/1000 time = 10.010963916778564
I0123 17:17:06.524011 140046437961728 ddar.py:60] Depth 6/1000 time = 9.819702625274658
I0123 17:17:23.319963 140046437961728 ddar.py:60] Depth 7/1000 time = 16.391109466552734
I0123 17:17:39.621894 140046437961728 ddar.py:60] Depth 8/1000 time = 16.301629781723022
I0123 17:18:01.218064 140046437961728 ddar.py:60] Depth 9/1000 time = 21.595735788345337
I0123 17:18:29.646993 140046437961728 ddar.py:60] Depth 10/1000 time = 28.428488969802856
I0123 17:18:59.173093 140046437961728 ddar.py:60] Depth 11/1000 time = 29.525671005249023
I0123 17:19:31.388586 140046437961728 ddar.py:60] Depth 12/1000 time = 32.21519327163696
I0123 17:20:03.958739 140046437961728 ddar.py:60] Depth 13/1000 time = 32.53906059265137
I0123 17:20:37.824939 140046437961728 ddar.py:60] Depth 14/1000 time = 33.51457929611206
I0123 17:21:18.512231 140046437961728 ddar.py:60] Depth 15/1000 time = 40.68698501586914
I0123 17:21:59.441837 140046437961728 ddar.py:60] Depth 16/1000 time = 40.899840116500854
I0123 17:22:41.993906 140046437961728 ddar.py:60] Depth 17/1000 time = 42.4429247379303
I0123 17:22:41.994328 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:22:41.994444 140046437961728 alphageometry.py:566] LM output (score=-2.293111): "o : T c h c o 20 ;"
I0123 17:22:41.994481 140046437961728 alphageometry.py:567] Translation: "o = on_tline o c c h"

I0123 17:22:41.994532 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c c h ? para e f m n"
I0123 17:22:41.994734 140046437961728 graph.py:498] 
I0123 17:22:41.994793 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c c h ? para e f m n
I0123 17:22:44.114446 140046437961728 ddar.py:60] Depth 1/1000 time = 1.9663891792297363
I0123 17:22:49.652371 140046437961728 ddar.py:60] Depth 2/1000 time = 5.537748336791992
I0123 17:22:58.457380 140046437961728 ddar.py:60] Depth 3/1000 time = 8.804777383804321
I0123 17:23:07.426773 140046437961728 ddar.py:60] Depth 4/1000 time = 8.969084739685059
I0123 17:23:18.162211 140046437961728 ddar.py:60] Depth 5/1000 time = 10.735217809677124
I0123 17:23:36.547142 140046437961728 ddar.py:60] Depth 6/1000 time = 17.953052520751953
I0123 17:23:54.645491 140046437961728 ddar.py:60] Depth 7/1000 time = 18.098073959350586
I0123 17:24:19.013185 140046437961728 ddar.py:60] Depth 8/1000 time = 24.36743664741516
I0123 17:24:49.717035 140046437961728 ddar.py:60] Depth 9/1000 time = 30.70351552963257
I0123 17:25:23.049310 140046437961728 ddar.py:60] Depth 10/1000 time = 33.33185315132141
I0123 17:25:58.970488 140046437961728 ddar.py:60] Depth 11/1000 time = 35.92083144187927
I0123 17:26:35.781392 140046437961728 ddar.py:60] Depth 12/1000 time = 36.78049373626709
I0123 17:27:13.912162 140046437961728 ddar.py:60] Depth 13/1000 time = 37.7508590221405
I0123 17:27:59.678591 140046437961728 ddar.py:60] Depth 14/1000 time = 45.76609659194946
I0123 17:28:46.210071 140046437961728 ddar.py:60] Depth 15/1000 time = 46.50719499588013
I0123 17:29:33.946605 140046437961728 ddar.py:60] Depth 16/1000 time = 47.63153910636902
I0123 17:29:33.946877 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:29:33.946967 140046437961728 alphageometry.py:566] LM output (score=-2.384959): "o : T c f c o 20 ;"
I0123 17:29:33.947003 140046437961728 alphageometry.py:567] Translation: "o = on_tline o c c f"

I0123 17:29:33.947051 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c c f ? para e f m n"
I0123 17:29:33.947228 140046437961728 graph.py:498] 
I0123 17:29:33.947287 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c c f ? para e f m n
I0123 17:29:36.060855 140046437961728 ddar.py:60] Depth 1/1000 time = 1.9606869220733643
I0123 17:29:41.168797 140046437961728 ddar.py:60] Depth 2/1000 time = 5.10776948928833
I0123 17:29:48.593098 140046437961728 ddar.py:60] Depth 3/1000 time = 7.424105644226074
I0123 17:29:56.027810 140046437961728 ddar.py:60] Depth 4/1000 time = 7.434494495391846
I0123 17:30:04.980487 140046437961728 ddar.py:60] Depth 5/1000 time = 8.952480792999268
I0123 17:30:21.247689 140046437961728 ddar.py:60] Depth 6/1000 time = 15.873268127441406
I0123 17:30:36.833828 140046437961728 ddar.py:60] Depth 7/1000 time = 15.58582854270935
I0123 17:30:58.075161 140046437961728 ddar.py:60] Depth 8/1000 time = 21.24091601371765
I0123 17:31:25.278699 140046437961728 ddar.py:60] Depth 9/1000 time = 27.20308232307434
I0123 17:31:54.232421 140046437961728 ddar.py:60] Depth 10/1000 time = 28.953248500823975
I0123 17:32:25.740181 140046437961728 ddar.py:60] Depth 11/1000 time = 31.5073082447052
I0123 17:32:58.102559 140046437961728 ddar.py:60] Depth 12/1000 time = 32.330546379089355
I0123 17:33:31.400113 140046437961728 ddar.py:60] Depth 13/1000 time = 32.93634295463562
I0123 17:34:11.407716 140046437961728 ddar.py:60] Depth 14/1000 time = 40.00713777542114
I0123 17:34:52.467912 140046437961728 ddar.py:60] Depth 15/1000 time = 41.03669810295105
I0123 17:35:34.174312 140046437961728 ddar.py:60] Depth 16/1000 time = 41.597594022750854
I0123 17:35:34.174752 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:35:34.174880 140046437961728 alphageometry.py:566] LM output (score=-2.549935): "o : T l n l o 20 ;"
I0123 17:35:34.174917 140046437961728 alphageometry.py:567] Translation: "o = on_tline o l l n"

I0123 17:35:34.174973 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o l l n ? para e f m n"
I0123 17:35:34.175175 140046437961728 graph.py:498] 
I0123 17:35:34.175233 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o l l n ? para e f m n
I0123 17:35:36.334109 140046437961728 ddar.py:60] Depth 1/1000 time = 2.0032384395599365
I0123 17:35:41.820339 140046437961728 ddar.py:60] Depth 2/1000 time = 5.486063003540039
I0123 17:35:50.492723 140046437961728 ddar.py:60] Depth 3/1000 time = 8.672123670578003
I0123 17:35:59.401064 140046437961728 ddar.py:60] Depth 4/1000 time = 8.908020973205566
I0123 17:36:10.067367 140046437961728 ddar.py:60] Depth 5/1000 time = 10.666042566299438
I0123 17:36:28.747209 140046437961728 ddar.py:60] Depth 6/1000 time = 18.246020317077637
I0123 17:36:46.631311 140046437961728 ddar.py:60] Depth 7/1000 time = 17.88381552696228
I0123 17:37:10.981340 140046437961728 ddar.py:60] Depth 8/1000 time = 24.3497371673584
I0123 17:37:42.077468 140046437961728 ddar.py:60] Depth 9/1000 time = 31.09585428237915
I0123 17:38:15.600824 140046437961728 ddar.py:60] Depth 10/1000 time = 33.52300953865051
I0123 17:38:51.860887 140046437961728 ddar.py:60] Depth 11/1000 time = 36.25954532623291
I0123 17:39:28.848701 140046437961728 ddar.py:60] Depth 12/1000 time = 36.95684027671814
I0123 17:40:06.714969 140046437961728 ddar.py:60] Depth 13/1000 time = 37.48264265060425
I0123 17:40:52.517475 140046437961728 ddar.py:60] Depth 14/1000 time = 45.802016496658325
I0123 17:41:39.123739 140046437961728 ddar.py:60] Depth 15/1000 time = 46.582056283950806
I0123 17:42:26.961125 140046437961728 ddar.py:60] Depth 16/1000 time = 47.732293367385864
I0123 17:42:26.961405 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:42:26.961497 140046437961728 alphageometry.py:566] LM output (score=-2.616080): "o : T c i c o 20 ;"
I0123 17:42:26.961533 140046437961728 alphageometry.py:567] Translation: "o = on_tline o c c i"

I0123 17:42:26.961572 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c c i ? para e f m n"
I0123 17:42:26.961756 140046437961728 graph.py:498] 
I0123 17:42:26.961816 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c c i ? para e f m n
I0123 17:42:29.102169 140046437961728 ddar.py:60] Depth 1/1000 time = 1.985957145690918
I0123 17:42:34.404385 140046437961728 ddar.py:60] Depth 2/1000 time = 5.301980495452881
I0123 17:42:43.114312 140046437961728 ddar.py:60] Depth 3/1000 time = 8.709612369537354
I0123 17:42:52.261428 140046437961728 ddar.py:60] Depth 4/1000 time = 9.146896839141846
I0123 17:43:02.923088 140046437961728 ddar.py:60] Depth 5/1000 time = 10.66144347190857
I0123 17:43:20.842806 140046437961728 ddar.py:60] Depth 6/1000 time = 17.493610858917236
I0123 17:43:38.830268 140046437961728 ddar.py:60] Depth 7/1000 time = 17.987061738967896
I0123 17:44:02.588664 140046437961728 ddar.py:60] Depth 8/1000 time = 23.75797390937805
I0123 17:44:33.556750 140046437961728 ddar.py:60] Depth 9/1000 time = 30.96760320663452
I0123 17:45:07.036441 140046437961728 ddar.py:60] Depth 10/1000 time = 33.479193449020386
I0123 17:45:43.413033 140046437961728 ddar.py:60] Depth 11/1000 time = 36.37611365318298
I0123 17:46:20.251316 140046437961728 ddar.py:60] Depth 12/1000 time = 36.808003187179565
I0123 17:46:58.654394 140046437961728 ddar.py:60] Depth 13/1000 time = 38.01041293144226
I0123 17:47:44.769302 140046437961728 ddar.py:60] Depth 14/1000 time = 46.11441159248352
I0123 17:48:31.842095 140046437961728 ddar.py:60] Depth 15/1000 time = 47.04852867126465
I0123 17:49:19.996971 140046437961728 ddar.py:60] Depth 16/1000 time = 48.04789972305298
I0123 17:49:19.997396 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:49:19.997517 140046437961728 alphageometry.py:566] LM output (score=-2.628236): "o : T c e e o 20 ;"
I0123 17:49:19.997553 140046437961728 alphageometry.py:567] Translation: "o = on_tline o e c e"

I0123 17:49:19.997616 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o e c e ? para e f m n"
I0123 17:49:19.997836 140046437961728 graph.py:498] 
I0123 17:49:19.997895 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o e c e ? para e f m n
I0123 17:49:22.210847 140046437961728 ddar.py:60] Depth 1/1000 time = 2.058610677719116
I0123 17:49:27.010171 140046437961728 ddar.py:60] Depth 2/1000 time = 4.799121618270874
I0123 17:49:34.445178 140046437961728 ddar.py:60] Depth 3/1000 time = 7.434814691543579
I0123 17:49:42.016926 140046437961728 ddar.py:60] Depth 4/1000 time = 7.571506500244141
I0123 17:49:51.106272 140046437961728 ddar.py:60] Depth 5/1000 time = 9.089033603668213
I0123 17:50:07.473133 140046437961728 ddar.py:60] Depth 6/1000 time = 15.968230724334717
I0123 17:50:23.102824 140046437961728 ddar.py:60] Depth 7/1000 time = 15.629462480545044
I0123 17:50:44.460131 140046437961728 ddar.py:60] Depth 8/1000 time = 21.35705327987671
I0123 17:51:10.866611 140046437961728 ddar.py:60] Depth 9/1000 time = 26.40613627433777
I0123 17:51:39.861968 140046437961728 ddar.py:60] Depth 10/1000 time = 28.994893312454224
I0123 17:52:11.366832 140046437961728 ddar.py:60] Depth 11/1000 time = 31.504377365112305
I0123 17:52:43.400185 140046437961728 ddar.py:60] Depth 12/1000 time = 32.00434231758118
I0123 17:53:16.749698 140046437961728 ddar.py:60] Depth 13/1000 time = 32.98771619796753
I0123 17:53:56.789966 140046437961728 ddar.py:60] Depth 14/1000 time = 40.039851665496826
I0123 17:54:37.211049 140046437961728 ddar.py:60] Depth 15/1000 time = 40.39693236351013
I0123 17:55:19.476343 140046437961728 ddar.py:60] Depth 16/1000 time = 42.16353106498718
I0123 17:55:19.476654 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:55:19.476752 140046437961728 alphageometry.py:566] LM output (score=-2.654336): "o : T b c c o 20 ;"
I0123 17:55:19.476788 140046437961728 alphageometry.py:567] Translation: "o = on_tline o c b c"

I0123 17:55:19.476825 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c b c ? para e f m n"
I0123 17:55:19.477006 140046437961728 graph.py:498] 
I0123 17:55:19.477066 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c b c ? para e f m n
I0123 17:55:21.289758 140046437961728 ddar.py:60] Depth 1/1000 time = 1.658158779144287
I0123 17:55:26.564652 140046437961728 ddar.py:60] Depth 2/1000 time = 5.274665117263794
I0123 17:55:34.103480 140046437961728 ddar.py:60] Depth 3/1000 time = 7.5385119915008545
I0123 17:55:41.680566 140046437961728 ddar.py:60] Depth 4/1000 time = 7.57690954208374
I0123 17:55:50.303604 140046437961728 ddar.py:60] Depth 5/1000 time = 8.622778177261353
I0123 17:56:06.825553 140046437961728 ddar.py:60] Depth 6/1000 time = 16.125734090805054
I0123 17:56:22.568426 140046437961728 ddar.py:60] Depth 7/1000 time = 15.742495775222778
I0123 17:56:44.039143 140046437961728 ddar.py:60] Depth 8/1000 time = 21.47047996520996
I0123 17:57:10.360383 140046437961728 ddar.py:60] Depth 9/1000 time = 26.320906400680542
I0123 17:57:39.540238 140046437961728 ddar.py:60] Depth 10/1000 time = 29.179420709609985
I0123 17:58:11.175853 140046437961728 ddar.py:60] Depth 11/1000 time = 31.635316610336304
I0123 17:58:43.284318 140046437961728 ddar.py:60] Depth 12/1000 time = 32.076627254486084
I0123 17:59:16.236274 140046437961728 ddar.py:60] Depth 13/1000 time = 32.59280180931091
I0123 17:59:56.356715 140046437961728 ddar.py:60] Depth 14/1000 time = 40.11998176574707
I0123 18:00:36.855763 140046437961728 ddar.py:60] Depth 15/1000 time = 40.4687557220459
I0123 18:01:18.817258 140046437961728 ddar.py:60] Depth 16/1000 time = 41.84310436248779
I0123 18:01:18.817815 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:01:18.817951 140046437961728 alphageometry.py:566] LM output (score=-2.693098): "o : C b g o 20 D b g g o 21 ;"
I0123 18:01:18.817989 140046437961728 alphageometry.py:567] Translation: "o = on_line o b g, on_circle o g b"

I0123 18:01:18.818043 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_line o b g, on_circle o g b ? para e f m n"
I0123 18:01:18.818244 140046437961728 graph.py:498] 
I0123 18:01:18.818302 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_line o b g, on_circle o g b ? para e f m n
I0123 18:01:21.409413 140046437961728 ddar.py:60] Depth 1/1000 time = 2.4202463626861572
I0123 18:01:27.412183 140046437961728 ddar.py:60] Depth 2/1000 time = 6.002581834793091
I0123 18:01:35.797034 140046437961728 ddar.py:60] Depth 3/1000 time = 8.384657621383667
I0123 18:01:44.268874 140046437961728 ddar.py:60] Depth 4/1000 time = 8.471620559692383
I0123 18:01:54.402543 140046437961728 ddar.py:60] Depth 5/1000 time = 10.13346266746521
I0123 18:02:04.491311 140046437961728 ddar.py:60] Depth 6/1000 time = 10.08834195137024
I0123 18:02:21.800488 140046437961728 ddar.py:60] Depth 7/1000 time = 16.909334659576416
I0123 18:02:39.254025 140046437961728 ddar.py:60] Depth 8/1000 time = 17.45315170288086
I0123 18:03:01.543338 140046437961728 ddar.py:60] Depth 9/1000 time = 22.28898310661316
I0123 18:03:30.733165 140046437961728 ddar.py:60] Depth 10/1000 time = 29.189355850219727
I0123 18:04:01.438710 140046437961728 ddar.py:60] Depth 11/1000 time = 30.705058336257935
I0123 18:04:34.719763 140046437961728 ddar.py:60] Depth 12/1000 time = 33.280661821365356
I0123 18:05:08.405190 140046437961728 ddar.py:60] Depth 13/1000 time = 33.65391182899475
I0123 18:05:43.622180 140046437961728 ddar.py:60] Depth 14/1000 time = 34.85283541679382
I0123 18:06:25.474584 140046437961728 ddar.py:60] Depth 15/1000 time = 41.852036476135254
I0123 18:07:07.997140 140046437961728 ddar.py:60] Depth 16/1000 time = 42.49239373207092
I0123 18:07:51.806550 140046437961728 ddar.py:60] Depth 17/1000 time = 43.70634937286377
I0123 18:07:51.806893 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:07:51.807000 140046437961728 alphageometry.py:566] LM output (score=-2.786413): "o : T b g g o 20 ;"
I0123 18:07:51.807038 140046437961728 alphageometry.py:567] Translation: "o = on_tline o g b g"

I0123 18:07:51.807077 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o g b g ? para e f m n"
I0123 18:07:51.807261 140046437961728 graph.py:498] 
I0123 18:07:51.807320 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o g b g ? para e f m n
I0123 18:07:53.600369 140046437961728 ddar.py:60] Depth 1/1000 time = 1.6381433010101318
I0123 18:07:59.315885 140046437961728 ddar.py:60] Depth 2/1000 time = 5.715243577957153
I0123 18:08:08.072970 140046437961728 ddar.py:60] Depth 3/1000 time = 8.756880760192871
I0123 18:08:17.011431 140046437961728 ddar.py:60] Depth 4/1000 time = 8.938243389129639
I0123 18:08:27.167163 140046437961728 ddar.py:60] Depth 5/1000 time = 10.15550684928894
I0123 18:08:45.877922 140046437961728 ddar.py:60] Depth 6/1000 time = 18.272380590438843
I0123 18:09:03.772812 140046437961728 ddar.py:60] Depth 7/1000 time = 17.894585132598877
I0123 18:09:27.815718 140046437961728 ddar.py:60] Depth 8/1000 time = 24.04247283935547
I0123 18:09:58.880164 140046437961728 ddar.py:60] Depth 9/1000 time = 31.063997268676758
I0123 18:10:32.400215 140046437961728 ddar.py:60] Depth 10/1000 time = 33.51962232589722
I0123 18:11:08.042370 140046437961728 ddar.py:60] Depth 11/1000 time = 35.64181995391846
I0123 18:11:44.811600 140046437961728 ddar.py:60] Depth 12/1000 time = 36.73872351646423
I0123 18:12:22.990120 140046437961728 ddar.py:60] Depth 13/1000 time = 37.79865765571594
I0123 18:13:09.044591 140046437961728 ddar.py:60] Depth 14/1000 time = 46.05413770675659
I0123 18:13:54.968905 140046437961728 ddar.py:60] Depth 15/1000 time = 45.9001898765564
I0123 18:14:42.949634 140046437961728 ddar.py:60] Depth 16/1000 time = 47.874019145965576
I0123 18:14:42.950078 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:14:42.950206 140046437961728 alphageometry.py:566] LM output (score=-2.802945): "o : T a e a o 20 ;"
I0123 18:14:42.950243 140046437961728 alphageometry.py:567] Translation: "o = on_tline o a a e"

I0123 18:14:42.950297 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o a a e ? para e f m n"
I0123 18:14:42.950497 140046437961728 graph.py:498] 
I0123 18:14:42.950554 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o a a e ? para e f m n
I0123 18:14:45.311009 140046437961728 ddar.py:60] Depth 1/1000 time = 2.206533670425415
I0123 18:14:50.131848 140046437961728 ddar.py:60] Depth 2/1000 time = 4.820661783218384
I0123 18:14:57.665842 140046437961728 ddar.py:60] Depth 3/1000 time = 7.533802509307861
I0123 18:15:05.449428 140046437961728 ddar.py:60] Depth 4/1000 time = 7.783386707305908
I0123 18:15:14.135227 140046437961728 ddar.py:60] Depth 5/1000 time = 8.685583114624023
I0123 18:15:31.081872 140046437961728 ddar.py:60] Depth 6/1000 time = 16.54906916618347
I0123 18:15:47.086400 140046437961728 ddar.py:60] Depth 7/1000 time = 16.00424814224243
I0123 18:16:08.434238 140046437961728 ddar.py:60] Depth 8/1000 time = 21.347445011138916
I0123 18:16:35.697186 140046437961728 ddar.py:60] Depth 9/1000 time = 27.26250195503235
I0123 18:17:05.302884 140046437961728 ddar.py:60] Depth 10/1000 time = 29.605226755142212
I0123 18:17:37.555073 140046437961728 ddar.py:60] Depth 11/1000 time = 32.25176191329956
I0123 18:18:09.482835 140046437961728 ddar.py:60] Depth 12/1000 time = 31.899307250976562
I0123 18:18:43.725178 140046437961728 ddar.py:60] Depth 13/1000 time = 33.87869477272034
I0123 18:19:24.511568 140046437961728 ddar.py:60] Depth 14/1000 time = 40.78606581687927
I0123 18:20:04.993903 140046437961728 ddar.py:60] Depth 15/1000 time = 40.45879626274109
I0123 18:20:47.521806 140046437961728 ddar.py:60] Depth 16/1000 time = 42.42711639404297
I0123 18:20:47.522279 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:20:47.522415 140046437961728 alphageometry.py:566] LM output (score=-2.805339): "o : T c k c o 20 ;"
I0123 18:20:47.522456 140046437961728 alphageometry.py:567] Translation: "o = on_tline o c c k"

I0123 18:20:47.522512 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c c k ? para e f m n"
I0123 18:20:47.522734 140046437961728 graph.py:498] 
I0123 18:20:47.522799 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o c c k ? para e f m n
I0123 18:20:49.324146 140046437961728 ddar.py:60] Depth 1/1000 time = 1.646925449371338
I0123 18:20:55.181478 140046437961728 ddar.py:60] Depth 2/1000 time = 5.8571412563323975
I0123 18:21:04.206098 140046437961728 ddar.py:60] Depth 3/1000 time = 9.024433851242065
I0123 18:21:12.839326 140046437961728 ddar.py:60] Depth 4/1000 time = 8.632956266403198
I0123 18:21:23.870729 140046437961728 ddar.py:60] Depth 5/1000 time = 11.031066179275513
I0123 18:21:42.361039 140046437961728 ddar.py:60] Depth 6/1000 time = 18.04985237121582
I0123 18:22:00.904093 140046437961728 ddar.py:60] Depth 7/1000 time = 18.54274082183838
I0123 18:22:25.206077 140046437961728 ddar.py:60] Depth 8/1000 time = 24.301692724227905
I0123 18:22:56.453005 140046437961728 ddar.py:60] Depth 9/1000 time = 31.246581554412842
I0123 18:23:29.870399 140046437961728 ddar.py:60] Depth 10/1000 time = 33.416948080062866
I0123 18:24:05.605555 140046437961728 ddar.py:60] Depth 11/1000 time = 35.73459815979004
I0123 18:24:42.378098 140046437961728 ddar.py:60] Depth 12/1000 time = 36.74244046211243
I0123 18:25:20.361674 140046437961728 ddar.py:60] Depth 13/1000 time = 37.59692144393921
I0123 18:26:05.605154 140046437961728 ddar.py:60] Depth 14/1000 time = 45.242982149124146
I0123 18:26:51.889429 140046437961728 ddar.py:60] Depth 15/1000 time = 46.259913206100464
I0123 18:27:39.200924 140046437961728 ddar.py:60] Depth 16/1000 time = 47.2059166431427
I0123 18:27:39.201395 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:27:39.201532 140046437961728 alphageometry.py:566] LM output (score=-2.872705): "o : T b g e o 20 ;"
I0123 18:27:39.201573 140046437961728 alphageometry.py:567] Translation: "o = on_tline o e b g"

I0123 18:27:39.201629 140046437961728 alphageometry.py:576] Solving: "a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o e b g ? para e f m n"
I0123 18:27:39.201843 140046437961728 graph.py:498] 
I0123 18:27:39.201906 140046437961728 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g e b a; h = circle h e a c; i = circle i e b c; j = circle j f b a; k = circle k f a c; l = circle l f b c; m = circle m g h i; n = circle n j k l; o = on_tline o e b g ? para e f m n
I0123 18:27:41.592863 140046437961728 ddar.py:60] Depth 1/1000 time = 2.2364633083343506
I0123 18:27:46.812562 140046437961728 ddar.py:60] Depth 2/1000 time = 5.219508647918701
I0123 18:27:55.647282 140046437961728 ddar.py:60] Depth 3/1000 time = 8.834462881088257
I0123 18:28:04.577698 140046437961728 ddar.py:60] Depth 4/1000 time = 8.930073738098145
I0123 18:28:14.750713 140046437961728 ddar.py:60] Depth 5/1000 time = 10.172741174697876
I0123 18:28:33.851958 140046437961728 ddar.py:60] Depth 6/1000 time = 18.665441751480103
I0123 18:28:52.010155 140046437961728 ddar.py:60] Depth 7/1000 time = 18.157958030700684
I0123 18:29:16.476857 140046437961728 ddar.py:60] Depth 8/1000 time = 24.46636724472046
I0123 18:29:47.535445 140046437961728 ddar.py:60] Depth 9/1000 time = 31.05813431739807
I0123 18:30:21.560728 140046437961728 ddar.py:60] Depth 10/1000 time = 34.0248589515686
I0123 18:30:57.831217 140046437961728 ddar.py:60] Depth 11/1000 time = 36.270092725753784
I0123 18:31:35.225078 140046437961728 ddar.py:60] Depth 12/1000 time = 37.3636736869812
I0123 18:32:14.139595 140046437961728 ddar.py:60] Depth 13/1000 time = 38.52986478805542
I0123 18:33:00.071706 140046437961728 ddar.py:60] Depth 14/1000 time = 45.93173384666443
I0123 18:33:47.248904 140046437961728 ddar.py:60] Depth 15/1000 time = 47.15324258804321
I0123 18:33:47.354473 140046437961728 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:33:47.354524 140046437961728 alphageometry.py:585] Timeout.
