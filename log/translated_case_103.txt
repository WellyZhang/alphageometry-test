I0123 12:40:46.391536 139837198389248 inference_utils.py:69] Parsing gin configuration.
I0123 12:40:46.391727 139837198389248 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:40:46.392093 139837198389248 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:40:46.392128 139837198389248 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:40:46.392157 139837198389248 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:40:46.392184 139837198389248 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:40:46.392211 139837198389248 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:40:46.392236 139837198389248 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:40:46.392262 139837198389248 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:40:46.392287 139837198389248 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:40:46.392313 139837198389248 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:40:46.392338 139837198389248 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:40:46.392402 139837198389248 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:40:46.392580 139837198389248 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:40:46.392875 139837198389248 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:40:46.392984 139837198389248 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:40:46.399690 139837198389248 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:40:46.399824 139837198389248 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:40:46.400149 139837198389248 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:40:46.400255 139837198389248 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:40:46.400535 139837198389248 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:40:46.400632 139837198389248 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:40:46.401036 139837198389248 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:40:46.401135 139837198389248 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:40:46.405293 139837198389248 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:40:46.512106 139837198389248 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:40:46.513120 139837198389248 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:40:46.519736 139837198389248 training_loop.py:335] Process 0 of 1
I0123 12:40:46.519794 139837198389248 training_loop.py:336] Local device count = 1
I0123 12:40:46.519835 139837198389248 training_loop.py:337] Number of replicas = 1
I0123 12:40:46.519868 139837198389248 training_loop.py:339] Using random number seed 42
I0123 12:40:47.028825 139837198389248 training_loop.py:359] Initializing the model.
I0123 12:40:47.416858 139837198389248 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.417388 139837198389248 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:40:47.417514 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.417656 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.417740 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.417820 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.417889 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.417957 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.418023 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.418090 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.418157 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.418224 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.418290 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.418356 139837198389248 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:40:47.418399 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:47.418446 139837198389248 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:40:47.418564 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:47.418604 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:47.418634 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:47.420669 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.426251 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:47.440744 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.441619 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:47.446324 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:47.457949 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:47.458039 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:47.458082 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:47.458118 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.458182 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.460201 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.460278 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.461015 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.463713 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.470094 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.471371 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.471455 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:47.471492 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:47.471560 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.471696 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:47.472064 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:47.472112 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.474060 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.474165 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.477180 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.477263 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:47.477732 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:47.488173 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.497171 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.497271 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.497574 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.497673 139837198389248 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:40:47.497793 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:47.497834 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:47.497867 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:47.499905 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.502362 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:47.508069 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.508341 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:47.511020 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:47.514914 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:47.514972 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:47.515009 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:47.515042 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.515105 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.515686 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.515763 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.516130 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.516896 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.519435 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.520109 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.520187 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:47.520222 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:47.520283 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.520413 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:47.520736 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:47.520780 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.522710 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.522804 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.525367 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.525446 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:47.525945 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:47.528256 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.530179 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.530273 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.530568 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.530647 139837198389248 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:40:47.530757 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:47.530796 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:47.530828 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:47.532760 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.535184 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:47.541215 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.541482 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:47.544189 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:47.548068 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:47.548123 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:47.548160 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:47.548193 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.548255 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.548823 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.548899 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.549261 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.550049 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.552675 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.553303 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.553379 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:47.553414 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:47.553474 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.553600 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:47.553944 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:47.553989 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.555919 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.556012 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.558615 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.558700 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:47.559134 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:47.561418 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.563372 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.563467 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.563763 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.563843 139837198389248 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:40:47.563952 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:47.563992 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:47.564025 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:47.565941 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.568367 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:47.574037 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.574302 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:47.576999 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:47.580808 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:47.580864 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:47.580900 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:47.580932 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.580992 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.581552 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.581631 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.582002 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.582781 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.585354 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.585986 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.586065 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:47.586100 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:47.586159 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.586289 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:47.586615 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:47.586658 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.588567 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.588659 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.591229 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.591317 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:47.591747 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:47.594011 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.595984 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.596077 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.596374 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.596456 139837198389248 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:40:47.596567 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:47.596606 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:47.596638 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:47.598473 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.600866 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:47.606515 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.606777 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:47.609526 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:47.613342 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:47.613398 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:47.613439 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:47.613471 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.613533 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.614470 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.614547 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.614908 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.615694 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.618254 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.618869 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.618944 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:47.618979 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:47.619039 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.619174 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:47.619503 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:47.619546 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.621461 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.621555 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.624166 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.624246 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:47.624685 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:47.627039 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.628977 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.629072 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.629374 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.629455 139837198389248 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:40:47.629567 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:47.629606 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:47.629645 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:47.631509 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.634054 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:47.639738 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.639997 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:47.642687 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:47.646501 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:47.646558 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:47.646595 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:47.646627 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.646688 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.647252 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.647327 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.647689 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.648473 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.650997 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.651617 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.651697 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:47.651733 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:47.651798 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.651926 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:47.652249 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:47.652292 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.654291 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.654384 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.656970 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.657049 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:47.657487 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:47.659849 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.661812 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.661910 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.662207 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.662286 139837198389248 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:40:47.662400 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:47.662439 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:47.662471 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:47.664393 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.666872 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:47.672599 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.672866 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:47.675587 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:47.679439 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:47.679496 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:47.679533 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:47.679564 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.679627 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.680201 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.680281 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.680649 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.681429 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.683959 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.684632 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.684710 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:47.684746 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:47.684805 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.684935 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:47.685263 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:47.685307 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.687250 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.687343 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.689911 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.689991 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:47.690786 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:47.693088 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:47.695025 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.695132 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:47.695430 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:47.695510 139837198389248 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:40:47.695621 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:47.695660 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:47.695692 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.042925 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.046519 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.052762 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.053122 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.056012 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:48.060165 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.060229 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.060271 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.060307 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.060369 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.061098 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.061175 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.061550 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.062367 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.065042 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.065727 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.065808 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.065844 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.065910 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.066040 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.066399 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.066442 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.068406 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.068501 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.071174 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.071256 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.071708 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.074150 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.076188 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.076303 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.076608 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.076694 139837198389248 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:40:48.076812 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.076852 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.076884 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.078809 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.081320 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.087039 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.087311 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.096910 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:48.103080 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.103148 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.103195 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.103229 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.103357 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.104164 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.104242 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.104616 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.105461 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.108180 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.108831 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.108907 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.108943 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.109005 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.109134 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.109544 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.109588 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.111644 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.111746 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.114544 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.114623 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.115082 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.117628 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.119594 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.119690 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.119996 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.120086 139837198389248 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:40:48.120218 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.120265 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.120299 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.122310 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.124908 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.131056 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.131339 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.134088 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:48.137983 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.138037 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.138073 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.138104 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.138165 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.138732 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.138808 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.139174 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.139973 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.142585 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.143242 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.143319 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.143355 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.143416 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.143559 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.143886 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.143930 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.145976 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.146070 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.148695 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.148773 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.149209 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.151631 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.153610 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.153712 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.154008 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.154093 139837198389248 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:40:48.154206 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.154245 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.154277 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.156202 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.158718 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.164527 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.164805 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.167522 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:48.171488 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.171544 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.171581 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.171612 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.171676 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.172246 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.172321 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.172681 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.173485 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.176101 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.176789 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.176867 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.176903 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.176965 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.177093 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.177421 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.177464 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.179390 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.179486 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.184126 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.184206 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.184689 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.186979 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.188908 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.189002 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.189299 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.189384 139837198389248 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:40:48.189501 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.189541 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.189573 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.191517 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.193950 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.199612 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.199878 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.202570 139837198389248 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:40:48.206414 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.206470 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.206507 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.206540 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.206604 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.207162 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.207237 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.207596 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.208362 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.211251 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.211879 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.211956 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.211992 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.212052 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.212179 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.212503 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.212548 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.214470 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.214563 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.217133 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.217212 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.217650 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.219893 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.221814 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.221909 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.222200 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.222558 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.222632 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.222701 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.222761 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.222818 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.222874 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.222929 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.222983 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.223037 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.223090 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.223144 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.223197 139837198389248 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:40:48.223241 139837198389248 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:40:48.227026 139837198389248 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:40:48.275810 139837198389248 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.275903 139837198389248 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:40:48.275959 139837198389248 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:40:48.276066 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.276105 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.276135 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.276197 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.278609 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.284024 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.284281 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.286890 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.303600 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.303656 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.303692 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.303723 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.303785 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.304933 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.305010 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.305727 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.307702 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.312524 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.313847 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.313933 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.313969 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.314029 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.314162 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.314271 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.314310 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.316241 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.316334 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.318824 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.318905 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.319014 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.321252 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.323212 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.323308 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.323603 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.323685 139837198389248 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:40:48.323797 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.323836 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.323868 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.323934 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.326217 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.331707 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.331969 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.334687 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.347897 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.347951 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.347988 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.348020 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.348086 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.348643 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.348719 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.349081 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.349787 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.352317 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.352936 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.353014 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.353055 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.353115 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.353247 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.353357 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.353399 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.355357 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.355451 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.357913 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.357994 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.358104 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.360335 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.362287 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.362382 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.362676 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.362758 139837198389248 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:40:48.362869 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.362908 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.362940 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.363004 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.365267 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.370774 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.371037 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.373758 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.386589 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.386645 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.386682 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.386714 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.386775 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.387341 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.387439 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.387803 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.388494 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.391016 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.391649 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.391726 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.391760 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.391824 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.391953 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.392061 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.392099 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.394045 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.394140 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.396584 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.396662 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.396770 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.399019 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.400959 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.401066 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.401360 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.401442 139837198389248 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:40:48.401552 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.401591 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.401623 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.401700 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.403998 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.409544 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.409821 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.412519 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.425321 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.425377 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.425413 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.425445 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.425507 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.426076 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.426152 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.426510 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.427204 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.429740 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.430365 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.430442 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.430477 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.430536 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.430672 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.430783 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.430822 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.432773 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.432867 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.435320 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.435400 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.435512 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.437748 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.439629 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.439724 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.440016 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.440096 139837198389248 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:40:48.440206 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.440246 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.440278 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.440343 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.442971 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.448499 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.448764 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.451420 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.464356 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.464413 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.464450 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.464481 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.464547 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.465105 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.465184 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.465544 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.466253 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.468829 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.469464 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.469541 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.469576 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.469636 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.469789 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.469901 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.469940 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.471838 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.471932 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.474389 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.474469 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.474580 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.476875 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.478764 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.478859 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.479151 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.479232 139837198389248 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:40:48.479343 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.479383 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.479415 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.479479 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.481781 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.487291 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.487551 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.490284 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.503145 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.503201 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.503237 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.503269 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.503330 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.503892 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.503968 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.504329 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.505028 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.507537 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.508160 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.508235 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.508270 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.508332 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.508461 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.508576 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.508615 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.510568 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.510662 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.513119 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.513196 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.513305 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.515552 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.517405 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.517502 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.517806 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.517890 139837198389248 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:40:48.518001 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.518040 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.518072 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.518138 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.520401 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.525995 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.526257 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.528870 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.541682 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.541738 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.541776 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.541807 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.541872 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.542429 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.542505 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.542866 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.543556 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.546077 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.547066 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.547144 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.547179 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.547239 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.547369 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.547478 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.547521 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.549425 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.549519 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.551970 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.552048 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.552158 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.554405 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.556341 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.556436 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.556728 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.556810 139837198389248 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:40:48.556922 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.556961 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.556993 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.557057 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.559337 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.564810 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.565087 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.567806 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.580585 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.580641 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.580677 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.580709 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.580770 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.581371 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.581446 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.581822 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.582512 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.585014 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.585653 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.585731 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.585765 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.585824 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.585954 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.586063 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.586107 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.588031 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.588123 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.590635 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.590715 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.590830 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.593038 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.594895 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.594990 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.595277 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.595358 139837198389248 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:40:48.595467 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.595507 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.595539 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.595602 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.597870 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.603375 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.603640 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.606266 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.618971 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.619025 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.619061 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.619092 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.619154 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.619706 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.619781 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.620141 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.620820 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.623343 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.624019 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.624097 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.624132 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.624192 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.624320 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.624429 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.624468 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.626381 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.626476 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.628903 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.628983 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.629093 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.631325 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.633268 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.633363 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.633661 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.633744 139837198389248 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:40:48.633858 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.633898 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.633930 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.633995 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.636276 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.641778 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.642041 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.644748 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.657826 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.657881 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.657918 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.657950 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.658011 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.658619 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.658694 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.659048 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.659741 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.662244 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.662864 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.662940 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.662976 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.663034 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.663164 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.663274 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.663312 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.665189 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.665285 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.667774 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.667853 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.667961 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.670186 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.672052 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.672146 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.672436 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.672516 139837198389248 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:40:48.672627 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.672667 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.672698 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.672761 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.675036 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.680565 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.680824 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.683466 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.696248 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.696303 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.696340 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.696372 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.696435 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.696998 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.697073 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.697430 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.698127 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.700625 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.701295 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.701372 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.701409 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.701467 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.701597 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.701718 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.701760 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.703665 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.703764 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.706218 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.706299 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.706412 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.708609 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.710556 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.710652 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.710943 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.711025 139837198389248 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:40:48.711135 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.711174 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.711205 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.711267 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.713514 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.718987 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.719248 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.721962 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.734694 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.734750 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.734786 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.734819 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.734882 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.735434 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.735510 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.735867 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.736603 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.739135 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.739766 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.739844 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.739880 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.739938 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.740070 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.740186 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.740226 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.742108 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.742202 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.744641 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.744721 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.744828 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.747123 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.748994 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.749088 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.749376 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.749462 139837198389248 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:40:48.752367 139837198389248 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:40:48.808764 139837198389248 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.808855 139837198389248 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:40:48.808910 139837198389248 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:40:48.809015 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.809053 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.809084 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.809149 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.811836 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.817203 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.817465 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.820044 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.832447 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.832503 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.832540 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.832572 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.832633 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.833183 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.833257 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.833609 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.834285 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.836788 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.837401 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.837477 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.837512 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.837571 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.837708 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.837828 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.837867 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.839693 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.839786 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.842191 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.842269 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.842377 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.844608 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.846473 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.846567 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.846855 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.846936 139837198389248 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:40:48.847046 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.847086 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.847118 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.847181 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.849441 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.854800 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.855058 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.857730 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.870049 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.870104 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.870140 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.870172 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.870234 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.870781 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.870856 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.871212 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.871887 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.874409 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.875024 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.875100 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.875135 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.875194 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.875322 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.875431 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.875476 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.877307 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.877400 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.879815 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.879896 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.880006 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.882297 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.884165 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.884260 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.884549 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.884629 139837198389248 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:40:48.884738 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.884778 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.884809 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.884872 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.887126 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.892498 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.892758 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.895449 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.907864 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.907919 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.907955 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.907986 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.908048 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.908600 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.908675 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.909034 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.909722 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.912242 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.912865 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.912942 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.912977 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.913036 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.913162 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.913268 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.913306 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.915183 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.915277 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.917726 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.917806 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.917917 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.920633 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.922496 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.922592 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.922897 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.922993 139837198389248 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:40:48.923105 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.923144 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.923176 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.923238 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.925508 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.930872 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.931131 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.933859 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.946304 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.946359 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.946398 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.946443 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.946506 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.947058 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.947132 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.947488 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.948167 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.950720 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.951339 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.951414 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.951448 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.951508 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.951634 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.951739 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.951781 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.953658 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.953750 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.956166 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.956242 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.956349 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.958656 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.960521 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.960613 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.960903 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.960983 139837198389248 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:40:48.961090 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.961128 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.961158 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.961220 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.963488 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:48.968919 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.969178 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:48.971879 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:48.984471 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:48.984525 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:48.984559 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:48.984590 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.984652 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.985204 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.985276 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.985632 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.986329 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.988892 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.989508 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.989582 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:48.989616 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:48.989681 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.989811 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:48.989917 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:48.989953 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.991814 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.991909 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.994338 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.994415 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:48.994520 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:48.996784 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:48.998649 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.998742 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:48.999027 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:48.999106 139837198389248 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:40:48.999213 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:48.999251 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:48.999281 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:48.999345 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.001590 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:49.007017 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.007277 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:49.010003 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:49.022669 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:49.022724 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:49.022758 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:49.022788 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.022849 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.023396 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.023469 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.023827 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.024513 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.027092 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.027714 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.027790 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:49.027823 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:49.027881 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.028006 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:49.028113 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:49.028150 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.030025 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.030122 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.032521 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.032597 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:49.032703 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:49.035399 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.037260 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.037351 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.037638 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.037727 139837198389248 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:40:49.037833 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:49.037870 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:49.037900 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:49.037962 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.040224 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:49.045658 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.045917 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:49.048613 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:49.061221 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:49.061276 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:49.061311 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:49.061341 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.061401 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.061980 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.062055 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.062411 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.063093 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.065655 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.066291 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.066366 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:49.066399 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:49.066458 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.066583 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:49.066689 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:49.066726 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.068611 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.068702 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.071143 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.071221 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:49.071328 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:49.073606 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.075471 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.075564 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.075849 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.075928 139837198389248 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:40:49.076037 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:49.076075 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:49.076105 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:49.076167 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.078418 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:49.083842 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.084104 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:49.086802 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:49.099352 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:49.099407 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:49.099442 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:49.099473 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.099534 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.100094 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.100168 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.100521 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.101200 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.103750 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.104372 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.104446 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:49.104480 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:49.104537 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.104661 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:49.104766 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:49.104803 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.106691 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.106783 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.109188 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.109273 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:49.109382 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:49.111659 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.113497 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.113590 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.113888 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.113968 139837198389248 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:40:49.114073 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:49.114109 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:49.114139 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:49.114201 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.116431 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:49.121876 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.122140 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:49.124816 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:49.137428 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:49.137482 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:49.137516 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:49.137546 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.137651 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.138225 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.138299 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.138657 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.139341 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.141914 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.142528 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.142602 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:49.142635 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:49.142693 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.142819 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:49.142924 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:49.142961 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.144832 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.144923 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.147352 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.147436 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:49.147545 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:49.150213 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.152079 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.152171 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.152456 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.152535 139837198389248 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:40:49.152643 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:49.152680 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:49.152710 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:49.152772 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.155025 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:49.160478 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.160740 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:49.163445 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:49.176051 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:49.176105 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:49.176139 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:49.176169 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.176231 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.176785 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.176859 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.177214 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.177911 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.180482 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.181111 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.181210 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:49.181245 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:49.181303 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.181430 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:49.181535 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:49.181572 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.183790 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.183884 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.186293 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.186372 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:49.186484 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:49.188727 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.190582 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.190675 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.190962 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.191043 139837198389248 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:40:49.191150 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:49.191187 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:49.191217 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:49.191280 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.193509 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:49.198892 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.199152 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:49.201836 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:49.214399 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:49.214452 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:49.214487 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:49.214518 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.214578 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.215135 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.215209 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.215568 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.216262 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.218840 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.219468 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.219543 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:49.219577 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:49.219634 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.219760 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:49.219867 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:49.219905 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.221797 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.221889 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.224308 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.224385 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:49.224492 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:49.226784 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.228635 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.228728 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.229015 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.229094 139837198389248 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:40:49.229201 139837198389248 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:40:49.229238 139837198389248 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:40:49.229269 139837198389248 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:40:49.229331 139837198389248 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.231594 139837198389248 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:40:49.237008 139837198389248 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.237269 139837198389248 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:40:49.239980 139837198389248 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:40:49.252635 139837198389248 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:40:49.252689 139837198389248 attention.py:418] Single window, no scan.
I0123 12:40:49.252723 139837198389248 transformer_layer.py:389] tlayer: self-attention.
I0123 12:40:49.252753 139837198389248 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.252813 139837198389248 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.253373 139837198389248 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.253452 139837198389248 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.253823 139837198389248 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.254511 139837198389248 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.257079 139837198389248 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.257705 139837198389248 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.257781 139837198389248 transformer_layer.py:468] tlayer: End windows.
I0123 12:40:49.257814 139837198389248 transformer_layer.py:472] tlayer: final FFN.
I0123 12:40:49.257871 139837198389248 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.258001 139837198389248 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:40:49.258110 139837198389248 nn_components.py:325] mlp: activation = None
I0123 12:40:49.258148 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.260034 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.260124 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.262545 139837198389248 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.262624 139837198389248 transformer_base.py:443] tbase: final FFN
I0123 12:40:49.262732 139837198389248 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:40:49.265388 139837198389248 nn_components.py:329] mlp: final activation = None
I0123 12:40:49.267273 139837198389248 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.267367 139837198389248 nn_components.py:261] mlp: residual
I0123 12:40:49.267657 139837198389248 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:49.267741 139837198389248 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:40:49.270606 139837198389248 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:40:53.966259 139837198389248 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:40:54.499725 139837198389248 training_loop.py:409] No working directory specified.
I0123 12:40:54.499869 139837198389248 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:40:54.500857 139837198389248 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:40:57.885801 139837198389248 training_loop.py:447] Only restoring trainable parameters.
I0123 12:40:57.886733 139837198389248 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:40:57.886796 139837198389248 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.886844 139837198389248 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.886886 139837198389248 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.886926 139837198389248 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.886963 139837198389248 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.886999 139837198389248 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887036 139837198389248 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887072 139837198389248 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.887108 139837198389248 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.887143 139837198389248 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887179 139837198389248 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.887215 139837198389248 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.887253 139837198389248 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.887292 139837198389248 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887327 139837198389248 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.887362 139837198389248 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887398 139837198389248 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887433 139837198389248 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.887468 139837198389248 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.887525 139837198389248 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887562 139837198389248 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.887597 139837198389248 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.887631 139837198389248 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.887666 139837198389248 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887700 139837198389248 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.887733 139837198389248 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887768 139837198389248 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887801 139837198389248 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.887835 139837198389248 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.887869 139837198389248 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.887902 139837198389248 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.887936 139837198389248 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.887969 139837198389248 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.888002 139837198389248 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888035 139837198389248 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.888068 139837198389248 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888101 139837198389248 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888134 139837198389248 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.888167 139837198389248 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.888201 139837198389248 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888235 139837198389248 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.888267 139837198389248 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.888300 139837198389248 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.888333 139837198389248 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888366 139837198389248 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.888405 139837198389248 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888440 139837198389248 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888474 139837198389248 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.888506 139837198389248 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.888540 139837198389248 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888574 139837198389248 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.888607 139837198389248 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.888640 139837198389248 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.888673 139837198389248 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888706 139837198389248 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.888741 139837198389248 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888775 139837198389248 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888808 139837198389248 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.888843 139837198389248 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.888876 139837198389248 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.888911 139837198389248 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.888945 139837198389248 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.888978 139837198389248 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.889012 139837198389248 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889046 139837198389248 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.889080 139837198389248 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889113 139837198389248 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889147 139837198389248 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.889181 139837198389248 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.889215 139837198389248 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889249 139837198389248 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.889283 139837198389248 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.889322 139837198389248 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.889358 139837198389248 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889392 139837198389248 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.889425 139837198389248 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889458 139837198389248 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889492 139837198389248 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.889524 139837198389248 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.889558 139837198389248 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889591 139837198389248 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.889624 139837198389248 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.889685 139837198389248 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.889720 139837198389248 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889754 139837198389248 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.889788 139837198389248 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889822 139837198389248 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889854 139837198389248 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.889888 139837198389248 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.889921 139837198389248 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.889955 139837198389248 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.889989 139837198389248 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.890022 139837198389248 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.890056 139837198389248 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890090 139837198389248 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.890125 139837198389248 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890158 139837198389248 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890192 139837198389248 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.890227 139837198389248 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.890267 139837198389248 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890303 139837198389248 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.890337 139837198389248 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.890371 139837198389248 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.890405 139837198389248 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890439 139837198389248 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.890474 139837198389248 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890507 139837198389248 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890541 139837198389248 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.890574 139837198389248 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.890608 139837198389248 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890641 139837198389248 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.890674 139837198389248 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:40:57.890708 139837198389248 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:40:57.890743 139837198389248 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890777 139837198389248 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.890811 139837198389248 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890846 139837198389248 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890880 139837198389248 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:40:57.890915 139837198389248 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:40:57.890948 139837198389248 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:40:57.890981 139837198389248 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:40:57.891009 139837198389248 training_loop.py:725] Total parameters: 152072288
I0123 12:40:57.891341 139837198389248 training_loop.py:739] Total state size: 0
I0123 12:40:57.913748 139837198389248 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:40:57.914161 139837198389248 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:40:57.914564 139837198389248 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:40:57.915022 139837198389248 training_loop.py:89] registering functions: dict_keys([])
I0123 12:40:57.938095 139837198389248 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = incenter e c b a; f = on_circle f d a, on_bline f a b; g = on_circle g d b, on_bline g b a; h = foot h c f d; i = mirror i c h; j = lc_tangent j b e, on_line j c e; k = foot k j b a; l = on_pline l e a b, on_line l b c; m = foot m k c e; n = mirror n k m; o = on_line o c n, on_line o e l; p = on_circle p d g, on_line p o g; q = on_line q c p, on_line q e o; r = on_line r c o, on_line r i e; s = on_circle s d f, on_line s q f ? perp f s s g
I0123 12:41:07.338107 139837198389248 ddar.py:60] Depth 1/1000 time = 9.274157047271729
I0123 12:41:07.346599 139837198389248 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D F G P Q S : Points
DA = DB [00]
DB = DC [01]
DF = DA [02]
FA = FB [03]
DG = DB [04]
GB = GA [05]
DP = DG [06]
Q,F,S are collinear [07]
DS = DF [08]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. DG = DB [04] & DA = DB [00] & DF = DA [02] & DS = DF [08] & DP = DG [06] & DB = DC [01]   S,G,F,A are concyclic [09]
002. DG = DB [04] & DA = DB [00] & DF = DA [02] & DS = DF [08] & DP = DG [06] & DB = DC [01]   S,G,B,F are concyclic [10]
003. S,G,F,A are concyclic [09]   SGA = SFA [11]
004. FA = FB [03] & GB = GA [05] (SSS)  FBG = GAF [12]
005. S,G,B,F are concyclic [10]   SGB = SFB [13]
006. Q,F,S are collinear [07] & SGA = SFA [11] & FBG = GAF [12] & SGB = SFB [13]   FS  SG
==========================

