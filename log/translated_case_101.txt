I0123 22:37:58.399374 139953274175488 inference_utils.py:69] Parsing gin configuration.
I0123 22:37:58.399479 139953274175488 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 22:37:58.399680 139953274175488 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 22:37:58.399714 139953274175488 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 22:37:58.399744 139953274175488 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 22:37:58.399772 139953274175488 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 22:37:58.399803 139953274175488 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 22:37:58.399832 139953274175488 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 22:37:58.399861 139953274175488 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 22:37:58.399888 139953274175488 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 22:37:58.399914 139953274175488 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 22:37:58.399941 139953274175488 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 22:37:58.399986 139953274175488 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 22:37:58.400120 139953274175488 resource_reader.py:55] Path not found: base_htrans.gin
I0123 22:37:58.400324 139953274175488 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 22:37:58.400426 139953274175488 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 22:37:58.406723 139953274175488 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 22:37:58.406845 139953274175488 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 22:37:58.407169 139953274175488 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 22:37:58.407277 139953274175488 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 22:37:58.407552 139953274175488 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 22:37:58.407655 139953274175488 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 22:37:58.408062 139953274175488 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 22:37:58.408165 139953274175488 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 22:37:58.411725 139953274175488 training_loop.py:334] ==== Training loop: initializing model ====
I0123 22:37:58.503721 139953274175488 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 22:37:58.504499 139953274175488 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 22:37:58.511269 139953274175488 training_loop.py:335] Process 0 of 1
I0123 22:37:58.511331 139953274175488 training_loop.py:336] Local device count = 1
I0123 22:37:58.511374 139953274175488 training_loop.py:337] Number of replicas = 1
I0123 22:37:58.511407 139953274175488 training_loop.py:339] Using random number seed 42
I0123 22:37:58.969045 139953274175488 training_loop.py:359] Initializing the model.
I0123 22:37:59.400607 139953274175488 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.400844 139953274175488 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 22:37:59.400947 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401028 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401107 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401195 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401269 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401355 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401427 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401498 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401570 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401647 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401721 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401793 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:37:59.401834 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.401880 139953274175488 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:37:59.401996 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.402040 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.402072 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.404263 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.409542 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.420147 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.420423 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.424710 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.435190 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.435254 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.435294 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.435328 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.435394 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.436564 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.436647 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.437355 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.439785 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.445862 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.447159 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.447244 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.447282 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.447347 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.447478 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.447811 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.447861 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.449765 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.449870 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.452739 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.452825 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.453315 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.463347 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.472047 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.472152 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.472451 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.472536 139953274175488 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:37:59.472648 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.472689 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.472723 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.474561 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.477025 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.482525 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.482792 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.485394 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.489155 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.489217 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.489255 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.489287 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.489352 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.489918 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.490000 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.490364 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.491126 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.493583 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.494215 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.494297 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.494333 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.494391 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.494520 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.494847 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.494895 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.496820 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.496919 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.499398 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.499483 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.499907 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.502189 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.504075 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.504175 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.504465 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.504550 139953274175488 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:37:59.504658 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.504699 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.504732 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.506967 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.509325 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.514817 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.515084 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.517814 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.521618 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.521686 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.521724 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.521756 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.521821 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.522380 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.522460 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.522816 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.523588 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.526059 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.526725 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.526808 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.526845 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.526903 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.527032 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.527361 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.527410 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.529280 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.529378 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.531845 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.531936 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.532416 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.534684 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.536608 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.536711 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.537002 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.537087 139953274175488 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:37:59.537199 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.537243 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.537275 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.539165 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.541546 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.547129 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.547396 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.550018 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.553796 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.553857 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.553899 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.553934 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.553997 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.554561 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.554641 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.554998 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.555775 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.558318 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.558948 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.559031 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.559068 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.559128 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.559264 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.559596 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.559644 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.561533 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.561634 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.564178 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.564270 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.564695 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.566968 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.568868 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.568967 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.569259 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.569345 139953274175488 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:37:59.569457 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.569501 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.569534 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.572131 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.574646 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.580296 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.580572 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.583578 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.587416 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.587479 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.587517 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.587550 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.587615 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.588191 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.588273 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.588637 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.589416 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.591971 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.592603 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.592685 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.592722 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.592784 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.592922 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.593249 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.593297 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.595207 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.595308 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.597861 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.597947 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.598382 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.600650 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.602612 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.602713 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.603010 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.603098 139953274175488 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:37:59.603212 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.603257 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.603289 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.605105 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.607488 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.613127 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.613388 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.616132 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.619874 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.619935 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.619973 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.620005 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.620070 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.620680 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.620765 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.621131 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.621923 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.624424 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.625050 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.625133 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.625170 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.625231 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.625363 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.625704 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.625752 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.627648 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.627747 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.630310 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.630396 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.630827 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.633147 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.635079 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.635180 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.635475 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.635561 139953274175488 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:37:59.635672 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.635717 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.635749 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.637585 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.640034 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.645626 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.645899 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.648519 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.652321 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.652383 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.652421 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.652452 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.652516 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.653090 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.653175 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.653531 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.654309 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.656769 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.657391 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.657476 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.657511 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.657570 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.657711 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.658036 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.658084 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.660337 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.660437 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.662905 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.662993 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.663425 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.800758 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.802990 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.803149 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.803473 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.803570 139953274175488 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:37:59.803688 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.803734 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.803768 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.805824 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.808346 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.814192 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.814476 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.817200 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.821191 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.821255 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.821296 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.821330 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.821400 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.822023 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.822106 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.822480 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.823282 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.825946 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.826593 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.826678 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.826717 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.826779 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.826912 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.827252 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.827301 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.829234 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.829334 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.831891 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.831977 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.832470 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.834811 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.836750 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.836862 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.837159 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.837247 139953274175488 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:37:59.837363 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.837406 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.837440 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.839373 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.841768 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.847487 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.847757 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.850466 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.854287 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.854349 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.854390 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.854424 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.854490 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.855063 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.855145 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.855511 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.856298 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.858906 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.859530 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.859615 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.859652 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.859714 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.859849 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.860180 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.860229 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.862176 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.862280 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.864856 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.864944 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.865388 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.867728 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.869839 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.869943 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.870246 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.870342 139953274175488 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:37:59.870460 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.870506 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.870540 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.872462 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.874877 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.880934 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.881208 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.883964 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.887909 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.887972 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.888011 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.888045 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.888115 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.888691 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.888776 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.889146 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.889990 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.892538 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.893177 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.893261 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.893298 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.893359 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.893496 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.893837 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.893887 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.895819 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.895918 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.898502 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.898593 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.899033 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.901338 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.903370 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.903472 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.903768 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.903864 139953274175488 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:37:59.903981 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.904027 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.904061 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.905918 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.908389 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.914033 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.914308 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.916981 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.920774 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.920836 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.920875 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.920909 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.921019 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.921584 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.921673 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.922050 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.922836 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.925364 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.926006 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.926090 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.926127 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.926192 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.926329 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.926658 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.926707 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.928696 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.928796 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.931546 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.931633 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.932070 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.934422 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.936343 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.936443 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.936742 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.936828 139953274175488 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:37:59.936950 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:37:59.936995 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:37:59.937029 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:37:59.938875 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.941337 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:37:59.946999 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.947271 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:37:59.949936 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:37:59.954100 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:37:59.954162 139953274175488 attention.py:418] Single window, no scan.
I0123 22:37:59.954201 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:37:59.954235 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.954305 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.954870 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.954953 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.955318 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.956104 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.958632 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.959271 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.959355 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:37:59.959392 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:37:59.959455 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.959597 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:37:59.959928 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:37:59.959977 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.961965 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.962070 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.964597 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.964688 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:37:59.965131 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:37:59.967470 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:37:59.969405 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.969508 139953274175488 nn_components.py:261] mlp: residual
I0123 22:37:59.969814 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:37:59.970108 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970184 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970254 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970315 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970373 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970430 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970486 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970542 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970597 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970652 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970705 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970760 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:37:59.970800 139953274175488 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:37:59.974349 139953274175488 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:38:00.022411 139953274175488 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.022501 139953274175488 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:38:00.022558 139953274175488 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:38:00.022665 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.022710 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.022742 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.022807 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.025242 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.030719 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.030986 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.033632 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.050006 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.050070 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.050107 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.050139 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.050203 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.051317 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.051400 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.052096 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.054090 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.059165 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.060466 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.060560 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.060600 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.060662 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.060799 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.060916 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.060961 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.062861 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.062962 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.065362 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.065448 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.065558 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.067785 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.069738 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.069840 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.070130 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.070218 139953274175488 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:38:00.070327 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.070369 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.070402 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.070468 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.072708 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.078132 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.078396 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.081058 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.094123 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.094186 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.094224 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.094256 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.094321 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.094882 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.094965 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.095327 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.096026 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.098536 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.099154 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.099236 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.099280 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.099340 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.099482 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.099598 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.099642 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.101548 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.101655 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.104042 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.104128 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.104240 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.106474 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.108400 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.108501 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.108791 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.108878 139953274175488 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:38:00.108988 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.109030 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.109063 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.109127 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.111360 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.116799 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.117063 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.119746 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.132352 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.132414 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.132453 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.132485 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.132550 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.133114 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.133195 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.133557 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.134276 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.136846 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.137477 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.137560 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.137596 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.137668 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.137803 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.137918 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.137962 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.139883 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.139982 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.142388 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.142477 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.142589 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.144795 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.146731 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.146832 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.147119 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.147206 139953274175488 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:38:00.147317 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.147361 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.147393 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.147459 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.149724 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.155193 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.155458 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.158342 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.171198 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.171261 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.171301 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.171334 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.171402 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.171958 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.172039 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.172396 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.173097 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.175553 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.176176 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.176259 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.176295 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.176354 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.176496 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.176611 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.176655 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.178869 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.178970 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.181378 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.181465 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.181576 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.183803 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.185673 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.185775 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.186065 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.186152 139953274175488 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:38:00.186264 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.186308 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.186342 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.186405 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.188700 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.194143 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.194415 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.197014 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.209629 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.209697 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.209735 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.209767 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.209833 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.210397 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.210478 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.210839 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.211539 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.214060 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.214687 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.214773 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.214810 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.214870 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.215015 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.215132 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.215181 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.217055 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.217154 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.219552 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.219637 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.219746 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.222015 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.223874 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.223973 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.224258 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.224343 139953274175488 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:38:00.224454 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.224499 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.224530 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.224594 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.229931 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.235566 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.235852 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.238633 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.251519 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.251584 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.251625 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.251657 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.251721 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.252316 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.252400 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.252772 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.253483 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.256064 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.256701 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.256785 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.256822 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.256888 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.257025 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.257157 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.257203 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.259173 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.259276 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.261744 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.261831 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.261945 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.264431 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.266343 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.266447 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.266736 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.266825 139953274175488 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:38:00.266942 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.266990 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.267024 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.267089 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.269341 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.274919 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.275186 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.277809 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.290930 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.290992 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.291029 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.291060 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.291126 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.291693 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.291774 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.292134 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.292827 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.295334 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.296012 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.296094 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.296131 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.296190 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.296325 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.296440 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.296491 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.298384 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.298484 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.300880 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.300966 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.301079 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.303293 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.305251 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.305352 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.305649 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.305740 139953274175488 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:38:00.305855 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.305900 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.305933 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.305999 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.308244 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.313749 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.314028 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.316726 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.329470 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.329533 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.329572 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.329604 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.329690 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.330300 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.330382 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.330741 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.331439 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.333900 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.334534 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.334617 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.334660 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.334723 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.334857 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.334969 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.335021 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.336896 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.336996 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.339445 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.339536 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.339648 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.341866 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.343727 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.343829 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.344115 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.344200 139953274175488 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:38:00.344311 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.344355 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.344389 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.344454 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.346690 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.352197 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.352459 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.355062 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.367739 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.367802 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.367840 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.367873 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.367938 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.368496 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.368578 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.368937 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.369631 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.372122 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.372799 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.372884 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.372921 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.372981 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.373114 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.373228 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.373273 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.375171 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.375271 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.377660 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.377749 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.377862 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.380079 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.382009 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.382109 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.382396 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.382484 139953274175488 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:38:00.382597 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.382642 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.382676 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.382741 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.384981 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.390421 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.390683 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.393668 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.406220 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.406282 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.406320 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.406352 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.406416 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.407027 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.407111 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.407472 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.408181 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.410653 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.411277 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.411360 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.411396 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.411454 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.411592 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.411707 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.411752 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.413618 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.413731 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.416167 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.416255 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.416366 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.418590 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.420448 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.420548 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.420836 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.420923 139953274175488 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:38:00.421034 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.421078 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.421111 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.421174 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.423406 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.428873 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.429136 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.431745 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.444297 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.444358 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.444400 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.444433 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.444497 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.445055 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.445136 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.445489 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.446185 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.448637 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.449301 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.449384 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.449419 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.449477 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.449608 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.449727 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.449771 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.451629 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.451734 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.454123 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.454209 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.454317 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.456500 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.458411 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.458512 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.458795 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.458881 139953274175488 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:38:00.458990 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.459034 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.459066 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.459128 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.461334 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.466723 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.466989 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.469591 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.482148 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.482210 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.482247 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.482279 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.482344 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.482906 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.482987 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.483349 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.484050 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.486599 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.487224 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.487308 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.487344 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.487404 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.487540 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.487657 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.487701 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.489559 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.489669 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.492065 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.492151 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.492259 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.495063 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.496924 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.497025 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.497312 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.497406 139953274175488 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:38:00.500270 139953274175488 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:38:00.555507 139953274175488 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.555599 139953274175488 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:38:00.555655 139953274175488 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:38:00.555766 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.555810 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.555843 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.555907 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.558246 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.563610 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.563874 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.566626 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.578956 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.579018 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.579057 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.579089 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.579154 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.579705 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.579786 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.580142 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.580827 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.583322 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.583939 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.584022 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.584059 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.584118 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.584250 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.584373 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.584418 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.586292 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.586393 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.588772 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.588857 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.588969 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.591225 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.593068 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.593169 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.593458 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.593546 139953274175488 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:38:00.593663 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.593705 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.593739 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.593803 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.596200 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.601557 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.601838 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.604458 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.616662 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.616725 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.616765 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.616797 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.616863 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.617416 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.617498 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.617861 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.618543 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.621015 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.621631 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.621722 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.621760 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.621819 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.621948 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.622061 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.622111 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.623934 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.624032 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.626434 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.626523 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.626639 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.628916 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.630801 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.630908 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.631211 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.631319 139953274175488 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:38:00.631443 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.631489 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.631522 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.631587 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.633811 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.639130 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.639394 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.642028 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.654135 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.654197 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.654235 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.654268 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.654333 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.654884 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.654966 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.655318 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.655988 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.658914 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.659534 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.659618 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.659655 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.659714 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.659843 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.659956 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.660000 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.661840 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.661941 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.664286 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.664372 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.664482 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.666730 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.668568 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.668669 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.668958 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.669045 139953274175488 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:38:00.669156 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.669200 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.669232 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.669295 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.671516 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.676820 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.677083 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.679715 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.691920 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.691982 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.692023 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.692063 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.692129 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.692689 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.692770 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.693126 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.693814 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.696303 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.696922 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.697004 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.697040 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.697100 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.697229 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.697342 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.697387 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.699232 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.699331 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.701704 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.701787 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.701896 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.704160 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.706010 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.706110 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.706393 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.706477 139953274175488 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:38:00.706584 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.706628 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.706659 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.706722 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.708925 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.714253 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.714513 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.717152 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.729524 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.729584 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.729621 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.729659 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.729725 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.730278 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.730358 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.730713 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.731390 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.733898 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.734515 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.734596 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.734632 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.734689 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.734820 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.734933 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.734976 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.736812 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.736918 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.739300 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.739384 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.739494 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.741760 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.743597 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.743696 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.743979 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.744065 139953274175488 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:38:00.744175 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.744219 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.744252 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.744315 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.746531 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.751864 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.752125 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.754785 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.767087 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.767147 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.767182 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.767214 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.767277 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.767833 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.767914 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.768266 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.768944 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.771882 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.772508 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.772591 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.772626 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.772684 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.772818 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.772931 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.772974 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.774828 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.774933 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.777315 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.777398 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.777505 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.779779 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.781654 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.781754 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.782041 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.782128 139953274175488 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:38:00.782237 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.782280 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.782312 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.782375 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.784574 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.789931 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.790190 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.792822 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.805269 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.805330 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.805366 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.805397 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.805462 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.806022 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.806102 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.806455 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.807139 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.809637 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.810260 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.810343 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.810378 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.810436 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.810566 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.810676 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.810719 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.812563 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.812659 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.815033 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.815117 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.815225 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.817484 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.819342 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.819443 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.819732 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.819817 139953274175488 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:38:00.819926 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.819970 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.820001 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.820064 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.822286 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.827656 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.827918 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.830582 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.843009 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.843070 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.843106 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.843137 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.843201 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.843754 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.843833 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.844192 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.844871 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.847359 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.847978 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.848058 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.848094 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.848155 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.848288 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.848401 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.848444 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.850300 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.850398 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.852745 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.852835 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.852945 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.855179 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.857010 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.857108 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.857390 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.857475 139953274175488 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:38:00.857582 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.857623 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.857661 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.857725 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.859913 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.865283 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.865543 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.868179 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.880564 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.880624 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.880661 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.880692 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.880757 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.881317 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.881396 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.881752 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.882442 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.885316 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.885951 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.886034 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.886069 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.886126 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.886257 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.886370 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.886413 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.888263 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.888360 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.890718 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.890810 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.890923 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.893177 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.895030 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.895130 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.895413 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.895499 139953274175488 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:38:00.895606 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.895649 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.895680 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.895742 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.897960 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.903340 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.903602 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.906267 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.918663 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.918723 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.918759 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.918790 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.918854 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.919412 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.919491 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.919847 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.920534 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.923041 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.923670 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.923751 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.923787 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.923843 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.923973 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.924084 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.924126 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.926453 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.926553 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.928908 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.928990 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.929105 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.931346 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.933168 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.933265 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.933550 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.933635 139953274175488 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:38:00.933751 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.933794 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.933824 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.933885 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.936089 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.941447 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.941717 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.944371 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.956727 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.956787 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.956824 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.956855 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.956919 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.957474 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.957556 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.957917 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.958607 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.961135 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.961759 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.961842 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.961877 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.961933 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.962063 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:00.962175 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:00.962220 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.964080 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.964177 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.966538 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.966622 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:00.966730 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:00.968995 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:00.970835 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.970934 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:00.971217 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.971302 139953274175488 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:38:00.971410 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:38:00.971454 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:38:00.971486 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:38:00.971548 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.973769 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:38:00.979093 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.979356 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:38:00.982011 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:38:00.994343 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:38:00.994403 139953274175488 attention.py:418] Single window, no scan.
I0123 22:38:00.994440 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:38:00.994471 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.994534 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.995095 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.995176 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.995532 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.996224 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.999086 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.999710 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:00.999790 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:38:00.999825 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:38:00.999882 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:01.000010 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:38:01.000128 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:38:01.000170 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:01.002022 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:01.002119 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:01.004471 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:01.004555 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:38:01.004672 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:38:01.006941 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:38:01.008810 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:01.008909 139953274175488 nn_components.py:261] mlp: residual
I0123 22:38:01.009194 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:01.009282 139953274175488 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:38:01.012076 139953274175488 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:38:05.387457 139953274175488 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 22:38:05.898739 139953274175488 training_loop.py:409] No working directory specified.
I0123 22:38:05.898857 139953274175488 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 22:38:05.899586 139953274175488 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 22:38:08.825730 139953274175488 training_loop.py:447] Only restoring trainable parameters.
I0123 22:38:08.826435 139953274175488 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 22:38:08.826500 139953274175488 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.826548 139953274175488 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.826592 139953274175488 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.826637 139953274175488 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.826681 139953274175488 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.826721 139953274175488 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.826762 139953274175488 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.826801 139953274175488 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.826840 139953274175488 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.826880 139953274175488 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.826919 139953274175488 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.826962 139953274175488 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.827003 139953274175488 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.827044 139953274175488 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827083 139953274175488 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.827123 139953274175488 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827164 139953274175488 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827203 139953274175488 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.827242 139953274175488 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.827298 139953274175488 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827340 139953274175488 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.827379 139953274175488 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.827417 139953274175488 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.827456 139953274175488 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827494 139953274175488 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.827534 139953274175488 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827573 139953274175488 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827613 139953274175488 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.827653 139953274175488 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.827690 139953274175488 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827730 139953274175488 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.827769 139953274175488 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.827809 139953274175488 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.827847 139953274175488 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827886 139953274175488 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.827924 139953274175488 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.827962 139953274175488 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828001 139953274175488 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.828040 139953274175488 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.828079 139953274175488 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828117 139953274175488 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.828156 139953274175488 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.828194 139953274175488 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.828233 139953274175488 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828270 139953274175488 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.828317 139953274175488 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828359 139953274175488 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828398 139953274175488 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.828438 139953274175488 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.828475 139953274175488 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828514 139953274175488 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.828552 139953274175488 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.828591 139953274175488 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.828630 139953274175488 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828669 139953274175488 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.828708 139953274175488 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828745 139953274175488 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828784 139953274175488 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.828821 139953274175488 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.828860 139953274175488 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.828898 139953274175488 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.828938 139953274175488 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.828977 139953274175488 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.829015 139953274175488 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.829055 139953274175488 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.829093 139953274175488 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.829133 139953274175488 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.829170 139953274175488 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.829209 139953274175488 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.829247 139953274175488 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.829286 139953274175488 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.829325 139953274175488 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.829371 139953274175488 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.829614 139953274175488 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.829664 139953274175488 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.829705 139953274175488 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.829744 139953274175488 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.829782 139953274175488 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.829821 139953274175488 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.829861 139953274175488 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.829898 139953274175488 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.829937 139953274175488 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.829975 139953274175488 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.830014 139953274175488 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830051 139953274175488 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.830091 139953274175488 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830128 139953274175488 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830168 139953274175488 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.830207 139953274175488 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.830244 139953274175488 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830285 139953274175488 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.830322 139953274175488 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.830361 139953274175488 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.830399 139953274175488 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830438 139953274175488 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.830476 139953274175488 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830515 139953274175488 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830556 139953274175488 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.830593 139953274175488 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.830640 139953274175488 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830680 139953274175488 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.830721 139953274175488 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.830759 139953274175488 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.830798 139953274175488 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830838 139953274175488 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.830876 139953274175488 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830916 139953274175488 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.830953 139953274175488 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.830993 139953274175488 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.831030 139953274175488 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.831070 139953274175488 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.831108 139953274175488 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:38:08.831148 139953274175488 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:38:08.831188 139953274175488 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.831226 139953274175488 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.831266 139953274175488 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.831305 139953274175488 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.831345 139953274175488 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:38:08.831382 139953274175488 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:38:08.831422 139953274175488 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:38:08.831460 139953274175488 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:38:08.831492 139953274175488 training_loop.py:725] Total parameters: 152072288
I0123 22:38:08.831736 139953274175488 training_loop.py:739] Total state size: 0
I0123 22:38:08.854534 139953274175488 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 22:38:08.854816 139953274175488 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 22:38:08.855191 139953274175488 training_loop.py:652] Compiling mode beam_search with jit.
I0123 22:38:08.855542 139953274175488 training_loop.py:89] registering functions: dict_keys([])
I0123 22:38:08.872719 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g ? perp d c c n
I0123 22:38:09.997119 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0797970294952393
I0123 22:38:13.197203 139953274175488 ddar.py:60] Depth 2/1000 time = 3.1999013423919678
I0123 22:38:19.578779 139953274175488 ddar.py:60] Depth 3/1000 time = 6.381383180618286
I0123 22:38:29.355114 139953274175488 ddar.py:60] Depth 4/1000 time = 9.77611780166626
I0123 22:38:39.263566 139953274175488 ddar.py:60] Depth 5/1000 time = 9.908161878585815
I0123 22:38:49.330967 139953274175488 ddar.py:60] Depth 6/1000 time = 10.067026853561401
I0123 22:38:59.490427 139953274175488 ddar.py:60] Depth 7/1000 time = 10.1377592086792
I0123 22:39:09.964463 139953274175488 ddar.py:60] Depth 8/1000 time = 10.47370958328247
I0123 22:39:20.401740 139953274175488 ddar.py:60] Depth 9/1000 time = 10.266194820404053
I0123 22:39:30.878883 139953274175488 ddar.py:60] Depth 10/1000 time = 10.468810319900513
I0123 22:39:30.879301 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:39:30.879429 139953274175488 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 22:39:30.879473 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00
I0123 22:39:30.879508 139953274175488 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00
I0123 22:39:31.027573 139953274175488 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.027762 139953274175488 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 22:39:31.027872 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.027950 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028025 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028097 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028168 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028237 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028307 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028376 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028443 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028512 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028581 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028648 139953274175488 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:39:31.028692 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.028750 139953274175488 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:39:31.028863 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.028903 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.028936 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.030819 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.033297 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.039046 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.039323 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.041951 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.045984 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.046047 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.046086 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.046120 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.046187 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.046829 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.046910 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.047281 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.048078 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.050652 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.051293 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.051375 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.051411 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.051473 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.051605 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.051936 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.051985 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.053967 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.054066 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.056512 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.056595 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.057018 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.059350 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.061264 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.061363 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.061655 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.061741 139953274175488 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:39:31.061848 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.061896 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.061930 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.063750 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.066029 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.071483 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.071742 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.074311 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.077899 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.077957 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.077993 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.078023 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.078086 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.078630 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.078708 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.079059 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.079813 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.082244 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.083322 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.083405 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.083440 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.083497 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.083624 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.083945 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.083992 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.085875 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.085972 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.088364 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.088447 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.088868 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.091153 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.093037 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.093136 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.093427 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.093512 139953274175488 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:39:31.093619 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.093667 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.093709 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.095468 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.097753 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.103569 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.103831 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.106374 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.110016 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.110076 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.110112 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.110143 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.110207 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.110810 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.110892 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.111247 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.112009 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.114422 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.115033 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.115114 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.115148 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.115204 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.115335 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.115654 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.115701 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.117656 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.117753 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.120150 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.120233 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.120653 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.122886 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.124772 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.124871 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.125155 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.125240 139953274175488 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:39:31.125348 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.125390 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.125421 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.127286 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.129579 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.135192 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.135460 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.138012 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.141646 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.141706 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.141743 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.141775 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.141839 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.142387 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.142469 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.142820 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.143574 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.145976 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.146585 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.146667 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.146702 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.146760 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.146891 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.147254 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.147302 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.149170 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.149266 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.151658 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.151741 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.152165 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.154535 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.156499 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.156597 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.156885 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.156970 139953274175488 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:39:31.157077 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.157120 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.157152 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.158922 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.161226 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.166817 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.167076 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.169620 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.173261 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.173320 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.173357 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.173387 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.173453 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.174057 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.174138 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.174489 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.175254 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.177685 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.178300 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.178382 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.178417 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.178474 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.178605 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.178919 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.178965 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.180915 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.181011 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.183406 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.183490 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.183912 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.186169 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.188053 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.188151 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.188437 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.188522 139953274175488 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:39:31.188631 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.188675 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.188707 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.190884 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.193172 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.198721 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.198980 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.201560 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.205178 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.205238 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.205274 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.205305 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.205368 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.205919 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.206000 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.206348 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.207095 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.209489 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.210156 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.210238 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.210273 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.210330 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.210489 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.210805 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.210851 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.212739 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.212835 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.215237 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.215320 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.215734 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.218029 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.219928 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.220026 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.220317 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.220402 139953274175488 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:39:31.220510 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.220554 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.220586 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.222362 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.224664 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.230285 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.230547 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.233046 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.236649 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.236710 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.236746 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.236778 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.236841 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.237439 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.237521 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.237878 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.238640 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.241042 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.241661 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.241745 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.241780 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.241838 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.241972 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.242286 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.242333 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.244274 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.244372 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.246779 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.246864 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.247285 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.249483 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.251372 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.251472 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.251759 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.251843 139953274175488 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:39:31.251951 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.251995 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.252026 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.253935 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.256200 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.261700 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.261968 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.264539 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.268172 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.268232 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.268269 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.268300 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.268365 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.268917 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.268998 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.269347 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.270107 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.272526 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.273192 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.273275 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.273310 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.273367 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.273499 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.273829 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.273877 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.275769 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.275865 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.278296 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.278379 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.278803 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.281096 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.282998 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.283097 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.283384 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.283468 139953274175488 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:39:31.283576 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.283619 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.283651 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.285426 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.287895 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.293463 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.293728 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.296247 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.299830 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.299890 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.299926 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.299957 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.300020 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.300945 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.301027 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.301386 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.302157 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.304584 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.305193 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.305274 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.305309 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.305365 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.305495 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.305820 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.305867 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.307727 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.307823 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.310297 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.310381 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.310799 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.313042 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.314939 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.315038 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.315325 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.315408 139953274175488 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:39:31.315517 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.315559 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.315590 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.317336 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.319695 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.325205 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.325459 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.327988 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.331587 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.331646 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.331682 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.331713 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.331775 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.332374 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.332453 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.332801 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.333548 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.335999 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.336608 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.336689 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.336724 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.336781 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.336910 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.337229 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.337274 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.339153 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.339253 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.341720 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.341803 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.342221 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.344446 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.346341 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.346440 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.346726 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.346810 139953274175488 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:39:31.346918 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.346960 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.346991 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.348751 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.351117 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.356622 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.356881 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.359405 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.362997 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.363065 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.363102 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.363134 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.363199 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.363804 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.363886 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.364243 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.365009 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.367446 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.368055 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.368138 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.368173 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.368231 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.368363 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.368683 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.368730 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.370613 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.370709 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.373159 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.373244 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.373675 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.375888 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.377776 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.377905 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.378227 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.378325 139953274175488 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:39:31.378438 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.378482 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.378515 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.380335 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.382724 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.388573 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.388836 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.391378 139953274175488 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:31.395005 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.395064 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.395110 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.395147 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.395212 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.395822 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.395903 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.396258 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.397023 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.399458 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.400073 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.400154 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.400190 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.400249 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.400382 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.400700 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.400746 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.402619 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.402715 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.405175 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.405258 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.405680 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.407911 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.409777 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.409875 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.410160 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.410406 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410477 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410535 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410590 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410645 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410699 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410751 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410804 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410857 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410911 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.410972 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.411027 139953274175488 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:39:31.411065 139953274175488 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:39:31.413930 139953274175488 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:31.458032 139953274175488 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.458121 139953274175488 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:39:31.458174 139953274175488 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:39:31.458276 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.458318 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.458348 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.458407 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.460705 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.465933 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.466188 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.468707 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.481301 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.481362 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.481398 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.481430 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.481493 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.482059 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.482137 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.482493 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.483179 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.485992 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.486614 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.486697 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.486733 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.486973 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.487102 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.487207 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.487247 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.489121 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.489219 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.491567 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.491661 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.491774 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.494001 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.495817 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.495917 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.496206 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.496292 139953274175488 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:39:31.496402 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.496443 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.496473 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.496533 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.498732 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.504029 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.504291 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.506910 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.519179 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.519239 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.519276 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.519307 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.519369 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.519914 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.519994 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.520348 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.521082 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.523521 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.524144 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.524226 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.524261 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.524319 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.524446 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.524555 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.524595 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.526413 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.526510 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.528854 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.528938 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.529055 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.531280 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.533112 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.533211 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.533506 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.533593 139953274175488 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:39:31.533713 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.533755 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.533787 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.533850 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.536077 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.541398 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.541670 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.544312 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.556610 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.556671 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.556708 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.556740 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.556801 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.557352 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.557433 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.557802 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.558539 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.560971 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.561588 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.561678 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.561715 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.561776 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.561903 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.562012 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.562052 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.563888 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.563986 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.566399 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.566484 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.566596 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.568872 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.570758 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.570858 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.571151 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.571237 139953274175488 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:39:31.571348 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.571389 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.571420 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.571482 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.573714 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.579084 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.579357 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.582034 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.594533 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.594595 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.594632 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.594664 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.594726 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.595273 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.595354 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.595711 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.596841 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.599298 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.599916 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.599999 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.600036 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.600096 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.600228 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.600341 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.600383 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.602245 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.602344 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.604753 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.604837 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.604948 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.607234 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.609089 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.609189 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.609481 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.609568 139953274175488 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:39:31.609687 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.609729 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.609761 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.609824 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.612057 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.617404 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.617681 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.620331 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.632707 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.632770 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.632807 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.632840 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.632904 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.633454 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.633535 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.633905 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.634645 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.637095 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.637726 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.637811 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.637849 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.637908 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.638040 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.638151 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.638192 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.640049 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.640147 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.642548 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.642634 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.642745 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.644999 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.646859 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.646960 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.647250 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.647337 139953274175488 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:39:31.647447 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.647488 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.647520 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.647581 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.649818 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.655200 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.655463 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.658115 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.670526 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.670587 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.670627 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.670661 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.670725 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.671284 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.671365 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.671727 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.672477 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.674915 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.675534 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.675617 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.675654 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.675713 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.675843 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.675952 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.675991 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.677851 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.677950 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.680340 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.680426 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.680536 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.682784 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.684634 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.684741 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.685032 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.685119 139953274175488 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:39:31.685231 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.685271 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.685303 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.685366 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.687595 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.693004 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.693270 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.695925 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.708225 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.708285 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.708323 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.708356 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.708419 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.708972 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.709053 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.709409 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.710553 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.713009 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.713628 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.713719 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.713755 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.713814 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.713946 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.714055 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.714095 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.715928 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.716026 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.718419 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.718504 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.718615 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.720863 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.722732 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.722841 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.723134 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.723220 139953274175488 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:39:31.723330 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.723371 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.723402 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.723465 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.725709 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.731096 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.731363 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.734013 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.746309 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.746371 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.746408 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.746440 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.746503 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.747056 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.747136 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.747490 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.748224 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.750651 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.751265 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.751346 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.751381 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.751439 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.751568 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.751674 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.751713 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.753518 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.753616 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.755980 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.756064 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.756172 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.758408 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.760268 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.760375 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.760676 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.760763 139953274175488 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:39:31.760874 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.760915 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.760947 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.761009 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.763254 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.768594 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.768856 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.771512 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.783890 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.783952 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.783990 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.784023 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.784085 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.784639 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.784720 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.785079 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.785775 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.788291 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.788918 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.789002 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.789038 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.789097 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.789229 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.789340 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.789380 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.791234 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.791333 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.793747 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.793833 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.793944 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.796211 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.798098 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.798200 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.798495 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.798590 139953274175488 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:39:31.798703 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.798744 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.798776 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.798838 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.801061 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.806419 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.806684 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.809340 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.821720 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.821781 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.821819 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.821851 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.821913 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.822468 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.822549 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.822906 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.823596 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.826491 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.827117 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.827200 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.827236 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.827294 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.827425 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.827535 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.827576 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.829435 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.829532 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.831927 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.832013 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.832123 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.834398 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.836259 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.836359 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.836654 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.836749 139953274175488 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:39:31.836863 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.836904 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.836936 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.837000 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.839232 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.844546 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.844810 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.847445 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.859700 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.859765 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.859803 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.859836 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.859903 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.860491 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.860576 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.860952 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.861689 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.864301 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.864921 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.865004 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.865039 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.865098 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.865227 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.865336 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.865376 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.867285 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.867386 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.869818 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.869902 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.870015 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.872358 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.874208 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.874308 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.874598 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.874685 139953274175488 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:39:31.874805 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.874847 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.874879 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.874942 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.877165 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.882564 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.882828 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.885618 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.898279 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.898342 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.898382 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.898416 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.898482 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.899061 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.899145 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.899524 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.900250 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.902810 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.903460 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.903546 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.903584 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.903657 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.903806 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.903916 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.903955 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.905823 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.905922 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.908396 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.908479 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:31.908589 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:31.910881 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.912789 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.912888 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:31.913180 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.913270 139953274175488 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:39:31.916184 139953274175488 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:31.966684 139953274175488 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.966774 139953274175488 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:39:31.966829 139953274175488 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:39:31.966936 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:31.966978 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:31.967010 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:31.967074 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.969361 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:31.974866 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.975131 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:31.977715 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:31.990368 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:31.990429 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:31.990467 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:31.990500 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.990563 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.991121 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.991203 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.991564 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.992250 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.994673 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.995288 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.995371 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:31.995407 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:31.995468 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.995598 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:31.995710 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:31.995750 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:31.997656 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:31.997754 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.000132 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.000217 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.000328 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.002505 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.004335 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.004443 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.004746 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.004833 139953274175488 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:39:32.004943 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.004983 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.005015 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.005079 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.007308 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.013170 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.013436 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.016009 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.028367 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.028429 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.028470 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.028503 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.028566 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.029118 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.029199 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.029554 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.030245 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.032665 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.033283 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.033366 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.033403 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.033464 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.033592 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.033710 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.033753 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.035653 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.035751 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.038142 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.038226 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.038336 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.040519 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.042368 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.042477 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.042770 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.042856 139953274175488 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:39:32.042966 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.043008 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.043039 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.043102 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.045310 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.050784 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.051052 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.053626 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.066059 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.066122 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.066163 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.066196 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.066258 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.066819 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.066901 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.067261 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.067952 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.070413 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.071029 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.071112 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.071148 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.071209 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.071336 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.071446 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.071486 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.073372 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.073470 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.075855 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.075942 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.076052 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.078240 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.080090 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.080190 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.080493 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.080580 139953274175488 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:39:32.080689 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.080730 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.080762 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.080824 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.083047 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.088491 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.088758 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.091320 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.103681 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.103742 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.103780 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.103813 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.103874 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.104424 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.104505 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.104867 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.105557 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.108001 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.108615 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.108697 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.108733 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.108793 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.108923 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.109033 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.109075 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.110988 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.111089 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.113465 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.113549 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.113667 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.115839 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.117686 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.117788 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.118080 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.118176 139953274175488 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:39:32.118288 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.118330 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.118363 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.118426 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.120646 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.126556 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.126821 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.129386 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.141686 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.141748 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.141786 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.141819 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.141880 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.142436 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.142518 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.142874 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.143563 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.146021 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.146640 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.146723 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.146760 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.146820 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.146952 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.147062 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.147103 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.149017 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.149117 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.151576 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.151678 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.151801 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.153982 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.155833 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.155933 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.156224 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.156318 139953274175488 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:39:32.156430 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.156471 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.156502 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.156564 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.158791 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.164229 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.164490 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.167073 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.179373 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.179434 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.179471 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.179504 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.179567 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.180119 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.180201 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.180558 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.181241 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.183705 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.184322 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.184405 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.184441 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.184500 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.184632 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.184741 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.184782 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.186694 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.186793 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.189170 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.189254 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.189364 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.191536 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.193367 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.193466 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.193765 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.193853 139953274175488 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:39:32.193971 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.194013 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.194045 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.194107 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.196311 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.201714 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.201982 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.204529 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.216739 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.216798 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.216835 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.216868 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.216931 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.217484 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.217567 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.217935 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.218621 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.221040 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.221664 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.221747 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.221785 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.221845 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.221977 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.222086 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.222127 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.224008 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.224107 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.226475 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.226560 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.226670 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.228835 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.230675 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.230779 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.231069 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.231157 139953274175488 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:39:32.231265 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.231316 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.231351 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.231414 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.233633 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.239371 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.239634 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.242169 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.254284 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.254345 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.254381 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.254412 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.254474 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.255023 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.255104 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.255455 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.256129 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.258561 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.259177 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.259259 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.259295 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.259353 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.259482 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.259590 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.259630 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.261501 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.261597 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.263927 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.264011 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.264117 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.266242 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.268054 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.268152 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.268438 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.268521 139953274175488 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:39:32.268629 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.268669 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.268708 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.268771 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.270950 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.276294 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.276553 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.279083 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.291206 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.291268 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.291305 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.291335 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.291394 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.291942 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.292021 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.292371 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.293040 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.295417 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.296024 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.296104 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.296139 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.296196 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.296322 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.296428 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.296468 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.298340 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.298437 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.300759 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.300842 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.300949 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.303101 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.304906 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.305003 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.305291 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.305376 139953274175488 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:39:32.305482 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.305521 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.305562 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.305625 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.307814 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.313163 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.313423 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.315938 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.328270 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.328331 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.328369 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.328401 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.328462 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.329015 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.329097 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.329454 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.330149 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.332565 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.333182 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.333263 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.333299 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.333358 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.333485 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.333594 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.333634 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.335548 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.335645 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.338020 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.338104 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.338215 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.340396 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.342238 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.342338 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.342627 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.342712 139953274175488 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:39:32.342820 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.342860 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.342892 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.342965 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.345182 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.351010 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.351276 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.353862 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.564371 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.564516 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.564557 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.564591 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.564675 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.565311 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.565392 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.565769 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.566500 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.569161 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.569803 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.569886 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.569922 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.569986 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.570116 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.570234 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.570275 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.572247 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.572345 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.574819 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.574903 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.575014 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.577318 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.579214 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.579314 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.579611 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.579699 139953274175488 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:39:32.579810 139953274175488 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:32.579852 139953274175488 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:32.579884 139953274175488 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:32.579950 139953274175488 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.582234 139953274175488 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:32.587796 139953274175488 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.588064 139953274175488 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:32.590687 139953274175488 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:32.603295 139953274175488 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:32.603356 139953274175488 attention.py:418] Single window, no scan.
I0123 22:39:32.603395 139953274175488 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:32.603429 139953274175488 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.603490 139953274175488 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.604052 139953274175488 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.604133 139953274175488 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.604493 139953274175488 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.605184 139953274175488 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.607758 139953274175488 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.608380 139953274175488 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.608461 139953274175488 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:32.608497 139953274175488 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:32.608558 139953274175488 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.608686 139953274175488 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:32.608798 139953274175488 nn_components.py:325] mlp: activation = None
I0123 22:39:32.608837 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.610722 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.610819 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.613242 139953274175488 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.613325 139953274175488 transformer_base.py:443] tbase: final FFN
I0123 22:39:32.613437 139953274175488 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:32.615728 139953274175488 nn_components.py:329] mlp: final activation = None
I0123 22:39:32.617589 139953274175488 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.617697 139953274175488 nn_components.py:261] mlp: residual
I0123 22:39:32.617990 139953274175488 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:32.618083 139953274175488 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:39:32.620918 139953274175488 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:44.885922 139953274175488 alphageometry.py:566] LM output (score=-1.830340): "o : D d i d o 22 ;"
I0123 22:39:44.886108 139953274175488 alphageometry.py:567] Translation: "o = on_circle o d i"

I0123 22:39:44.886159 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i ? perp d c c n"
I0123 22:39:44.886340 139953274175488 graph.py:498] 
I0123 22:39:44.886399 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i ? perp d c c n
I0123 22:39:46.550418 139953274175488 ddar.py:60] Depth 1/1000 time = 1.618624210357666
I0123 22:39:51.531950 139953274175488 ddar.py:60] Depth 2/1000 time = 4.981350898742676
I0123 22:40:03.275563 139953274175488 ddar.py:60] Depth 3/1000 time = 11.743431806564331
I0123 22:40:19.552511 139953274175488 ddar.py:60] Depth 4/1000 time = 16.276735544204712
I0123 22:40:35.899991 139953274175488 ddar.py:60] Depth 5/1000 time = 16.347234964370728
I0123 22:40:52.312701 139953274175488 ddar.py:60] Depth 6/1000 time = 16.412471771240234
I0123 22:41:09.089473 139953274175488 ddar.py:60] Depth 7/1000 time = 16.747634887695312
I0123 22:41:26.324641 139953274175488 ddar.py:60] Depth 8/1000 time = 17.23489475250244
I0123 22:41:43.753428 139953274175488 ddar.py:60] Depth 9/1000 time = 17.170900583267212
I0123 22:42:00.727080 139953274175488 ddar.py:60] Depth 10/1000 time = 16.96479368209839
I0123 22:42:00.727366 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:42:00.727470 139953274175488 alphageometry.py:566] LM output (score=-1.964208): "o : T c d c o 22 ;"
I0123 22:42:00.727512 139953274175488 alphageometry.py:567] Translation: "o = on_tline o c c d"

I0123 22:42:00.727555 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c c d ? perp d c c n"
I0123 22:42:00.727734 139953274175488 graph.py:498] 
I0123 22:42:00.727802 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c c d ? perp d c c n
I0123 22:42:02.195168 139953274175488 ddar.py:60] Depth 1/1000 time = 1.4189612865447998
I0123 22:42:05.256661 139953274175488 ddar.py:60] Depth 2/1000 time = 3.0613019466400146
I0123 22:42:14.450599 139953274175488 ddar.py:60] Depth 3/1000 time = 9.193707942962646
I0123 22:42:26.787158 139953274175488 ddar.py:60] Depth 4/1000 time = 12.336304664611816
I0123 22:42:38.792904 139953274175488 ddar.py:60] Depth 5/1000 time = 12.005422830581665
I0123 22:42:50.855650 139953274175488 ddar.py:60] Depth 6/1000 time = 12.062374114990234
I0123 22:43:03.378118 139953274175488 ddar.py:60] Depth 7/1000 time = 12.494297742843628
I0123 22:43:15.733937 139953274175488 ddar.py:60] Depth 8/1000 time = 12.355483055114746
I0123 22:43:28.692843 139953274175488 ddar.py:60] Depth 9/1000 time = 12.76899766921997
I0123 22:43:41.598503 139953274175488 ddar.py:60] Depth 10/1000 time = 12.897379159927368
I0123 22:43:41.598898 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:43:41.599011 139953274175488 alphageometry.py:566] LM output (score=-2.365585): "o : T c o f j 22 ;"
I0123 22:43:41.599051 139953274175488 alphageometry.py:567] Translation: "o = on_tline o c f j"

I0123 22:43:41.599095 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c f j ? perp d c c n"
I0123 22:43:41.599299 139953274175488 graph.py:498] 
I0123 22:43:41.599369 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c f j ? perp d c c n
I0123 22:43:42.897351 139953274175488 ddar.py:60] Depth 1/1000 time = 1.2492196559906006
I0123 22:43:46.610964 139953274175488 ddar.py:60] Depth 2/1000 time = 3.7134296894073486
I0123 22:43:54.871667 139953274175488 ddar.py:60] Depth 3/1000 time = 8.26045298576355
I0123 22:44:05.418588 139953274175488 ddar.py:60] Depth 4/1000 time = 10.546597480773926
I0123 22:44:15.702603 139953274175488 ddar.py:60] Depth 5/1000 time = 10.283715963363647
I0123 22:44:26.019294 139953274175488 ddar.py:60] Depth 6/1000 time = 10.316320657730103
I0123 22:44:36.972078 139953274175488 ddar.py:60] Depth 7/1000 time = 10.923321723937988
I0123 22:44:47.767373 139953274175488 ddar.py:60] Depth 8/1000 time = 10.795045137405396
I0123 22:44:58.563858 139953274175488 ddar.py:60] Depth 9/1000 time = 10.614600419998169
I0123 22:45:09.135860 139953274175488 ddar.py:60] Depth 10/1000 time = 10.5646333694458
I0123 22:45:09.136243 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:45:09.136351 139953274175488 alphageometry.py:566] LM output (score=-2.413160): "o : T f j f o 22 ;"
I0123 22:45:09.136390 139953274175488 alphageometry.py:567] Translation: "o = on_tline o f f j"

I0123 22:45:09.136431 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o f f j ? perp d c c n"
I0123 22:45:09.136607 139953274175488 graph.py:498] 
I0123 22:45:09.136673 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o f f j ? perp d c c n
I0123 22:45:10.460832 139953274175488 ddar.py:60] Depth 1/1000 time = 1.2751970291137695
I0123 22:45:14.026337 139953274175488 ddar.py:60] Depth 2/1000 time = 3.5653131008148193
I0123 22:45:22.494304 139953274175488 ddar.py:60] Depth 3/1000 time = 8.467731237411499
I0123 22:45:33.379738 139953274175488 ddar.py:60] Depth 4/1000 time = 10.885131359100342
I0123 22:45:44.413126 139953274175488 ddar.py:60] Depth 5/1000 time = 11.03314208984375
I0123 22:45:55.138112 139953274175488 ddar.py:60] Depth 6/1000 time = 10.724729776382446
I0123 22:46:06.449774 139953274175488 ddar.py:60] Depth 7/1000 time = 11.282920837402344
I0123 22:46:18.118251 139953274175488 ddar.py:60] Depth 8/1000 time = 11.668193340301514
I0123 22:46:29.273752 139953274175488 ddar.py:60] Depth 9/1000 time = 10.981613159179688
I0123 22:46:40.247500 139953274175488 ddar.py:60] Depth 10/1000 time = 10.966290950775146
I0123 22:46:40.247742 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:46:40.247839 139953274175488 alphageometry.py:566] LM output (score=-2.482226): "o : C e g o 22 D e o g o 23 ;"
I0123 22:46:40.247877 139953274175488 alphageometry.py:567] Translation: "o = on_line o e g, on_bline o g e"

I0123 22:46:40.247916 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_line o e g, on_bline o g e ? perp d c c n"
I0123 22:46:40.248112 139953274175488 graph.py:498] 
I0123 22:46:40.248180 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_line o e g, on_bline o g e ? perp d c c n
I0123 22:46:41.648272 139953274175488 ddar.py:60] Depth 1/1000 time = 1.3496081829071045
I0123 22:46:45.852356 139953274175488 ddar.py:60] Depth 2/1000 time = 4.203847646713257
I0123 22:46:55.900947 139953274175488 ddar.py:60] Depth 3/1000 time = 10.04829740524292
I0123 22:47:10.838104 139953274175488 ddar.py:60] Depth 4/1000 time = 14.93684697151184
I0123 22:47:27.751188 139953274175488 ddar.py:60] Depth 5/1000 time = 16.91258692741394
I0123 22:47:44.683866 139953274175488 ddar.py:60] Depth 6/1000 time = 16.932276248931885
I0123 22:48:01.651474 139953274175488 ddar.py:60] Depth 7/1000 time = 16.966572046279907
I0123 22:48:19.098004 139953274175488 ddar.py:60] Depth 8/1000 time = 17.41750144958496
I0123 22:48:37.048547 139953274175488 ddar.py:60] Depth 9/1000 time = 17.950075149536133
I0123 22:48:55.170125 139953274175488 ddar.py:60] Depth 10/1000 time = 18.121138334274292
I0123 22:49:13.033646 139953274175488 ddar.py:60] Depth 11/1000 time = 17.863218784332275
I0123 22:49:31.846315 139953274175488 ddar.py:60] Depth 12/1000 time = 18.502618551254272
I0123 22:49:50.307678 139953274175488 ddar.py:60] Depth 13/1000 time = 18.445806741714478
I0123 22:49:50.308104 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:49:50.308212 139953274175488 alphageometry.py:566] LM output (score=-2.492184): "o : T a d a o 22 ;"
I0123 22:49:50.308254 139953274175488 alphageometry.py:567] Translation: "o = on_tline o a a d"

I0123 22:49:50.308302 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a a d ? perp d c c n"
I0123 22:49:50.308486 139953274175488 graph.py:498] 
I0123 22:49:50.308557 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a a d ? perp d c c n
I0123 22:49:51.643214 139953274175488 ddar.py:60] Depth 1/1000 time = 1.2862827777862549
I0123 22:49:56.000474 139953274175488 ddar.py:60] Depth 2/1000 time = 4.3570709228515625
I0123 22:50:03.117901 139953274175488 ddar.py:60] Depth 3/1000 time = 7.117203712463379
I0123 22:50:14.168986 139953274175488 ddar.py:60] Depth 4/1000 time = 11.050860166549683
I0123 22:50:24.782408 139953274175488 ddar.py:60] Depth 5/1000 time = 10.613124370574951
I0123 22:50:35.445785 139953274175488 ddar.py:60] Depth 6/1000 time = 10.66308856010437
I0123 22:50:46.542719 139953274175488 ddar.py:60] Depth 7/1000 time = 11.069703340530396
I0123 22:50:57.845865 139953274175488 ddar.py:60] Depth 8/1000 time = 11.302885055541992
I0123 22:51:08.606945 139953274175488 ddar.py:60] Depth 9/1000 time = 10.585611343383789
I0123 22:51:19.508805 139953274175488 ddar.py:60] Depth 10/1000 time = 10.893697738647461
I0123 22:51:19.509069 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:51:19.509176 139953274175488 alphageometry.py:566] LM output (score=-2.507349): "o : T g h g o 22 ;"
I0123 22:51:19.509216 139953274175488 alphageometry.py:567] Translation: "o = on_tline o g g h"

I0123 22:51:19.509258 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o g g h ? perp d c c n"
I0123 22:51:19.509459 139953274175488 graph.py:498] 
I0123 22:51:19.509532 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o g g h ? perp d c c n
I0123 22:51:20.878056 139953274175488 ddar.py:60] Depth 1/1000 time = 1.3192999362945557
I0123 22:51:24.655497 139953274175488 ddar.py:60] Depth 2/1000 time = 3.777221202850342
I0123 22:51:32.381618 139953274175488 ddar.py:60] Depth 3/1000 time = 7.72589373588562
I0123 22:51:43.310097 139953274175488 ddar.py:60] Depth 4/1000 time = 10.928184986114502
I0123 22:51:54.096118 139953274175488 ddar.py:60] Depth 5/1000 time = 10.785609006881714
I0123 22:52:04.905627 139953274175488 ddar.py:60] Depth 6/1000 time = 10.809176206588745
I0123 22:52:16.110372 139953274175488 ddar.py:60] Depth 7/1000 time = 11.180488109588623
I0123 22:52:27.536366 139953274175488 ddar.py:60] Depth 8/1000 time = 11.425665140151978
I0123 22:52:39.180306 139953274175488 ddar.py:60] Depth 9/1000 time = 11.472246408462524
I0123 22:52:50.275461 139953274175488 ddar.py:60] Depth 10/1000 time = 11.08664321899414
I0123 22:52:50.275893 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:52:50.276038 139953274175488 alphageometry.py:566] LM output (score=-2.526732): "o : T f j j o 22 ;"
I0123 22:52:50.276082 139953274175488 alphageometry.py:567] Translation: "o = on_tline o j f j"

I0123 22:52:50.276144 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o j f j ? perp d c c n"
I0123 22:52:50.276350 139953274175488 graph.py:498] 
I0123 22:52:50.276419 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o j f j ? perp d c c n
I0123 22:52:51.705743 139953274175488 ddar.py:60] Depth 1/1000 time = 1.3802249431610107
I0123 22:52:55.704055 139953274175488 ddar.py:60] Depth 2/1000 time = 3.998129367828369
I0123 22:53:03.747902 139953274175488 ddar.py:60] Depth 3/1000 time = 8.04366683959961
I0123 22:53:13.775362 139953274175488 ddar.py:60] Depth 4/1000 time = 10.027196407318115
I0123 22:53:24.253882 139953274175488 ddar.py:60] Depth 5/1000 time = 10.478190183639526
I0123 22:53:34.901839 139953274175488 ddar.py:60] Depth 6/1000 time = 10.647692918777466
I0123 22:53:45.681160 139953274175488 ddar.py:60] Depth 7/1000 time = 10.749516010284424
I0123 22:53:56.801904 139953274175488 ddar.py:60] Depth 8/1000 time = 11.120418787002563
I0123 22:54:07.958397 139953274175488 ddar.py:60] Depth 9/1000 time = 10.979350328445435
I0123 22:54:18.611248 139953274175488 ddar.py:60] Depth 10/1000 time = 10.645250082015991
I0123 22:54:18.611832 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:54:18.611969 139953274175488 alphageometry.py:566] LM output (score=-2.638430): "o : T d g d o 22 ;"
I0123 22:54:18.612015 139953274175488 alphageometry.py:567] Translation: "o = on_tline o d d g"

I0123 22:54:18.612076 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o d d g ? perp d c c n"
I0123 22:54:18.612288 139953274175488 graph.py:498] 
I0123 22:54:18.612358 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o d d g ? perp d c c n
I0123 22:54:20.062634 139953274175488 ddar.py:60] Depth 1/1000 time = 1.4003193378448486
I0123 22:54:23.473238 139953274175488 ddar.py:60] Depth 2/1000 time = 3.4104113578796387
I0123 22:54:31.218878 139953274175488 ddar.py:60] Depth 3/1000 time = 7.7454423904418945
I0123 22:54:41.831659 139953274175488 ddar.py:60] Depth 4/1000 time = 10.612534523010254
I0123 22:54:52.445251 139953274175488 ddar.py:60] Depth 5/1000 time = 10.613305807113647
I0123 22:55:02.779850 139953274175488 ddar.py:60] Depth 6/1000 time = 10.334288358688354
I0123 22:55:13.972236 139953274175488 ddar.py:60] Depth 7/1000 time = 11.168081521987915
I0123 22:55:25.206084 139953274175488 ddar.py:60] Depth 8/1000 time = 11.233566284179688
I0123 22:55:36.167766 139953274175488 ddar.py:60] Depth 9/1000 time = 10.790445566177368
I0123 22:55:47.418340 139953274175488 ddar.py:60] Depth 10/1000 time = 11.242400646209717
I0123 22:55:47.418642 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:55:47.418748 139953274175488 alphageometry.py:566] LM output (score=-2.654643): "o : T d f d o 22 ;"
I0123 22:55:47.418791 139953274175488 alphageometry.py:567] Translation: "o = on_tline o d d f"

I0123 22:55:47.418836 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o d d f ? perp d c c n"
I0123 22:55:47.419017 139953274175488 graph.py:498] 
I0123 22:55:47.419088 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o d d f ? perp d c c n
I0123 22:55:48.848882 139953274175488 ddar.py:60] Depth 1/1000 time = 1.3799986839294434
I0123 22:55:51.845297 139953274175488 ddar.py:60] Depth 2/1000 time = 2.9962356090545654
I0123 22:55:58.635867 139953274175488 ddar.py:60] Depth 3/1000 time = 6.790378093719482
I0123 22:56:08.937073 139953274175488 ddar.py:60] Depth 4/1000 time = 10.300968408584595
I0123 22:56:19.461229 139953274175488 ddar.py:60] Depth 5/1000 time = 10.523895502090454
I0123 22:56:29.956173 139953274175488 ddar.py:60] Depth 6/1000 time = 10.494645833969116
I0123 22:56:40.771258 139953274175488 ddar.py:60] Depth 7/1000 time = 10.789713144302368
I0123 22:56:51.564722 139953274175488 ddar.py:60] Depth 8/1000 time = 10.793171644210815
I0123 22:57:02.654489 139953274175488 ddar.py:60] Depth 9/1000 time = 10.915607690811157
I0123 22:57:13.411541 139953274175488 ddar.py:60] Depth 10/1000 time = 10.749046564102173
I0123 22:57:13.411818 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:57:13.411915 139953274175488 alphageometry.py:566] LM output (score=-2.658728): "o : D d g d o 22 ;"
I0123 22:57:13.411956 139953274175488 alphageometry.py:567] Translation: "o = on_circle o d g"

I0123 22:57:13.412000 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d g ? perp d c c n"
I0123 22:57:13.412192 139953274175488 graph.py:498] 
I0123 22:57:13.412258 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d g ? perp d c c n
I0123 22:57:15.047940 139953274175488 ddar.py:60] Depth 1/1000 time = 1.5882174968719482
I0123 22:57:20.346056 139953274175488 ddar.py:60] Depth 2/1000 time = 5.297932863235474
I0123 22:57:31.930886 139953274175488 ddar.py:60] Depth 3/1000 time = 11.584613561630249
I0123 22:57:48.589723 139953274175488 ddar.py:60] Depth 4/1000 time = 16.658520936965942
I0123 22:58:05.290787 139953274175488 ddar.py:60] Depth 5/1000 time = 16.70065665245056
I0123 22:58:21.587598 139953274175488 ddar.py:60] Depth 6/1000 time = 16.29650616645813
I0123 22:58:38.673046 139953274175488 ddar.py:60] Depth 7/1000 time = 17.05626678466797
I0123 22:58:56.199862 139953274175488 ddar.py:60] Depth 8/1000 time = 17.526348114013672
I0123 22:59:13.617227 139953274175488 ddar.py:60] Depth 9/1000 time = 17.170296907424927
I0123 22:59:31.151908 139953274175488 ddar.py:60] Depth 10/1000 time = 17.525778770446777
I0123 22:59:31.152172 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:59:31.152270 139953274175488 alphageometry.py:566] LM output (score=-2.668610): "o : T a o c d 22 ;"
I0123 22:59:31.152311 139953274175488 alphageometry.py:567] Translation: "o = on_tline o a c d"

I0123 22:59:31.152353 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a c d ? perp d c c n"
I0123 22:59:31.152535 139953274175488 graph.py:498] 
I0123 22:59:31.152601 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a c d ? perp d c c n
I0123 22:59:32.201582 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0006957054138184
I0123 22:59:35.866338 139953274175488 ddar.py:60] Depth 2/1000 time = 3.664525032043457
I0123 22:59:44.420576 139953274175488 ddar.py:60] Depth 3/1000 time = 8.554016351699829
I0123 22:59:56.373596 139953274175488 ddar.py:60] Depth 4/1000 time = 11.952719926834106
I0123 23:00:08.576515 139953274175488 ddar.py:60] Depth 5/1000 time = 12.202497959136963
I0123 23:00:20.828559 139953274175488 ddar.py:60] Depth 6/1000 time = 12.251749753952026
I0123 23:00:33.609071 139953274175488 ddar.py:60] Depth 7/1000 time = 12.754464864730835
I0123 23:00:46.048138 139953274175488 ddar.py:60] Depth 8/1000 time = 12.438769578933716
I0123 23:00:59.081127 139953274175488 ddar.py:60] Depth 9/1000 time = 12.848276615142822
I0123 23:01:11.537820 139953274175488 ddar.py:60] Depth 10/1000 time = 12.448242664337158
I0123 23:01:11.538107 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:01:11.538213 139953274175488 alphageometry.py:566] LM output (score=-2.679945): "o : T a o f j 22 ;"
I0123 23:01:11.538254 139953274175488 alphageometry.py:567] Translation: "o = on_tline o a f j"

I0123 23:01:11.538297 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a f j ? perp d c c n"
I0123 23:01:11.538501 139953274175488 graph.py:498] 
I0123 23:01:11.538572 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a f j ? perp d c c n
I0123 23:01:13.009763 139953274175488 ddar.py:60] Depth 1/1000 time = 1.4221901893615723
I0123 23:01:16.649655 139953274175488 ddar.py:60] Depth 2/1000 time = 3.6396889686584473
I0123 23:01:24.131762 139953274175488 ddar.py:60] Depth 3/1000 time = 7.481866836547852
I0123 23:01:34.127219 139953274175488 ddar.py:60] Depth 4/1000 time = 9.995155334472656
I0123 23:01:44.302270 139953274175488 ddar.py:60] Depth 5/1000 time = 10.174746751785278
I0123 23:01:54.568775 139953274175488 ddar.py:60] Depth 6/1000 time = 10.266137838363647
I0123 23:02:05.270894 139953274175488 ddar.py:60] Depth 7/1000 time = 10.672714948654175
I0123 23:02:16.047249 139953274175488 ddar.py:60] Depth 8/1000 time = 10.775979280471802
I0123 23:02:27.335818 139953274175488 ddar.py:60] Depth 9/1000 time = 11.112116575241089
I0123 23:02:37.876155 139953274175488 ddar.py:60] Depth 10/1000 time = 10.533847570419312
I0123 23:02:37.876397 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:02:37.876511 139953274175488 alphageometry.py:566] LM output (score=-2.720139): "o : T g h h o 22 ;"
I0123 23:02:37.876552 139953274175488 alphageometry.py:567] Translation: "o = on_tline o h g h"

I0123 23:02:37.876591 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o h g h ? perp d c c n"
I0123 23:02:37.876764 139953274175488 graph.py:498] 
I0123 23:02:37.876832 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o h g h ? perp d c c n
I0123 23:02:39.532733 139953274175488 ddar.py:60] Depth 1/1000 time = 1.6130077838897705
I0123 23:02:43.728316 139953274175488 ddar.py:60] Depth 2/1000 time = 4.19539213180542
I0123 23:02:51.832227 139953274175488 ddar.py:60] Depth 3/1000 time = 8.103698253631592
I0123 23:03:03.474177 139953274175488 ddar.py:60] Depth 4/1000 time = 11.64170527458191
I0123 23:03:15.369446 139953274175488 ddar.py:60] Depth 5/1000 time = 11.894973993301392
I0123 23:03:26.880498 139953274175488 ddar.py:60] Depth 6/1000 time = 11.510770797729492
I0123 23:03:38.877954 139953274175488 ddar.py:60] Depth 7/1000 time = 11.975270748138428
I0123 23:03:51.110537 139953274175488 ddar.py:60] Depth 8/1000 time = 12.232253551483154
I0123 23:04:03.230368 139953274175488 ddar.py:60] Depth 9/1000 time = 11.951472043991089
I0123 23:04:15.117821 139953274175488 ddar.py:60] Depth 10/1000 time = 11.879266738891602
I0123 23:04:15.118248 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:04:15.118386 139953274175488 alphageometry.py:566] LM output (score=-2.723205): "o : T d h h o 22 ;"
I0123 23:04:15.118427 139953274175488 alphageometry.py:567] Translation: "o = on_tline o h d h"

I0123 23:04:15.118490 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o h d h ? perp d c c n"
I0123 23:04:15.118706 139953274175488 graph.py:498] 
I0123 23:04:15.118774 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o h d h ? perp d c c n
I0123 23:04:16.645489 139953274175488 ddar.py:60] Depth 1/1000 time = 1.4777038097381592
I0123 23:04:20.430878 139953274175488 ddar.py:60] Depth 2/1000 time = 3.7851786613464355
I0123 23:04:28.343934 139953274175488 ddar.py:60] Depth 3/1000 time = 7.912860870361328
I0123 23:04:39.754805 139953274175488 ddar.py:60] Depth 4/1000 time = 11.410565614700317
I0123 23:04:51.069608 139953274175488 ddar.py:60] Depth 5/1000 time = 11.31441855430603
I0123 23:05:02.528803 139953274175488 ddar.py:60] Depth 6/1000 time = 11.45890188217163
I0123 23:05:14.396971 139953274175488 ddar.py:60] Depth 7/1000 time = 11.840228796005249
I0123 23:05:26.535539 139953274175488 ddar.py:60] Depth 8/1000 time = 12.138274669647217
I0123 23:05:38.888641 139953274175488 ddar.py:60] Depth 9/1000 time = 12.16020941734314
I0123 23:05:51.005270 139953274175488 ddar.py:60] Depth 10/1000 time = 12.108134031295776
I0123 23:05:51.005521 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:05:51.005629 139953274175488 alphageometry.py:566] LM output (score=-2.793395): "o : T b o c d 22 ;"
I0123 23:05:51.005678 139953274175488 alphageometry.py:567] Translation: "o = on_tline o b c d"

I0123 23:05:51.005719 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b c d ? perp d c c n"
I0123 23:05:51.005896 139953274175488 graph.py:498] 
I0123 23:05:51.005963 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b c d ? perp d c c n
I0123 23:05:52.053142 139953274175488 ddar.py:60] Depth 1/1000 time = 0.9984574317932129
I0123 23:05:56.168802 139953274175488 ddar.py:60] Depth 2/1000 time = 4.115467548370361
I0123 23:06:05.222368 139953274175488 ddar.py:60] Depth 3/1000 time = 9.053330659866333
I0123 23:06:17.853148 139953274175488 ddar.py:60] Depth 4/1000 time = 12.630504846572876
I0123 23:06:29.864544 139953274175488 ddar.py:60] Depth 5/1000 time = 12.011088848114014
I0123 23:06:42.386486 139953274175488 ddar.py:60] Depth 6/1000 time = 12.521660566329956
I0123 23:06:54.801089 139953274175488 ddar.py:60] Depth 7/1000 time = 12.387302875518799
I0123 23:07:07.858010 139953274175488 ddar.py:60] Depth 8/1000 time = 13.056623697280884
I0123 23:07:20.730980 139953274175488 ddar.py:60] Depth 9/1000 time = 12.680343389511108
I0123 23:07:33.888912 139953274175488 ddar.py:60] Depth 10/1000 time = 13.149543285369873
I0123 23:07:33.889183 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:07:33.889281 139953274175488 alphageometry.py:566] LM output (score=-2.820904): "o : T b o g h 22 ;"
I0123 23:07:33.889325 139953274175488 alphageometry.py:567] Translation: "o = on_tline o b g h"

I0123 23:07:33.889368 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b g h ? perp d c c n"
I0123 23:07:33.889547 139953274175488 graph.py:498] 
I0123 23:07:33.889616 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b g h ? perp d c c n
I0123 23:07:34.949498 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0112650394439697
I0123 23:07:38.423781 139953274175488 ddar.py:60] Depth 2/1000 time = 3.4740896224975586
I0123 23:07:46.398433 139953274175488 ddar.py:60] Depth 3/1000 time = 7.974412679672241
I0123 23:07:58.165204 139953274175488 ddar.py:60] Depth 4/1000 time = 11.766433715820312
I0123 23:08:09.734570 139953274175488 ddar.py:60] Depth 5/1000 time = 11.569013118743896
I0123 23:08:20.888736 139953274175488 ddar.py:60] Depth 6/1000 time = 11.15376329421997
I0123 23:08:32.545154 139953274175488 ddar.py:60] Depth 7/1000 time = 11.632439613342285
I0123 23:08:44.851163 139953274175488 ddar.py:60] Depth 8/1000 time = 12.305599689483643
I0123 23:08:56.539088 139953274175488 ddar.py:60] Depth 9/1000 time = 11.51234769821167
I0123 23:09:08.001513 139953274175488 ddar.py:60] Depth 10/1000 time = 11.454018592834473
I0123 23:09:08.001795 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:09:08.001909 139953274175488 alphageometry.py:566] LM output (score=-2.826410): "o : D d e d o 22 ;"
I0123 23:09:08.001949 139953274175488 alphageometry.py:567] Translation: "o = on_circle o d e"

I0123 23:09:08.001993 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d e ? perp d c c n"
I0123 23:09:08.002182 139953274175488 graph.py:498] 
I0123 23:09:08.002252 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d e ? perp d c c n
I0123 23:09:09.093108 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0399601459503174
I0123 23:09:13.589468 139953274175488 ddar.py:60] Depth 2/1000 time = 4.496091842651367
I0123 23:09:27.015850 139953274175488 ddar.py:60] Depth 3/1000 time = 13.426038026809692
I0123 23:09:43.910837 139953274175488 ddar.py:60] Depth 4/1000 time = 16.89470386505127
I0123 23:10:01.746378 139953274175488 ddar.py:60] Depth 5/1000 time = 17.835202932357788
I0123 23:10:19.084494 139953274175488 ddar.py:60] Depth 6/1000 time = 17.337742805480957
I0123 23:10:37.191052 139953274175488 ddar.py:60] Depth 7/1000 time = 18.07997417449951
I0123 23:10:55.173938 139953274175488 ddar.py:60] Depth 8/1000 time = 17.982478141784668
I0123 23:11:13.426075 139953274175488 ddar.py:60] Depth 9/1000 time = 17.98619270324707
I0123 23:11:31.862893 139953274175488 ddar.py:60] Depth 10/1000 time = 18.428117513656616
I0123 23:11:31.863275 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:11:31.863384 139953274175488 alphageometry.py:566] LM output (score=-2.867423): "o : T b o f j 22 ;"
I0123 23:11:31.863425 139953274175488 alphageometry.py:567] Translation: "o = on_tline o b f j"

I0123 23:11:31.863471 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b f j ? perp d c c n"
I0123 23:11:31.863655 139953274175488 graph.py:498] 
I0123 23:11:31.863721 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b f j ? perp d c c n
I0123 23:11:32.920342 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0078437328338623
I0123 23:11:36.614650 139953274175488 ddar.py:60] Depth 2/1000 time = 3.6941182613372803
I0123 23:11:45.599192 139953274175488 ddar.py:60] Depth 3/1000 time = 8.98427700996399
I0123 23:11:56.154550 139953274175488 ddar.py:60] Depth 4/1000 time = 10.555023670196533
I0123 23:12:06.999520 139953274175488 ddar.py:60] Depth 5/1000 time = 10.844659328460693
I0123 23:12:17.882472 139953274175488 ddar.py:60] Depth 6/1000 time = 10.882583856582642
I0123 23:12:29.399234 139953274175488 ddar.py:60] Depth 7/1000 time = 11.48696231842041
I0123 23:12:40.986833 139953274175488 ddar.py:60] Depth 8/1000 time = 11.587194681167603
I0123 23:12:52.844031 139953274175488 ddar.py:60] Depth 9/1000 time = 11.679034233093262
I0123 23:13:04.494753 139953274175488 ddar.py:60] Depth 10/1000 time = 11.642889499664307
I0123 23:13:04.495032 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:13:04.495149 139953274175488 alphageometry.py:566] LM output (score=-2.875446): "o : T f i f o 22 ;"
I0123 23:13:04.495191 139953274175488 alphageometry.py:567] Translation: "o = on_tline o f f i"

I0123 23:13:04.495237 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o f f i ? perp d c c n"
I0123 23:13:04.495424 139953274175488 graph.py:498] 
I0123 23:13:04.495494 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o f f i ? perp d c c n
I0123 23:13:05.566538 139953274175488 ddar.py:60] Depth 1/1000 time = 1.022298812866211
I0123 23:13:09.382587 139953274175488 ddar.py:60] Depth 2/1000 time = 3.8158509731292725
I0123 23:13:17.862717 139953274175488 ddar.py:60] Depth 3/1000 time = 8.479909896850586
I0123 23:13:29.843971 139953274175488 ddar.py:60] Depth 4/1000 time = 11.98100733757019
I0123 23:13:42.023245 139953274175488 ddar.py:60] Depth 5/1000 time = 12.178953409194946
I0123 23:13:54.247507 139953274175488 ddar.py:60] Depth 6/1000 time = 12.223894357681274
I0123 23:14:06.713464 139953274175488 ddar.py:60] Depth 7/1000 time = 12.437448978424072
I0123 23:14:19.575500 139953274175488 ddar.py:60] Depth 8/1000 time = 12.861719131469727
I0123 23:14:32.865515 139953274175488 ddar.py:60] Depth 9/1000 time = 13.087500095367432
I0123 23:14:45.981911 139953274175488 ddar.py:60] Depth 10/1000 time = 13.107796907424927
I0123 23:14:45.982198 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:14:45.982320 139953274175488 alphageometry.py:566] LM output (score=-2.884675): "o : T b o d e 22 ;"
I0123 23:14:45.982365 139953274175488 alphageometry.py:567] Translation: "o = on_tline o b d e"

I0123 23:14:45.982411 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b d e ? perp d c c n"
I0123 23:14:45.982604 139953274175488 graph.py:498] 
I0123 23:14:45.982678 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b d e ? perp d c c n
I0123 23:14:47.541825 139953274175488 ddar.py:60] Depth 1/1000 time = 1.5096592903137207
I0123 23:14:51.532109 139953274175488 ddar.py:60] Depth 2/1000 time = 3.9900929927825928
I0123 23:14:59.114862 139953274175488 ddar.py:60] Depth 3/1000 time = 7.582535982131958
I0123 23:15:09.558665 139953274175488 ddar.py:60] Depth 4/1000 time = 10.443561553955078
I0123 23:15:20.147745 139953274175488 ddar.py:60] Depth 5/1000 time = 10.588746547698975
I0123 23:15:30.719750 139953274175488 ddar.py:60] Depth 6/1000 time = 10.571615219116211
I0123 23:15:41.882684 139953274175488 ddar.py:60] Depth 7/1000 time = 11.137856245040894
I0123 23:15:52.994692 139953274175488 ddar.py:60] Depth 8/1000 time = 11.11161994934082
I0123 23:16:04.157532 139953274175488 ddar.py:60] Depth 9/1000 time = 10.983068704605103
I0123 23:16:15.134905 139953274175488 ddar.py:60] Depth 10/1000 time = 10.969151735305786
I0123 23:16:15.135309 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:16:15.135426 139953274175488 alphageometry.py:566] LM output (score=-2.911665): "o : T c o e g 22 ;"
I0123 23:16:15.135466 139953274175488 alphageometry.py:567] Translation: "o = on_tline o c e g"

I0123 23:16:15.135508 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c e g ? perp d c c n"
I0123 23:16:15.135686 139953274175488 graph.py:498] 
I0123 23:16:15.135754 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c e g ? perp d c c n
I0123 23:16:16.762969 139953274175488 ddar.py:60] Depth 1/1000 time = 1.5782582759857178
I0123 23:16:20.770910 139953274175488 ddar.py:60] Depth 2/1000 time = 4.007701635360718
I0123 23:16:29.009250 139953274175488 ddar.py:60] Depth 3/1000 time = 8.238069295883179
I0123 23:16:40.821502 139953274175488 ddar.py:60] Depth 4/1000 time = 11.811941623687744
I0123 23:16:51.941743 139953274175488 ddar.py:60] Depth 5/1000 time = 11.119814157485962
I0123 23:17:03.066554 139953274175488 ddar.py:60] Depth 6/1000 time = 11.124465465545654
I0123 23:17:14.771061 139953274175488 ddar.py:60] Depth 7/1000 time = 11.676774024963379
I0123 23:17:26.534467 139953274175488 ddar.py:60] Depth 8/1000 time = 11.763113737106323
I0123 23:17:38.263956 139953274175488 ddar.py:60] Depth 9/1000 time = 11.55927300453186
I0123 23:17:49.231299 139953274175488 ddar.py:60] Depth 10/1000 time = 10.959149599075317
I0123 23:17:49.231705 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:17:49.231834 139953274175488 alphageometry.py:566] LM output (score=-2.912017): "o : T d h d o 22 ;"
I0123 23:17:49.231872 139953274175488 alphageometry.py:567] Translation: "o = on_tline o d d h"

I0123 23:17:49.231928 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o d d h ? perp d c c n"
I0123 23:17:49.232132 139953274175488 graph.py:498] 
I0123 23:17:49.232194 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o d d h ? perp d c c n
I0123 23:17:50.892509 139953274175488 ddar.py:60] Depth 1/1000 time = 1.610961675643921
I0123 23:17:54.877703 139953274175488 ddar.py:60] Depth 2/1000 time = 3.985015869140625
I0123 23:18:02.712237 139953274175488 ddar.py:60] Depth 3/1000 time = 7.834352731704712
I0123 23:18:15.117673 139953274175488 ddar.py:60] Depth 4/1000 time = 12.405168533325195
I0123 23:18:27.808736 139953274175488 ddar.py:60] Depth 5/1000 time = 12.690701246261597
I0123 23:18:40.588960 139953274175488 ddar.py:60] Depth 6/1000 time = 12.779818534851074
I0123 23:18:53.675412 139953274175488 ddar.py:60] Depth 7/1000 time = 13.057348012924194
I0123 23:19:07.117864 139953274175488 ddar.py:60] Depth 8/1000 time = 13.442049026489258
I0123 23:19:20.342148 139953274175488 ddar.py:60] Depth 9/1000 time = 13.028034448623657
I0123 23:19:33.414114 139953274175488 ddar.py:60] Depth 10/1000 time = 13.063737630844116
I0123 23:19:33.414631 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:19:33.414736 139953274175488 alphageometry.py:566] LM output (score=-2.929058): "o : T c o g h 22 ;"
I0123 23:19:33.414773 139953274175488 alphageometry.py:567] Translation: "o = on_tline o c g h"

I0123 23:19:33.414816 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c g h ? perp d c c n"
I0123 23:19:33.414999 139953274175488 graph.py:498] 
I0123 23:19:33.415063 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c g h ? perp d c c n
I0123 23:19:34.475669 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0119071006774902
I0123 23:19:38.144870 139953274175488 ddar.py:60] Depth 2/1000 time = 3.6690244674682617
I0123 23:19:45.989299 139953274175488 ddar.py:60] Depth 3/1000 time = 7.8442442417144775
I0123 23:19:56.290356 139953274175488 ddar.py:60] Depth 4/1000 time = 10.30078911781311
I0123 23:20:06.955980 139953274175488 ddar.py:60] Depth 5/1000 time = 10.665258407592773
I0123 23:20:17.629870 139953274175488 ddar.py:60] Depth 6/1000 time = 10.673583507537842
I0123 23:20:28.735702 139953274175488 ddar.py:60] Depth 7/1000 time = 11.082333087921143
I0123 23:20:39.978271 139953274175488 ddar.py:60] Depth 8/1000 time = 11.242266654968262
I0123 23:20:51.203704 139953274175488 ddar.py:60] Depth 9/1000 time = 11.048892974853516
I0123 23:21:01.616804 139953274175488 ddar.py:60] Depth 10/1000 time = 10.404897451400757
I0123 23:21:01.617197 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:21:01.617328 139953274175488 alphageometry.py:566] LM output (score=-2.930040): "o : T d e d o 22 ;"
I0123 23:21:01.617366 139953274175488 alphageometry.py:567] Translation: "o = on_tline o d d e"

I0123 23:21:01.617424 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o d d e ? perp d c c n"
I0123 23:21:01.617619 139953274175488 graph.py:498] 
I0123 23:21:01.617735 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o d d e ? perp d c c n
I0123 23:21:03.327159 139953274175488 ddar.py:60] Depth 1/1000 time = 1.659099817276001
I0123 23:21:07.023447 139953274175488 ddar.py:60] Depth 2/1000 time = 3.6960952281951904
I0123 23:21:14.013720 139953274175488 ddar.py:60] Depth 3/1000 time = 6.990095138549805
I0123 23:21:24.851358 139953274175488 ddar.py:60] Depth 4/1000 time = 10.837397336959839
I0123 23:21:35.841124 139953274175488 ddar.py:60] Depth 5/1000 time = 10.98948073387146
I0123 23:21:46.886243 139953274175488 ddar.py:60] Depth 6/1000 time = 11.044854402542114
I0123 23:21:58.386039 139953274175488 ddar.py:60] Depth 7/1000 time = 11.473811864852905
I0123 23:22:10.082658 139953274175488 ddar.py:60] Depth 8/1000 time = 11.696345567703247
I0123 23:22:20.894051 139953274175488 ddar.py:60] Depth 9/1000 time = 10.639962196350098
I0123 23:22:32.271594 139953274175488 ddar.py:60] Depth 10/1000 time = 11.369346141815186
I0123 23:22:32.271854 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:22:32.271950 139953274175488 alphageometry.py:566] LM output (score=-2.953931): "o : T e g f o 22 ;"
I0123 23:22:32.271986 139953274175488 alphageometry.py:567] Translation: "o = on_tline o f e g"

I0123 23:22:32.272026 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o f e g ? perp d c c n"
I0123 23:22:32.272201 139953274175488 graph.py:498] 
I0123 23:22:32.272263 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o f e g ? perp d c c n
I0123 23:22:33.934819 139953274175488 ddar.py:60] Depth 1/1000 time = 1.6135103702545166
I0123 23:22:37.242172 139953274175488 ddar.py:60] Depth 2/1000 time = 3.307173252105713
I0123 23:22:46.380526 139953274175488 ddar.py:60] Depth 3/1000 time = 9.138108015060425
I0123 23:22:58.416669 139953274175488 ddar.py:60] Depth 4/1000 time = 12.035800457000732
I0123 23:23:09.886384 139953274175488 ddar.py:60] Depth 5/1000 time = 11.469437599182129
I0123 23:23:20.652365 139953274175488 ddar.py:60] Depth 6/1000 time = 10.765737771987915
I0123 23:23:33.296233 139953274175488 ddar.py:60] Depth 7/1000 time = 12.616677284240723
I0123 23:23:44.638528 139953274175488 ddar.py:60] Depth 8/1000 time = 11.342031717300415
I0123 23:23:56.631036 139953274175488 ddar.py:60] Depth 9/1000 time = 11.82122802734375
I0123 23:24:08.391511 139953274175488 ddar.py:60] Depth 10/1000 time = 11.752068281173706
I0123 23:24:08.391938 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:24:08.392050 139953274175488 alphageometry.py:566] LM output (score=-3.010039): "o : T a o g h 22 ;"
I0123 23:24:08.392088 139953274175488 alphageometry.py:567] Translation: "o = on_tline o a g h"

I0123 23:24:08.392131 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a g h ? perp d c c n"
I0123 23:24:08.392314 139953274175488 graph.py:498] 
I0123 23:24:08.392378 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a g h ? perp d c c n
I0123 23:24:09.458002 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0157756805419922
I0123 23:24:13.213768 139953274175488 ddar.py:60] Depth 2/1000 time = 3.75546932220459
I0123 23:24:21.192849 139953274175488 ddar.py:60] Depth 3/1000 time = 7.978867053985596
I0123 23:24:31.286774 139953274175488 ddar.py:60] Depth 4/1000 time = 10.093660354614258
I0123 23:24:41.933251 139953274175488 ddar.py:60] Depth 5/1000 time = 10.64611029624939
I0123 23:24:52.619572 139953274175488 ddar.py:60] Depth 6/1000 time = 10.686017036437988
I0123 23:25:03.739487 139953274175488 ddar.py:60] Depth 7/1000 time = 11.0956392288208
I0123 23:25:14.998502 139953274175488 ddar.py:60] Depth 8/1000 time = 11.258731842041016
I0123 23:25:25.684806 139953274175488 ddar.py:60] Depth 9/1000 time = 10.51370906829834
I0123 23:25:36.907735 139953274175488 ddar.py:60] Depth 10/1000 time = 11.214672803878784
I0123 23:25:36.908336 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:25:36.908472 139953274175488 alphageometry.py:566] LM output (score=-3.016224): "o : P c d f o 22 ;"
I0123 23:25:36.908510 139953274175488 alphageometry.py:567] Translation: "o = on_pline o f c d"

I0123 23:25:36.908567 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_pline o f c d ? perp d c c n"
I0123 23:25:36.908763 139953274175488 graph.py:498] 
I0123 23:25:36.908824 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_pline o f c d ? perp d c c n
I0123 23:25:38.653760 139953274175488 ddar.py:60] Depth 1/1000 time = 1.6966311931610107
I0123 23:25:42.181991 139953274175488 ddar.py:60] Depth 2/1000 time = 3.528043031692505
I0123 23:25:50.261755 139953274175488 ddar.py:60] Depth 3/1000 time = 8.079565286636353
I0123 23:26:00.581253 139953274175488 ddar.py:60] Depth 4/1000 time = 10.319244861602783
I0123 23:26:11.588010 139953274175488 ddar.py:60] Depth 5/1000 time = 11.006416082382202
I0123 23:26:22.602083 139953274175488 ddar.py:60] Depth 6/1000 time = 11.013638496398926
I0123 23:26:33.389426 139953274175488 ddar.py:60] Depth 7/1000 time = 10.763586521148682
I0123 23:26:44.956012 139953274175488 ddar.py:60] Depth 8/1000 time = 11.566267728805542
I0123 23:26:56.637487 139953274175488 ddar.py:60] Depth 9/1000 time = 11.507906198501587
I0123 23:27:08.145490 139953274175488 ddar.py:60] Depth 10/1000 time = 11.4998140335083
I0123 23:27:08.145899 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:27:08.146003 139953274175488 alphageometry.py:566] LM output (score=-3.090643): "o : T a o e f 22 ;"
I0123 23:27:08.146040 139953274175488 alphageometry.py:567] Translation: "o = on_tline o a e f"

I0123 23:27:08.146084 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a e f ? perp d c c n"
I0123 23:27:08.146257 139953274175488 graph.py:498] 
I0123 23:27:08.146319 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o a e f ? perp d c c n
I0123 23:27:09.219355 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0236611366271973
I0123 23:27:12.902557 139953274175488 ddar.py:60] Depth 2/1000 time = 3.6830220222473145
I0123 23:27:20.659471 139953274175488 ddar.py:60] Depth 3/1000 time = 7.756748914718628
I0123 23:27:32.505115 139953274175488 ddar.py:60] Depth 4/1000 time = 11.845392942428589
I0123 23:27:42.785360 139953274175488 ddar.py:60] Depth 5/1000 time = 10.279921770095825
I0123 23:27:53.826711 139953274175488 ddar.py:60] Depth 6/1000 time = 11.040982723236084
I0123 23:28:05.426253 139953274175488 ddar.py:60] Depth 7/1000 time = 11.57613229751587
I0123 23:28:16.342489 139953274175488 ddar.py:60] Depth 8/1000 time = 10.915859937667847
I0123 23:28:27.883310 139953274175488 ddar.py:60] Depth 9/1000 time = 11.370991945266724
I0123 23:28:39.285372 139953274175488 ddar.py:60] Depth 10/1000 time = 11.393629312515259
I0123 23:28:39.285817 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:28:39.285919 139953274175488 alphageometry.py:566] LM output (score=-3.110773): "o : T b o e g 22 ;"
I0123 23:28:39.285956 139953274175488 alphageometry.py:567] Translation: "o = on_tline o b e g"

I0123 23:28:39.285996 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b e g ? perp d c c n"
I0123 23:28:39.286172 139953274175488 graph.py:498] 
I0123 23:28:39.286234 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b e g ? perp d c c n
I0123 23:28:40.359692 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0252838134765625
I0123 23:28:44.451138 139953274175488 ddar.py:60] Depth 2/1000 time = 4.091268062591553
I0123 23:28:52.349650 139953274175488 ddar.py:60] Depth 3/1000 time = 7.89825439453125
I0123 23:29:04.490437 139953274175488 ddar.py:60] Depth 4/1000 time = 12.14045262336731
I0123 23:29:15.074575 139953274175488 ddar.py:60] Depth 5/1000 time = 10.58381962776184
I0123 23:29:26.350560 139953274175488 ddar.py:60] Depth 6/1000 time = 11.27562689781189
I0123 23:29:38.451891 139953274175488 ddar.py:60] Depth 7/1000 time = 12.074295282363892
I0123 23:29:50.340467 139953274175488 ddar.py:60] Depth 8/1000 time = 11.888247013092041
I0123 23:30:01.610369 139953274175488 ddar.py:60] Depth 9/1000 time = 11.09770655632019
I0123 23:30:13.365182 139953274175488 ddar.py:60] Depth 10/1000 time = 11.746355295181274
I0123 23:30:13.365716 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:30:13.365850 139953274175488 alphageometry.py:566] LM output (score=-3.111779): "o : T b o e i 22 ;"
I0123 23:30:13.365887 139953274175488 alphageometry.py:567] Translation: "o = on_tline o b e i"

I0123 23:30:13.365945 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b e i ? perp d c c n"
I0123 23:30:13.366150 139953274175488 graph.py:498] 
I0123 23:30:13.366212 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o b e i ? perp d c c n
I0123 23:30:14.440444 139953274175488 ddar.py:60] Depth 1/1000 time = 1.0258007049560547
I0123 23:30:18.636911 139953274175488 ddar.py:60] Depth 2/1000 time = 4.196275472640991
I0123 23:30:26.410376 139953274175488 ddar.py:60] Depth 3/1000 time = 7.773292064666748
I0123 23:30:38.659743 139953274175488 ddar.py:60] Depth 4/1000 time = 12.249150514602661
I0123 23:30:49.780146 139953274175488 ddar.py:60] Depth 5/1000 time = 11.120124101638794
I0123 23:31:01.657757 139953274175488 ddar.py:60] Depth 6/1000 time = 11.877316951751709
I0123 23:31:13.949395 139953274175488 ddar.py:60] Depth 7/1000 time = 12.263400077819824
I0123 23:31:25.796241 139953274175488 ddar.py:60] Depth 8/1000 time = 11.84660291671753
I0123 23:31:38.811872 139953274175488 ddar.py:60] Depth 9/1000 time = 12.8142728805542
I0123 23:31:51.958601 139953274175488 ddar.py:60] Depth 10/1000 time = 13.137362480163574
I0123 23:31:51.958978 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:31:51.959219 139953274175488 alphageometry.py:566] LM output (score=-3.120647): "o : T c o e i 22 ;"
I0123 23:31:51.959259 139953274175488 alphageometry.py:567] Translation: "o = on_tline o c e i"

I0123 23:31:51.959303 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c e i ? perp d c c n"
I0123 23:31:51.959526 139953274175488 graph.py:498] 
I0123 23:31:51.959626 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_tline o c e i ? perp d c c n
I0123 23:31:53.045006 139953274175488 ddar.py:60] Depth 1/1000 time = 1.02996826171875
I0123 23:31:56.684522 139953274175488 ddar.py:60] Depth 2/1000 time = 3.6392505168914795
I0123 23:32:05.533343 139953274175488 ddar.py:60] Depth 3/1000 time = 8.848477125167847
I0123 23:32:18.523227 139953274175488 ddar.py:60] Depth 4/1000 time = 12.989445686340332
I0123 23:32:30.373347 139953274175488 ddar.py:60] Depth 5/1000 time = 11.849739074707031
I0123 23:32:43.028216 139953274175488 ddar.py:60] Depth 6/1000 time = 12.654294490814209
I0123 23:32:56.085524 139953274175488 ddar.py:60] Depth 7/1000 time = 13.028182983398438
I0123 23:33:09.325612 139953274175488 ddar.py:60] Depth 8/1000 time = 13.239479064941406
I0123 23:33:21.817988 139953274175488 ddar.py:60] Depth 9/1000 time = 12.293772459030151
I0123 23:33:34.918731 139953274175488 ddar.py:60] Depth 10/1000 time = 13.092000961303711
I0123 23:33:34.919296 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:33:34.919516 139953274175488 alphageometry.py:540] Depth 1. There are 32 nodes to expand:
I0123 23:33:34.919595 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : D d i d o 22 ; x00
I0123 23:33:34.919635 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T c d c o 22 ; x00
I0123 23:33:34.919671 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T c o f j 22 ; x00
I0123 23:33:34.919729 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T f j f o 22 ; x00
I0123 23:33:34.919764 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : C e g o 22 D e o g o 23 ; x00
I0123 23:33:34.919796 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T a d a o 22 ; x00
I0123 23:33:34.919828 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T g h g o 22 ; x00
I0123 23:33:34.919860 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T f j j o 22 ; x00
I0123 23:33:34.919891 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T d g d o 22 ; x00
I0123 23:33:34.919919 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T d f d o 22 ; x00
I0123 23:33:34.919951 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : D d g d o 22 ; x00
I0123 23:33:34.919981 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T a o c d 22 ; x00
I0123 23:33:34.920014 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T a o f j 22 ; x00
I0123 23:33:34.920047 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T g h h o 22 ; x00
I0123 23:33:34.920078 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T d h h o 22 ; x00
I0123 23:33:34.920110 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T b o c d 22 ; x00
I0123 23:33:34.920142 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T b o g h 22 ; x00
I0123 23:33:34.920171 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : D d e d o 22 ; x00
I0123 23:33:34.920200 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T b o f j 22 ; x00
I0123 23:33:34.920230 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T f i f o 22 ; x00
I0123 23:33:34.920260 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T b o d e 22 ; x00
I0123 23:33:34.920296 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T c o e g 22 ; x00
I0123 23:33:34.920328 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T d h d o 22 ; x00
I0123 23:33:34.920359 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T c o g h 22 ; x00
I0123 23:33:34.920388 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T d e d o 22 ; x00
I0123 23:33:34.920417 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T e g f o 22 ; x00
I0123 23:33:34.920445 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T a o g h 22 ; x00
I0123 23:33:34.920474 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : P c d f o 22 ; x00
I0123 23:33:34.920503 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T a o e f 22 ; x00
I0123 23:33:34.920534 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T b o e g 22 ; x00
I0123 23:33:34.920562 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T b o e i 22 ; x00
I0123 23:33:34.920585 139953274175488 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : T c o e i 22 ; x00
I0123 23:33:34.920615 139953274175488 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C b c f 04 T b c d f 05 ; g : C a b g 06 T a b d g 07 ; h : C e f h 08 T e f g h 09 ; i : C g h i 10 D d g d i 11 ; j : C c i j 12 D d i d j 13 ; k : D a k e k 14 D a k j k 15 ; l : D b l f l 16 D b l j l 17 ; m : C k l m 18 T j m k l 19 ; n : C d g n 20 C j m n 21 ? T d c c n {F1} x00 o : D d i d o 22 ; x00
I0123 23:33:40.385389 139953274175488 alphageometry.py:566] LM output (score=-1.367119): "p : C c f p 23 D c f c p 24 ;"
I0123 23:33:40.385747 139953274175488 alphageometry.py:567] Translation: "p = on_line p c f, on_circle p c f"

I0123 23:33:40.385809 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p c f, on_circle p c f ? perp d c c n"
I0123 23:33:40.385993 139953274175488 graph.py:498] 
I0123 23:33:40.386061 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p c f, on_circle p c f ? perp d c c n
I0123 23:33:43.048634 139953274175488 ddar.py:60] Depth 1/1000 time = 2.6051690578460693
I0123 23:33:48.896384 139953274175488 ddar.py:60] Depth 2/1000 time = 5.847531080245972
I0123 23:34:03.615245 139953274175488 ddar.py:60] Depth 3/1000 time = 14.718616724014282
I0123 23:34:23.926000 139953274175488 ddar.py:60] Depth 4/1000 time = 20.31038475036621
I0123 23:34:44.839357 139953274175488 ddar.py:60] Depth 5/1000 time = 20.91280746459961
I0123 23:35:05.789670 139953274175488 ddar.py:60] Depth 6/1000 time = 20.949865102767944
I0123 23:35:27.569654 139953274175488 ddar.py:60] Depth 7/1000 time = 21.778810024261475
I0123 23:35:49.080667 139953274175488 ddar.py:60] Depth 8/1000 time = 21.479825496673584
I0123 23:36:11.065114 139953274175488 ddar.py:60] Depth 9/1000 time = 21.98377776145935
I0123 23:36:33.751610 139953274175488 ddar.py:60] Depth 10/1000 time = 22.37826371192932
I0123 23:36:55.495463 139953274175488 ddar.py:60] Depth 11/1000 time = 21.733309507369995
I0123 23:36:55.496072 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:36:55.496151 139953274175488 alphageometry.py:566] LM output (score=-1.456656): "p : C f j p 23 D f p j p 24 ;"
I0123 23:36:55.496188 139953274175488 alphageometry.py:567] Translation: "p = on_line p f j, on_bline p j f"

I0123 23:36:55.496232 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p f j, on_bline p j f ? perp d c c n"
I0123 23:36:55.496448 139953274175488 graph.py:498] 
I0123 23:36:55.496519 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p f j, on_bline p j f ? perp d c c n
I0123 23:36:57.219732 139953274175488 ddar.py:60] Depth 1/1000 time = 1.6640079021453857
I0123 23:37:03.290528 139953274175488 ddar.py:60] Depth 2/1000 time = 6.070420980453491
I0123 23:37:20.983590 139953274175488 ddar.py:60] Depth 3/1000 time = 17.69269371032715
I0123 23:37:40.029106 139953274175488 ddar.py:60] Depth 4/1000 time = 19.04487633705139
I0123 23:38:00.370488 139953274175488 ddar.py:60] Depth 5/1000 time = 20.340723037719727
I0123 23:38:20.824992 139953274175488 ddar.py:60] Depth 6/1000 time = 20.453805208206177
I0123 23:38:41.286825 139953274175488 ddar.py:60] Depth 7/1000 time = 20.460214376449585
I0123 23:39:01.973511 139953274175488 ddar.py:60] Depth 8/1000 time = 20.646595239639282
I0123 23:39:22.868226 139953274175488 ddar.py:60] Depth 9/1000 time = 20.894251823425293
I0123 23:39:43.711148 139953274175488 ddar.py:60] Depth 10/1000 time = 20.53568744659424
I0123 23:40:04.262759 139953274175488 ddar.py:60] Depth 11/1000 time = 20.545252561569214
I0123 23:40:04.263338 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:40:04.263454 139953274175488 alphageometry.py:566] LM output (score=-1.751807): "p : C e g p 23 D e p g p 24 ;"
I0123 23:40:04.263493 139953274175488 alphageometry.py:567] Translation: "p = on_line p e g, on_bline p g e"

I0123 23:40:04.263554 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p e g, on_bline p g e ? perp d c c n"
I0123 23:40:04.263795 139953274175488 graph.py:498] 
I0123 23:40:04.263863 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p e g, on_bline p g e ? perp d c c n
I0123 23:40:05.978535 139953274175488 ddar.py:60] Depth 1/1000 time = 1.6547977924346924
I0123 23:40:12.278494 139953274175488 ddar.py:60] Depth 2/1000 time = 6.299703121185303
I0123 23:40:32.405495 139953274175488 ddar.py:60] Depth 3/1000 time = 20.126661777496338
I0123 23:40:54.538419 139953274175488 ddar.py:60] Depth 4/1000 time = 22.13223886489868
I0123 23:41:22.886876 139953274175488 ddar.py:60] Depth 5/1000 time = 28.34780478477478
I0123 23:41:50.288862 139953274175488 ddar.py:60] Depth 6/1000 time = 27.4014413356781
I0123 23:42:18.629934 139953274175488 ddar.py:60] Depth 7/1000 time = 28.33967876434326
I0123 23:42:47.798961 139953274175488 ddar.py:60] Depth 8/1000 time = 29.132272720336914
I0123 23:43:16.859011 139953274175488 ddar.py:60] Depth 9/1000 time = 29.059335708618164
I0123 23:43:46.793267 139953274175488 ddar.py:60] Depth 10/1000 time = 29.933510780334473
I0123 23:44:15.877255 139953274175488 ddar.py:60] Depth 11/1000 time = 29.08168363571167
I0123 23:44:45.987075 139953274175488 ddar.py:60] Depth 12/1000 time = 29.663265705108643
I0123 23:45:16.929111 139953274175488 ddar.py:60] Depth 13/1000 time = 30.92388367652893
I0123 23:45:16.929988 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:45:16.930119 139953274175488 alphageometry.py:566] LM output (score=-2.139385): "p : P c p e h 23 ;"
I0123 23:45:16.930159 139953274175488 alphageometry.py:567] Translation: "p = on_pline p c e h"

I0123 23:45:16.930234 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_pline p c e h ? perp d c c n"
I0123 23:45:16.930478 139953274175488 graph.py:498] 
I0123 23:45:16.930544 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_pline p c e h ? perp d c c n
I0123 23:45:18.532745 139953274175488 ddar.py:60] Depth 1/1000 time = 1.5462687015533447
I0123 23:45:24.330605 139953274175488 ddar.py:60] Depth 2/1000 time = 5.7975451946258545
I0123 23:45:36.429471 139953274175488 ddar.py:60] Depth 3/1000 time = 12.09840703010559
I0123 23:45:54.477245 139953274175488 ddar.py:60] Depth 4/1000 time = 18.047260284423828
I0123 23:46:12.436109 139953274175488 ddar.py:60] Depth 5/1000 time = 17.958388566970825
I0123 23:46:29.583537 139953274175488 ddar.py:60] Depth 6/1000 time = 17.146892070770264
I0123 23:46:47.969936 139953274175488 ddar.py:60] Depth 7/1000 time = 18.354262590408325
I0123 23:47:06.804833 139953274175488 ddar.py:60] Depth 8/1000 time = 18.83421516418457
I0123 23:47:25.092255 139953274175488 ddar.py:60] Depth 9/1000 time = 18.028477907180786
I0123 23:47:44.000356 139953274175488 ddar.py:60] Depth 10/1000 time = 18.898702383041382
I0123 23:47:44.000960 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:47:44.001098 139953274175488 alphageometry.py:566] LM output (score=-2.157613): "p : C c e p 23 D c e c p 24 ;"
I0123 23:47:44.001139 139953274175488 alphageometry.py:567] Translation: "p = on_line p c e, on_circle p c e"

I0123 23:47:44.001214 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p c e, on_circle p c e ? perp d c c n"
I0123 23:47:44.001478 139953274175488 graph.py:498] 
I0123 23:47:44.001552 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p c e, on_circle p c e ? perp d c c n
I0123 23:47:46.667234 139953274175488 ddar.py:60] Depth 1/1000 time = 2.6046087741851807
I0123 23:47:52.795695 139953274175488 ddar.py:60] Depth 2/1000 time = 6.128167629241943
I0123 23:48:07.009022 139953274175488 ddar.py:60] Depth 3/1000 time = 14.212950468063354
I0123 23:48:26.836473 139953274175488 ddar.py:60] Depth 4/1000 time = 19.827089548110962
I0123 23:48:48.093348 139953274175488 ddar.py:60] Depth 5/1000 time = 21.256383657455444
I0123 23:49:09.427727 139953274175488 ddar.py:60] Depth 6/1000 time = 21.33385968208313
I0123 23:49:30.687054 139953274175488 ddar.py:60] Depth 7/1000 time = 21.257771015167236
I0123 23:49:52.465741 139953274175488 ddar.py:60] Depth 8/1000 time = 21.745227813720703
I0123 23:50:14.639276 139953274175488 ddar.py:60] Depth 9/1000 time = 22.17277979850769
I0123 23:50:36.909621 139953274175488 ddar.py:60] Depth 10/1000 time = 21.96641778945923
I0123 23:50:58.860375 139953274175488 ddar.py:60] Depth 11/1000 time = 21.93970775604248
I0123 23:50:58.860961 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:50:58.861049 139953274175488 alphageometry.py:566] LM output (score=-2.186298): "p : C f g p 23 D f p g p 24 ;"
I0123 23:50:58.861088 139953274175488 alphageometry.py:567] Translation: "p = on_line p f g, on_bline p g f"

I0123 23:50:58.861147 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p f g, on_bline p g f ? perp d c c n"
I0123 23:50:58.861370 139953274175488 graph.py:498] 
I0123 23:50:58.861441 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p f g, on_bline p g f ? perp d c c n
I0123 23:51:00.563228 139953274175488 ddar.py:60] Depth 1/1000 time = 1.6425378322601318
I0123 23:51:06.863453 139953274175488 ddar.py:60] Depth 2/1000 time = 6.299978971481323
I0123 23:51:24.230941 139953274175488 ddar.py:60] Depth 3/1000 time = 17.367138385772705
I0123 23:51:46.632681 139953274175488 ddar.py:60] Depth 4/1000 time = 22.401199340820312
I0123 23:52:13.616629 139953274175488 ddar.py:60] Depth 5/1000 time = 26.983402729034424
I0123 23:52:41.348481 139953274175488 ddar.py:60] Depth 6/1000 time = 27.73108696937561
I0123 23:53:10.045703 139953274175488 ddar.py:60] Depth 7/1000 time = 28.695764780044556
I0123 23:53:38.215873 139953274175488 ddar.py:60] Depth 8/1000 time = 28.134679317474365
I0123 23:54:07.365935 139953274175488 ddar.py:60] Depth 9/1000 time = 29.14929747581482
I0123 23:54:36.759264 139953274175488 ddar.py:60] Depth 10/1000 time = 29.39255714416504
I0123 23:55:07.149422 139953274175488 ddar.py:60] Depth 11/1000 time = 30.389383554458618
I0123 23:55:37.547049 139953274175488 ddar.py:60] Depth 12/1000 time = 29.955809593200684
I0123 23:56:07.522922 139953274175488 ddar.py:60] Depth 13/1000 time = 29.957937002182007
I0123 23:56:07.523795 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:56:07.523941 139953274175488 alphageometry.py:566] LM output (score=-2.233210): "p : C e j p 23 D e p j p 24 ;"
I0123 23:56:07.523981 139953274175488 alphageometry.py:567] Translation: "p = on_line p e j, on_bline p j e"

I0123 23:56:07.524068 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p e j, on_bline p j e ? perp d c c n"
I0123 23:56:07.524317 139953274175488 graph.py:498] 
I0123 23:56:07.524395 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p e j, on_bline p j e ? perp d c c n
I0123 23:56:09.266030 139953274175488 ddar.py:60] Depth 1/1000 time = 1.681588888168335
I0123 23:56:15.505432 139953274175488 ddar.py:60] Depth 2/1000 time = 6.239112377166748
I0123 23:56:34.666861 139953274175488 ddar.py:60] Depth 3/1000 time = 19.161043167114258
I0123 23:56:55.719396 139953274175488 ddar.py:60] Depth 4/1000 time = 21.05208158493042
I0123 23:57:15.834222 139953274175488 ddar.py:60] Depth 5/1000 time = 20.114341497421265
I0123 23:57:36.818646 139953274175488 ddar.py:60] Depth 6/1000 time = 20.983901500701904
I0123 23:57:57.870492 139953274175488 ddar.py:60] Depth 7/1000 time = 21.050434589385986
I0123 23:58:19.199173 139953274175488 ddar.py:60] Depth 8/1000 time = 21.290295124053955
I0123 23:58:40.520238 139953274175488 ddar.py:60] Depth 9/1000 time = 21.320348501205444
I0123 23:59:02.027754 139953274175488 ddar.py:60] Depth 10/1000 time = 21.20523715019226
I0123 23:59:23.280437 139953274175488 ddar.py:60] Depth 11/1000 time = 21.24603033065796
I0123 23:59:23.281064 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 23:59:23.281192 139953274175488 alphageometry.py:566] LM output (score=-2.242615): "p : P c p f h 23 ;"
I0123 23:59:23.281234 139953274175488 alphageometry.py:567] Translation: "p = on_pline p c f h"

I0123 23:59:23.281309 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_pline p c f h ? perp d c c n"
I0123 23:59:23.281558 139953274175488 graph.py:498] 
I0123 23:59:23.281636 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_pline p c f h ? perp d c c n
I0123 23:59:25.820828 139953274175488 ddar.py:60] Depth 1/1000 time = 2.481903553009033
I0123 23:59:30.785823 139953274175488 ddar.py:60] Depth 2/1000 time = 4.964694023132324
I0123 23:59:43.702541 139953274175488 ddar.py:60] Depth 3/1000 time = 12.916326522827148
I0124 00:00:00.714296 139953274175488 ddar.py:60] Depth 4/1000 time = 17.011314153671265
I0124 00:00:18.048665 139953274175488 ddar.py:60] Depth 5/1000 time = 17.33367085456848
I0124 00:00:34.570987 139953274175488 ddar.py:60] Depth 6/1000 time = 16.52160358428955
I0124 00:00:51.573055 139953274175488 ddar.py:60] Depth 7/1000 time = 16.971639156341553
I0124 00:01:09.715732 139953274175488 ddar.py:60] Depth 8/1000 time = 18.14200258255005
I0124 00:01:28.358217 139953274175488 ddar.py:60] Depth 9/1000 time = 18.3795325756073
I0124 00:01:45.753449 139953274175488 ddar.py:60] Depth 10/1000 time = 17.38609027862549
I0124 00:01:45.754060 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:01:45.754192 139953274175488 alphageometry.py:566] LM output (score=-2.261770): "p : C a e p 23 D a p e p 24 ;"
I0124 00:01:45.754235 139953274175488 alphageometry.py:567] Translation: "p = on_line p a e, on_bline p e a"

I0124 00:01:45.754309 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p a e, on_bline p e a ? perp d c c n"
I0124 00:01:45.754552 139953274175488 graph.py:498] 
I0124 00:01:45.754628 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p a e, on_bline p e a ? perp d c c n
I0124 00:01:48.544168 139953274175488 ddar.py:60] Depth 1/1000 time = 2.728943109512329
I0124 00:01:54.723365 139953274175488 ddar.py:60] Depth 2/1000 time = 6.17893123626709
I0124 00:02:09.482914 139953274175488 ddar.py:60] Depth 3/1000 time = 14.759167909622192
I0124 00:02:28.345549 139953274175488 ddar.py:60] Depth 4/1000 time = 18.861992359161377
I0124 00:02:48.177964 139953274175488 ddar.py:60] Depth 5/1000 time = 19.831682682037354
I0124 00:03:08.023482 139953274175488 ddar.py:60] Depth 6/1000 time = 19.844889879226685
I0124 00:03:27.804290 139953274175488 ddar.py:60] Depth 7/1000 time = 19.77935481071472
I0124 00:03:47.891674 139953274175488 ddar.py:60] Depth 8/1000 time = 20.055102586746216
I0124 00:04:08.134136 139953274175488 ddar.py:60] Depth 9/1000 time = 20.24196457862854
I0124 00:04:28.115789 139953274175488 ddar.py:60] Depth 10/1000 time = 19.706788539886475
I0124 00:04:47.090360 139953274175488 ddar.py:60] Depth 11/1000 time = 18.968459606170654
I0124 00:04:47.090992 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:04:47.091099 139953274175488 alphageometry.py:566] LM output (score=-2.304370): "p : P c p e f 23 ;"
I0124 00:04:47.091141 139953274175488 alphageometry.py:567] Translation: "p = on_pline p c e f"

I0124 00:04:47.091192 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_pline p c e f ? perp d c c n"
I0124 00:04:47.091404 139953274175488 graph.py:498] 
I0124 00:04:47.091475 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_pline p c e f ? perp d c c n
I0124 00:04:49.621456 139953274175488 ddar.py:60] Depth 1/1000 time = 2.4742815494537354
I0124 00:04:55.472616 139953274175488 ddar.py:60] Depth 2/1000 time = 5.8508124351501465
I0124 00:05:08.530070 139953274175488 ddar.py:60] Depth 3/1000 time = 13.057095766067505
I0124 00:05:26.052522 139953274175488 ddar.py:60] Depth 4/1000 time = 17.521881341934204
I0124 00:05:44.317920 139953274175488 ddar.py:60] Depth 5/1000 time = 18.26470923423767
I0124 00:06:01.787170 139953274175488 ddar.py:60] Depth 6/1000 time = 17.468652725219727
I0124 00:06:20.630717 139953274175488 ddar.py:60] Depth 7/1000 time = 18.811758279800415
I0124 00:06:38.882322 139953274175488 ddar.py:60] Depth 8/1000 time = 18.251121759414673
I0124 00:06:58.102151 139953274175488 ddar.py:60] Depth 9/1000 time = 18.962397575378418
I0124 00:07:17.116656 139953274175488 ddar.py:60] Depth 10/1000 time = 19.005467891693115
I0124 00:07:17.117096 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:07:17.117170 139953274175488 alphageometry.py:566] LM output (score=-2.332988): "p : C f i p 23 D f p i p 24 ;"
I0124 00:07:17.117209 139953274175488 alphageometry.py:567] Translation: "p = on_line p f i, on_bline p i f"

I0124 00:07:17.117265 139953274175488 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p f i, on_bline p i f ? perp d c c n"
I0124 00:07:17.117479 139953274175488 graph.py:498] 
I0124 00:07:17.117557 139953274175488 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = foot e d c a; f = foot f d c b; g = foot g d b a; h = foot h g e f; i = on_circle i d g, on_line i h g; j = on_circle j d i, on_line j c i; k = circle k j a e; l = circle l j b f; m = foot m j k l; n = on_line n j m, on_line n d g; o = on_circle o d i; p = on_line p f i, on_bline p i f ? perp d c c n
I0124 00:07:18.823329 139953274175488 ddar.py:60] Depth 1/1000 time = 1.6461684703826904
I0124 00:07:25.050011 139953274175488 ddar.py:60] Depth 2/1000 time = 6.226282835006714
I0124 00:07:42.914859 139953274175488 ddar.py:60] Depth 3/1000 time = 17.8644859790802
I0124 00:08:05.153997 139953274175488 ddar.py:60] Depth 4/1000 time = 22.238597631454468
I0124 00:08:27.698738 139953274175488 ddar.py:60] Depth 5/1000 time = 22.544244050979614
I0124 00:08:50.433295 139953274175488 ddar.py:60] Depth 6/1000 time = 22.734062671661377
I0124 00:08:50.434623 139953274175488 alphageometry.py:221] DD+AR failed to solve the problem.
I0124 00:08:50.434683 139953274175488 alphageometry.py:585] Timeout.
