I0123 12:31:26.917902 140190696792064 inference_utils.py:69] Parsing gin configuration.
I0123 12:31:26.918003 140190696792064 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:31:26.918209 140190696792064 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:31:26.918243 140190696792064 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:31:26.918272 140190696792064 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:31:26.918300 140190696792064 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:31:26.918326 140190696792064 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:31:26.918353 140190696792064 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:31:26.918380 140190696792064 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:31:26.918410 140190696792064 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:31:26.918440 140190696792064 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:31:26.918466 140190696792064 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:31:26.918514 140190696792064 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:31:26.918649 140190696792064 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:31:26.918857 140190696792064 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:31:26.918956 140190696792064 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:31:26.925383 140190696792064 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:31:26.925502 140190696792064 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:31:26.925833 140190696792064 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:31:26.925940 140190696792064 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:31:26.926229 140190696792064 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:31:26.926331 140190696792064 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:31:26.926749 140190696792064 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:31:26.926851 140190696792064 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:31:26.930536 140190696792064 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:31:27.034756 140190696792064 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:31:27.035499 140190696792064 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:31:27.042173 140190696792064 training_loop.py:335] Process 0 of 1
I0123 12:31:27.042228 140190696792064 training_loop.py:336] Local device count = 1
I0123 12:31:27.042270 140190696792064 training_loop.py:337] Number of replicas = 1
I0123 12:31:27.042302 140190696792064 training_loop.py:339] Using random number seed 42
I0123 12:31:27.506513 140190696792064 training_loop.py:359] Initializing the model.
I0123 12:31:27.879166 140190696792064 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.879410 140190696792064 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:31:27.879516 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.879597 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.879677 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.879761 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.879836 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.879908 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.879979 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.880050 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.880123 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.880196 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.880267 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.880337 140190696792064 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:27.880378 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:27.880425 140190696792064 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:31:27.880544 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:27.880585 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:27.880616 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:27.882700 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.888211 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:27.899106 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.899390 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:27.903869 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:27.915303 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:27.915363 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:27.915402 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:27.915435 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.915501 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.916719 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.916801 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.917549 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.920094 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.926406 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.927783 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.927868 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:27.927904 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:27.927966 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.928096 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:27.928443 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:27.928490 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:27.930468 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.930575 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:27.933537 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.933622 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:27.934136 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:27.944580 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:27.953664 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.953765 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:27.954080 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.954166 140190696792064 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:31:27.954282 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:27.954324 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:27.954356 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:27.956264 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.958792 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:27.964468 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.964738 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:27.967434 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:27.971340 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:27.971399 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:27.971437 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:27.971470 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.971536 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.972112 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.972189 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.972557 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.973333 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.975855 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.976500 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.976579 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:27.976614 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:27.976673 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.976803 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:27.977125 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:27.977169 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:27.979146 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.979242 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:27.981755 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.981839 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:27.982267 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:27.984613 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:27.986541 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.986638 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:27.986936 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.987017 140190696792064 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:31:27.987127 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:27.987166 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:27.987198 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:27.989456 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.991863 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:27.997468 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:27.997750 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.000421 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.004355 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.004411 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.004449 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.004482 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.004545 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.005128 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.005207 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.005572 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.006374 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.008920 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.009599 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.009687 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.009724 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.009786 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.009921 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.010253 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.010298 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.012247 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.012343 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.015107 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.015198 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.015693 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.018049 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.020030 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.020127 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.020431 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.020513 140190696792064 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:31:28.020625 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.020663 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.020695 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.022640 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.025073 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.030828 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.031103 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.033868 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.037767 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.037823 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.037860 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.037892 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.037955 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.038532 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.038612 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.038984 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.039776 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.042381 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.043019 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.043099 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.043135 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.043196 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.043328 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.043661 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.043706 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.045689 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.045789 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.048475 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.048561 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.049005 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.051332 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.053284 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.053382 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.053694 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.053781 140190696792064 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:31:28.053892 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.053932 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.053965 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.055913 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.058369 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.064169 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.064438 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.067507 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.071376 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.071433 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.071475 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.071507 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.071572 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.072149 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.072228 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.072601 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.073380 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.075989 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.076621 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.076700 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.076737 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.076797 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.076934 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.077260 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.077304 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.079270 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.079371 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.081983 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.082066 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.082509 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.084824 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.086831 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.086930 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.087229 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.087311 140190696792064 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:31:28.087422 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.087462 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.087494 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.089362 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.091826 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.097588 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.097861 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.100602 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.104431 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.104488 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.104525 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.104557 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.104619 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.105230 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.105310 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.105682 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.106473 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.109020 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.109658 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.109738 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.109774 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.109835 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.109966 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.110291 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.110336 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.112271 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.112365 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.114985 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.115067 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.115500 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.117845 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.119807 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.119905 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.120199 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.120281 140190696792064 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:31:28.120392 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.120430 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.120461 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.122319 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.124820 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.130573 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.130845 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.133552 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.138489 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.138592 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.138631 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.138664 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.138738 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.139329 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.139409 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.139785 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.140594 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.143167 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.143807 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.143886 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.143925 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.143987 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.144118 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.144463 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.144508 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.146849 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.146947 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.149520 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.149603 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.150060 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.291581 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.293820 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.293977 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.294300 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.294394 140190696792064 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:31:28.294514 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.294556 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.294589 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.296667 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.299226 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.305052 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.305338 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.308074 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.312065 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.312124 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.312161 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.312194 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.312258 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.312889 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.312969 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.313335 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.314142 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.316776 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.317425 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.317506 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.317542 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.317603 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.317744 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.318081 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.318125 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.320236 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.320332 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.323093 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.323175 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.323665 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.326012 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.327963 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.328068 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.328366 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.328449 140190696792064 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:31:28.328560 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.328600 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.328632 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.330586 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.333008 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.338742 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.339013 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.341757 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.345608 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.345669 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.345707 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.345739 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.345802 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.346369 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.346448 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.346813 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.347604 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.350199 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.350827 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.350906 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.350943 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.351003 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.351133 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.351464 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.351509 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.353438 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.353533 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.356134 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.356216 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.356654 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.358971 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.360897 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.360994 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.361288 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.361376 140190696792064 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:31:28.361489 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.361529 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.361561 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.363480 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.365908 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.371836 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.372102 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.374774 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.378503 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.378560 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.378596 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.378627 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.378689 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.379237 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.379317 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.379671 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.380500 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.382978 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.383592 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.383670 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.383706 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.383763 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.383896 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.384217 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.384260 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.386154 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.386249 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.388801 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.388884 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.389314 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.391584 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.393552 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.393654 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.393949 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.394037 140190696792064 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:31:28.394148 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.394187 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.394217 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.396043 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.398511 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.404069 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.404344 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.407028 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.410794 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.410851 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.410887 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.410917 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.411022 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.411582 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.411660 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.412021 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.412794 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.415290 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.415920 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.415998 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.416033 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.416092 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.416222 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.416540 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.416583 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.418550 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.418647 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.421390 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.421471 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.422039 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.424346 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.426260 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.426358 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.426648 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.426731 140190696792064 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:31:28.426847 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.426887 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.426917 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.428752 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.431213 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.436836 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.437102 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.439747 140190696792064 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:28.443879 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.443935 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.443971 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.444003 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.444066 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.444626 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.444703 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.445062 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.445841 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.448332 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.448957 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.449036 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.449070 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.449130 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.449261 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.449581 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.449624 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.451587 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.451682 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.454202 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.454286 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.454711 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.457010 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.458920 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.459017 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.459307 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.459597 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.459668 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.459733 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.459789 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.459842 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.459895 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.459949 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.460001 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.460053 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.460107 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.460158 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.460210 140190696792064 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:28.460249 140190696792064 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:31:28.463787 140190696792064 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:28.511826 140190696792064 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.511913 140190696792064 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:31:28.511967 140190696792064 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:31:28.512073 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.512112 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.512142 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.512206 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.514685 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.520194 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.520456 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.523201 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.539964 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.540021 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.540057 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.540088 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.540152 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.541300 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.541379 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.542109 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.544139 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.548931 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.550250 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.550337 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.550373 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.550434 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.550570 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.550680 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.550720 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.552632 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.552728 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.555172 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.555254 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.555364 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.557621 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.559585 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.559682 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.559976 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.560059 140190696792064 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:31:28.560171 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.560210 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.560240 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.560304 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.562597 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.568212 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.568475 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.571160 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.584244 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.584300 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.584335 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.584366 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.584429 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.584982 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.585064 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.585422 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.586117 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.588607 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.589226 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.589305 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.589344 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.589404 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.589533 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.589646 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.589685 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.591612 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.591708 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.594125 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.594207 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.594315 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.596525 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.598463 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.598561 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.598848 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.598929 140190696792064 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:31:28.599037 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.599075 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.599105 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.599168 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.601431 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.606960 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.607220 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.609946 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.622850 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.622907 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.622943 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.622974 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.623038 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.623595 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.623673 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.624040 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.624742 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.627285 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.627921 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.627999 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.628034 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.628098 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.628227 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.628335 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.628372 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.630339 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.630437 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.632894 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.632974 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.633082 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.635321 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.637262 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.637358 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.637655 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.637740 140190696792064 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:31:28.637849 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.637888 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.637919 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.637982 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.640260 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.645753 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.646014 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.648707 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.661537 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.661593 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.661629 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.661667 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.661732 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.662286 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.662364 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.662728 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.663424 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.665931 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.666555 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.666635 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.666670 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.666732 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.666867 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.666976 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.667014 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.669262 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.669357 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.671795 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.671877 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.671985 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.674228 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.676091 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.676186 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.676471 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.676552 140190696792064 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:31:28.676661 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.676700 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.676732 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.676795 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.679133 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.684626 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.684896 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.687547 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.700481 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.700537 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.700572 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.700603 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.700665 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.701229 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.701307 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.701677 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.702379 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.704923 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.705548 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.705627 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.705670 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.705731 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.705868 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.705979 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.706018 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.707911 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.708005 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.710434 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.710516 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.710623 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.712925 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.714812 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.714910 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.715198 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.715281 140190696792064 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:31:28.715389 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.715428 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.715459 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.715523 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.717798 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.723284 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.723547 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.726247 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.739027 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.739084 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.739120 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.739151 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.739214 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.739784 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.739861 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.740216 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.740917 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.743423 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.744050 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.744131 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.744166 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.744225 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.744357 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.744472 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.744511 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.746472 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.746568 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.748985 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.749070 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.749178 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.751406 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.753267 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.753362 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.753656 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.753741 140190696792064 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:31:28.753849 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.753887 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.753918 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.753981 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.756246 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.761785 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.762046 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.764661 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.777839 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.777895 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.777930 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.777961 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.778028 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.778591 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.778669 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.779027 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.779716 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.782350 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.783019 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.783098 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.783133 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.783193 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.783327 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.783437 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.783479 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.785364 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.785457 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.787903 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.787984 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.788091 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.790310 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.792229 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.792326 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.792613 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.792695 140190696792064 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:31:28.792806 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.792845 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.792876 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.792941 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.799760 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.805412 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.805717 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.808498 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.821435 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.821494 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.821531 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.821563 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.821625 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.822279 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.822357 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.822723 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.823430 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.825981 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.826605 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.826686 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.826721 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.826783 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.826919 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.827033 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.827077 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.828985 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.829080 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.831602 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.831683 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.831794 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.834066 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.835965 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.836062 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.836351 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.836435 140190696792064 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:31:28.836546 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.836588 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.836620 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.836684 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.838971 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.844537 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.844799 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.847453 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.860361 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.860419 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.860457 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.860489 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.860554 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.861146 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.861226 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.861596 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.862330 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.864872 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.865538 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.865617 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.865660 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.865720 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.865853 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.865965 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.866005 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.867956 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.868051 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.870509 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.870594 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.870710 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.872977 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.874979 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.875081 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.875381 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.875467 140190696792064 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:31:28.875581 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.875621 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.875652 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.875715 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.877986 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.883616 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.883878 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.886946 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.899932 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.899988 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.900023 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.900053 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.900116 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.900724 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.900802 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.901161 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.901866 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.904433 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.905059 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.905137 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.905171 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.905230 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.905363 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.905473 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.905511 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.907472 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.907579 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.910060 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.910147 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.910259 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.912541 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.914462 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.914564 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.914863 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.914949 140190696792064 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:31:28.915063 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.915103 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.915136 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.915203 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.917479 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.923158 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.923430 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.926115 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.939170 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.939229 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.939266 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.939298 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.939367 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.939936 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.940013 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.940367 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.941061 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.943596 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.944265 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.944343 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.944378 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.944436 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.944568 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.944679 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.944718 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.946650 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.946757 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.949229 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.949308 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.949416 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.951690 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.953618 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.953723 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.954016 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.954101 140190696792064 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:31:28.954212 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:28.954252 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:28.954284 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:28.954348 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.956647 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:28.962165 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.962434 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:28.965109 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:28.977999 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:28.978056 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:28.978094 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:28.978126 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.978192 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.978777 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.978858 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.979235 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.979943 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.982509 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.983134 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.983213 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:28.983247 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:28.983306 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.983442 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:28.983553 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:28.983592 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.985479 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.985573 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.988006 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.988088 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:28.988195 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:28.990819 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:28.992697 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.992793 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:28.993081 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:28.993172 140190696792064 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:31:28.996077 140190696792064 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:29.052709 140190696792064 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.052794 140190696792064 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:31:29.052850 140190696792064 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:31:29.052958 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.052996 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.053027 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.053090 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.055477 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.060911 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.061171 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.063765 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.076271 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.076328 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.076363 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.076394 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.076456 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.077011 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.077088 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.077445 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.078149 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.080650 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.081263 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.081341 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.081377 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.081435 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.081566 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.081688 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.081729 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.083561 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.083656 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.086080 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.086162 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.086272 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.088532 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.090407 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.090504 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.090792 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.090875 140190696792064 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:31:29.090983 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.091022 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.091053 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.091117 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.093360 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.098741 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.099000 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.101665 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.114086 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.114144 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.114180 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.114212 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.114275 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.114825 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.114902 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.115260 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.115937 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.118468 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.119106 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.119187 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.119224 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.119285 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.119418 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.119530 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.119577 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.121424 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.121519 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.123919 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.124001 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.124109 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.126374 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.128206 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.128302 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.128589 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.128671 140190696792064 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:31:29.128782 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.128821 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.128852 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.128915 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.131145 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.136578 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.136834 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.139498 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.151929 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.151986 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.152022 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.152052 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.152114 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.152665 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.152744 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.153103 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.153794 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.156750 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.157377 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.157456 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.157493 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.157555 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.157695 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.157808 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.157848 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.159745 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.159841 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.162298 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.162379 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.162488 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.164767 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.166655 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.166754 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.167044 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.167127 140190696792064 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:31:29.167237 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.167275 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.167307 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.167371 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.169635 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.175071 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.175335 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.178033 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.190717 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.190774 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.190812 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.190855 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.190920 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.191481 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.191557 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.191909 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.192597 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.195150 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.195777 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.195855 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.195889 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.195946 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.196077 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.196184 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.196223 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.198103 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.198198 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.200615 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.200694 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.200801 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.203095 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.204960 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.205054 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.205338 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.205420 140190696792064 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:31:29.205526 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.205563 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.205592 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.205664 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.207912 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.213304 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.213561 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.216261 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.228822 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.228877 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.228911 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.228940 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.229001 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.229552 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.229628 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.229989 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.230683 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.233188 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.233825 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.233903 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.233936 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.233994 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.234125 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.234235 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.234272 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.236275 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.236373 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.238820 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.238899 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.239005 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.241257 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.243123 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.243219 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.243505 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.243586 140190696792064 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:31:29.243693 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.243730 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.243760 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.243822 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.246072 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.251460 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.251718 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.254404 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.266955 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.267010 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.267044 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.267073 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.267134 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.267691 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.267767 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.268123 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.268820 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.271765 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.272386 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.272462 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.272495 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.272550 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.272674 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.272779 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.272816 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.274692 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.274791 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.277178 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.277256 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.277361 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.279658 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.281514 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.281610 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.281902 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.281983 140190696792064 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:31:29.282089 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.282126 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.282155 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.282216 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.284456 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.289848 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.290104 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.292793 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.305414 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.305469 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.305502 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.305532 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.305593 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.306159 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.306235 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.306592 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.307275 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.309813 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.310438 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.310514 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.310547 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.310605 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.310731 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.310839 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.310876 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.312731 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.312823 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.315225 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.315304 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.315415 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.317701 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.319584 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.319679 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.319968 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.320050 140190696792064 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:31:29.320156 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.320194 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.320224 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.320286 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.322536 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.327992 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.328258 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.330990 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.343876 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.343931 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.343965 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.343994 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.344056 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.344616 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.344693 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.345044 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.345734 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.348251 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.348869 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.348945 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.348979 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.349036 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.349160 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.349266 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.349303 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.351164 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.351257 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.353624 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.353720 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.353829 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.356087 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.357943 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.358038 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.358320 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.358401 140190696792064 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:31:29.358506 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.358542 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.358571 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.358631 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.360830 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.366176 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.366434 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.369084 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.381530 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.381585 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.381618 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.381654 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.381721 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.382271 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.382345 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.382700 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.383384 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.386314 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.386935 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.387011 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.387045 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.387101 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.387226 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.387332 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.387369 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.389219 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.389312 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.391712 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.391796 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.391904 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.394167 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.396023 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.396116 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.396396 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.396476 140190696792064 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:31:29.396582 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.396619 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.396647 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.396707 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.398924 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.404302 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.404556 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.407214 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.419731 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.419785 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.419820 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.419850 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.419911 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.420472 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.420548 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.420900 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.421586 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.424110 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.424727 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.424803 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.424837 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.424893 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.425019 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.425124 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.425160 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.427541 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.427635 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.430014 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.430093 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.430205 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.432429 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.434257 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.434351 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.434635 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.434715 140190696792064 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:31:29.434819 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.434856 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.434885 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.434945 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.437153 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.442518 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.442929 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.445757 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.458235 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.458290 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.458324 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.458354 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.458415 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.458962 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.459038 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.459397 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.460074 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.462593 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.463206 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.463282 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.463315 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.463371 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.463495 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.463599 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.463635 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.465475 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.465566 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.467957 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.468036 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.468141 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.470404 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.472239 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.472332 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.472612 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.472692 140190696792064 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:31:29.472796 140190696792064 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:29.472833 140190696792064 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:29.472862 140190696792064 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:29.472922 140190696792064 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.475144 140190696792064 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:29.480475 140190696792064 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.480733 140190696792064 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:29.483412 140190696792064 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:29.495852 140190696792064 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:29.495907 140190696792064 attention.py:418] Single window, no scan.
I0123 12:31:29.495941 140190696792064 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:29.495971 140190696792064 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.496031 140190696792064 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.496580 140190696792064 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.496656 140190696792064 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.497006 140190696792064 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.497697 140190696792064 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.500557 140190696792064 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.501165 140190696792064 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.501241 140190696792064 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:29.501275 140190696792064 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:29.501331 140190696792064 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.501455 140190696792064 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:29.501565 140190696792064 nn_components.py:325] mlp: activation = None
I0123 12:31:29.501602 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.503458 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.503549 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.505936 140190696792064 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.506018 140190696792064 transformer_base.py:443] tbase: final FFN
I0123 12:31:29.506122 140190696792064 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:29.508370 140190696792064 nn_components.py:329] mlp: final activation = None
I0123 12:31:29.510220 140190696792064 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.510315 140190696792064 nn_components.py:261] mlp: residual
I0123 12:31:29.510601 140190696792064 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:29.510686 140190696792064 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:31:29.513499 140190696792064 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:33.974501 140190696792064 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:31:34.532124 140190696792064 training_loop.py:409] No working directory specified.
I0123 12:31:34.532250 140190696792064 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:31:34.533025 140190696792064 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:31:37.602361 140190696792064 training_loop.py:447] Only restoring trainable parameters.
I0123 12:31:37.603055 140190696792064 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:31:37.603113 140190696792064 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.603160 140190696792064 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.603202 140190696792064 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.603243 140190696792064 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.603283 140190696792064 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.603323 140190696792064 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.603363 140190696792064 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.603402 140190696792064 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.603441 140190696792064 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.603480 140190696792064 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.603518 140190696792064 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.603557 140190696792064 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.603594 140190696792064 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.603632 140190696792064 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.603669 140190696792064 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.603706 140190696792064 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.603744 140190696792064 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.603781 140190696792064 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.603817 140190696792064 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.603865 140190696792064 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.603904 140190696792064 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.603942 140190696792064 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.603978 140190696792064 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.604015 140190696792064 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604053 140190696792064 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.604090 140190696792064 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604127 140190696792064 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604165 140190696792064 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.604202 140190696792064 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.604239 140190696792064 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604276 140190696792064 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.604313 140190696792064 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.604351 140190696792064 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.604387 140190696792064 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604425 140190696792064 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.604463 140190696792064 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604499 140190696792064 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604535 140190696792064 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.604571 140190696792064 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.604607 140190696792064 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604643 140190696792064 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.604681 140190696792064 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.604717 140190696792064 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.604754 140190696792064 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604791 140190696792064 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.604834 140190696792064 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604875 140190696792064 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.604912 140190696792064 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.604949 140190696792064 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.604987 140190696792064 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605024 140190696792064 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.605060 140190696792064 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.605097 140190696792064 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.605134 140190696792064 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605170 140190696792064 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.605207 140190696792064 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605244 140190696792064 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605280 140190696792064 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.605317 140190696792064 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.605354 140190696792064 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605391 140190696792064 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.605428 140190696792064 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.605465 140190696792064 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.605502 140190696792064 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605538 140190696792064 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.605575 140190696792064 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605611 140190696792064 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605662 140190696792064 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.605707 140190696792064 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.605746 140190696792064 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605783 140190696792064 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.605820 140190696792064 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.605862 140190696792064 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.605900 140190696792064 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.605938 140190696792064 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.605975 140190696792064 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606011 140190696792064 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606048 140190696792064 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.606086 140190696792064 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.606124 140190696792064 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606162 140190696792064 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.606199 140190696792064 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.606237 140190696792064 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.606275 140190696792064 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606311 140190696792064 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.606348 140190696792064 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606397 140190696792064 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606441 140190696792064 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.606479 140190696792064 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.606517 140190696792064 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606555 140190696792064 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.606594 140190696792064 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.606632 140190696792064 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.606669 140190696792064 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606706 140190696792064 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.606744 140190696792064 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606781 140190696792064 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606818 140190696792064 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.606856 140190696792064 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.606899 140190696792064 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.606939 140190696792064 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.606978 140190696792064 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.607016 140190696792064 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.607055 140190696792064 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.607093 140190696792064 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.607131 140190696792064 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.607169 140190696792064 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.607207 140190696792064 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.607244 140190696792064 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.607281 140190696792064 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.607319 140190696792064 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.607357 140190696792064 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:37.607393 140190696792064 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:37.607431 140190696792064 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.607469 140190696792064 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.607506 140190696792064 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.607544 140190696792064 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.607582 140190696792064 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:37.607619 140190696792064 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:37.607657 140190696792064 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:37.607694 140190696792064 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:37.607725 140190696792064 training_loop.py:725] Total parameters: 152072288
I0123 12:31:37.607937 140190696792064 training_loop.py:739] Total state size: 0
I0123 12:31:37.627948 140190696792064 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:31:37.628208 140190696792064 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:31:37.628668 140190696792064 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:31:37.628984 140190696792064 training_loop.py:89] registering functions: dict_keys([])
I0123 12:31:37.644942 140190696792064 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = circle e c a d; f = midpoint f a b; g = on_circle g e d, on_line g f d; h = foot h a g e; i = mirror i a h; j = foot j b a c; k = foot k b a i; l = foot l b c i ? cong b j b k
I0123 12:31:38.738427 140190696792064 ddar.py:60] Depth 1/1000 time = 1.0455365180969238
I0123 12:31:42.272433 140190696792064 ddar.py:60] Depth 2/1000 time = 3.5338497161865234
I0123 12:31:54.914411 140190696792064 ddar.py:60] Depth 3/1000 time = 12.641749858856201
I0123 12:31:54.919753 140190696792064 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K : Points
DA = DB [00]
DB = DC [01]
EC = EA [02]
B,F,A are collinear [03]
FA = FB [04]
EG = ED [05]
D,F,G are collinear [06]
AH  EG [07]
H,G,E are collinear [08]
H,I,A are collinear [09]
C,J,A are collinear [10]
BJ  AC [11]
I,K,A are collinear [12]
AI  KB [13]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. J,C,A are collinear [10] & I,K,A are collinear [12] & H,I,A are collinear [09] & AH  EG [07] & BJ  AC [11] & AI  KB [13]   BJA = BKA [14]
002. BJA = BKA [14]   J,B,K,A are concyclic [15]
003. DA = DB [00] & FA = FB [04]   AB  DF [16]
004. B,F,A are collinear [03] & H,I,A are collinear [09] & H,E,G are collinear [08] & AH  EG [07] & AB  DF [16]   BFD = IHG [17]
005. EG = ED [05]   EDG = DGE [18]
006. H,E,G are collinear [08] & EDG = DGE [18] & D,F,G are collinear [06]   (DF-HG) = EDF [19]
007. BFD = IHG [17] & (DF-HG) = EDF [19]   (BF-DE) = (HI-DF) [20]
008. B,F,A are collinear [03] & AB  DF [16]   BF  DF [21]
009. DB = DC [01] & DA = DB [00]   DA = DC [22]
010. DA = DC [22] & EC = EA [02]   AC  DE [23]
011. C,A,J are collinear [10] & DE  AC [23]   DE  CJ [24]
012. BF  DF [21] & DE  CJ [24]   (BF-CJ) = FDE [25]
013. J,B,K,A are concyclic [15]   JKB = JAB [26]
014. I,K,A are collinear [12] & H,I,A are collinear [09] & (BF-DE) = (HI-DF) [20] & B,F,A are collinear [03] & (BF-CJ) = FDE [25] & C,J,A are collinear [10] & JKB = JAB [26] & AI  KB [13] & AH  EG [07]   JKB = BAK [27]
015. J,B,K,A are concyclic [15] & JKB = BAK [27]   JB = BK
==========================

