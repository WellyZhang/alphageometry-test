I0123 11:01:05.209033 139732788322304 inference_utils.py:69] Parsing gin configuration.
I0123 11:01:05.209140 139732788322304 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:01:05.209382 139732788322304 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:01:05.209418 139732788322304 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:01:05.209450 139732788322304 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:01:05.209480 139732788322304 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:01:05.209508 139732788322304 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:01:05.209537 139732788322304 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:01:05.209565 139732788322304 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:01:05.209593 139732788322304 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:01:05.209621 139732788322304 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:01:05.209653 139732788322304 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:01:05.209705 139732788322304 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:01:05.209842 139732788322304 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:01:05.210244 139732788322304 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:01:05.210356 139732788322304 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:01:05.216617 139732788322304 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:01:05.216749 139732788322304 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:01:05.217085 139732788322304 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:01:05.217197 139732788322304 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:01:05.217491 139732788322304 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:01:05.217600 139732788322304 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:01:05.218037 139732788322304 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:01:05.218144 139732788322304 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:01:05.221866 139732788322304 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:01:05.311291 139732788322304 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:01:05.312010 139732788322304 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:01:05.318845 139732788322304 training_loop.py:335] Process 0 of 1
I0123 11:01:05.318904 139732788322304 training_loop.py:336] Local device count = 1
I0123 11:01:05.318944 139732788322304 training_loop.py:337] Number of replicas = 1
I0123 11:01:05.318976 139732788322304 training_loop.py:339] Using random number seed 42
I0123 11:01:05.772098 139732788322304 training_loop.py:359] Initializing the model.
I0123 11:01:06.160272 139732788322304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.160524 139732788322304 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:01:06.160630 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.160712 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.160792 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.160878 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.160956 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.161030 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.161104 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.161177 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.161250 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.161321 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.161395 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.161466 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:06.161508 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.161555 139732788322304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:06.161683 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.161727 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.161760 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.163779 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.169101 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.179800 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.180082 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.184531 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.195369 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.195432 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.195473 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.195508 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.195575 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.196779 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.196862 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.197597 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.200120 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.205931 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.207679 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.207765 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.207803 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.207868 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.208001 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.208344 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.208395 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.210360 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.210468 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.213364 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.213450 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.213962 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.224328 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.233258 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.233364 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.233682 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.233772 139732788322304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:06.233893 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.233936 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.233970 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.235850 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.238571 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.244254 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.244526 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.247203 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.251145 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.251206 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.251245 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.251278 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.251343 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.251930 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.252011 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.252379 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.253167 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.255654 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.256293 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.256376 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.256413 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.256477 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.256610 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.256937 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.256983 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.259569 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.259726 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.262318 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.262406 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.262847 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.265242 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.267273 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.267375 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.267674 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.267764 139732788322304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:06.267880 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.267922 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.267956 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.269914 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.272310 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.278346 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.278631 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.281328 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.285335 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.285398 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.285439 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.285473 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.285539 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.286146 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.286228 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.286598 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.287389 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.289927 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.290607 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.290691 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.290729 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.290791 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.290925 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.291262 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.291312 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.293252 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.293352 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.295924 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.296016 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.296514 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.298854 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.300814 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.300918 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.301218 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.301305 139732788322304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:06.301421 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.301464 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.301499 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.303452 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.305907 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.311668 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.311940 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.314629 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.318542 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.318602 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.318641 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.318674 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.318740 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.319321 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.319402 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.319769 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.320564 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.323153 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.323786 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.323869 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.323906 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.323973 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.324113 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.324450 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.324499 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.326436 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.326535 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.329108 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.329203 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.329651 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.331958 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.333917 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.334020 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.334318 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.334404 139732788322304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:06.334519 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.334562 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.334595 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.336526 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.338947 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.344636 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.344911 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.347629 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.351471 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.351532 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.351570 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.351602 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.351666 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.352247 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.352332 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.352701 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.353502 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.356408 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.357056 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.357140 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.357186 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.357253 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.357392 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.357730 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.357781 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.359706 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.359804 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.362382 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.362467 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.362905 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.365220 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.367256 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.367357 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.367656 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.367743 139732788322304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:06.367858 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.367902 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.367936 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.369997 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.372425 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.378142 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.378416 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.381147 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.384949 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.385010 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.385049 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.385083 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.385149 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.385775 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.385857 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.386218 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.387008 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.389488 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.390129 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.390211 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.390249 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.390316 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.390451 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.390783 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.390832 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.392751 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.392849 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.395421 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.395507 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.395951 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.398298 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.400242 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.400343 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.400655 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.400760 139732788322304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:06.400874 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.400917 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.400952 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.402891 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.405361 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.411173 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.411463 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.414206 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.418093 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.418154 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.418193 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.418227 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.418293 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.418867 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.418948 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.419315 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.420110 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.422626 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.423267 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.423350 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.423389 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.423452 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.423588 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.423918 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.423968 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.425980 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.426079 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.428646 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.428731 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.429317 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.432148 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.434108 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.434516 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.434818 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.434911 139732788322304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:06.435026 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.435069 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.435103 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.577448 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.580625 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.586592 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.586897 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.589637 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.593699 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.593764 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.593805 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.593840 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.593910 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.594546 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.594631 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.595014 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.595814 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.598464 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.599127 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.599211 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.599249 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.599314 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.599447 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.599801 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.599850 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.601785 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.601885 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.604503 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.604589 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.605036 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.607410 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.609365 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.609479 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.609788 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.609878 139732788322304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:06.609995 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.610038 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.610073 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.612118 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.614747 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.620545 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.620823 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.623551 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.627476 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.627537 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.627577 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.627610 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.627676 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.628261 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.628341 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.628708 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.629496 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.632121 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.632770 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.632852 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.632890 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.632954 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.633088 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.633424 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.633472 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.635415 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.635519 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.638460 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.638546 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.638984 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.641316 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.643332 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.643432 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.643728 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.643824 139732788322304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:06.643944 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.643986 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.644021 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.645910 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.648393 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.653959 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.654237 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.657294 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.661067 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.661126 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.661164 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.661196 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.661261 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.661895 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.661978 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.662349 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.663133 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.665634 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.666285 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.666368 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.666405 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.666469 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.666601 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.666934 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.666988 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.669085 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.669184 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.671966 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.672052 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.672482 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.674808 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.676713 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.676813 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.677106 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.677201 139732788322304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:06.677319 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.677361 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.677393 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.679246 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.681692 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.687251 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.687521 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.690193 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.694018 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.694078 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.694116 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.694149 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.694214 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.694796 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.694876 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.695240 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.696018 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.698521 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.699164 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.699248 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.699286 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.699347 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.699484 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.699813 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.699864 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.701813 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.701915 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.704688 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.704772 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.705210 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.707554 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.709477 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.709577 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.709880 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.709967 139732788322304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:06.710092 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.710135 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.710167 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.712116 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.714522 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.720154 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.720425 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.723060 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:06.726920 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.726980 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.727017 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.727048 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.727111 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.727691 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.727772 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.728135 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.728913 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.731404 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.732402 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.732487 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.732532 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.732619 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.732794 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.733161 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.733213 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.735187 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.735285 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.737799 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.737885 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.738372 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.740659 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.742583 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.742685 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.742982 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.743279 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743355 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743426 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743485 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743543 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743597 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743653 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743707 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743762 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743816 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743869 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743923 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:06.743962 139732788322304 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:06.747520 139732788322304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:06.796366 139732788322304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.796459 139732788322304 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:06.796518 139732788322304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:06.796629 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.796672 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.796706 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.796773 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.799448 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.805029 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.805293 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.807973 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:06.824901 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.824963 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.825002 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.825036 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.825101 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.826272 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.826356 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.827079 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.829132 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.833933 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.835278 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.835371 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.835413 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.835478 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.835621 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.835739 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.835781 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.837742 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.837843 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.840383 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.840468 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.840583 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.842850 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.844840 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.844941 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.845237 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.845325 139732788322304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:06.845439 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.845482 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.845515 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.845583 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.847871 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.853447 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.853735 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.856444 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:06.870160 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.870223 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.870263 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.870298 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.870366 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.870965 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.871060 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.871507 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.872252 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.874859 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.875530 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.875618 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.875664 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.875729 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.875870 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.875993 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.876039 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.878030 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.878135 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.880777 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.880865 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.880984 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.883369 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.885403 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.885504 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.885809 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.885898 139732788322304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:06.886015 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.886060 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.886096 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.886164 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.888492 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.894127 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.894397 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.897119 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:06.910871 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.910935 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.910975 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.911010 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.911078 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.911684 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.911776 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.912145 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.912855 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.915432 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.916107 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.916193 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.916231 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.916301 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.916437 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.916551 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.916594 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.922489 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.922635 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.925335 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.925422 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.925539 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.927988 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.929987 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.930089 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.930393 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.930488 139732788322304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:06.930613 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.930662 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.930698 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.930772 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.933146 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.938897 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.939180 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.942015 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:06.955511 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.955578 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.955629 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.955671 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.955748 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.956350 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.956433 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.956835 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.957546 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.960166 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.960804 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.960886 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:06.960925 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:06.960989 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.961138 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:06.961255 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:06.961299 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.963364 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.963467 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.965937 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.966024 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:06.966141 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:06.968461 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:06.970378 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.970483 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:06.970785 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.970874 139732788322304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:06.970991 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:06.971036 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:06.971072 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:06.971143 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.973832 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:06.979378 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.979649 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:06.982287 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:06.995274 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:06.995334 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:06.995375 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:06.995410 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.995481 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.996057 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.996138 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.996504 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.997232 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:06.999809 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.000457 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.000543 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.000583 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.000647 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.000789 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.000905 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.000948 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.002887 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.002986 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.005652 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.005739 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.005852 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.008194 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.010188 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.010306 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.010612 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.010703 139732788322304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:07.010822 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.010866 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.010902 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.010971 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.013314 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.018877 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.019142 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.021884 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.034999 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.035064 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.035104 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.035138 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.035203 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.035780 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.035862 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.036231 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.036952 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.039484 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.040116 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.040198 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.040235 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.040297 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.040432 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.040559 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.040603 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.042592 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.042692 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.045150 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.045235 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.045348 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.047658 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.049575 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.049681 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.049974 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.050063 139732788322304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:07.050177 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.050220 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.050253 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.050320 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.052610 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.058268 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.058534 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.061166 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.074278 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.074340 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.074379 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.074414 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.074479 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.075059 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.075144 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.075512 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.076215 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.078724 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.079746 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.079832 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.079870 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.079934 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.080070 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.080187 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.080235 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.082222 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.082323 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.084817 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.084903 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.085017 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.087310 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.089272 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.089372 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.089676 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.089765 139732788322304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:07.089881 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.089925 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.089959 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.090024 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.092298 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.097826 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.098118 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.100900 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.113897 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.113958 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.113998 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.114032 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.114098 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.114724 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.114807 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.115174 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.115876 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.118381 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.119058 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.119144 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.119181 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.119252 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.119392 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.119507 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.119556 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.121497 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.121596 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.124076 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.124161 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.124272 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.126602 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.128502 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.128601 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.128889 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.128975 139732788322304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:07.129086 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.129130 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.129164 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.129230 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.131499 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.137074 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.137342 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.139986 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.153096 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.153156 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.153195 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.153228 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.153292 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.153874 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.153955 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.154318 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.155020 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.157526 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.158215 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.158299 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.158339 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.158403 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.158539 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.158656 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.158698 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.160596 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.160695 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.163140 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.163225 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.163341 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.165598 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.167594 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.167697 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.167988 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.168077 139732788322304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:07.168191 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.168235 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.168270 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.168337 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.170631 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.176179 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.176454 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.179188 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.192490 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.192551 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.192590 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.192625 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.192689 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.193311 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.193394 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.193761 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.194473 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.196965 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.197601 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.197691 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.197730 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.197793 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.197929 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.198044 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.198086 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.199981 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.200086 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.202581 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.202669 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.202786 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.205065 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.206975 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.207076 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.207363 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.207450 139732788322304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:07.207563 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.207606 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.207639 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.207705 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.209987 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.215570 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.215834 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.218540 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.231466 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.231527 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.231566 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.231600 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.231664 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.232231 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.232313 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.232671 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.233364 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.235848 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.236526 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.236609 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.236646 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.236709 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.236844 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.236958 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.237001 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.238936 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.239051 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.241615 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.241711 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.241831 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.244176 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.246228 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.246334 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.246636 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.246726 139732788322304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:07.246844 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.246889 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.246925 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.246994 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.249356 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.255363 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.255642 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.258440 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.271554 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.271616 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.271655 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.271689 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.271756 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.272333 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.272416 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.272775 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.273534 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.276068 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.276709 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.276794 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.276832 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.276894 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.277030 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.277145 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.277188 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.279114 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.279214 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.281668 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.281754 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.281874 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.284198 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.286121 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.286222 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.286515 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.286612 139732788322304 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:07.289533 139732788322304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:07.346065 139732788322304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.346156 139732788322304 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:07.346213 139732788322304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:07.346323 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.346365 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.346399 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.346466 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.349171 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.354780 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.355073 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.357917 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.370960 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.371024 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.371065 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.371103 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.371173 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.371771 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.371854 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.372219 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.372916 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.375539 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.376181 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.376265 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.376303 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.376366 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.376501 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.376629 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.376672 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.378595 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.378699 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.381174 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.381259 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.381375 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.383743 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.385635 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.385751 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.386048 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.386144 139732788322304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:07.386262 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.386307 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.386343 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.386414 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.388739 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.394250 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.394527 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.397410 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.410297 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.410362 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.410403 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.410438 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.410506 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.411092 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.411176 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.411550 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.412237 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.414784 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.415415 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.415498 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.415535 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.415599 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.415731 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.415844 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.415895 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.417781 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.417882 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.420357 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.420442 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.420556 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.422855 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.424716 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.424818 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.425108 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.425197 139732788322304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:07.425312 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.425356 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.425389 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.425457 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.427796 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.433252 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.433517 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.436208 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.448964 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.449025 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.449063 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.449097 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.449162 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.449736 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.449818 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.450189 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.450905 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.453469 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.454109 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.454192 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.454230 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.454292 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.454425 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.454541 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.454584 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.456744 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.456844 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.459331 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.459421 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.459541 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.462297 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.464193 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.464294 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.464586 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.464676 139732788322304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:07.464792 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.464836 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.464870 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.464936 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.467286 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.472771 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.473038 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.475811 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.488695 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.488758 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.488800 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.488848 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.488917 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.489488 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.489570 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.489945 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.490682 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.493283 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.493918 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.493999 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.494036 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.494097 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.494231 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.494343 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.494388 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.496293 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.496390 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.498848 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.498935 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.499053 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.501401 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.503291 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.503391 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.503683 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.503770 139732788322304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:07.503885 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.503926 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.503958 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.504025 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.506297 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.511859 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.512124 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.514854 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.527825 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.527886 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.527923 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.527957 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.528022 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.528591 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.528672 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.529036 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.529744 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.532312 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.532940 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.533020 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.533056 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.533118 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.533248 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.533364 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.533406 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.535314 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.535421 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.537879 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.537962 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.538076 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.540412 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.542331 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.542434 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.542734 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.542824 139732788322304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:07.542942 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.542984 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.543018 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.543085 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.545432 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.551003 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.551279 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.554088 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.567182 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.567242 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.567283 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.567317 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.567384 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.567950 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.568031 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.568395 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.569096 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.571687 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.572316 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.572398 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.572434 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.572496 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.572628 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.572740 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.572782 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.574799 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.574911 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.577427 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.577510 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.577623 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.580380 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.582317 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.582419 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.582712 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.582799 139732788322304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:07.582914 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.582956 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.582988 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.583054 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.585415 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.591152 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.591428 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.594249 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.607491 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.607553 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.607592 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.607626 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.607690 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.608259 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.608340 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.608698 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.609402 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.612052 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.612722 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.612807 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.612844 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.612909 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.613047 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.613165 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.613208 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.615405 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.615508 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.617987 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.618081 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.618199 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.620577 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.622519 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.622623 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.622925 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.623015 139732788322304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:07.623132 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.623176 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.623210 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.623278 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.625545 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.631229 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.631505 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.634255 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.647445 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.647509 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.647547 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.647581 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.647647 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.648218 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.648299 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.648657 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.649354 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.652005 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.652648 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.652731 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.652767 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.652829 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.652960 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.653072 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.653114 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.655090 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.655191 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.657664 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.657755 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.657871 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.660409 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.662338 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.662442 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.662743 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.662832 139732788322304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:07.662948 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.662993 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.663027 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.663095 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.665443 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.671049 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.671325 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.674088 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.687233 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.687296 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.687335 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.687369 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.687434 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.688014 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.688095 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.688476 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.689208 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.691876 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.692508 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.692589 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.692626 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.692687 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.692817 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.692928 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.692969 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.694912 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.695015 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.697500 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.697605 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.697733 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.700578 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.702533 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.702636 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.702936 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.703026 139732788322304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:07.703143 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.703186 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.703221 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.703289 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.705596 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.711262 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.711539 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.714290 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.727348 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.727409 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.727447 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.727479 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.727544 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.728114 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.728199 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.728560 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.729266 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.731833 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.732460 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.732542 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.732578 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.732638 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.732767 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.732879 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.732920 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.735373 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.735475 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.737886 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.737970 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.738090 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.740385 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.742244 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.742343 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.742628 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.742712 139732788322304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:07.742823 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.742863 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.742895 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.742960 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.745206 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.750727 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.750997 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.753715 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.766921 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.766981 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.767018 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.767051 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.767114 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.767689 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.767769 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.768132 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.768830 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.771464 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.772139 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.772223 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.772262 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.772326 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.772467 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.772585 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.772628 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.774592 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.774694 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.777141 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.777224 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.777334 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.779675 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.781565 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.781671 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.781961 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.782048 139732788322304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:07.782160 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:07.782201 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:07.782233 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:07.782300 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.784606 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:07.790130 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.790393 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:07.793095 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:07.806075 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:07.806134 139732788322304 attention.py:418] Single window, no scan.
I0123 11:01:07.806174 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:07.806207 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.806277 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.806842 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.806923 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.807284 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.807984 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.810588 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.811253 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.811338 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:07.811376 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:07.811439 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.811576 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:07.811692 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:01:07.811732 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.813688 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.813795 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.816268 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.816351 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:01:07.816461 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:07.819199 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:01:07.821091 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.821190 139732788322304 nn_components.py:261] mlp: residual
I0123 11:01:07.821475 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:07.821567 139732788322304 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:07.824482 139732788322304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:12.239122 139732788322304 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:01:12.734510 139732788322304 training_loop.py:409] No working directory specified.
I0123 11:01:12.734643 139732788322304 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:01:12.735421 139732788322304 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:01:15.819567 139732788322304 training_loop.py:447] Only restoring trainable parameters.
I0123 11:01:15.820280 139732788322304 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:01:15.820342 139732788322304 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.820388 139732788322304 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.820432 139732788322304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.820474 139732788322304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.820513 139732788322304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.820555 139732788322304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.820595 139732788322304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.820634 139732788322304 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.820674 139732788322304 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.820713 139732788322304 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.820750 139732788322304 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.820789 139732788322304 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.820828 139732788322304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.820866 139732788322304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.820903 139732788322304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.820942 139732788322304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.820980 139732788322304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821016 139732788322304 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.821054 139732788322304 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.821109 139732788322304 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821149 139732788322304 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.821187 139732788322304 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.821226 139732788322304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.821264 139732788322304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821301 139732788322304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.821341 139732788322304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821377 139732788322304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821415 139732788322304 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.821451 139732788322304 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.821489 139732788322304 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821526 139732788322304 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.821563 139732788322304 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.821601 139732788322304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.821637 139732788322304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821695 139732788322304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.821734 139732788322304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821773 139732788322304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821811 139732788322304 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.821848 139732788322304 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.821887 139732788322304 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.821925 139732788322304 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.821962 139732788322304 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.821999 139732788322304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.822038 139732788322304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822074 139732788322304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.822118 139732788322304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822158 139732788322304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822195 139732788322304 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.822232 139732788322304 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.822268 139732788322304 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822306 139732788322304 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.822343 139732788322304 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.822380 139732788322304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.822417 139732788322304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822454 139732788322304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.822491 139732788322304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822526 139732788322304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822565 139732788322304 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.822600 139732788322304 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.822636 139732788322304 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822673 139732788322304 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.822710 139732788322304 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.822746 139732788322304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.822783 139732788322304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822821 139732788322304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.822858 139732788322304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822895 139732788322304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.822931 139732788322304 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.822968 139732788322304 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.823004 139732788322304 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823040 139732788322304 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.823077 139732788322304 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.823121 139732788322304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.823160 139732788322304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823197 139732788322304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.823235 139732788322304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823272 139732788322304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823308 139732788322304 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.823346 139732788322304 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.823382 139732788322304 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823419 139732788322304 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.823456 139732788322304 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.823493 139732788322304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.823529 139732788322304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823566 139732788322304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.823602 139732788322304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823640 139732788322304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823676 139732788322304 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.823713 139732788322304 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.823750 139732788322304 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823786 139732788322304 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.823822 139732788322304 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.823858 139732788322304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.823895 139732788322304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.823931 139732788322304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.823966 139732788322304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824001 139732788322304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824040 139732788322304 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.824077 139732788322304 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.824120 139732788322304 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824160 139732788322304 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.824197 139732788322304 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.824234 139732788322304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.824270 139732788322304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824309 139732788322304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.824344 139732788322304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824380 139732788322304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824419 139732788322304 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.824455 139732788322304 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.824491 139732788322304 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824527 139732788322304 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.824564 139732788322304 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:15.824599 139732788322304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:15.824635 139732788322304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824673 139732788322304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.824710 139732788322304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824747 139732788322304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824783 139732788322304 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:15.824820 139732788322304 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:15.824857 139732788322304 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:15.824892 139732788322304 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:15.824921 139732788322304 training_loop.py:725] Total parameters: 152072288
I0123 11:01:15.825132 139732788322304 training_loop.py:739] Total state size: 0
I0123 11:01:15.845434 139732788322304 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:01:15.845735 139732788322304 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:01:15.846281 139732788322304 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:01:15.846626 139732788322304 training_loop.py:89] registering functions: dict_keys([])
I0123 11:01:15.862637 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m ? para e f k o
I0123 11:01:25.339653 139732788322304 ddar.py:60] Depth 1/1000 time = 9.397707223892212
I0123 11:01:46.363389 139732788322304 ddar.py:60] Depth 2/1000 time = 21.023496627807617
I0123 11:02:12.622912 139732788322304 ddar.py:60] Depth 3/1000 time = 26.259195804595947
I0123 11:02:39.927742 139732788322304 ddar.py:60] Depth 4/1000 time = 27.304563522338867
I0123 11:03:07.015505 139732788322304 ddar.py:60] Depth 5/1000 time = 27.086808681488037
I0123 11:03:41.260657 139732788322304 ddar.py:60] Depth 6/1000 time = 33.864447832107544
I0123 11:04:15.090369 139732788322304 ddar.py:60] Depth 7/1000 time = 33.829411029815674
I0123 11:04:15.106140 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:04:15.106245 139732788322304 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 11:04:15.106285 139732788322304 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 ; f : C a c f 03 D c e c f 04 ; g : C a c g 05 D c e c g 06 ; h : C b e h 07 D b h e h 08 ; i : C a b i 09 T b h h i 10 ; j : C h i j 11 D b d d j 12 ; k : C h i k 13 D b d d k 14 ; l : C a f l 15 D a l f l 16 ; m : C a b m 17 T a l l m 18 ; n : C l m n 19 D a d d n 20 ; o : C l m o 21 D a d d o 22 ? P e f k o {F1} x00
I0123 11:04:15.106318 139732788322304 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 ; f : C a c f 03 D c e c f 04 ; g : C a c g 05 D c e c g 06 ; h : C b e h 07 D b h e h 08 ; i : C a b i 09 T b h h i 10 ; j : C h i j 11 D b d d j 12 ; k : C h i k 13 D b d d k 14 ; l : C a f l 15 D a l f l 16 ; m : C a b m 17 T a l l m 18 ; n : C l m n 19 D a d d n 20 ; o : C l m o 21 D a d d o 22 ? P e f k o {F1} x00
I0123 11:04:15.251912 139732788322304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.252098 139732788322304 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:04:15.252198 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252277 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252353 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252425 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252497 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252568 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252640 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252711 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252782 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252853 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252922 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.252992 139732788322304 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:04:15.253033 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.253079 139732788322304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:04:15.253192 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.253246 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.253279 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.255293 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.257843 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.263724 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.264010 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.266750 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.270755 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.270817 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.270856 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.270892 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.270958 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.271637 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.271720 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.272102 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.272902 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.275521 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.276177 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.276260 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.276297 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.276360 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.276496 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.276844 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.276892 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.278964 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.279064 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.281626 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.281721 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.282168 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.284559 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.286568 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.286671 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.286974 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.287061 139732788322304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:04:15.287176 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.287220 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.287261 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.289198 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.291632 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.297468 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.297749 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.300480 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.304497 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.304558 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.304597 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.304630 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.304697 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.305455 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.305536 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.305917 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.306716 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.309263 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.309983 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.310070 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.310108 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.310171 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.310308 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.310645 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.310691 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.312676 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.312776 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.315323 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.315411 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.315853 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.318303 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.320325 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.320427 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.320734 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.320822 139732788322304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:04:15.320938 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.320981 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.321015 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.322916 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.325335 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.331614 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.331883 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.334573 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.338386 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.338447 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.338485 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.338518 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.338584 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.339227 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.339309 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.339678 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.340471 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.342981 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.343618 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.343701 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.343738 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.343802 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.343938 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.344270 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.344318 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.346299 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.346397 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.348899 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.348982 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.349425 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.351784 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.353770 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.353868 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.354156 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.354241 139732788322304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:04:15.354352 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.354394 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.354427 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.356359 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.358726 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.364457 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.364726 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.367333 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.371143 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.371205 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.371244 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.371276 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.371343 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.371928 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.372010 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.372381 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.373151 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.375655 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.376303 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.376386 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.376424 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.376486 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.376623 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.377013 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.377062 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.379070 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.379170 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.381712 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.381797 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.382245 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.384539 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.386622 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.386724 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.387027 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.387116 139732788322304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:04:15.387232 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.387274 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.387308 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.389179 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.391597 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.397495 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.397772 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.400445 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.404290 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.404351 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.404389 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.404422 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.404541 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.405311 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.405394 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.405773 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.406584 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.409147 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.409805 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.409890 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.410105 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.410168 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.410300 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.410637 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.410686 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.412724 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.412822 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.415357 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.415442 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.415883 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.418222 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.420164 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.420261 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.420546 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.420633 139732788322304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:04:15.420749 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.420793 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.420825 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.422787 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.425199 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.430953 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.431231 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.433954 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.437754 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.437812 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.437850 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.437883 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.437945 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.438506 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.438585 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.438939 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.439711 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.442169 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.443242 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.443327 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.443365 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.443429 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.443596 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.443938 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.443986 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.445969 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.446068 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.448552 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.448637 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.449069 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.451407 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.453342 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.453443 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.453750 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.453839 139732788322304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:04:15.453955 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.453998 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.454031 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.455894 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.458225 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.463963 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.464245 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.466836 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.470538 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.470596 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.470633 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.470664 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.470779 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.471347 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.471426 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.471784 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.472553 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.475038 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.475686 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.475765 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.475800 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.475860 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.475988 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.476306 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.476351 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.478313 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.478409 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.480910 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.480992 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.481419 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.483651 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.485555 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.485657 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.485948 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.486036 139732788322304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:04:15.486152 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.486196 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.486228 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.488145 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.490709 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.496511 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.496771 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.499386 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.503054 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.503112 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.503149 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.503181 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.503245 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.503802 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.503880 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.504233 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.504987 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.507423 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.508102 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.508183 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.508219 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.508280 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.508413 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.508738 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.508785 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.510693 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.510789 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.513208 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.513288 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.513718 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.516032 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.517927 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.518025 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.518314 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.518398 139732788322304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:04:15.518507 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.518548 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.518579 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.520372 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.522661 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.528321 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.528576 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.531149 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.534843 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.534904 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.534941 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.534974 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.535041 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.535682 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.535764 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.536132 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.536905 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.539374 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.540020 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.540102 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.540140 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.540202 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.540336 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.540664 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.540712 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.542676 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.542772 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.545315 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.545401 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.545855 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.548131 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.550063 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.550164 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.550467 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.550555 139732788322304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:04:15.550671 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.550713 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.550747 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.552580 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.555333 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.561038 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.561304 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.563897 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.567671 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.567742 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.567781 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.567815 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.567938 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.568530 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.568612 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.568971 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.569745 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.572232 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.572885 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.572968 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.573006 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.573069 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.573204 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.573530 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.573577 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.575492 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.575589 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.578164 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.578250 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.578689 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.580947 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.582932 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.583035 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.583342 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.583431 139732788322304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:04:15.583547 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.583591 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.583625 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.585417 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.587840 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.593454 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.593735 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.596495 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.600207 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.600265 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.600310 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.600344 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.600464 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.601033 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.601113 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.601473 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.602261 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.604794 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.605448 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.605532 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.605570 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.605632 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.605770 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.606090 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.606137 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.608029 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.608125 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.610695 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.610778 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.611202 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.613468 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.615399 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.615501 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.615803 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.615891 139732788322304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:04:15.616006 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.616050 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.616084 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.617912 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.620307 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.626370 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.626642 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.629227 139732788322304 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:04:15.632937 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.632996 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.633034 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.633067 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.633192 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.633774 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.633854 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.634214 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.634982 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.637415 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.638047 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.638130 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.638166 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.638227 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.638357 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.638680 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.638727 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.640643 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.640738 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.643261 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.643345 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.643771 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.646045 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.648014 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.648110 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.648399 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.648654 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.648727 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.648787 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.648844 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.648900 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.648955 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.649010 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.649065 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.649120 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.649176 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.649229 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.649283 139732788322304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:04:15.649329 139732788322304 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:04:15.652230 139732788322304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:04:15.697505 139732788322304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.697596 139732788322304 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:04:15.697660 139732788322304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:04:15.697774 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.697817 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.697849 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.697916 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.700336 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.705843 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.706109 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.708758 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:15.721708 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.721768 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.721805 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.721837 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.721900 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.722493 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.722574 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.722950 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.723674 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.726608 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.727255 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.727338 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.727376 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.727441 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.727576 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.727693 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.727735 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.729676 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.729774 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.732260 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.732345 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.732461 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.734823 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.736712 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.736814 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.737117 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.737204 139732788322304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:04:15.737319 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.737361 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.737394 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.737460 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.739780 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.745286 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.745560 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.748675 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:15.761779 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.761840 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.761877 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.761909 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.761973 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.762539 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.762621 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.762993 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.763758 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.766317 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.766968 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.767052 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.767091 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.767154 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.767289 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.767405 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.767446 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.769302 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.769397 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.771892 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.771975 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.772087 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.774386 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.776321 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.776422 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.776723 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.776815 139732788322304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:04:15.776929 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.776970 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.777003 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.777069 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.779379 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.784880 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.785143 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.787911 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:15.800671 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.800731 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.800767 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.800799 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.800863 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.801439 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.801522 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.801894 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.802660 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.805197 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.806051 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.806138 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.806175 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.806240 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.806375 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.806491 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.806534 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.808456 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.808556 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.811052 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.811133 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.811244 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.813585 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.815522 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.815625 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.815916 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.815999 139732788322304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:04:15.816114 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.816156 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.816189 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.816255 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.818574 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.824102 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.824375 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.827157 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:15.840036 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.840097 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.840137 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.840171 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.840238 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.840821 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.840903 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.841274 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.842050 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.844536 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.845182 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.845266 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.845304 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.845367 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.845505 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.845625 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.845675 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.847587 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.847684 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.850176 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.850262 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.850377 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.852718 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.854650 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.854761 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.855065 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.855153 139732788322304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:04:15.855269 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.855310 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.855345 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.855412 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.857737 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.863456 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.863729 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.866901 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:15.880175 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.880236 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.880275 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.880308 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.880373 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.880954 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.881036 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.881402 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.882186 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.884694 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.885342 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.885425 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.885463 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.885526 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.885669 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.885791 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.885835 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.887748 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.887847 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.890324 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.890411 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.890526 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.892863 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.894784 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.894895 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.895199 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.895288 139732788322304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:04:15.895405 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.895447 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.895482 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.895549 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.897870 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.903481 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.903751 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.906521 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:15.919479 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.919539 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.919578 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.919611 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.919679 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.920262 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.920345 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.920711 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.921475 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.923983 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.924631 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.924712 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.924750 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.924820 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.924952 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.925067 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.925109 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.927024 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.927125 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.929579 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.929674 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.929792 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.932127 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.934040 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.934141 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.934451 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.934540 139732788322304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:04:15.934657 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.934700 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.934733 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.934800 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.937104 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.942757 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.943031 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.945792 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:15.958778 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.958840 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.958879 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.958912 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.958979 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.959559 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.959642 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.960012 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.960777 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.963324 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.963972 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.964056 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:15.964093 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:15.964155 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.964290 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:15.964407 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:15.964451 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.966388 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.966489 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.968965 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.969051 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:15.969169 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:15.971531 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:15.973449 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.973550 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:15.973868 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.973966 139732788322304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:04:15.974084 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:15.974127 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:15.974161 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:15.974226 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.976540 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:15.982556 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.982832 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:15.986007 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:15.998943 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:15.999003 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:15.999042 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:15.999074 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.999140 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.999720 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:15.999801 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.000165 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.000922 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.003474 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.004125 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.004209 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.004247 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.004311 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.004448 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.004567 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.004610 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.006537 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.006639 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.009137 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.009222 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.009338 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.011686 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.013614 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.013724 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.014029 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.014126 139732788322304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:04:16.014244 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.014285 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.014319 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.014386 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.016682 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.022295 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.022570 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.025322 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.038171 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.038232 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.038272 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.038305 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.038370 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.038953 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.039035 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.039405 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.040121 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.042736 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.043375 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.043458 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.043496 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.043560 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.043694 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.043810 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.043853 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.045772 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.045868 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.048347 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.048433 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.048549 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.050909 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.052832 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.052930 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.053222 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.053309 139732788322304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:04:16.053434 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.053478 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.053511 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.053579 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.055897 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.061496 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.061770 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.064532 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.077244 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.077304 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.077341 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.077373 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.077436 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.078006 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.078086 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.078446 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.079138 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.082073 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.082695 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.082777 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.082813 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.082873 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.083004 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.083116 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.083156 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.085046 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.085145 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.087649 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.087733 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.087843 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.090099 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.092034 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.092134 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.092434 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.092524 139732788322304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:04:16.092638 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.092689 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.092722 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.092787 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.095031 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.100524 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.100792 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.103914 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.116578 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.116638 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.116675 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.116707 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.116771 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.117349 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.117432 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.117812 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.118526 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.121029 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.121655 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.121741 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.121781 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.121845 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.121979 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.122094 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.122135 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.124047 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.124146 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.126611 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.126698 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.126813 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.129155 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.131077 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.131179 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.131485 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.131573 139732788322304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:04:16.131687 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.131730 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.131772 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.131842 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.134175 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.139799 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.140062 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.142820 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.155616 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.155674 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.155711 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.155743 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.155806 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.156370 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.156450 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.156808 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.157495 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.160060 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.160679 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.160761 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.160796 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.160856 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.160986 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.161096 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.161137 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.163031 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.163134 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.165626 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.165720 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.165836 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.168162 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.170082 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.170185 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.170485 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.170578 139732788322304 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:04:16.173484 139732788322304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:04:16.225208 139732788322304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.225310 139732788322304 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:04:16.225369 139732788322304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:04:16.225482 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.225524 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.225557 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.225624 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.228015 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.233646 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.233918 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.236571 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.249677 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.249737 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.249775 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.249808 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.249871 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.250450 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.250533 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.250904 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.251620 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.254155 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.254799 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.254882 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.254920 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.254984 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.255118 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.255233 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.255275 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.257259 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.257359 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.259858 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.259942 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.260054 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.262315 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.264249 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.264351 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.264654 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.264756 139732788322304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:04:16.264873 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.264916 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.264950 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.265018 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.267343 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.273018 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.273294 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.276003 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.289560 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.289622 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.289668 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.289704 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.289771 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.290357 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.290440 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.290810 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.291521 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.294087 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.294738 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.294821 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.294858 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.294922 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.295055 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.295171 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.295212 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.297205 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.297304 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.299819 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.299905 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.300022 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.302600 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.304534 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.304635 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.304924 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.305017 139732788322304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:04:16.305131 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.305174 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.305208 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.305274 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.307601 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.313274 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.313549 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.316238 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.329016 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.329075 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.329113 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.329145 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.329211 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.329799 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.329883 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.330260 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.330983 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.333489 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.334120 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.334203 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.334239 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.334301 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.334431 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.334547 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.334589 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.336571 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.336669 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.339080 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.339162 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.339274 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.341504 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.343361 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.343461 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.343754 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.343845 139732788322304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:04:16.343956 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.343998 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.344031 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.344097 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.346405 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.352058 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.352331 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.354963 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.367623 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.367681 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.367717 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.367749 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.367813 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.368371 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.368450 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.368805 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.369493 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.372003 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.372621 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.372700 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.372735 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.372797 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.372927 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.373041 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.373083 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.375038 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.375138 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.377627 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.377715 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.377827 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.380119 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.382045 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.382148 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.382449 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.382538 139732788322304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:04:16.382661 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.382704 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.382736 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.382800 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.385087 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.390634 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.390906 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.393541 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.406740 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.406802 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.406840 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.406872 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.406939 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.407519 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.407601 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.407973 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.408684 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.411109 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.411728 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.411809 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.411846 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.411906 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.412043 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.412162 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.412206 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.414213 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.414314 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.416797 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.416883 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.417001 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.419302 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.421322 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.421424 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.421744 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.421833 139732788322304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:04:16.421945 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.421997 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.422034 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.422103 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.424458 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.430200 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.430473 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.433153 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.446235 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.446297 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.446336 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.446370 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.446436 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.447021 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.447103 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.447476 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.448189 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.450758 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.451406 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.451489 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.451526 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.451591 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.451725 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.451843 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.451884 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.454004 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.454105 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.456600 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.456685 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.456802 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.459101 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.461035 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.461136 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.461436 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.461523 139732788322304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:04:16.461646 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.461691 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.461735 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.461806 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.464147 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.469828 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.470106 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.472760 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.485810 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.485870 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.485906 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.485939 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.486003 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.486589 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.486672 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.487044 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.487757 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.490278 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.490931 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.491015 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.491053 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.491117 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.491253 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.491370 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.491412 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.493543 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.493639 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.496132 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.496220 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.496337 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.498612 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.500536 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.500638 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.500941 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.501030 139732788322304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:04:16.501143 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.501186 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.501219 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.501296 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.503607 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.509263 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.509529 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.512206 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.525507 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.525567 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.525603 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.525635 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.525707 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.526282 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.526364 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.526741 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.527453 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.529945 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.530564 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.530643 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.530678 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.530740 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.530869 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.530983 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.531024 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.532983 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.533077 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.535461 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.535542 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.535652 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.537916 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.539933 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.540028 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.540316 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.540401 139732788322304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:04:16.540510 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.540553 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.540585 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.540657 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.542948 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.548516 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.548780 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.551413 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.564194 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.564255 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.564294 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.564327 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.564393 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.564958 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.565036 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.565387 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.566098 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.568646 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.569287 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.569369 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.569407 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.569470 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.569609 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.569732 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.569774 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.571748 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.571842 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.574282 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.574365 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.574481 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.576676 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.578585 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.578686 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.578989 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.579074 139732788322304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:04:16.579188 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.579231 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.579265 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.579331 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.581656 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.587328 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.587602 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.590269 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.603165 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.603224 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.603261 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.603293 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.603357 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.603918 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.603996 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.604351 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.605061 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.607549 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.608170 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.608252 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.608289 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.608353 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.608484 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.608598 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.608640 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.610631 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.610731 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.613203 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.613283 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.613396 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.615633 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.617544 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.617650 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.617953 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.618040 139732788322304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:04:16.618154 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.618196 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.618229 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.618296 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.620604 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.626245 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.626516 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.629165 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.642097 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.642156 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.642193 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.642224 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.642284 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.642835 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.642912 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.643275 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.643981 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.646485 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.647105 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.647186 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.647222 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.647284 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.647416 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.647532 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.647574 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.649538 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.649635 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.652090 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.652173 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.652288 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.654544 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.656462 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.656563 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.656854 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.656939 139732788322304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:04:16.657047 139732788322304 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:04:16.657088 139732788322304 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:04:16.657119 139732788322304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:04:16.657181 139732788322304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.659485 139732788322304 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:04:16.665090 139732788322304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.665354 139732788322304 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:04:16.668006 139732788322304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:04:16.680571 139732788322304 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:04:16.680632 139732788322304 attention.py:418] Single window, no scan.
I0123 11:04:16.680669 139732788322304 transformer_layer.py:389] tlayer: self-attention.
I0123 11:04:16.680701 139732788322304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.680763 139732788322304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.681328 139732788322304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.681410 139732788322304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.681793 139732788322304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.682508 139732788322304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.685026 139732788322304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.685649 139732788322304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.685733 139732788322304 transformer_layer.py:468] tlayer: End windows.
I0123 11:04:16.685770 139732788322304 transformer_layer.py:472] tlayer: final FFN.
I0123 11:04:16.685832 139732788322304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.685969 139732788322304 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:04:16.686086 139732788322304 nn_components.py:325] mlp: activation = None
I0123 11:04:16.686130 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.688125 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.688225 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.690803 139732788322304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.690887 139732788322304 transformer_base.py:443] tbase: final FFN
I0123 11:04:16.691004 139732788322304 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:04:16.693248 139732788322304 nn_components.py:329] mlp: final activation = None
I0123 11:04:16.695162 139732788322304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.695264 139732788322304 nn_components.py:261] mlp: residual
I0123 11:04:16.695566 139732788322304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:16.695657 139732788322304 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:04:16.698573 139732788322304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:04:31.105558 139732788322304 alphageometry.py:566] LM output (score=-2.089789): "p : C b g p 23 D b p g p 24 ;"
I0123 11:04:31.105794 139732788322304 alphageometry.py:567] Translation: "p = on_line p b g, on_bline p g b"

I0123 11:04:31.105848 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_line p b g, on_bline p g b ? para e f k o"
I0123 11:04:31.106053 139732788322304 graph.py:498] 
I0123 11:04:31.106117 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_line p b g, on_bline p g b ? para e f k o
I0123 11:04:40.981299 139732788322304 ddar.py:60] Depth 1/1000 time = 9.744933843612671
I0123 11:05:06.452073 139732788322304 ddar.py:60] Depth 2/1000 time = 25.470571756362915
I0123 11:05:34.458138 139732788322304 ddar.py:60] Depth 3/1000 time = 28.00582981109619
I0123 11:06:03.682107 139732788322304 ddar.py:60] Depth 4/1000 time = 29.22368621826172
I0123 11:06:33.428864 139732788322304 ddar.py:60] Depth 5/1000 time = 29.745203256607056
I0123 11:07:10.283282 139732788322304 ddar.py:60] Depth 6/1000 time = 36.46674060821533
I0123 11:07:46.248685 139732788322304 ddar.py:60] Depth 7/1000 time = 35.96498394012451
I0123 11:07:46.273925 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:07:46.274083 139732788322304 alphageometry.py:566] LM output (score=-2.228877): "p : C e g p 23 D e p g p 24 ;"
I0123 11:07:46.274121 139732788322304 alphageometry.py:567] Translation: "p = on_line p e g, on_bline p g e"

I0123 11:07:46.274176 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_line p e g, on_bline p g e ? para e f k o"
I0123 11:07:46.274381 139732788322304 graph.py:498] 
I0123 11:07:46.274442 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_line p e g, on_bline p g e ? para e f k o
I0123 11:07:56.026671 139732788322304 ddar.py:60] Depth 1/1000 time = 9.568878650665283
I0123 11:08:21.425302 139732788322304 ddar.py:60] Depth 2/1000 time = 25.398378372192383
I0123 11:08:50.449965 139732788322304 ddar.py:60] Depth 3/1000 time = 29.024272918701172
I0123 11:09:20.744736 139732788322304 ddar.py:60] Depth 4/1000 time = 30.294337034225464
I0123 11:09:50.114336 139732788322304 ddar.py:60] Depth 5/1000 time = 29.368346452713013
I0123 11:10:27.780222 139732788322304 ddar.py:60] Depth 6/1000 time = 37.27535891532898
I0123 11:11:04.882406 139732788322304 ddar.py:60] Depth 7/1000 time = 37.101500034332275
I0123 11:11:04.905833 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:11:04.905964 139732788322304 alphageometry.py:566] LM output (score=-2.395928): "p : T f g g p 23 ;"
I0123 11:11:04.906004 139732788322304 alphageometry.py:567] Translation: "p = on_tline p g f g"

I0123 11:11:04.906059 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p g f g ? para e f k o"
I0123 11:11:04.906295 139732788322304 graph.py:498] 
I0123 11:11:04.906363 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p g f g ? para e f k o
I0123 11:11:15.688739 139732788322304 ddar.py:60] Depth 1/1000 time = 10.662132501602173
I0123 11:11:40.427846 139732788322304 ddar.py:60] Depth 2/1000 time = 24.738861322402954
I0123 11:12:08.071846 139732788322304 ddar.py:60] Depth 3/1000 time = 27.643640995025635
I0123 11:12:36.397101 139732788322304 ddar.py:60] Depth 4/1000 time = 28.324935913085938
I0123 11:13:04.873139 139732788322304 ddar.py:60] Depth 5/1000 time = 28.4747474193573
I0123 11:13:41.760836 139732788322304 ddar.py:60] Depth 6/1000 time = 36.48414444923401
I0123 11:14:16.899214 139732788322304 ddar.py:60] Depth 7/1000 time = 35.137948513031006
I0123 11:14:16.915475 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:14:16.915616 139732788322304 alphageometry.py:566] LM output (score=-2.463655): "p : D a p b p 23 ;"
I0123 11:14:16.915658 139732788322304 alphageometry.py:567] Translation: "p = on_bline p b a"

I0123 11:14:16.915698 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p b a ? para e f k o"
I0123 11:14:16.915894 139732788322304 graph.py:498] 
I0123 11:14:16.915961 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p b a ? para e f k o
I0123 11:14:27.168334 139732788322304 ddar.py:60] Depth 1/1000 time = 10.138641834259033
I0123 11:14:52.849292 139732788322304 ddar.py:60] Depth 2/1000 time = 25.680591344833374
I0123 11:15:22.016579 139732788322304 ddar.py:60] Depth 3/1000 time = 29.166923999786377
I0123 11:15:53.041900 139732788322304 ddar.py:60] Depth 4/1000 time = 31.024957418441772
I0123 11:16:24.693616 139732788322304 ddar.py:60] Depth 5/1000 time = 31.650532484054565
I0123 11:17:03.330586 139732788322304 ddar.py:60] Depth 6/1000 time = 38.19063472747803
I0123 11:17:41.476530 139732788322304 ddar.py:60] Depth 7/1000 time = 38.14545822143555
I0123 11:17:41.493820 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:17:41.493955 139732788322304 alphageometry.py:566] LM output (score=-2.493995): "p : C b i p 23 D b p i p 24 ;"
I0123 11:17:41.493993 139732788322304 alphageometry.py:567] Translation: "p = on_line p b i, on_bline p i b"

I0123 11:17:41.494049 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_line p b i, on_bline p i b ? para e f k o"
I0123 11:17:41.494272 139732788322304 graph.py:498] 
I0123 11:17:41.494336 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_line p b i, on_bline p i b ? para e f k o
I0123 11:17:53.299843 139732788322304 ddar.py:60] Depth 1/1000 time = 11.66680097579956
I0123 11:18:18.765405 139732788322304 ddar.py:60] Depth 2/1000 time = 25.465317010879517
I0123 11:18:49.367980 139732788322304 ddar.py:60] Depth 3/1000 time = 30.60218334197998
I0123 11:19:20.968319 139732788322304 ddar.py:60] Depth 4/1000 time = 31.599968910217285
I0123 11:19:53.274393 139732788322304 ddar.py:60] Depth 5/1000 time = 32.30499458312988
I0123 11:20:33.950126 139732788322304 ddar.py:60] Depth 6/1000 time = 40.29222822189331
I0123 11:21:14.446386 139732788322304 ddar.py:60] Depth 7/1000 time = 40.495906352996826
I0123 11:21:14.466941 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:21:14.467092 139732788322304 alphageometry.py:566] LM output (score=-2.529821): "p : D a p c p 23 ;"
I0123 11:21:14.467406 139732788322304 alphageometry.py:567] Translation: "p = on_bline p c a"

I0123 11:21:14.467472 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p c a ? para e f k o"
I0123 11:21:14.467691 139732788322304 graph.py:498] 
I0123 11:21:14.467757 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p c a ? para e f k o
I0123 11:21:24.255336 139732788322304 ddar.py:60] Depth 1/1000 time = 9.691459655761719
I0123 11:21:50.503617 139732788322304 ddar.py:60] Depth 2/1000 time = 26.24808621406555
I0123 11:22:20.246834 139732788322304 ddar.py:60] Depth 3/1000 time = 29.742920398712158
I0123 11:22:50.533709 139732788322304 ddar.py:60] Depth 4/1000 time = 30.28641700744629
I0123 11:23:20.956359 139732788322304 ddar.py:60] Depth 5/1000 time = 30.421516180038452
I0123 11:23:59.423324 139732788322304 ddar.py:60] Depth 6/1000 time = 38.04392981529236
I0123 11:24:37.560517 139732788322304 ddar.py:60] Depth 7/1000 time = 38.1368088722229
I0123 11:24:37.577107 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:24:37.577279 139732788322304 alphageometry.py:566] LM output (score=-2.588125): "p : T f g j p 23 ;"
I0123 11:24:37.577317 139732788322304 alphageometry.py:567] Translation: "p = on_tline p j f g"

I0123 11:24:37.577374 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p j f g ? para e f k o"
I0123 11:24:37.577585 139732788322304 graph.py:498] 
I0123 11:24:37.577654 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p j f g ? para e f k o
I0123 11:24:47.752497 139732788322304 ddar.py:60] Depth 1/1000 time = 10.03199815750122
I0123 11:25:13.072179 139732788322304 ddar.py:60] Depth 2/1000 time = 25.319315671920776
I0123 11:25:41.780434 139732788322304 ddar.py:60] Depth 3/1000 time = 28.707907676696777
I0123 11:26:11.074043 139732788322304 ddar.py:60] Depth 4/1000 time = 29.293362140655518
I0123 11:26:39.920970 139732788322304 ddar.py:60] Depth 5/1000 time = 28.845924377441406
I0123 11:27:16.402171 139732788322304 ddar.py:60] Depth 6/1000 time = 36.09470176696777
I0123 11:27:52.323363 139732788322304 ddar.py:60] Depth 7/1000 time = 35.92087435722351
I0123 11:27:52.339374 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:27:52.339480 139732788322304 alphageometry.py:566] LM output (score=-2.600293): "p : D a p j p 23 ;"
I0123 11:27:52.339518 139732788322304 alphageometry.py:567] Translation: "p = on_bline p j a"

I0123 11:27:52.339557 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p j a ? para e f k o"
I0123 11:27:52.339747 139732788322304 graph.py:498] 
I0123 11:27:52.339807 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p j a ? para e f k o
I0123 11:28:03.048910 139732788322304 ddar.py:60] Depth 1/1000 time = 10.593836545944214
I0123 11:28:28.849657 139732788322304 ddar.py:60] Depth 2/1000 time = 25.80045795440674
I0123 11:28:59.467890 139732788322304 ddar.py:60] Depth 3/1000 time = 30.61786198616028
I0123 11:29:30.648695 139732788322304 ddar.py:60] Depth 4/1000 time = 31.18048596382141
I0123 11:30:02.359469 139732788322304 ddar.py:60] Depth 5/1000 time = 31.709798336029053
I0123 11:30:41.034223 139732788322304 ddar.py:60] Depth 6/1000 time = 38.21713042259216
I0123 11:31:19.406027 139732788322304 ddar.py:60] Depth 7/1000 time = 38.3714394569397
I0123 11:31:19.423537 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:31:19.423671 139732788322304 alphageometry.py:566] LM output (score=-2.602868): "p : T a g g p 23 ;"
I0123 11:31:19.423710 139732788322304 alphageometry.py:567] Translation: "p = on_tline p g a g"

I0123 11:31:19.423766 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p g a g ? para e f k o"
I0123 11:31:19.423976 139732788322304 graph.py:498] 
I0123 11:31:19.424039 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p g a g ? para e f k o
I0123 11:31:30.042082 139732788322304 ddar.py:60] Depth 1/1000 time = 10.524964570999146
I0123 11:31:53.670542 139732788322304 ddar.py:60] Depth 2/1000 time = 23.628188610076904
I0123 11:32:21.888504 139732788322304 ddar.py:60] Depth 3/1000 time = 28.217605590820312
I0123 11:32:51.365113 139732788322304 ddar.py:60] Depth 4/1000 time = 29.47629165649414
I0123 11:33:20.461030 139732788322304 ddar.py:60] Depth 5/1000 time = 29.094797372817993
I0123 11:33:56.874479 139732788322304 ddar.py:60] Depth 6/1000 time = 36.02990698814392
I0123 11:34:33.069059 139732788322304 ddar.py:60] Depth 7/1000 time = 36.194116830825806
I0123 11:34:33.082860 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:34:33.082991 139732788322304 alphageometry.py:566] LM output (score=-2.625736): "p : D a c a p 23 D a c c p 24 ;"
I0123 11:34:33.083030 139732788322304 alphageometry.py:567] Translation: "p = on_circle p a c, on_circle p c a"

I0123 11:34:33.083080 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_circle p a c, on_circle p c a ? para e f k o"
I0123 11:34:33.083294 139732788322304 graph.py:498] 
I0123 11:34:33.083355 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_circle p a c, on_circle p c a ? para e f k o
I0123 11:34:43.811638 139732788322304 ddar.py:60] Depth 1/1000 time = 10.612875938415527
I0123 11:35:10.986275 139732788322304 ddar.py:60] Depth 2/1000 time = 27.174434423446655
I0123 11:35:39.892210 139732788322304 ddar.py:60] Depth 3/1000 time = 28.90564250946045
I0123 11:36:10.984803 139732788322304 ddar.py:60] Depth 4/1000 time = 31.092203617095947
I0123 11:36:42.279877 139732788322304 ddar.py:60] Depth 5/1000 time = 31.294071197509766
I0123 11:37:12.648901 139732788322304 ddar.py:60] Depth 6/1000 time = 30.36405062675476
I0123 11:37:51.471634 139732788322304 ddar.py:60] Depth 7/1000 time = 38.4001567363739
I0123 11:38:30.225154 139732788322304 ddar.py:60] Depth 8/1000 time = 38.75306057929993
I0123 11:38:30.242907 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:38:30.243043 139732788322304 alphageometry.py:566] LM output (score=-2.656616): "p : D a b a p 23 D a b b p 24 ;"
I0123 11:38:30.243082 139732788322304 alphageometry.py:567] Translation: "p = on_circle p a b, on_circle p b a"

I0123 11:38:30.243136 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_circle p a b, on_circle p b a ? para e f k o"
I0123 11:38:30.243349 139732788322304 graph.py:498] 
I0123 11:38:30.243412 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_circle p a b, on_circle p b a ? para e f k o
I0123 11:38:41.000065 139732788322304 ddar.py:60] Depth 1/1000 time = 10.572242975234985
I0123 11:39:05.473280 139732788322304 ddar.py:60] Depth 2/1000 time = 24.4729585647583
I0123 11:39:37.153621 139732788322304 ddar.py:60] Depth 3/1000 time = 31.679901123046875
I0123 11:40:10.319757 139732788322304 ddar.py:60] Depth 4/1000 time = 33.16566443443298
I0123 11:40:43.065901 139732788322304 ddar.py:60] Depth 5/1000 time = 32.74491095542908
I0123 11:41:15.311724 139732788322304 ddar.py:60] Depth 6/1000 time = 32.24126100540161
I0123 11:41:55.388867 139732788322304 ddar.py:60] Depth 7/1000 time = 39.62381672859192
I0123 11:42:35.498450 139732788322304 ddar.py:60] Depth 8/1000 time = 40.1091525554657
I0123 11:42:35.515206 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:42:35.515308 139732788322304 alphageometry.py:566] LM output (score=-2.691685): "p : D e g g p 23 T e g g p 24 ;"
I0123 11:42:35.515347 139732788322304 alphageometry.py:567] Translation: "p = on_circle p g e, on_tline p g e g"

I0123 11:42:35.515389 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_circle p g e, on_tline p g e g ? para e f k o"
I0123 11:42:35.515585 139732788322304 graph.py:498] 
I0123 11:42:35.515650 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_circle p g e, on_tline p g e g ? para e f k o
I0123 11:42:46.189189 139732788322304 ddar.py:60] Depth 1/1000 time = 10.356300115585327
I0123 11:43:12.666632 139732788322304 ddar.py:60] Depth 2/1000 time = 26.477197885513306
I0123 11:43:40.933142 139732788322304 ddar.py:60] Depth 3/1000 time = 28.26610827445984
I0123 11:44:10.225170 139732788322304 ddar.py:60] Depth 4/1000 time = 29.291651487350464
I0123 11:44:40.189349 139732788322304 ddar.py:60] Depth 5/1000 time = 29.9632306098938
I0123 11:45:09.024206 139732788322304 ddar.py:60] Depth 6/1000 time = 28.83126425743103
I0123 11:45:46.912897 139732788322304 ddar.py:60] Depth 7/1000 time = 37.459781646728516
I0123 11:46:23.552142 139732788322304 ddar.py:60] Depth 8/1000 time = 36.63878846168518
I0123 11:46:23.568640 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:46:23.568746 139732788322304 alphageometry.py:566] LM output (score=-2.711022): "p : D a p c p 23 D c p d p 24 ;"
I0123 11:46:23.568784 139732788322304 alphageometry.py:567] Translation: "p = on_bline p c a, on_bline p d c"

I0123 11:46:23.568829 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p c a, on_bline p d c ? para e f k o"
I0123 11:46:23.569017 139732788322304 graph.py:498] 
I0123 11:46:23.569081 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p c a, on_bline p d c ? para e f k o
I0123 11:46:34.698080 139732788322304 ddar.py:60] Depth 1/1000 time = 11.027913331985474
I0123 11:46:59.830585 139732788322304 ddar.py:60] Depth 2/1000 time = 25.132253646850586
I0123 11:47:29.425990 139732788322304 ddar.py:60] Depth 3/1000 time = 29.594990491867065
I0123 11:48:00.765393 139732788322304 ddar.py:60] Depth 4/1000 time = 31.339023113250732
I0123 11:48:31.143943 139732788322304 ddar.py:60] Depth 5/1000 time = 30.377576112747192
I0123 11:49:10.739391 139732788322304 ddar.py:60] Depth 6/1000 time = 39.13699197769165
I0123 11:49:49.891231 139732788322304 ddar.py:60] Depth 7/1000 time = 39.1513569355011
I0123 11:49:49.908350 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:49:49.908514 139732788322304 alphageometry.py:566] LM output (score=-2.814458): "p : T f g i p 23 ;"
I0123 11:49:49.908557 139732788322304 alphageometry.py:567] Translation: "p = on_tline p i f g"

I0123 11:49:49.908609 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p i f g ? para e f k o"
I0123 11:49:49.908817 139732788322304 graph.py:498] 
I0123 11:49:49.908881 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p i f g ? para e f k o
I0123 11:50:00.343836 139732788322304 ddar.py:60] Depth 1/1000 time = 10.354391098022461
I0123 11:50:23.581120 139732788322304 ddar.py:60] Depth 2/1000 time = 23.237081289291382
I0123 11:50:51.437323 139732788322304 ddar.py:60] Depth 3/1000 time = 27.855956554412842
I0123 11:51:20.622101 139732788322304 ddar.py:60] Depth 4/1000 time = 29.184442281723022
I0123 11:51:50.271604 139732788322304 ddar.py:60] Depth 5/1000 time = 29.64836621284485
I0123 11:52:27.333477 139732788322304 ddar.py:60] Depth 6/1000 time = 36.663636207580566
I0123 11:53:03.735451 139732788322304 ddar.py:60] Depth 7/1000 time = 36.40145778656006
I0123 11:53:03.751839 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:53:03.751988 139732788322304 alphageometry.py:566] LM output (score=-2.855740): "p : T f g h p 23 ;"
I0123 11:53:03.752026 139732788322304 alphageometry.py:567] Translation: "p = on_tline p h f g"

I0123 11:53:03.752081 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p h f g ? para e f k o"
I0123 11:53:03.752290 139732788322304 graph.py:498] 
I0123 11:53:03.752352 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p h f g ? para e f k o
I0123 11:53:14.252897 139732788322304 ddar.py:60] Depth 1/1000 time = 10.420713186264038
I0123 11:53:40.146537 139732788322304 ddar.py:60] Depth 2/1000 time = 25.893414974212646
I0123 11:54:09.126808 139732788322304 ddar.py:60] Depth 3/1000 time = 28.979963064193726
I0123 11:54:38.531038 139732788322304 ddar.py:60] Depth 4/1000 time = 29.403834581375122
I0123 11:55:08.544139 139732788322304 ddar.py:60] Depth 5/1000 time = 30.012001514434814
I0123 11:55:45.583227 139732788322304 ddar.py:60] Depth 6/1000 time = 36.653757095336914
I0123 11:56:22.537747 139732788322304 ddar.py:60] Depth 7/1000 time = 36.954092502593994
I0123 11:56:22.550807 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:56:22.550916 139732788322304 alphageometry.py:566] LM output (score=-2.863289): "p : T a b a p 23 ;"
I0123 11:56:22.550956 139732788322304 alphageometry.py:567] Translation: "p = on_tline p a a b"

I0123 11:56:22.550998 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p a a b ? para e f k o"
I0123 11:56:22.551198 139732788322304 graph.py:498] 
I0123 11:56:22.551264 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p a a b ? para e f k o
I0123 11:56:33.627387 139732788322304 ddar.py:60] Depth 1/1000 time = 10.997413635253906
I0123 11:56:57.718539 139732788322304 ddar.py:60] Depth 2/1000 time = 24.090888023376465
I0123 11:57:28.118593 139732788322304 ddar.py:60] Depth 3/1000 time = 30.399672985076904
I0123 11:57:59.177780 139732788322304 ddar.py:60] Depth 4/1000 time = 31.058839321136475
I0123 11:58:30.398268 139732788322304 ddar.py:60] Depth 5/1000 time = 31.21942901611328
I0123 11:59:08.889086 139732788322304 ddar.py:60] Depth 6/1000 time = 38.060784101486206
I0123 11:59:46.204272 139732788322304 ddar.py:60] Depth 7/1000 time = 37.31483221054077
I0123 11:59:46.223152 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:59:46.223340 139732788322304 alphageometry.py:566] LM output (score=-2.870322): "p : D a p c p 23 D a p d p 24 ;"
I0123 11:59:46.223382 139732788322304 alphageometry.py:567] Translation: "p = on_bline p c a, on_bline p d a"

I0123 11:59:46.223437 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p c a, on_bline p d a ? para e f k o"
I0123 11:59:46.223654 139732788322304 graph.py:498] 
I0123 11:59:46.223719 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p c a, on_bline p d a ? para e f k o
I0123 11:59:57.973160 139732788322304 ddar.py:60] Depth 1/1000 time = 11.596062660217285
I0123 12:00:23.471993 139732788322304 ddar.py:60] Depth 2/1000 time = 25.498466968536377
I0123 12:00:54.142153 139732788322304 ddar.py:60] Depth 3/1000 time = 30.669796466827393
I0123 12:01:24.898165 139732788322304 ddar.py:60] Depth 4/1000 time = 30.75566554069519
I0123 12:01:56.998565 139732788322304 ddar.py:60] Depth 5/1000 time = 32.09930181503296
I0123 12:02:36.686837 139732788322304 ddar.py:60] Depth 6/1000 time = 39.23088312149048
I0123 12:03:16.515037 139732788322304 ddar.py:60] Depth 7/1000 time = 39.827829122543335
I0123 12:03:16.533039 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:03:16.533171 139732788322304 alphageometry.py:566] LM output (score=-2.937949): "p : D e h i p 23 D e i h p 24 ;"
I0123 12:03:16.533210 139732788322304 alphageometry.py:567] Translation: "p = eqdistance p i e h, eqdistance p h e i"

I0123 12:03:16.533266 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = eqdistance p i e h, eqdistance p h e i ? para e f k o"
I0123 12:03:16.533482 139732788322304 graph.py:498] 
I0123 12:03:16.533546 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = eqdistance p i e h, eqdistance p h e i ? para e f k o
I0123 12:03:27.210783 139732788322304 ddar.py:60] Depth 1/1000 time = 10.551652431488037
I0123 12:03:54.646569 139732788322304 ddar.py:60] Depth 2/1000 time = 27.43541383743286
I0123 12:04:25.647517 139732788322304 ddar.py:60] Depth 3/1000 time = 31.00060200691223
I0123 12:04:58.545111 139732788322304 ddar.py:60] Depth 4/1000 time = 32.89727711677551
I0123 12:05:32.127712 139732788322304 ddar.py:60] Depth 5/1000 time = 33.58137631416321
I0123 12:06:13.401918 139732788322304 ddar.py:60] Depth 6/1000 time = 40.853453159332275
I0123 12:06:53.801633 139732788322304 ddar.py:60] Depth 7/1000 time = 40.39932942390442
I0123 12:06:53.829519 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:06:53.829661 139732788322304 alphageometry.py:566] LM output (score=-2.964768): "p : T e f f p 23 ;"
I0123 12:06:53.829703 139732788322304 alphageometry.py:567] Translation: "p = on_tline p f e f"

I0123 12:06:53.829759 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p f e f ? para e f k o"
I0123 12:06:53.829972 139732788322304 graph.py:498] 
I0123 12:06:53.830035 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p f e f ? para e f k o
I0123 12:07:04.118871 139732788322304 ddar.py:60] Depth 1/1000 time = 10.187941312789917
I0123 12:07:28.016135 139732788322304 ddar.py:60] Depth 2/1000 time = 23.897021532058716
I0123 12:07:55.691360 139732788322304 ddar.py:60] Depth 3/1000 time = 27.674978494644165
I0123 12:08:24.744200 139732788322304 ddar.py:60] Depth 4/1000 time = 29.052542209625244
I0123 12:08:53.776968 139732788322304 ddar.py:60] Depth 5/1000 time = 29.031745195388794
I0123 12:09:31.281916 139732788322304 ddar.py:60] Depth 6/1000 time = 37.09688138961792
I0123 12:10:06.629885 139732788322304 ddar.py:60] Depth 7/1000 time = 35.34762144088745
I0123 12:10:06.646457 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:10:06.646604 139732788322304 alphageometry.py:566] LM output (score=-3.020689): "p : T d p e f 23 ;"
I0123 12:10:06.646644 139732788322304 alphageometry.py:567] Translation: "p = on_tline p d e f"

I0123 12:10:06.646703 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p d e f ? para e f k o"
I0123 12:10:06.646919 139732788322304 graph.py:498] 
I0123 12:10:06.646981 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p d e f ? para e f k o
I0123 12:10:17.711076 139732788322304 ddar.py:60] Depth 1/1000 time = 10.84664249420166
I0123 12:10:43.641016 139732788322304 ddar.py:60] Depth 2/1000 time = 25.92973566055298
I0123 12:11:12.520323 139732788322304 ddar.py:60] Depth 3/1000 time = 28.879011154174805
I0123 12:11:41.382204 139732788322304 ddar.py:60] Depth 4/1000 time = 28.861499309539795
I0123 12:12:10.645720 139732788322304 ddar.py:60] Depth 5/1000 time = 29.262590169906616
I0123 12:12:47.950470 139732788322304 ddar.py:60] Depth 6/1000 time = 36.9023711681366
I0123 12:13:24.319141 139732788322304 ddar.py:60] Depth 7/1000 time = 36.36836862564087
I0123 12:13:24.335092 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:13:24.335236 139732788322304 alphageometry.py:566] LM output (score=-3.029495): "p : D a p b p 23 D b p d p 24 ;"
I0123 12:13:24.335279 139732788322304 alphageometry.py:567] Translation: "p = on_bline p b a, on_bline p d b"

I0123 12:13:24.335320 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p b a, on_bline p d b ? para e f k o"
I0123 12:13:24.335523 139732788322304 graph.py:498] 
I0123 12:13:24.335586 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p b a, on_bline p d b ? para e f k o
I0123 12:13:36.069823 139732788322304 ddar.py:60] Depth 1/1000 time = 11.58416485786438
I0123 12:14:00.887207 139732788322304 ddar.py:60] Depth 2/1000 time = 24.817173719406128
I0123 12:14:33.244426 139732788322304 ddar.py:60] Depth 3/1000 time = 32.356961250305176
I0123 12:15:05.651799 139732788322304 ddar.py:60] Depth 4/1000 time = 32.4070725440979
I0123 12:15:38.082140 139732788322304 ddar.py:60] Depth 5/1000 time = 32.42935347557068
I0123 12:16:19.190564 139732788322304 ddar.py:60] Depth 6/1000 time = 40.613250970840454
I0123 12:16:58.979547 139732788322304 ddar.py:60] Depth 7/1000 time = 39.788615465164185
I0123 12:16:58.997371 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:16:58.997517 139732788322304 alphageometry.py:566] LM output (score=-3.053627): "p : C b i p 23 D b i i p 24 ;"
I0123 12:16:58.997557 139732788322304 alphageometry.py:567] Translation: "p = on_line p b i, on_circle p i b"

I0123 12:16:58.997616 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_line p b i, on_circle p i b ? para e f k o"
I0123 12:16:58.997843 139732788322304 graph.py:498] 
I0123 12:16:58.997909 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_line p b i, on_circle p i b ? para e f k o
I0123 12:17:12.934615 139732788322304 ddar.py:60] Depth 1/1000 time = 13.747382402420044
I0123 12:17:42.142682 139732788322304 ddar.py:60] Depth 2/1000 time = 29.207800149917603
I0123 12:18:14.580174 139732788322304 ddar.py:60] Depth 3/1000 time = 32.43713092803955
I0123 12:18:48.287917 139732788322304 ddar.py:60] Depth 4/1000 time = 33.70743441581726
I0123 12:19:22.177493 139732788322304 ddar.py:60] Depth 5/1000 time = 33.88852548599243
I0123 12:20:03.946847 139732788322304 ddar.py:60] Depth 6/1000 time = 41.365955114364624
I0123 12:20:44.998467 139732788322304 ddar.py:60] Depth 7/1000 time = 41.051262855529785
I0123 12:20:45.017024 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:20:45.017133 139732788322304 alphageometry.py:566] LM output (score=-3.058413): "p : D e h i p 23 P e h i p 24 ;"
I0123 12:20:45.017171 139732788322304 alphageometry.py:567] Translation: "p = eqdistance p i e h, on_pline p i e h"

I0123 12:20:45.017222 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = eqdistance p i e h, on_pline p i e h ? para e f k o"
I0123 12:20:45.017424 139732788322304 graph.py:498] 
I0123 12:20:45.017486 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = eqdistance p i e h, on_pline p i e h ? para e f k o
I0123 12:20:56.230762 139732788322304 ddar.py:60] Depth 1/1000 time = 11.122409105300903
I0123 12:21:25.128667 139732788322304 ddar.py:60] Depth 2/1000 time = 28.897639751434326
I0123 12:21:56.829321 139732788322304 ddar.py:60] Depth 3/1000 time = 31.700369119644165
I0123 12:22:30.859277 139732788322304 ddar.py:60] Depth 4/1000 time = 34.02961540222168
I0123 12:23:03.209039 139732788322304 ddar.py:60] Depth 5/1000 time = 32.348405599594116
I0123 12:23:45.579124 139732788322304 ddar.py:60] Depth 6/1000 time = 41.913663148880005
I0123 12:24:26.681539 139732788322304 ddar.py:60] Depth 7/1000 time = 41.102051973342896
I0123 12:24:26.700156 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:24:26.700284 139732788322304 alphageometry.py:566] LM output (score=-3.072155): "p : T f g f p 23 ;"
I0123 12:24:26.700323 139732788322304 alphageometry.py:567] Translation: "p = on_tline p f f g"

I0123 12:24:26.700378 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p f f g ? para e f k o"
I0123 12:24:26.700586 139732788322304 graph.py:498] 
I0123 12:24:26.700650 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_tline p f f g ? para e f k o
I0123 12:24:37.212320 139732788322304 ddar.py:60] Depth 1/1000 time = 10.430128335952759
I0123 12:25:02.981038 139732788322304 ddar.py:60] Depth 2/1000 time = 25.768317461013794
I0123 12:25:31.977818 139732788322304 ddar.py:60] Depth 3/1000 time = 28.996405363082886
I0123 12:26:02.609306 139732788322304 ddar.py:60] Depth 4/1000 time = 30.63111400604248
I0123 12:26:32.382090 139732788322304 ddar.py:60] Depth 5/1000 time = 29.771430253982544
I0123 12:27:10.226842 139732788322304 ddar.py:60] Depth 6/1000 time = 37.456257343292236
I0123 12:27:47.264984 139732788322304 ddar.py:60] Depth 7/1000 time = 37.03771090507507
I0123 12:27:47.281087 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:27:47.281208 139732788322304 alphageometry.py:566] LM output (score=-3.099042): "p : D a p e p 23 D e p f p 24 ;"
I0123 12:27:47.281247 139732788322304 alphageometry.py:567] Translation: "p = on_bline p e a, on_bline p f e"

I0123 12:27:47.281293 139732788322304 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p e a, on_bline p f e ? para e f k o"
I0123 12:27:47.281498 139732788322304 graph.py:498] 
I0123 12:27:47.281560 139732788322304 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c b; f = on_circle f c e, on_line f c a; g = on_circle g c e, on_line g c a; h = midpoint h b e; i = lc_tangent i h b, on_line i a b; j = on_circle j d b, on_line j h i; k = on_circle k d b, on_line k h i; l = midpoint l f a; m = lc_tangent m l a, on_line m b a; n = on_circle n d a, on_line n l m; o = on_circle o d a, on_line o l m; p = on_bline p e a, on_bline p f e ? para e f k o
I0123 12:27:59.055560 139732788322304 ddar.py:60] Depth 1/1000 time = 11.609436273574829
I0123 12:28:31.670105 139732788322304 ddar.py:60] Depth 2/1000 time = 32.61432504653931
I0123 12:29:08.780347 139732788322304 ddar.py:60] Depth 3/1000 time = 37.10991644859314
I0123 12:29:49.679158 139732788322304 ddar.py:60] Depth 4/1000 time = 40.89834141731262
I0123 12:30:29.592799 139732788322304 ddar.py:60] Depth 5/1000 time = 39.912235498428345
I0123 12:31:19.977545 139732788322304 ddar.py:60] Depth 6/1000 time = 49.88719320297241
I0123 12:32:09.676383 139732788322304 ddar.py:60] Depth 7/1000 time = 49.69831895828247
I0123 12:32:09.700635 139732788322304 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:32:09.700691 139732788322304 alphageometry.py:585] Timeout.
